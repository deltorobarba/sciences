{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforcement.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xA4lJGPmRWW1",
        "Geh32fxYk4QD",
        "nGLeDmfAk7TY",
        "oXWolEAlIQI5",
        "tBIcsUlvYZP_",
        "Xhb_H1vVHVpH",
        "-jiuT2m9Ikaa",
        "RGblca9PG3y8",
        "CWBKkvu6I8ON",
        "2FzSIKa-KuLH",
        "Rhso417EdcDE",
        "aH81WPSoL-yL",
        "Em4sR7byZFUI",
        "ImV2HamKYd8k"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMbMaUDHB3j"
      },
      "source": [
        "# **Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMCcff-NHGgt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA4lJGPmRWW1"
      },
      "source": [
        "### **Markov Decision Process & Dynamic Programming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15ZqqPKmE7h"
      },
      "source": [
        "**Markov Decision Process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hseeMnE0RLzq"
      },
      "source": [
        "* MDP, the most typical decision making framework for RL. An MDP is typically defined by a 4-tuple (S, A, R, T + Œ≥)\n",
        "\n",
        "* The Markov property states that the future depends only on the present and not on the past. The Markov chain is a probabilistic model that solely depends on the current state and not the previous states, that is, the future is conditionally independent of past.\n",
        "\n",
        "* Moving from one state to another is called transition and its probability is called a transition probability. We can think of an example of anything in which next state depends only on the present state.\n",
        "\n",
        "* MDP is an extension of the Markov chain. It provides a mathematical framework for modeling decision-making situations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU50uFaQJ8Uq"
      },
      "source": [
        "**Bellman Equation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrurSZLwZSLf"
      },
      "source": [
        "* The **Bellman equation** for dynamic programming helps to solve MDP. To solve means finding the optimal policy and value functions. The optimal value function V*(S) is one that yields maximum value.\n",
        "\n",
        "* The value of a given state is equal to the max action (action which maximizes the value) of the reward of the optimal action in the given state and add a discount factor multiplied by the next state‚Äôs Value from the Bellman Equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szSBu4wmmKyO"
      },
      "source": [
        "**Dynamic programming (DP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGEyaPXLbcGp"
      },
      "source": [
        "is a technique for solving complex problems. In DP, instead of solving complex problems one at a time, we break the problem into simple subproblems, then for each sub-problem, we compute and store the solution. If the same subproblem occurs, we will not recompute, instead, we use the already computed solution.\n",
        "\n",
        "We solve a Bellman equation using two powerful algorithms:\n",
        "\n",
        "* Value Iteration (In value iteration, we start off with a random value function. As the value table is not optimized if randomly initialized we optimize it iteratively.)\n",
        "\n",
        "* Policy Iteration(In Policy Iteration the actions which the agent needs to take are decided or initialized first and the value table is created according to the policy.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geh32fxYk4QD"
      },
      "source": [
        "### **Elements**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGLeDmfAk7TY"
      },
      "source": [
        "##### **Learning Steps (SARTŒ≥)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzYevwusSqiW"
      },
      "source": [
        "**Almost all Reinforcement Learning problems can be modeled as MDP, represented by 5 elements:**\n",
        "\n",
        "\n",
        "1. $S$ is the state/observation space of an environment. A set of state (S) the agent can be in. \n",
        "\n",
        "2. $A$ is the set of actions the agent can perform, for moving from one state to another. \n",
        "\n",
        "3. $R(s, a)$ is a function that returns the reward received for taking action $a$ in state $s$ (probability of moving from one state to another state by performing some action).\n",
        "\n",
        "4. $T\\left(s^{\\prime} \\mid s, a\\right)$ is a transition probability function, specifying the probability that the environment will transition to state $s^{\\prime}$ if the agent takes action $a$ in state $s$ (probability of a reward acquired by the agent for moving from one state to another state by performing some action).\n",
        "\n",
        "5. A discount factor (Œ≥), which controls the importance of immediate and future rewards. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BF4ir24qb8h"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Our goal is to find a policy ùúã that maximizes the expected future (discounted) reward.\n",
        "\n",
        "Ziel ist es, eine Strategie (policy) zu finden, welche jedem Zustand die Aktion zuordnet, durch die der h√∂chste Gesamtgewinn (also die h√∂chste Summe √ºber alle erzielten Gewinne) erwartet werden kann.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjGxm0bcTM65"
      },
      "source": [
        "* Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision planning. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.\n",
        "\n",
        "* But the RL problem isn‚Äôt so kind to us. What makes a problem an RL problem, rather than a planning problem, is the **agent does not know all the elements of the MDP**, precluding it from being able to plan a solution. \n",
        "\n",
        "* Specifically, the agent does not know how the world will change in response to its actions (the transition function\n",
        "T), nor what immediate reward it will receive for doing so (the reward function R). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXWolEAlIQI5"
      },
      "source": [
        "##### **Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFEQ3viddRHx"
      },
      "source": [
        "* **Episodic tasks** are the tasks that have a terminal state (end). For example, in car racing game the end of game is a terminal state. Once the game is over, you start the next episode by restarting the game which will be a whole new beginning. In the above case r(Œ§) is the terminal state and the end of episode.\n",
        "\n",
        "* In **continuous task** there is no terminal state. For example, personal assitant do not have terminal state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBIcsUlvYZP_"
      },
      "source": [
        "##### **Rewards**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCxfe54WdaIA"
      },
      "source": [
        "* Based on the action our agent performs, it receives a **reward**. A reward is nothing but a numerical value, say, +1 for good action and -1 for a bad action. An agent tries to maximize the total amount of rewards (cumulative rewards) it receives from the environment instead of immediate rewards. \n",
        "\n",
        "* The total amount of rewards the agent receives from the environment is called returns. We can formulate the total amount of reward as following, where r(t+1) is the reward received by the agent at a time step t‚ÇÄ and so on:\n",
        "\n",
        "```\n",
        "R(t) = r(t+1)+r(t+2)+r(t+3)+r(t+4) ... +r(Œ§)\n",
        "```\n",
        "\n",
        "* A **reward function** defines the goal in a reinforcement learning problem. Roughly speaking, it maps each perceived state (or state-action pair) of the environment to a single number, a reward, indicating the intrinsic desirability of that state.\n",
        "\n",
        "* A reinforcement learning agent‚Äôs sole objective is to maximize the total reward it receives in the long run. The reward function defines what are the good and bad events for the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhb_H1vVHVpH"
      },
      "source": [
        "##### **Value**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpIHSmXYp0U"
      },
      "source": [
        "* The **value** of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. \n",
        "\n",
        "* Whereas **rewards** determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow, and the rewards available in those states. \n",
        "\n",
        "* To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more reÔ¨Åned and farsighted judgment of how pleased or displeased we are that our environment is in a particular state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jiuT2m9Ikaa"
      },
      "source": [
        "##### **Discount**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luX0AOEydEDt"
      },
      "source": [
        "* If we don‚Äôt have any final state for a continuous task, we can define our return for continuous tasks as\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "R(t) = r(t+1)+r(t+2)+r(t+3)‚Ä¶+r(Œ§) which will sum up to ‚àû\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "* That‚Äôs why we introduce the notion of a **discount factor**. We can redefine our return with a discount factor, as follows\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "R(t) = r(t+1)+Œ≥r(t+2)+Œ≥¬≤ r(t+3)+ ...\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "* The discount factor decides how much importance we give to the future rewards and immediate rewards. \n",
        "\n",
        "* The value of the discount factor lies within 0 to 1. Very low discount factor signifies importance to immediate reward while high discount signifies importance to future reward. The true value of the discount factor is application dependent but the optimal value of the discount factor lies between 0.2 to 0.8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGblca9PG3y8"
      },
      "source": [
        "##### **Policy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_MSZVskUcM"
      },
      "source": [
        "**Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YrfVZFBc8f-"
      },
      "source": [
        "* A **policy** defines the learning agent‚Äôs way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. \n",
        "\n",
        "* The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. A policy is often denoted by the symbol ùõë.\n",
        "\n",
        "* As we studied in the previous article it is a function which maps states to actions. It is denoted by œÄ. So, basically, a **policy function** says what action to perform in each state. \n",
        "\n",
        "* Our ultimate goal lies in finding the optimal policy which specifies the correct action to perform in each state, which maximizes the reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt0MlLRpY1KA"
      },
      "source": [
        "**Policy: Deterministic environment**\n",
        "\n",
        "An environment is said to be deterministic when we know the outcome based on the current state. For instance, in a chess game, we know the exact outcome of moving any player.\n",
        "\n",
        "* V(s) is the value for being in a certain state. V(s‚Äô) is the value for being in the next state that we will end up in after taking action a. \n",
        "\n",
        "* R(s, a) is the reward we get after taking action a in state s. As we can take different actions so we use maximum because our agent wants to be in the optimal state. Œ≥ is the discount factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLqbTFYGZ9eh"
      },
      "source": [
        "> $V(s)=\\max _{a}\\left(R(s, a)+\\gamma V\\left(s^{\\prime}\\right)\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlo8tMgY7Jq"
      },
      "source": [
        "**Policy: Stochastic environment**\n",
        "\n",
        "An environment is said to be stochastic when we cannot determine the outcome based on the current state. There will be a greater level of uncertainty. For example, we never know what number will show up when throwing a dice.\n",
        "\n",
        "* In a stochastic environment when we take an action it is not confirmed that we will end up in a particular next state and there is a probability of ending in a particular state. \n",
        "\n",
        "* P(s, a,s‚Äô) is the probability of ending is state s‚Äô from s by taking action a. This is summed up to a total number of future states. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivXHmkCAahwW"
      },
      "source": [
        "> $V(s)=\\max _{a}\\left(R(s, a)+\\gamma \\sum_{s^{\\prime}} P\\left(s, a, s^{\\prime}\\right) V\\left(s^{\\prime}\\right)\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRvHTojManbw"
      },
      "source": [
        "* For example, if by taking an action we can end up in 3 states s‚ÇÅ,s‚ÇÇ, and s‚ÇÉ from state s with a probability of 0.2, 0.2 and 0.6. The Bellman equation will be following.\n",
        "\n",
        "* We can solve the Bellman equation using a technique called **dynamic programming**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ECKMicawFM"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "V(s) = max‚Çê(R(s,a) + Œ≥(0.2*V(s‚ÇÅ) + 0.2*V(s‚ÇÇ) + 0.6*V(s‚ÇÉ) )\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1Jas7OkoC5"
      },
      "source": [
        "The stochastic policy is used when the environment is uncertain. We call this process a **Partially Observable Markov Decision Process (POMDP)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWBKkvu6I8ON"
      },
      "source": [
        "##### **State (Action) Value**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sbve44FcmdE"
      },
      "source": [
        "* **State value function** specifies how good it is for an agent to be in a particular state with a policy œÄ. A value function is often denoted by V(s). It denotes the value of a state following a policy. \n",
        "\n",
        "* The state value function depends on the policy and it varies depending on the policy we choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy50NC2Cc24D"
      },
      "source": [
        "> $V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_AF8RuFcLau"
      },
      "source": [
        "* A **State-action value function (Q function)** is also called the Q function. It specifies how good it is for an agent to perform a particular action in a state with a policy œÄ. The Q function is denoted by Q(s). It denotes the value of taking an action in a state following a policy œÄ.\n",
        "\n",
        "* The difference between the value function and the Q function is that the value function specifies the goodness of a state, while a Q function specifies the goodness of an action in a state. Similar to state value table we can make Q table which shows the value of all possible state action pairs. Whenever we say value function V(S) or Q function Q( S, a), it actually means the value table and Q table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryer9jPWciIP"
      },
      "source": [
        "> $Q^{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s, a_{t}=a\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FzSIKa-KuLH"
      },
      "source": [
        "### **Learning Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhso417EdcDE"
      },
      "source": [
        "##### **Model-based vs Model-free**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMZ4V18i-vm"
      },
      "source": [
        "The problem we're often dealing with in RL is that whenever you are in state s and make an action ùëé you might not necessarily know the next state ùë†‚Ä≤ that you'll end up in (the environment influences the agent).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxb0_DuHg7lb"
      },
      "source": [
        "**Model-based**\n",
        "\n",
        "* the agent learns a model of how the environment works from its observations and then plan a solution using that model. If the agent is currently in state $s_{1},$ takes action $a_{1},$ and then observes the environment transition to state $s_{2}$ with reward $r_{2}$, that information can be used to improve its estimate of $T\\left(s_{2} \\mid s_{1}, a_{1}\\right)$ and $R\\left(s_{1}, a_{1}\\right)$ which can be performed using supervised learning approaches.  Once the agent has adequately modelled the environment, it can use a planning algorithm with its learned model to find a policy. RL solutions that follow this framework are model-based $R L$ algorithms.\n",
        "\n",
        "* A model-based learning agent uses knowledge of the environment dynamics in order to make predictions of expected outcomes.\n",
        "\n",
        "* The agent exploits previously learned information to accomplish a task, so you **either have an access to the model**(environment) so you know the probability distribution over states that you end up in, **or you first try to build a model** (often - approximation) yourself. This might be useful because it allows you to do planning (you can \"think\" about making moves ahead without actually performing any actions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtKA2SJVWJxj"
      },
      "source": [
        "**Model-free** \n",
        "\n",
        "* As it turns out though, we don‚Äôt have to learn a model of the environment to find a good policy. \n",
        "\n",
        "* On Model-free learning the agent simply relies on a trial-and-error experience for performing the right action. So you're not given a model and you're not trying to explicitly figure out how it works. You just collect some experience and then derive (hopefully) optimal policy.\n",
        "\n",
        "* One of the most classic examples is **Q-learning**, which directly estimates the optimal Q-values of each action in each state (roughly, the utility of each action in each state), from which a policy may be derived by choosing the action with the highest Q-value in the current state. **Actor-critic and policy search methods** directly search over policy space to find policies that result in better reward from the environment. Because **these approaches do not learn a model of the environment** they are called model-free algorithms.\n",
        "\n",
        "* A model-free learning agent does not use knowledge of the environment dynamics in order to make predictions of expected outcomes. The model here might be provided explicitly by the developer - that could be code for physics to predict a mechanical system, or it might be the rules of a board game that the agent is allowed to know and query to predict outcomes of actions before taking them. Models can also be learned statistically from experience, although that is harder to make effective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBDgNSFGWcvw"
      },
      "source": [
        "So if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: **after learning, can the agent make predictions about what the next state and reward will be before it takes each action**? If it can, then it‚Äôs a model-based RL algorithm. if it cannot, it‚Äôs a model-free algorithm. This same idea may also apply to decision-making processes other than MDPs. [Source](https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH81WPSoL-yL"
      },
      "source": [
        "##### **Policy-based vs Value-based**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKCbBKs8d9he"
      },
      "source": [
        "\n",
        "* In **Policy-based** methods we explicitly build a representation of a policy (mapping œÄ:s‚Üía) and keep it in memory during learning (known as Policy Network).\n",
        "\n",
        "* In **Value-based** we don't store any explicit policy, only a value function. The policy is here implicit and can be derived directly from the value function (pick the action with the best value).\n",
        "\n",
        "  * The value network assigns value/score to the state of the game by calculating an expected cumulative score for the current state s . \n",
        "\n",
        "  * Every state goes through the value network. The states which gets more reward obviously get more value in the network.\n",
        "\n",
        "  * You can have an on-policy RL algorithm that is value-based. An example of such algorithm is SARSA, so not all value-based algorithms are off-policy. A value-based algorithm is just an algorithm that estimates the policy by first estimating the associated value function.\n",
        "\n",
        "* **Actor-critic** is a mix of the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4sR7byZFUI"
      },
      "source": [
        "##### **On-Policy vs Off-Policy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Li4armbjNY"
      },
      "source": [
        "**On-Policy Algorithms**\n",
        "\n",
        "* An on-policy agent learns statistically about how it is currently acting, and assuming a control problem, then uses that knowledge to change how it should act in future. \n",
        "\n",
        "* An example for on-policy is SARSA (expected SARSA though can be also used in off-policy)\n",
        "\n",
        "* 2 types of Policy-based: deterministic vs stochastic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO6eWKiUbuoi"
      },
      "source": [
        "**Off-Policy Algorithms**\n",
        "\n",
        "* An off-policy agent can learn statistically from other observed behaviours (including its own past behaviour, or random and exploratory behaviour) and use that knowledge to understand how a different target behaviour would perform.\n",
        "\n",
        "* Off-policy learning is a strict generalisation of on-policy learning and includes on-policy as a special case. However, off-policy learning is also often harder to perform since observations typically contain less relevant data.\n",
        "\n",
        "* An example for off-policy is Q-learning. [Q-learning is a special case of Expected SARSA](https://ai.stackexchange.com/questions/20419/is-expected-sarsa-an-off-policy-or-on-policy-algorithm?rq=1), where the target policy is greedy with respect to the action values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fosxXpMZMSpr"
      },
      "source": [
        "https://stats.stackexchange.com/questions/407230/what-is-the-difference-between-policy-based-on-policy-value-based-off-policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxcTXRLMLNlg"
      },
      "source": [
        "* Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances.\n",
        "\n",
        "*  It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H44VEqefh8NG"
      },
      "source": [
        "This division is based on whether you update your $Q$ values based on actions undertaken according to your current policy or not. Let's say your current policy is a completely random policy. You're in state $s$ and make an action $a$ that leads you to state $s^{\\prime}$. will you update your $Q(s, a)$ based on the best possible action you can take in $s^{\\prime}$ or based on an action according to your current policy (random action)? The first choice method is called off-policy and the latter - onpolicy. E.g. Q-learning does the first and SARSA does the latter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq4JyvDInEYR"
      },
      "source": [
        "To understand the difference between on-policy and off-policy, you need to understand that there are two phases of an RL algorithm: the learning (or training) phase and the inference (or behaviour) phase (after the training phase). **The distinction between on-policy and off-policy algorithms only concerns the training phase.**\n",
        "\n",
        "* During the learning phase, the RL agent needs to learn an estimate of the optimal value (or policy) function. Given that the agent still does not know the optimal policy, it often behaves sub-optimally. During training, the agent faces a dilemma: the exploration or exploitation dilemma. In the context of RL, exploration and exploitation are different concepts: exploration is the selection and execution (in the environment) of an action that is likely not optimal (according to the knowledge of the agent) and exploitation is the selection and execution of an action that is optimal according to the agent's knowledge (that is, according to the agent's current best estimate of the optimal policy). \n",
        "\n",
        "* During the training phase, the agent needs to explore and exploit: the exploration is required to discover more about the optimal strategy, but the exploitation is also required to know even more about the already visited and partially known states of the environment. During the learning phase, the agent thus can't just exploit the already visited states, but it also needs to explore possibly unvisited states. To explore possibly unvisited states, the agent often needs to perform a sub-optimal action.\n",
        "\n",
        "* An off-policy algorithm is an algorithm that, during training, uses a behaviour policy (that is, the policy it uses to select actions) that is different than the optimal policy it tries to estimate (the optimal policy). For example, ùëÑ-learning often uses an œµ-greedy policy (œµ percentage of the time it chooses a random or explorative action and 1‚àíœµ percentage of the time it chooses the action that is optimal, according to its current best estimate of the optimal policy) to behave (that is, to exploit and explore the environment), while, in its update rule, because of the max\n",
        "max\n",
        " operator, it assumes that the greedy action (that is, the current optimal action in a given state) is chosen.\n",
        "\n",
        "* An on-policy algorithm is an algorithm that, during training, chooses actions using a policy that is derived from the current estimate of the optimal policy, while the updates are also based on the current estimate of the optimal policy. For example, SARSA is an on-policy algorithm because it doesn't use the max\n",
        "operator in its update rule.\n",
        "\n",
        "* The difference between ùëÑ-learning (off-policy) and SARSA (on-policy) is respectively the use or not of the max operator in their update rule.\n",
        "\n",
        "* In the case of policy-based or policy search algorithm (e.g. REINFORCE), the distinction between on-policy and off-policy is often not made because, in this context, there isn't usually a clear separation between a behaviour policy (the policy to behave during training) and a target policy (the policy to be estimated).\n",
        "\n",
        "* You can think of actor-critic algorithms as value and policy-based because they use both a value and policy functions.\n",
        "\n",
        "* The usual examples of model-based algorithms are value and policy iterations, which are algorithms that use the transition and reward functions (of the given Markov decision process) to estimate the value function. However, it might be the case that you also have on-policy, off-policy, value-based or policy-based algorithms that are model-based, in some way, that is, they might use a model of the environment in some way\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImV2HamKYd8k"
      },
      "source": [
        "### **Simulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMuMiKNdYNz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "065cd217-ccff-4b6e-e0e0-d8277b538efd"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "#make environment\n",
        "env = gym.make('FrozenLake-v0')\n",
        "# as the environment is continues there cannot be finite number of states \n",
        "states = env.observation_space.n #used if discrete environment\n",
        "\n",
        "#check number of actions that can be \n",
        "actions = env.action_space.n\n",
        "\n",
        "#initialize value table randomly\n",
        "value_table = np.zeros((states,1))\n",
        "\n",
        "def value_iterations(env , n_iterations , gamma = 1.0 , threshold = 1e-30):\n",
        "    for i in range(n_iterations):\n",
        "        \n",
        "        new_valuetable = np.copy(value_table)\n",
        "        for state in range(states):\n",
        "            q_value = []\n",
        "            for action in range(actions):\n",
        "                next_state_reward = []\n",
        "                for next_state_parameters in env.env.P[state][action]:\n",
        "                    transition_prob, next_state, reward_prob, _ = next_state_parameters\n",
        "                    reward = transition_prob*(reward_prob+gamma*new_valuetable[next_state])\n",
        "                    next_state_reward.append(reward)\n",
        "                    \n",
        "                    \n",
        "                q_value.append((np.sum(next_state_reward)))\n",
        "            value_table[state] = max(q_value)\n",
        "            \n",
        "        if (np.sum(np.fabs(new_valuetable - value_table))<=threshold):\n",
        "            break\n",
        "    return value_table\n",
        "  \n",
        "\n",
        "def extract_policy(value_table, gamma = 1.0):\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "  for state in range(env.observation_space.n):\n",
        "    Q_table = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "      for next_sr in env.env.P[state][action]:\n",
        "        transition_prob, next_state, reward_prob, _ = next_sr\n",
        "        Q_table[action] += (transition_prob * (reward_prob + gamma *value_table[next_state]))\n",
        "    policy[state] = np.argmax(Q_table)\n",
        "  return policy\n",
        "value_table = value_iterations(env,10000)\n",
        "policy = extract_policy(value_table)\n",
        "print(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}