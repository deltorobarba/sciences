{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "divergence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UDmivRG6wJ2D",
        "u2LGANiFklj2",
        "_mkLGH0zwMko",
        "NhDdXzUqwR49",
        "lU3B-d5Ww08t",
        "ceCs4P8z17Q5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/divergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV",
        "colab_type": "text"
      },
      "source": [
        "# **Statistical Distance & Divergence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U5_6xH7tAQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDmivRG6wJ2D",
        "colab_type": "text"
      },
      "source": [
        "#### **Metric vs Statistical Distance & Divergences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3l9YTusyorX",
        "colab_type": "text"
      },
      "source": [
        "* In statistics and information geometry, divergence or a contrast function is a function which establishes the **\"distance\" of one probability distribution to the other** on a statistical manifold. \n",
        "\n",
        "* The **divergence is a weaker notion than that of the distance**, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and **need not satisfy the triangle inequality**.\n",
        "\n",
        "In statistics, probability theory, and information theory, **a statistical distance** quantifies the distance between two statistical objects, which can be\n",
        "\n",
        "* two random variables, or \n",
        "* two probability distributions or \n",
        "* two samples, or \n",
        "* the distance can be between an individual sample point and a population or \n",
        "* a wider sample of points.\n",
        "\n",
        "A distance between populations can be interpreted as measuring the distance between two probability distributions and hence they are essentially measures of distances between probability measures. \n",
        "\n",
        "* Where statistical distance measures relate to the differences between random variables, these may have statistical dependence, and hence these distances are not directly related to measures of distances between probability measures. \n",
        "\n",
        "* Again, a measure of distance between random variables may relate to the extent of dependence between them, rather than to their individual values.\n",
        "\n",
        "* **Statistical distance measures are mostly not metrics** and they need not be symmetric. **Some types of distance measures are referred to as (statistical) divergences**.\n",
        "\n",
        "**Properties of Distances as Metrics** (id they fullfill all 4 criteria)\n",
        "\n",
        "1. $d(x, y) \\geq 0 \\quad$ (non-negativity)\n",
        "2. $d(x, y)=0$ if and only if $x=y$ (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. $d(x, y)=d(y, x)$ (symmetry)\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (subadditivity / triangle inequality).\n",
        "\n",
        "**Many statistical distances are not metrics**, because they lack one or more properties of proper metrics. For example, \n",
        "\n",
        "* [pseudometrics](https://en.m.wikipedia.org/wiki/Pseudometric_space) violate the \"positive definiteness\" (alternatively, \"identity of indescernibles\") property (1 & 2 above); \n",
        "\n",
        "* [quasimetrics](https://en.m.wikipedia.org/wiki/Metric_(mathematics)#Quasimetrics) violate the symmetry property (3); and semimetrics violate the triangle inequality (4). \n",
        "\n",
        "* Statistical distances that satisfy (1) and (2) are referred to as divergences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gp5CIvP2Gxz",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Distance_(graph_theory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVLYU7qbDyCl",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ32qNn2D_Gk",
        "colab_type": "text"
      },
      "source": [
        "* In statistics and information geometry, there are many kinds of statistical distances, notably divergences, especially Bregman divergences and f-divergences. These include and generalize many of the notions of \"difference between two probability distributions\", and allow them to be studied geometrically, as statistical manifolds. \n",
        "\n",
        "* The most elementary is the squared Euclidean distance, which forms the basis of least squares; this is the most basic Bregman divergence. The most important in information theory is the relative entropy (Kullback–Leibler divergence), which allows one to analogously study maximum likelihood estimation geometrically; this is the most basic f-divergence, and is also a Bregman divergence (and is the only divergence that is both). \n",
        "\n",
        "* Statistical manifolds corresponding to Bregman divergences are flat manifolds in the corresponding geometry, allowing an analog of the Pythagorean theorem (which is traditionally true for squared Euclidean distance) to be used for linear inverse problems in inference by optimization theory.\n",
        "\n",
        "* Other important statistical distances include the Mahalanobis distance, the energy distance, and many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Bqq0S2uGHn",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_geometry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSf1EypWEKH6",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Statistical_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0HNuzkiuKmc",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2LGANiFklj2",
        "colab_type": "text"
      },
      "source": [
        "#### **List of Distances Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVR6FmOskqZn",
        "colab_type": "text"
      },
      "source": [
        "'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance\n",
        "\n",
        " 'canberra': hdbscan.dist_metrics.CanberraDistance\n",
        "\n",
        " 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance\n",
        "\n",
        " 'cityblock': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        " 'dice': hdbscan.dist_metrics.DiceDistance\n",
        "\n",
        " 'euclidean': hdbscan.dist_metrics.EuclideanDistance\n",
        "\n",
        " 'hamming': hdbscan.dist_metrics.HammingDistance\n",
        "\n",
        " 'haversine': hdbscan.dist_metrics.HaversineDistance\n",
        "\n",
        " 'infinity': hdbscan.dist_metrics.ChebyshevDistance\n",
        "\n",
        " 'jaccard': hdbscan.dist_metrics.JaccardDistance\n",
        "\n",
        " 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance\n",
        "\n",
        " 'l1': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        " 'l2': hdbscan.dist_metrics.EuclideanDistance\n",
        "\n",
        " 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance\n",
        "\n",
        " 'manhattan': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        " 'matching': hdbscan.dist_metrics.MatchingDistance\n",
        "\n",
        " 'minkowski': hdbscan.dist_metrics.MinkowskiDistance\n",
        "\n",
        " 'p': hdbscan.dist_metrics.MinkowskiDistance\n",
        "\n",
        " 'pyfunc': hdbscan.dist_metrics.PyFuncDistance\n",
        "\n",
        " 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance\n",
        "\n",
        " 'russellrao': hdbscan.dist_metrics.RussellRaoDistance\n",
        "\n",
        " 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance\n",
        "\n",
        " 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance\n",
        "\n",
        " 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance\n",
        "\n",
        " 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT69HLULk74-",
        "colab_type": "text"
      },
      "source": [
        "https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOhM_FCg1Tnz",
        "colab_type": "text"
      },
      "source": [
        "https://reference.wolfram.com/language/guide/DistanceAndSimilarityMeasures.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mkLGH0zwMko",
        "colab_type": "text"
      },
      "source": [
        "#### **f-Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBgXfQpwgtt",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/F-divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h682KAKdxX2_",
        "colab_type": "text"
      },
      "source": [
        "The Hellinger distance is a type of f-divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-OmXYHtwuW_",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Hellinger_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhDdXzUqwR49",
        "colab_type": "text"
      },
      "source": [
        "#### **Bregman Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r2XN4v8wkj3",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Bregman_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYgT1HtxIgJ",
        "colab_type": "text"
      },
      "source": [
        "The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BdWjOSuxDXh",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU3B-d5Ww08t",
        "colab_type": "text"
      },
      "source": [
        "#### **Kullback–Leibler divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZ-_wCxw9VS",
        "colab_type": "text"
      },
      "source": [
        "The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkPx5L0w4fb",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCs4P8z17Q5",
        "colab_type": "text"
      },
      "source": [
        "#### **Jensen–Shannon divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mWHJjm71-WE",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Jensen–Shannon_divergence"
      ]
    }
  ]
}