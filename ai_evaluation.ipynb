{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtqm2vO+odrJxSYsHTZblj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Observability & Evaluation**"
      ],
      "metadata": {
        "id": "45jfy2sWQt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Gecko for Multimodal Evaluation*"
      ],
      "metadata": {
        "id": "1N81st1oYFad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/evaluate-your-gen-media-models-on-vertex-ai"
      ],
      "metadata": {
        "id": "VWZPPGWpYJAU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge03DgPsQtMD"
      },
      "outputs": [],
      "source": [
        "# set up configurations for both rubric generation and rubric validation\n",
        "\n",
        "# Rubric Generation\n",
        "rubric_generation_config = RubricGenerationConfig(\n",
        "    prompt_template=RUBRIC_GENERATION_PROMPT,\n",
        "    parsing_fn=parse_json_to_qa_records,\n",
        ")\n",
        "# Rubric Validation\n",
        "pointwise_metric = PointwiseMetric(\n",
        "    metric=\"gecko_metric\",\n",
        "    metric_prompt_template=RUBRIC_VALIDATOR_PROMPT,\n",
        "    custom_output_config=CustomOutputConfig(\n",
        "        return_raw_output=True,\n",
        "        parsing_fn=parse_rubric_results,\n",
        "    ),\n",
        ")\n",
        "# Rubric Metric\n",
        "rubric_based_gecko = RubricBasedMetric(\n",
        "    generation_config=rubric_generation_config,\n",
        "    critique_metric=pointwise_metric,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare your dataset for evaluation\n",
        "\n",
        "prompts = [\n",
        "    \"steaming cup of coffee and a croissant on a table\",\n",
        "    \"steaming cup of coffee and toast in a cafe\",\n",
        "    # ... more prompts\n",
        "]\n",
        "images = [\n",
        "    '{\"contents\": [{\"parts\": [{\"file_data\": {\"mime_type\": \"image/png\", \"file_uri\": \"gs://cloud-samples-data/generative-ai/evaluation/images/coffee.png\"}}]}]}',\n",
        "    '{\"contents\": [{\"parts\": [{\"file_data\": {\"mime_type\": \"image/png\", \"file_uri\": \"gs://cloud-samples-data/generative-ai/evaluation/images/coffee.png\"}}]}]}',\n",
        "    # ... more image URIs\n",
        "]\n",
        "eval_dataset = pd.DataFrame(\n",
        "    {\n",
        "        \"prompt\": prompts,\n",
        "        \"image\": images, # or \"video\": videos for video evaluation\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "QIWAFzEnZQNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the rubrics based on your prompts using the configured rubric_based_gecko metric\n",
        "\n",
        "dataset_with_rubrics = rubric_based_gecko.generate_rubrics(eval_dataset)"
      ],
      "metadata": {
        "id": "N7GxQhHSZXch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the evaluation using the generated rubrics and your dataset\n",
        "\n",
        "eval_task = EvalTask(\n",
        "    dataset=dataset_with_rubrics,\n",
        "    metrics=[rubric_based_gecko],\n",
        ")\n",
        "eval_result = eval_task.evaluate(response_column_name=\"image\") # or \"video\""
      ],
      "metadata": {
        "id": "OzA3czZPZa3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After the evaluation runs, you can compute and analyze the final scores to understand\n",
        "# how well your generated content aligns with the detailed criteria derived from your prompts\n",
        "\n",
        "dataset_with_final_scores = compute_scores(eval_result.metrics_table)\n",
        "np.mean(dataset_with_final_scores[\"final_score\"])"
      ],
      "metadata": {
        "id": "dCzfASywZ0LW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}