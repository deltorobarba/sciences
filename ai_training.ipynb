{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wPtvvS62_Jvu",
        "k3rKfi9B_O5q"
      ],
      "authorship_tag": "ABX9TyOePlpv5/aLsaxNDpLrd024",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Training and Tuning**"
      ],
      "metadata": {
        "id": "45jfy2sWQt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "100+ Fine-tuning Unsloth AI notebooks: https://github.com/unslothai/notebooks/"
      ],
      "metadata": {
        "id": "oJBSChxyu3Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Create a LoRA adapter*"
      ],
      "metadata": {
        "id": "wPtvvS62_Jvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ðŸ¤— PEFT & bitsandbytes to finetune a LoRa checkpoint\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-3.1-8B\n",
        "\n",
        "https://huggingface.co/models?other=base_model:quantized:meta-llama/Llama-3.1-8B\n",
        "\n",
        "https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit\n",
        "\n",
        "Paper: https://arxiv.org/abs/2106.09685\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=Us5ZFp16PaU&t=393s"
      ],
      "metadata": {
        "id": "j5b_v4Vc4kB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YT PEFT Finetune-Bloom7B-tagger**\n",
        "\n",
        "Original: https://colab.research.google.com/drive/14xo6sj4dARk8lXZbOifHEn1f_70qNAwy?usp=sharing#scrollTo=4iwHGzKBN6wk\n"
      ],
      "metadata": {
        "id": "rgeQGtCYANg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-7b1\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")"
      ],
      "metadata": {
        "id": "ER_Eu48aACyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing the original weights\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "AiGjKhkLAWBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the LoRa Adapters\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "FrSbGIr7AdBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16, #attention heads\n",
        "    lora_alpha=32, #alpha scaling\n",
        "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "MSBVvf-mAgwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")"
      ],
      "metadata": {
        "id": "4PTYC-ckAixS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_columns(example):\n",
        "    example[\"prediction\"] = example[\"quote\"] + \" ->: \" + str(example[\"tags\"])\n",
        "    return example\n",
        "\n",
        "data['train'] = data['train'].map(merge_columns)\n",
        "data['train'][\"prediction\"][:5]"
      ],
      "metadata": {
        "id": "i0cMYtSeAlJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][0]"
      ],
      "metadata": {
        "id": "n_EEgNDAAoj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(lambda samples: tokenizer(samples['prediction']), batched=True)"
      ],
      "metadata": {
        "id": "xNNCJiSfAnUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RMG34t9bAqry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Share adapters on the ðŸ¤— Hub\n",
        "\n",
        "model.push_to_hub(\"samwit/bloom-7b1-lora-tagger\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"basic training\",\n",
        "                  private=True)"
      ],
      "metadata": {
        "id": "adPnn3fGAxUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load adapters from the Hub\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"samwit/bloom-7b1-lora-tagger\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "zI8mXtCbA1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llama-3.1 8b + Unsloth 2x faster finetuning**\n",
        "\n",
        "https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=QmUBVEnvCDJv\n"
      ],
      "metadata": {
        "id": "u15JYbTZAGJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge03DgPsQtMD"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "aScsFry8_eml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Appendix*"
      ],
      "metadata": {
        "id": "k3rKfi9B_O5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning LLM\n",
        "Official documentation on tuning large language models: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models\n",
        "Codelab tutorial on fine-tuning LLMs with Vertex AI: https://codelabs.developers.google.com/llm-finetuning-supervised\n",
        "Medium article providing insights into fine-tuning with Vertex AI: https://medium.com/google-cloud/fine-tuning-large-language-models-how-vertex-ai-takes-llms-to-the-next-level-3c113f4007da"
      ],
      "metadata": {
        "id": "1XdFhHDWXEOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.sandbox.google.com/drive/1CHtIWodkATMxWSkSdaZj60EkFTXymq26?resourcekey=0-YVhm1YZ9excXMSpaTTtxhA#scrollTo=35615fa2-c923-4bfd-b194-5265bbe81ce1\n",
        "\n",
        "managed tuning OSS\n"
      ],
      "metadata": {
        "id": "GpAnpq-c5wAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Model Garden\n",
        "\n",
        "PEFT: https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models\n",
        "\n",
        "Example: Mistral: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral_peft_tuning.ipynb\n",
        "\n",
        "Example: LLama3 https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_finetuning.ipynb\n",
        "\n",
        "Managed PEFT for open-weight models: pending\n",
        "\n",
        "Full finetuning: released\n",
        "\n",
        "DPO - Direct Preference Optimization: directly optimizes the LLM's policy using a dataset of human preferences. https://arxiv.org/pdf/2305.18290.pdf (pending)\n",
        "\n",
        "Continuous Pretraining: bring your own custom LoRA (end Q2) and tokenspace\n"
      ],
      "metadata": {
        "id": "uHWl_clr51OZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models"
      ],
      "metadata": {
        "id": "hYyiqHFC6FhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open source:** Ray, Horovod (training), Axolotl (finetune multiple GPU), Unsloth (finetune single GPU), Dask, Spark\n"
      ],
      "metadata": {
        "id": "Gn7b-NVO6I0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Garden Tuning: Explore Models https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models\n"
      ],
      "metadata": {
        "id": "_PY0Az1W6Lpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vertex Training: Pricing for training (https://cloud.google.com/vertex-ai/pricing#custom-trained_models) for example SVT for Gemini (https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning#supported_models)\n"
      ],
      "metadata": {
        "id": "yn9Mwpxh6QQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ray on Vertex (managed https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview) or Ray on GKE (with KubeRay https://cloud.google.com/kubernetes-engine/docs/add-on/ray-on-gke/concepts/overview), example see Gemma on Ray (https://developers.googleblog.com/en/get-started-with-gemma-on-ray-on-vertex-ai/), Scale AI on Ray on Vertex AI  (https://medium.com/google-cloud/ray-on-vertex-ai-lets-get-it-started-7a9f8360ea25)\n"
      ],
      "metadata": {
        "id": "aaPjib3r6WqZ"
      }
    }
  ]
}