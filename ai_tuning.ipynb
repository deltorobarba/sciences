{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfDbpW3M20u/sCA/lJsUg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PEFT Tuning LLama**"
      ],
      "metadata": {
        "id": "W2OBffP6c3U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama"
      ],
      "metadata": {
        "id": "5ybUxv-2GQGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-1-8"
      ],
      "metadata": {
        "id": "RjoLhRFcGEj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/models/open-model-tuning"
      ],
      "metadata": {
        "id": "T5zDhrwt8FY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning"
      ],
      "metadata": {
        "id": "2dJwMZkYzuGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Structure**\n",
        "\n",
        "Create a directory structure like this and upload the files in the bucket:\n",
        "\n",
        "```\n",
        "vertex-peft-pipeline/\n",
        "├── pipeline.py\n",
        "├── requirements.txt\n",
        "└── src/\n",
        "    ├── train.py\n",
        "    └── Dockerfile\n",
        "```"
      ],
      "metadata": {
        "id": "s3ctztzQfi-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparations**\n",
        "\n",
        "Set your project ID and number as variables\n",
        "```\n",
        "export PROJECT_ID=$(gcloud config get-value project)\n",
        "export PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format=\"value(projectNumber)\")\n",
        "```\n",
        "\n",
        "Grant the Artifact Registry Writer role to the Cloud Build service account\n",
        "```\n",
        "gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\" \\\n",
        "    --role=\"roles/artifactregistry.writer\"\n",
        "```\n",
        "\n",
        "Make sure to create an Artifact Registry Repository\n",
        "```\n",
        "gcloud artifacts repositories create peft-tuning-repo \\ --repository-format=docker \\ --location=us-central1 \\ --description=\"Docker repository for PEFT training images\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "NxqU8JMwdrXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a 'requirements.txt'**\n",
        "\n",
        "```\n",
        "google-cloud-aiplatform\n",
        "kfp\n",
        "```"
      ],
      "metadata": {
        "id": "oQrY31qSd_ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create pipeline.py**\n",
        "\n",
        "```\n",
        "import os\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
        "REGION = \"us-central1\"\n",
        "# We still need a staging bucket for the CustomJob\n",
        "STAGING_BUCKET = \"gs://deltorobarba-us-central1/pipeline-root\"\n",
        "DOCKER_REPO_NAME = \"peft-tuning-repo\"\n",
        "\n",
        "# This token should be managed securely, e.g., via Vertex AI Secrets\n",
        "HF_TOKEN = \"YOUR-HUGGINGFACE-TOKEN\"                     # <--- YOU NEED TO UPDATE THAT\n",
        "\n",
        "# --- Docker Image Configuration ---\n",
        "IMAGE_NAME = \"peft-lora-trainer\"\n",
        "IMAGE_TAG = \"latest\"\n",
        "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "    # --- Define the arguments for the training script ---\n",
        "    # These are the parameters that were previously in the pipeline definition\n",
        "    MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    DATASET_GCS_DIRECTORY = \"gs://deltorobarba-us-central1/data\"\n",
        "    EPOCHS = 2\n",
        "    # Define a unique output path for this job's artifacts\n",
        "    ADAPTER_GCS_URI = f\"{STAGING_BUCKET}/peft-output/{aiplatform.utils.timestamped_unique_name()}\"\n",
        "\n",
        "\n",
        "    # --- Create and run the CustomJob directly ---\n",
        "    job = aiplatform.CustomJob(\n",
        "        display_name=\"peft-lora-tuning-job\",\n",
        "        worker_pool_specs=[\n",
        "            {\n",
        "                \"machine_spec\": {\n",
        "                    \"machine_type\": \"g2-standard-4\",\n",
        "                    \"accelerator_type\": \"NVIDIA_L4\",\n",
        "                    \"accelerator_count\": 1,\n",
        "                },\n",
        "                \"replica_count\": 1,\n",
        "                \"container_spec\": {\n",
        "                    \"image_uri\": IMAGE_URI,\n",
        "                    \"args\": [\n",
        "                        \"--model-id\", MODEL_ID,\n",
        "                        \"--dataset-gcs-directory\", DATASET_GCS_DIRECTORY,\n",
        "                        \"--epochs\", str(EPOCHS),\n",
        "                        \"--adapter-gcs-uri\", ADAPTER_GCS_URI,\n",
        "                    ],\n",
        "                    \"env\": [{\"name\": \"HF_TOKEN\", \"value\": HF_TOKEN}],\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    job.run()\n",
        "\n",
        "    print(\"--- Custom job submitted. ---\")\n",
        "    print(f\"View the job here: {job.resource_name}\")\n",
        "    print(f\"Adapter will be saved to: {ADAPTER_GCS_URI}\")\n",
        "```"
      ],
      "metadata": {
        "id": "BuzhlHeaebDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dockerfile**\n",
        "\n",
        "```\n",
        "# In your Dockerfile\n",
        "\n",
        "# Use a PyTorch base image with CUDA support\n",
        "FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Install dependencies\n",
        "RUN pip install --no-cache-dir --upgrade pip\n",
        "# CORRECTED: Update libraries to their latest versions for Llama 3.1 support\n",
        "RUN pip install --no-cache-dir \\\n",
        "    \"transformers>=4.42.0\" \\\n",
        "    \"peft>=0.11.0\" \\\n",
        "    \"accelerate>=0.31.0\" \\\n",
        "    \"bitsandbytes>=0.43.0\" \\\n",
        "    \"datasets\" \\\n",
        "    \"torch\" \\\n",
        "    \"google-cloud-storage\" \\\n",
        "    \"google-cloud-aiplatform\" \\\n",
        "    \"gcsfs\"\n",
        "\n",
        "# Copy the training script into the container\n",
        "COPY train.py .\n",
        "# Set the entrypoint for the container\n",
        "ENTRYPOINT [\"python\", \"train.py\"]\n",
        "```"
      ],
      "metadata": {
        "id": "TtKfGEOTevJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create train.py**\n",
        "\n",
        "```\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from google.cloud import storage\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "def train_and_upload(args):\n",
        "    \"\"\"Loads model and dataset, performs PEFT training, and uploads to GCS.\"\"\"\n",
        "\n",
        "    # --- 1. Load Tokenizer and Model ---\n",
        "    print(\"--- 1. Loading tokenizer and model ---\")\n",
        "    \n",
        "    # Hugging Face token to access Llama 2\n",
        "    # In a production pipeline, use Vertex AI Secrets for this\n",
        "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"Hugging Face token not found in environment variable HF_TOKEN\")\n",
        "\n",
        "    # Configuration for loading the model in 4-bit precision\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, token=hf_token)\n",
        "    # Llama 2 does not have a pad token, so we set it to the end-of-sentence token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\", # Automatically place layers on available devices\n",
        "        token=hf_token,\n",
        "    )\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # --- 2. Configure PEFT LoRA ---\n",
        "    print(\"--- 2. Configuring PEFT LoRA ---\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        # Llama 3 has more projection layers (q, k, v, o)\n",
        "        # CHANGE THIS LINE:\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    \n",
        "    # Add LoRA adapter to the model\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # --- 3. Load and Preprocess Dataset ---\n",
        "    print(f\"--- 3. Loading and processing dataset from: {args.dataset_gcs_directory} ---\")\n",
        "    \n",
        "    # Construct the full path to the training file in the GCS bucket\n",
        "    train_file = f\"{args.dataset_gcs_directory}/train.jsonl\"\n",
        "    eval_file = f\"{args.dataset_gcs_directory}/val.jsonl\"\n",
        "\n",
        "    \n",
        "    # Load the dataset from the specified GCS path\n",
        "    # We specify the format is 'json' and provide the file path\n",
        "    dataset = load_dataset(\"json\", data_files={\"train\": train_file}, split=\"train\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # The 'messages' field from your data is already in the correct format.\n",
        "        # We can pass it directly to the tokenizer.\n",
        "        # Note: We are accessing examples['messages'] which exists in your data.\n",
        "        messages = examples['messages']\n",
        "    \n",
        "        tokenized_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            # Increase max_length as your new data format is much longer\n",
        "            max_length=512,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        return {\"input_ids\": tokenized_text}\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "\n",
        "    # --- 4. Set Up Trainer ---\n",
        "    print(\"--- 4. Setting up Trainer ---\")\n",
        "    \n",
        "    # Output directory for the training artifacts within the container\n",
        "    local_output_dir = \"/tmp/output\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=local_output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=args.epochs,\n",
        "        logging_steps=10,\n",
        "        fp16=True, # Use mixed precision\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\", # Disable reporting to wandb/tensorboard for this example\n",
        "        evaluation_strategy=\"steps\", # Evaluate at regular intervals\n",
        "        eval_steps=50, # How often to evaluate\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        eval_dataset=tokenized_eval_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    # --- 5. Start Training ---\n",
        "    print(\"--- 5. Starting training ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Training finished ---\")\n",
        "\n",
        "\n",
        "    # --- 6. Save and Upload Adapter ---\n",
        "    print(f\"--- 6. Saving adapter to {local_output_dir} and uploading to {args.adapter_gcs_uri} ---\")\n",
        "    # Save the PEFT adapter weights\n",
        "    trainer.save_model(local_output_dir)\n",
        "\n",
        "    # Upload the contents of the local output directory to GCS\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = args.adapter_gcs_uri.replace(\"gs://\", \"\").split(\"/\")[0]\n",
        "    destination_prefix = \"/\".join(args.adapter_gcs_uri.replace(\"gs://\", \"\").split(\"/\")[1:])\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    for root, _, files in os.walk(local_output_dir):\n",
        "        for filename in files:\n",
        "            local_path = os.path.join(root, filename)\n",
        "            gcs_path = os.path.join(destination_prefix, os.path.relpath(local_path, local_output_dir))\n",
        "            blob = bucket.blob(gcs_path)\n",
        "            blob.upload_from_filename(local_path)\n",
        "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")\n",
        "\n",
        "    print(\"--- Artifacts uploaded successfully ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model-id\", type=str, required=True, help=\"Base model ID from Hugging Face Hub\")\n",
        "    # Change dataset-id to dataset-gcs-directory to match the pipeline\n",
        "    parser.add_argument(\"--dataset-gcs-directory\", type=str, required=True, help=\"GCS directory containing train.jsonl\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--adapter-gcs-uri\", type=str, required=True, help=\"GCS URI to save the trained adapter\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    train_and_upload(args)\n",
        "```"
      ],
      "metadata": {
        "id": "fkID9nzte4By"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load files, build dockerfile and start tuning job**\n",
        "\n",
        "Open shell terminal in Google Cloud. Then copy the files from a GCS bucket into shell (the \".\" at the end means \"copy it to my current directory\"):\n",
        "\n",
        "```\n",
        "export GCS_SOURCE_PATH=\"gs://deltorobarba/tuning/vertex-peft-pipeline\"\n",
        "gcloud storage cp -r ${GCS_SOURCE_PATH} .\n",
        "```\n",
        "\n",
        "Navigate into the directory you just copied\n",
        "```\n",
        "cd vertex-peft-pipeline\n",
        "```\n",
        "\n",
        "Now run your build and pipeline submission commands\n",
        "```\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "Replace with your actual project ID and desired region\n",
        "```\n",
        "export PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
        "export REGION=\"us-central1\"\n",
        "export DOCKER_REPO_NAME=\"peft-tuning-repo\"\n",
        "export IMAGE_NAME=\"peft-lora-trainer\"\n",
        "export IMAGE_URI=\"${REGION}-docker.pkg.dev/${PROJECT_ID}/${DOCKER_REPO_NAME}/${IMAGE_NAME}:latest\"\n",
        "```\n",
        "\n",
        "Verify the Full Image Tag\n",
        "```\n",
        "echo ${IMAGE_URI}\n",
        "```\n",
        "\n",
        "Run the Correct Build Command, takes 5-10 min (make sure IAM permission are provided to compute and repository:\n",
        "```\n",
        "gcloud builds submit ./src --tag=\"${IMAGE_URI}\"\n",
        "```\n",
        "\n",
        "Now run pipeline\n",
        "```\n",
        "python pipeline.py\n",
        "```\n"
      ],
      "metadata": {
        "id": "WGG4j-Puc775"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Output Files**\n",
        "\n",
        "The files you see in your GCS bucket are the components of your LoRA adapter. You are not saving a whole new 8-billion-parameter model, but just the small, efficient adapter \"layers\" that you trained. Here are the most important files:\n",
        "\n",
        "* `adapter_model.safetensors`: This is the core of your output. It contains the actual trained weights of your LoRA adapter. The .safetensors format is a secure and fast way to store model weights.\n",
        "\n",
        "* `adapter_config.json`: This is the configuration file for your adapter. It tells the PEFT library how the adapter was built (e.g., the LoRA rank (r), target_modules, etc.), so it knows how to correctly load the weights from adapter_model.safetensors and apply them to the base model.\n",
        "\n",
        "* `tokenizer.json`, `tokenizer.model`, etc.: These files are a complete copy of the tokenizer from the base Llama 3.1 model. The trainer saves these to ensure that you use the exact same tokenizer for inference that you used during training, preventing any mismatch issues."
      ],
      "metadata": {
        "id": "Ei_MbJ1igAFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tuning of OpenAI GPT OSS on Google Cloud Vertex AI Colab Enterprise using NVIDIA A100\n",
        "\n",
        "  https://lnkd.in/eM-jasdb  \n",
        "\n",
        "Check Unsloth github and star it if you like to Fine Tune and Serve LLMs with a reduced memory footprint :  https://lnkd.in/eGgQaTQm\n",
        "\n",
        "\n",
        "\n",
        "Also explore Unsloth AI fantastic resources around Google DeepMind Gemma 3 and  Gemma 3n resources for smaller models that better fit edge and Web AI use cases\n",
        "\n",
        "Fine tuning Gemma 3 1B here : https://lnkd.in/e96kDCfH"
      ],
      "metadata": {
        "id": "5Iq4B7FKiBHP"
      }
    }
  ]
}