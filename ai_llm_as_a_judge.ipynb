{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_llm_as_a_judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LLM as a Judge**"
      ],
      "metadata": {
        "id": "dkHA4NglD5u8"
      },
      "id": "dkHA4NglD5u8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Update project ID, GCP bucket name and name of TMX file manually:\n",
        "\n",
        "PROJECT_ID = \"YOUR-PROJECT-ID\"               # <--- UPDATE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_NAME = \"translations-eval\" # <--- UPDATE THIS\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "TMX_GCS_PATH = \"samples.tmx\"    # <--- UPLOAD THIS\n",
        "LOCAL_TMX_FILE = \"samples.tmx\""
      ],
      "metadata": {
        "id": "x2GtCQTjx7-o"
      },
      "id": "x2GtCQTjx7-o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade google-cloud-aiplatform google-cloud-storage -q\n",
        "%pip install matplotlib seaborn langdetect -q\n",
        "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]\n",
        "\n",
        "!pip install google-cloud-translate==2.0.1 -q\n",
        "!pip install --upgrade google-cloud-translate -q\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.cloud import aiplatform, storage\n",
        "from google.cloud import translate_v3 as translate\n",
        "import vertexai\n",
        "from vertexai.tuning import sft\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For language detection\n",
        "from langdetect import detect, detect_langs"
      ],
      "metadata": {
        "id": "l4-2YlW2AhuU"
      },
      "id": "l4-2YlW2AhuU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Utility Functions\n",
        "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a file from GCS and returns the local path.\"\"\"\n",
        "    print(f\"--- Downloading {source_blob_name} ---\")\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "    print(f\"Successfully downloaded to {destination_file_name}\")\n",
        "    return destination_file_name\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to GCS and returns the GCS URI.\"\"\"\n",
        "    print(f\"--- Uploading {source_file_name} ---\")\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
        "    print(f\"Successfully uploaded to {gcs_uri}\")\n",
        "    return gcs_uri"
      ],
      "metadata": {
        "id": "ZgUnsbm4RfGV"
      },
      "id": "ZgUnsbm4RfGV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "ULqCi2aJ_II4"
      },
      "id": "ULqCi2aJ_II4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import vertexai\n",
        "\n",
        "#if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "#  PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "#LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\") #us-central1\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "bgHj8mjl_KtV"
      },
      "id": "bgHj8mjl_KtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai seaborn matplotlib -q\n",
        "\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "T6hyzClE_NYZ"
      },
      "id": "T6hyzClE_NYZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you may need to change that to your customer key\n",
        "if not client.vertexai:\n",
        "  print(\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "  print(\n",
        "      f\"Using Vertex AI with project: {client._api_client.project} in location:\"\n",
        "      f\" {client._api_client.location}\"\n",
        "  )\n",
        "elif client._api_client.api_key:\n",
        "  print(\n",
        "      \"Using Vertex AI in express mode with API key:\"\n",
        "      f\" {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3CVywhU_PtJ",
        "outputId": "1ee80ef0-9509-4924-aa5c-00b35fc196e0"
      },
      "id": "O3CVywhU_PtJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: lunar-352813 in location: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generic test if LLM model works\n",
        "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What is the name of the largest search engine?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "oBkjseKi_UND"
      },
      "id": "oBkjseKi_UND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LLM as a Judge\n",
        "\n",
        "\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "table_as_markdown = summary_df.to_markdown(index=False)\n",
        "prompt = f\"\"\"\n",
        "You are a helpful expert analyst specializing in machine learning and translation models.\n",
        "Your task is to interpret the following model evaluation results and explain them clearly.\n",
        "\n",
        "Here is the data:\n",
        "{table_as_markdown}\n",
        "\n",
        "Please provide an analysis that includes:\n",
        "1.  A high-level summary of the overall performance.\n",
        "2.  A simple explanation of what each metric (bleu, comet, metricx) measures.\n",
        "3.  A specific breakdown of each language's performance, identifying the strongest models and any with potential issues.\n",
        "4.  A concluding recommendation for next steps.\n",
        "\"\"\"\n",
        "\n",
        "model = GenerativeModel(\"gemini-2.5-pro\")\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "Jp739TexAfnI"
      },
      "id": "Jp739TexAfnI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}