{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hsDRYsMxUjfS",
        "p3IkHkaB4rSt",
        "AMSoJR0rlf3G",
        "dcFMdt_xthGK",
        "PR_nHvw-wozW",
        "PkHbbQ0YHcmW",
        "SSGbKRPK34VH",
        "jI0_l0el1j1u",
        "0-Lu47J5MYMj",
        "CtFnUPlheUKW",
        "p9cyxvqKIhGk",
        "t0wVu7UaIENk",
        "z9buLeTdI1Tp",
        "QYjPeXk3YiDa",
        "wdOoLdM0Fp_I",
        "UA8Zmuh7FRwE",
        "ncQDxd2nxffx",
        "kYZ-u0aXqvlz",
        "ZJsisvkNIxrv",
        "bIoYM5RYq5BD",
        "rwNM4NMnXEfV",
        "dddUNyIsyJEO",
        "UMSa4Pt2dvys",
        "Op-cit2CFEiK",
        "TiqOEUrSLR_5",
        "pF0aBln0tpWG",
        "71kEC0TTtwdt",
        "kY-gbnwtW_Tg",
        "5iVvtH_jA15X",
        "IcQxYCTe27bn",
        "5gEmqW10EUY7",
        "UnzMHYy2j-mZ",
        "Drnam8uix3L6",
        "S523jdTu5665",
        "Olk_ZtHZxxvB",
        "BLIqNf8wLAzg",
        "Z7Xj6gTt3FEA",
        "GuoalDqRyRyC",
        "cZPcI0kqrh4a",
        "0nJWHdt9gbt3",
        "yp-FcBnVLeFB",
        "vUaRXgHIgW2X",
        "iQFAjnERdvsq",
        "iptu2LVks6If",
        "jBh1Tr-vn_4f",
        "cqdowsM5oHEP",
        "fdcELPZDs-X5",
        "1fAAZcgWoiJB",
        "cyrUknMLo8bl",
        "M0kD-s4t82ez",
        "9GXuS8c94Iay",
        "gXZXzSSsrrUL",
        "RscthBMzOLUM",
        "7sf5tauFtttx",
        "jdJ5yNTZibYy",
        "AZ-w7Fv4O91T",
        "XdelGAe0UcAB",
        "OWqmJDTmXzvc",
        "P0qED2XtCTnH",
        "rMqIXS4-cxRG",
        "psPsoE2eVNiw",
        "f0O6pdoUYhec",
        "MkJxJltjAV7a",
        "dpys_ybfYkxU",
        "GYcJxiyZYuYO",
        "hZTQLaKGel6i",
        "3Ifz4t5vA6Y9",
        "KSaDCK89Y3_r",
        "v41iaGUgY8hR",
        "kSmK9R6Hbm2z",
        "cNo9Qsp7ZTQe",
        "QHclLheAYhtq",
        "Hz-PMZ5rZvs6",
        "WV3e_E50rXit",
        "M9vZNTRvVN_q",
        "0PltPkkfZ1Uw",
        "oQZJ3rU97I-z",
        "zpgWYl3LkxVc",
        "vSshMr2W7qVl",
        "R-pP8oZC7zxV",
        "EHcRBgP8rE9v",
        "Rjqwr4miC9_F",
        "jb3vfKj27tso",
        "LAH8nB-d7vrY",
        "bOsPQjh-8FP5",
        "Y-1lpMAgXzD9",
        "04vVZZU1Q4rZ",
        "Ak7yEH2CwIV7",
        "s8I3t063LsOf",
        "30ncah90297J",
        "YfOrrAOFvF6m",
        "-wUJznaApPv0",
        "yIoPrdtMpRd_",
        "xemut5KHpjBk",
        "cLeG1PswpSaL",
        "919bw_THpTYb",
        "h0hcBLukpUcp",
        "0p0KFlAypVZ9",
        "IbwJ8QbNvEbX",
        "0ChC-FyHZ41O",
        "PREcYcRRPEQ4",
        "_OexAhQsM_ex",
        "gS-QUpcFmXh1",
        "DxxVSdYniUxt",
        "iEPXBs9Yx37x",
        "bTmTARGQiRbm",
        "-6UGcA85iMH0",
        "0pnekZtsh-ON",
        "YW49URUwh8mn",
        "Qwm0Keqzh64L",
        "Fpc3Bxykh5KQ",
        "rJzNsbqaiC3n",
        "iqHduf9WDZMP",
        "jNQs-MXfBmVV",
        "Qa3-DIbUt0Ri",
        "UDNzWru2eKpK",
        "uOwaDiWZDTxx",
        "lh3x3_1K_e1m",
        "GssEWmSFaZdg",
        "f4lHqJyMa24i",
        "u6Cz4-k-vvpo",
        "Q-zfzmbO4ZOP",
        "Nj_F9wX47fTp",
        "ZiAK4jblJhPd",
        "WqR3v9EaJrvV",
        "JYooOdCtJ76C",
        "MMxCx0fZKDii",
        "fVQl5cbp7iqO",
        "PsodM6QW8wsN",
        "zTjh6Dkf9Imc",
        "icJDI9EQ9gc0",
        "VkuVX0zf959A",
        "Eg2UlbUr-YCc",
        "GCJMH1Oj-vBg",
        "YgcZdTDc_TCw",
        "V44oHA2a_1hG",
        "VcEin6DXApdH",
        "457rg_0-BFtj",
        "vq-kSppvBbU7",
        "F4gfVMumBdd4",
        "pBNDxf7EBkj-",
        "pAHNvHgpB0r-",
        "r-rllUswCV7X",
        "8h7LKwE9CYwF",
        "ihDto5NgCay8",
        "BJaouQGICdBE",
        "QHq7RwnYCe8j",
        "SOpIknui8ULm",
        "YUvl0oGH8ULh",
        "0SjFey3T8ULx",
        "PkPdLE5H8UL-",
        "hu1qRXiA8UL-",
        "XiU5eKGE8UMH",
        "dnkSoEwu8UMN",
        "q09fihf25FPw",
        "oEn5oZesJVdK",
        "HrtIDW9V0p96",
        "YWqVgPOwPCIJ",
        "4Q18zgBcxYJc",
        "ur1A1K3aOl3W",
        "QBuL0NbxO6cP",
        "Ym5Ax5nCOtuS",
        "M5NxmpI85zgL",
        "SN4YJmrE6HF3",
        "b8EVGNJmEfDo",
        "gmN2IqZ4FDsw",
        "nrkLBUH1hb36",
        "qYW5u_0Oi7l5",
        "0JfDpLK4KBqT",
        "4SpYIJ602L38",
        "SZkKviQcpLi5",
        "F1l9_Erf5T5S",
        "6quxRLo3U9Qf",
        "32OR42wtUS3y",
        "_Syw4dL96u31",
        "Gdr6F1zuU4MG",
        "9AR9PIXysIa4",
        "eloiR80j3LKg",
        "sprbeM9PikBv",
        "KYzlWwuA8hXl",
        "puX57AODxaUG",
        "SqYBnfUAD1yS",
        "w5zER_lMlN46",
        "DfJ1ATjNLmjN",
        "xcWGR3zwT-Fo",
        "aGvecdZZd-Kk",
        "rc8jFaF5d0R1",
        "L-vCM9-DX-j2",
        "6Nb2j8QPdg3L",
        "dolnoE19IyYG",
        "VQdL1yhzPGzI",
        "CQu9zwVlu7sn",
        "0USsGJE7Tfz3",
        "PhQKZZ-1M-GE",
        "521_dSZMrpGC",
        "LxkcVzsECAEO",
        "2KRiQv6BoMmA",
        "O8cFhBpbIFFd",
        "7bWObSh9Mh0N",
        "OeO_i6Z9-rLD",
        "xDUD9L1G7HIj",
        "fq5mO57kWnjQ",
        "OkGNbw_qosx7",
        "pHwO7VW3o75X",
        "4JijVDdYpeTw",
        "RTdYXBryoZqW",
        "fhdD4JnqZHKf",
        "hIwXOz85zxW_",
        "_DcbvY2o-JyH",
        "m066sar7fCUi",
        "U3xyzZzNOEhv",
        "82Lb_gBqUzi8",
        "fQEB0NP4Fk57",
        "y1flpzoQFTVa",
        "8lpmflh50O1X",
        "8chCbs1o_Iud",
        "8idRoUCpsoKR",
        "vUbYRNP_zXe4",
        "m0K1gPOoznzE",
        "LVOw3me6qbku",
        "ECcHaTG2nKiM",
        "P4KDiI20GSS8",
        "JGhitrlOQEHM",
        "T4A4TeH7SewU",
        "ErN__bggDh9m",
        "Vc8T8hpoiWFR",
        "IoZiS2oORjId",
        "4S4z_HeLRbtc",
        "2Aoj2PpxvDQV",
        "wc37qhDWywkD",
        "_j3LHUI5y6Co",
        "qCTiyeYsy0Y0",
        "fk2nIlNKjMY4",
        "F4uWOy1kiJIi",
        "h8VPlSUFiaHF",
        "sCrxJgguiyCW",
        "F1FnAXf0DeCY",
        "DjYXE5LZhFXn",
        "UvgTnPqOhs3q",
        "_cMWr-Beh3wr",
        "G1TQJG1ufr-D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/maths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Mathematics ü¶ã**"
      ],
      "metadata": {
        "id": "2VU6BC9nYCGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1000.png)"
      ],
      "metadata": {
        "id": "EChUkdiQsKjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Quantum**"
      ],
      "metadata": {
        "id": "hsDRYsMxUjfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Information Theory*"
      ],
      "metadata": {
        "id": "p3IkHkaB4rSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *CPTP Maps, Kraus Operators etc*"
      ],
      "metadata": {
        "id": "AMSoJR0rlf3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not all physical errors can be linear CPTP maps in a correlation space (quantum channels)**\n",
        "\n",
        "https://www.nature.com/articles/srep00508\n",
        "\n",
        "In the framework of quantum computational tensor network, which is a general framework of measurement-based quantum computation, the resource many-body state is represented in a tensor-network form (or a matrix-product form) and universal quantum computation is performed in a virtual linear space, which is called a correlation space, where tensors live. Since any unitary operation, state preparation and the projection measurement in the computational basis can be simulated in a correlation space, it is natural to expect that fault-tolerant quantum circuits can also be simulated in a correlation space. However, we point out that not all physical errors on physical qudits appear as linear completely-positive trace-preserving errors in a correlation space. Since the theories of fault-tolerant quantum circuits known so far assume such noises, this means that the simulation of fault-tolerant quantum circuits in a correlation space is not so straightforward for general resource states.\n",
        "\n"
      ],
      "metadata": {
        "id": "hvypLz1NhWFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the function space in a quantum computing operation (cptp map)?**\n",
        "\n",
        "In quantum computing, operations (also known as quantum gates or quantum channels) are often represented as Completely Positive Trace-Preserving (CPTP) maps. These maps represent the evolution of a quantum state in a quantum computation. \n",
        "\n",
        "**A quantum state is typically described as a density matrix**, which is a positive semidefinite operator with trace 1 on a Hilbert space. A Hilbert space is a complex vector space equipped with an inner product operation, and it is a key structure in quantum mechanics as it provides the stage on which quantum states exist and evolve. \n",
        "\n",
        "> **The \"function space\" in this context is the set of all possible quantum states, i.e., the set of all density matrices on the Hilbert space.**\n",
        "\n",
        "A CPTP map is a linear, completely positive, and trace-preserving transformation on this function space. It takes an initial quantum state (a point in the function space) and produces a final quantum state (another point in the same function space), describing the evolution of the state due to the quantum operation. \n",
        "\n",
        "In other words, **a CPTP map is a function that maps the function space of initial quantum states to the function space of final quantum states. Therefore, it is said to act on the \"function space\" of quantum states**.\n",
        "\n",
        "To make this a bit more concrete, consider a single qubit. Its Hilbert space is a two-dimensional complex vector space, and the set of density matrices on this space is the set of all 2x2 positive semidefinite matrices with trace 1. A single-qubit quantum operation is a CPTP map on this set of density matrices. \n",
        "\n",
        "CPTP maps are crucial in quantum information theory because they can represent not only idealized quantum gates but also realistic quantum operations that include effects such as decoherence and noise.\n"
      ],
      "metadata": {
        "id": "ztGyMd0ZfllE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Where does a quantum computer has an (exponential) advantage over a classical computer in machine learning tasks?**\n",
        "\n",
        "* Classical Data (learn classical Boolean circuits):\n",
        "  * average error: no\n",
        "  * worse error: no\n",
        "\n",
        "* Quantum Data (learn unknown cptp maps over known observables):\n",
        "  * average error: no - classical ML can learn unknown state p or observable O very well\n",
        "  * worse error: yes (predicting ex-\n",
        "pectation values of Pauli operators in an unknown n-\n",
        "qubit quantum state œÅ)"
      ],
      "metadata": {
        "id": "xdH07JFx1Szq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are ùëò-body reduced density matrices?**\n",
        "\n",
        "In quantum mechanics, the state of a system of particles can be represented by a density matrix, which is a complex, positive semi-definite matrix with a trace equal to 1. \n",
        "\n",
        "For a system of N particles, the full density matrix lives in a high-dimensional Hilbert space (the tensor product of the individual particle's Hilbert spaces), and contains complete information about the state of the system. However, it can often be difficult to work with such high-dimensional objects.\n",
        "\n",
        "A k-body reduced density matrix is a lower-dimensional object that contains information about only a subset of the particles in the system. \n",
        "\n",
        "Formally, the k-body reduced density matrix is obtained by taking the partial trace over N-k of the particles in the system. This process \"averages out\" the information about these N-k particles, leaving behind a density matrix that describes the remaining k particles.\n",
        "\n",
        "The k-body reduced density matrix is a useful tool in many areas of quantum physics, including quantum chemistry and quantum information theory. It allows us to study the behavior of small subsystems within a larger quantum system, even when we can't (or don't want to) keep track of the full state of all the particles. For example, the 1-body reduced density matrix (also known as the one-particle density matrix) is often used in quantum chemistry to describe electron behavior in molecular systems."
      ],
      "metadata": {
        "id": "780q8PXoJZja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CPTP Maps in Quantum Machine Learning**\n",
        "\n",
        "*Source: Information-theoretic bounds on quantum advantage in machine learning*\n",
        "\n",
        "*tldr: in ML experiments for an on average error quantum ML  (which operates on quantum data) cannot outperform classical ML (which operaties on classical data. qQuantum ML can outperform only for worst-case prediction error (rather than a small average prediction error), where an exponential separation becomes possible.*\n",
        "\n",
        "We are interested in predicting functions of the form\n",
        "\n",
        "$f(x)=\\operatorname{tr}(O \\mathcal{E}(|x\\rangle\\langle x|))$\n",
        "\n",
        "* where $x$ is a classical input, e.g., chemicals involved in the reaction, a description of the molecule, or the intensity of lasers that con- trol the neutral atoms\n",
        "* $\\mathcal{E}$ is an arbitrary (possibly unknown) completely positive and trace preserving (CPTP) map - it characterizes a quantum evolution happening in the lab. Depending on the parameter x, it produces the quantum state $\\mathcal{E}$(|x‚ü©‚ü®x|)\n",
        "* $O$ is a ceratin known observable (what the experimentalist measures at the end of the experiment.) \n",
        "\n",
        "This equation  encompasses any physical process that takes a classical input and produces a real number as output. The goal is to construct a function h(x) that accurately approximates f(x) after accessing the physical process $\\mathcal{E}$ as few times as possible.\n",
        "\n",
        "The goal is to predict the measurement outcome for new physical experiments, with new values of x that have not been encountered during the training process.\n",
        "\n",
        "1. all problems that are approximately learnable by a quantum ML model are also approximately learnable by some restricted clas- sical ML model which executes the quantum process $\\mathcal{E}$ a comparable number of times. This applies in particular, to predicting outputs of quantum- mechanical processes.\n",
        "\n",
        "  * For the task of learning classical Boolean circuits, fundamental limits on quantum advantage have been established in previous work [11‚Äì13, 31, 80, 94]. Theorem 1 generalizes these existing results to the task of learning outcomes of quantum processes.\n",
        "\n",
        "  * Therefore finding that classical and quantum ML have comparable power (for average-case prediction) boosts our hopes that the combination of classical ML and near-term quan- tum algorithms [47, 52, 53, 76, 79] may fruitfully ad- dress challenging quantum problems in physics, chem- istry, and materials science.\n",
        "\n",
        "2. quantum ML can have an exponential ad- vantage over classical ML for certain problems where the objective is achieving a specified worst-case prediction error. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OgKDwE8CoMNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are Lindblad operators?**\n",
        "\n",
        "Lindblad operators (also sometimes referred to as \"jump\" operators) are a fundamental component of Lindblad equations, which are used in the field of quantum mechanics to describe the evolution of quantum systems that interact with an environment. This interaction leads to effects such as decoherence and dissipation, which are not captured by the basic Schr√∂dinger equation.\n",
        "\n",
        "The general form of a Lindblad equation is:\n",
        "\n",
        "dœÅ/dt = -i[H, œÅ] + Œ£_k Œ≥_k (L_k œÅ L_k‚Ä† - 1/2 {L_k‚Ä† L_k, œÅ})\n",
        "\n",
        "Here:\n",
        "\n",
        "- H is the Hamiltonian of the system, which describes its energy and governs its unitary (non-dissipative) evolution.\n",
        "- œÅ is the density matrix of the system, which encapsulates the state of the system.\n",
        "- The brackets [ , ] and { , } denote the commutator and anticommutator respectively.\n",
        "- The L_k are the Lindblad operators, which describe how the system interacts with its environment.\n",
        "- The Œ≥_k are positive decay rates associated with each Lindblad operator.\n",
        "\n",
        "Each Lindblad operator L_k represents a certain type of interaction between the quantum system and its environment. The precise form of these operators depends on the specifics of the system and its environment. For example, in a system of quantum harmonic oscillators, the Lindblad operators might be the creation and annihilation operators for each oscillator.\n",
        "\n",
        "The non-unitary part of the Lindblad equation (the second term on the right-hand side) describes how these interactions cause the system to decay towards a steady state, and is responsible for phenomena such as relaxation and decoherence. The Lindblad equation is the most general form for the time evolution of a quantum system that ensures complete positivity and trace preservation of the density matrix, two crucial physical requirements."
      ],
      "metadata": {
        "id": "2TfI0tinl43W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**super-operator**\n",
        "\n",
        "In the field of quantum mechanics and quantum information theory, a superoperator is a type of operator that acts on other operators, rather than on states. In the context of quantum mechanics, these \"other operators\" are usually the density matrices that represent quantum states.\n",
        "\n",
        "This concept is needed because in quantum mechanics, the evolution of a system can often be represented as an operator acting on the state of the system. However, when we consider more complicated situations, like a quantum system interacting with an environment (leading to phenomena such as decoherence), the evolution of the system can involve operations that change the nature of the operator that represents the state itself. This kind of operation is represented by a superoperator.\n",
        "\n",
        "Formally, a superoperator Œ¶ acting on an operator A would be written as Œ¶(A), and this results in another operator.\n",
        "\n",
        "One common example of a superoperator is the Liouville superoperator, which appears in the Liouville-von Neumann equation describing the time evolution of a quantum system. If H is the Hamiltonian of the system and œÅ is the density operator, the Liouville superoperator L acting on œÅ is defined as:\n",
        "\n",
        "L(œÅ) = -i[H, œÅ]\n",
        "\n",
        "where [ , ] denotes the commutator.\n",
        "\n",
        "***Another important class of superoperators are the completely positive trace preserving (CPTP) maps***, which are used to describe the evolution of quantum systems in the presence of decoherence. A quantum operation (also called a quantum channel) is a CPTP map and can be represented as a superoperator acting on the density matrix of a quantum state."
      ],
      "metadata": {
        "id": "1-daOZv2mFJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are quantum channel and Kraus operators?**\n",
        "\n",
        "In quantum mechanics, a quantum channel is a mathematical model used to describe the evolution of quantum states due to physical processes, which could be the deterministic dynamics of an isolated quantum system, or could include the effects of noise, dissipation, and decoherence that result from interactions with an environment. Quantum channels are crucial in quantum information theory and quantum computing, where they are used to model realistic, noisy quantum operations.\n",
        "\n",
        "A quantum channel is a completely positive, trace-preserving (CPTP) map. These properties reflect the basic requirements that any physical process must satisfy:\n",
        "\n",
        "- **Complete positivity** ensures that the evolution is physically valid not only for individual quantum states, but also for quantum states that are part of larger systems. This is essential because quantum systems can be entangled, meaning that the state of one system can't be described independently of the state of another system.\n",
        "- **Trace preservation** ensures that the total probability remains 1, reflecting the probabilistic interpretation of quantum mechanics.\n",
        "\n",
        "One common way to represent a quantum channel is by using Kraus operators. Given a quantum channel Œõ, we can find a set of operators {K_i} such that the action of the channel on any state œÅ is given by\n",
        "\n",
        "Œõ(œÅ) = Œ£_i K_i œÅ K_i‚Ä†\n",
        "\n",
        "where ‚Ä† denotes the conjugate transpose, and the sum is over all i. The operators K_i are called the Kraus operators, or Kraus representation, of the channel.\n",
        "\n",
        "The Kraus operators must satisfy the completeness condition Œ£_i K_i‚Ä† K_i = I, where I is the identity operator, to ensure that the channel is trace-preserving.\n",
        "\n",
        "The Kraus representation is not unique; there can be many different sets of Kraus operators that represent the same quantum channel. The specific form of the Kraus operators can depend on the physical interpretation of the channel. For example, in a quantum system subject to dissipative processes or measurement, each Kraus operator might represent a different possible outcome or error process.\n"
      ],
      "metadata": {
        "id": "8XKy0JpymlW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a quantum channel?**\n",
        "\n",
        "A quantum channel is a mathematical construct used in quantum information theory to describe the process of transmitting quantum information from one location to another, or more generally, the evolution of quantum states in a system over time. This evolution can be due to the natural dynamics of the system or because of an interaction with an environment, which might cause phenomena like decoherence or noise.\n",
        "\n",
        "Mathematically, a quantum channel is a completely positive, trace-preserving (CPTP) map. It takes a density matrix (which represents a quantum state) as input and returns another density matrix as output.\n",
        "\n",
        "The \"completely positive\" part means that the map preserves the positive semi-definiteness of the density matrix, which is a requirement for it to represent a valid physical state. The \"trace-preserving\" part means that the total probability of all possible outcomes (which is represented by the trace of the density matrix) remains equal to 1.\n",
        "\n",
        "One common example of a quantum channel is the depolarizing channel, which introduces a certain probability of error into each qubit of information transmitted through it. There are many other types of quantum channels that model different kinds of physical processes, including lossy channels, dephasing channels, amplitude damping channels, and more. Each channel has a different impact on the quantum states that pass through it, often reflecting the effects of different types of environmental noise or decoherence."
      ],
      "metadata": {
        "id": "u6AwpEdL1zvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you give me an example of a quantum channel?**\n",
        "\n",
        "Certainly, a straightforward example of a quantum channel is the \"bit-flip channel.\" This channel models the possible flipping of a qubit due to errors in a quantum computing environment.\n",
        "\n",
        "Let's denote the basis states of our qubit as |0‚ü© and |1‚ü©. Suppose we have a single qubit quantum state œÅ which we send through the bit-flip channel. After going through the channel, each qubit can either stay the same or be flipped, i.e., |0‚ü© goes to |1‚ü© and |1‚ü© goes to |0‚ü©. \n",
        "\n",
        "Let's also say that the probability of a flip occurring is p. Then the probability that nothing happens is (1-p). \n",
        "\n",
        "The behavior of the bit-flip channel Œõ can be described as:\n",
        "\n",
        "Œõ(œÅ) = (1-p)œÅ + p XœÅX\n",
        "\n",
        "where X is the Pauli-X gate, which flips the state of a qubit.\n",
        "\n",
        "This description of the channel is essentially a probabilistic mixture of the identity operation (which leaves the state œÅ unchanged) with weight (1-p), and the bit-flip operation (XœÅX) with weight p.\n",
        "\n",
        "This is a completely positive, trace-preserving (CPTP) map: it's \"completely positive\" because it preserves the positive semi-definiteness of the density matrix, and it's \"trace-preserving\" because the sum of the probabilities of all outcomes is 1 ((1-p) + p = 1), ensuring that the total quantum state remains normalized.\n",
        "\n",
        "As a consequence of going through this channel, quantum information can be lost or corrupted, which is a significant consideration in the design of quantum computing systems and quantum error correction codes."
      ],
      "metadata": {
        "id": "qANEZaHQ2g4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For what do you need a kraus operator?**\n",
        "\n",
        "Kraus operators are a useful tool for representing and working with quantum channels, which are mathematical models that describe the evolution of quantum states due to physical processes. These processes can include both the deterministic dynamics of an isolated quantum system, and the effects of noise, dissipation, and decoherence due to interactions with an environment.\n",
        "\n",
        "Here are some specific uses for Kraus operators:\n",
        "\n",
        "1. **Modeling realistic quantum operations**: In quantum computing and quantum information theory, we often need to model not just ideal, noiseless operations (which are represented by unitary transformations), but also realistic, noisy operations. Kraus operators provide a way to do this. Each Kraus operator can represent a different possible outcome or error process that can occur during the operation.\n",
        "\n",
        "2. **Calculating the effect of a quantum channel**: Given the Kraus operators for a quantum channel, we can calculate how the channel will transform any quantum state. This is useful for predicting the results of quantum computations or quantum communications in the presence of noise.\n",
        "\n",
        "3. **Studying quantum decoherence**: Kraus operators provide a mathematical framework for studying decoherence, which is the process by which a quantum system loses its quantum properties due to interactions with its environment. Each Kraus operator can represent a different type of interaction with the environment.\n",
        "\n",
        "4. **Quantum error correction**: Kraus operators can represent different types of errors that can occur in a quantum system. This is crucial for designing quantum error correction codes, which are schemes for detecting and correcting errors in quantum computations or communications.\n",
        "\n",
        "5. **Quantum process tomography**: This is a technique for experimentally determining the Kraus operators of a quantum channel, which can provide insights into the physical processes occurring in a quantum system.\n",
        "\n",
        "Overall, Kraus operators provide a flexible and powerful mathematical tool for describing and analyzing the dynamics of quantum systems, especially in the presence of noise and other non-ideal effects.\n"
      ],
      "metadata": {
        "id": "n4Impp-7m0bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What‚Äòs the difference between kraus operator and lindblatt operator?**\n",
        "\n",
        "Kraus operators and Lindblad operators are both tools used to represent and study the evolution of quantum systems, especially when these systems are interacting with an environment. However, they are used in slightly different contexts and represent different types of processes.\n",
        "\n",
        "1. **Kraus Operators**: Kraus operators are used to represent quantum channels, which are completely positive, trace-preserving (CPTP) maps that describe the evolution of quantum states. The Kraus representation is a way to express a quantum channel as a sum of terms, each of which consists of a Kraus operator acting on the quantum state. The Kraus operators can represent different types of interactions with the environment or different possible outcomes of a quantum process.\n",
        "\n",
        "2. **Lindblad Operators**: Lindblad operators are used specifically to represent the generators of Markovian, open quantum systems in the Lindblad equation (also called the Gorini‚ÄìKossakowski‚ÄìSudarshan‚ÄìLindblad equation). The Lindblad equation describes the time evolution of a quantum system that is undergoing continuous, irreversible dynamics due to its interaction with a large, uncontrolled environment (a process called \"decoherence\"). Each Lindblad operator represents a different type of interaction with the environment.\n",
        "\n",
        "So, in brief: Kraus operators are used to represent discrete transformations of quantum states in general quantum channels, while Lindblad operators are used to represent continuous, Markovian dynamics in open quantum systems.\n",
        "\n",
        "It's also worth noting that the Lindblad equation is one way to derive Kraus operators for a quantum channel, specifically when the channel represents the evolution of a system over a small time interval under continuous, Markovian dynamics. In this case, the Lindblad operators determine the form of the Kraus operators."
      ],
      "metadata": {
        "id": "hFbdLJLMm4re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can one say that Kraus operators are bounded, and Lindblad operators unbounded? Or is that wrong?**\n",
        "\n",
        "In quantum mechanics, both Kraus and Lindblad operators act on a finite-dimensional Hilbert space (when we're considering finite quantum systems such as qubits, which are the building blocks of quantum computers), and hence they are indeed bounded operators. A bounded operator is an operator that doesn't amplify any vector in the Hilbert space by more than a certain factor, the operator norm.\n",
        "\n",
        "When the quantum system is finite-dimensional, the concepts of \"bounded\" and \"unbounded\" don't really come into play, because all operators on a finite-dimensional Hilbert space are automatically bounded.\n",
        "\n",
        "However, if you are considering infinite-dimensional systems, such as quantum fields or harmonic oscillators, then it becomes meaningful to talk about bounded and unbounded operators. In this case, Lindblad operators (the ones appearing in the Lindblad equation, governing the dynamics of open quantum systems) are typically unbounded.\n",
        "\n",
        "This difference arises because the Lindblad equation is a differential equation, and the Lindblad operators often represent physical quantities such as position and momentum, which are unbounded operators on infinite-dimensional Hilbert spaces. On the other hand, a Kraus representation of a quantum channel represents the action of the channel on the quantum state at a specific point in time, and this action can usually be expressed in terms of bounded operators.\n",
        "\n",
        "But to summarize, in the finite-dimensional case (which is the relevant case for most quantum computing applications), both Kraus and Lindblad operators are bounded. In the infinite-dimensional case, Kraus operators are generally still bounded, while Lindblad operators can be unbounded.\n"
      ],
      "metadata": {
        "id": "0SJBdUtqnADp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Channels and How could you formalize stochastic maps between basis changes into quantum circuits?**\n",
        "\n",
        "Basis changes in quantum circuits are usually modeled with unitary transformations. A unitary transformation is a linear transformation that preserves the inner product in the space, and in quantum mechanics, it corresponds to a deterministic evolution of the quantum state.\n",
        "\n",
        "However, if we're looking to introduce stochastic elements into quantum circuits, there are ways to do so, primarily through the use of quantum channels.\n",
        "\n",
        "A quantum channel is a completely positive, trace-preserving (CPTP) map that represents a physical process in quantum mechanics. While unitary transformations correspond to ideal, noiseless operations, quantum channels can represent realistic, noisy operations. This includes both coherent operations, represented by unitary transformations, and incoherent or stochastic operations, represented by non-unitary transformations.\n",
        "\n",
        "To introduce stochasticity, you can consider the notion of a mixed state, represented by a density matrix, which can be understood as a statistical mixture of different pure states. A quantum channel can take a pure state and produce a mixed state, representing a stochastic process. \n",
        "\n",
        "More precisely, a quantum channel Œõ can be represented by a set of Kraus operators {K_i}. These Kraus operators act on a state œÅ as follows:\n",
        "\n",
        "Œõ(œÅ) = Œ£_i K_i œÅ K_i‚Ä†\n",
        "\n",
        "where the sum is over all i, and ‚Ä† denotes the Hermitian adjoint (conjugate transpose). The Kraus operators are required to satisfy the completeness condition Œ£_i K_i‚Ä† K_i = I, where I is the identity operator.\n",
        "\n",
        "To encode a stochastic map into a quantum circuit, you'd need to find a suitable set of Kraus operators representing the stochastic process, and incorporate them into your circuit model.\n",
        "\n",
        "Keep in mind, however, that not all stochastic maps can be physically realized as quantum channels. The map must satisfy the conditions for a CPTP map, i.e., it must be completely positive and preserve the trace of the density matrix.\n",
        "\n",
        "Also note that while quantum channels provide a way to model stochastic processes, they are not usually implemented directly in quantum circuits on current quantum computers, which typically only perform unitary transformations directly. However, they can be used to model the effects of noise and errors in these circuits, and can be implemented indirectly using techniques like quantum error correction and quantum teleportation.\n",
        "\n"
      ],
      "metadata": {
        "id": "hOo54ImZma0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do you have an example of a Kraus operator and a Lindblad operator in the context of quantum computing?**\n",
        "\n",
        "Sure, I can provide some simple examples of both Kraus and Lindblad operators in the context of quantum computing.\n",
        "\n",
        "1. **Kraus Operators**: Kraus operators are used to describe the evolution of quantum states in the presence of noise or decoherence. They form a set of matrices {K_i} such that the sum of the Hermitian conjugates of each K_i times K_i is the identity matrix.\n",
        "\n",
        "    For instance, let's consider a single qubit undergoing bit flip error with a certain probability p. The error can be described by two Kraus operators:\n",
        "\n",
        "    K_0 = sqrt(1-p) * I,\n",
        "    K_1 = sqrt(p) * X,\n",
        "\n",
        "    where I is the identity operator and X is the Pauli-X operator. These operators satisfy the condition K_0‚Ä†K_0 + K_1‚Ä†K_1 = I, which ensures the resulting quantum state is still a valid density matrix.\n",
        "\n",
        "2. **Lindblad Operators**: Lindblad operators (also known as jump operators) are used in the Lindblad equation, which describes the time evolution of an open quantum system. \n",
        "\n",
        "    For instance, a common Lindblad operator for a single qubit undergoing amplitude damping (energy relaxation) is:\n",
        "\n",
        "    L = sqrt(Œ≥) * |0><1|,\n",
        "\n",
        "    where |0> and |1> are the computational basis states of the qubit, and Œ≥ is the damping rate. This Lindblad operator represents the process of the qubit transitioning from the excited state |1> to the ground state |0>.\n",
        "\n",
        "    In the context of a Lindbladian superoperator (describing the complete evolution), you might see this operator appearing in a term like L œÅ L‚Ä† in the master equation.\n",
        "\n",
        "These are quite simplified examples. In practice, the Kraus and Lindblad operators for a realistic quantum system could be more complex and might depend on the specific physical details of the system.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyvxk6BPnnh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an example of unbounded operators that lead to pathological behavior in the study of infinite-dimensional spaces in functional analysis?**\n",
        "\n",
        "Unbounded operators in infinite-dimensional spaces often require careful treatment because they can lead to \"pathological\" behavior, i.e., behavior that violates our usual intuitions or expectations based on finite-dimensional spaces.\n",
        "\n",
        "One classical example is the momentum operator in quantum mechanics, which is defined on the space of square-integrable functions over the real numbers (which is an infinite-dimensional space). The momentum operator is given by -iƒßd/dx, where i is the imaginary unit, ƒß is the reduced Planck constant, and d/dx is the derivative operator.\n",
        "\n",
        "This operator is unbounded, meaning there's no finite number B such that the norm of the momentum operator acting on any state is less than or equal to B times the norm of the state. As a result, the momentum operator can't be defined on the entire Hilbert space, but only on a dense subset (the set of differentiable functions, in this case).\n",
        "\n",
        "One \"pathological\" behavior of the momentum operator is that it doesn't have any eigenstates that belong to the Hilbert space. In quantum mechanics, we often want to find eigenstates of operators because these correspond to \"observable\" states of the system with definite values of the associated physical quantity. For the momentum operator, the eigenstates are plane waves, which are not square-integrable and hence do not belong to the Hilbert space.\n",
        "\n",
        "This behavior is quite different from what we see with bounded operators, where the eigenstates always belong to the same space as the operator. In order to deal with this kind of issue, physicists often have to introduce additional mathematical structures, such as rigged Hilbert spaces, or use indirect methods, such as the spectral theorem for unbounded self-adjoint operators, to make sense of the physical meaning of unbounded operators like the momentum operator."
      ],
      "metadata": {
        "id": "0ZubXnvmnE2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sign structure in quantum operators**\n",
        "\n",
        "The term \"sign structure\" is not standard terminology in quantum mechanics, so it may be interpreted in different ways depending on the context. In general, the mathematical operators used in quantum mechanics can have complex values and can act on complex-valued wavefunctions or on operators themselves, so they don't have a \"sign\" in the way that real numbers do. \n",
        "\n",
        "However, there are a few contexts where the notion of sign can be important in quantum mechanics:\n",
        "\n",
        "1. **Parity**: The parity operator is a specific quantum mechanical operator that represents spatial inversion (flipping the sign of the spatial coordinates). Eigenstates of the parity operator can have positive or negative eigenvalues, often interpreted as \"even\" or \"odd\" parity.\n",
        "\n",
        "2. **Spin**: In quantum mechanics, particles like electrons have a property called spin, which can take on positive or negative values (typically +1/2 or -1/2 for electrons). \n",
        "\n",
        "3. **Commutation relations**: Certain pairs of quantum operators have commutation relations that involve the imaginary unit \"i\", which could be interpreted as a kind of \"sign\" structure. For example, the position operator x and momentum operator p have the commutation relation [x, p] = iƒß, where ƒß is the reduced Planck's constant.\n",
        "\n",
        "4. **Phase factors**: In quantum mechanics, the overall sign (or more generally, phase factor) of a wavefunction is not observable, which leads to the principle of quantum superposition and interference effects.\n",
        "\n",
        "If you're asking about a specific aspect of quantum mechanics or a specific type of quantum operator, I'd be happy to provide more information!"
      ],
      "metadata": {
        "id": "fgSc_9VsmNdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by the sign of a trace?**\n",
        "\n",
        "The \"sign\" of a trace simply refers to whether the trace of a matrix (or operator in the context of quantum mechanics) is positive, negative, or zero. \n",
        "\n",
        "The trace of a matrix is the sum of its diagonal elements. If this sum is greater than zero, we say that the trace (and hence the matrix or operator) is positive. If the sum is less than zero, the trace is negative. If the sum is exactly zero, the trace is zero.\n",
        "\n",
        "This concept is often used in the context of quantum mechanics, where the trace of an operator can provide important physical information. For example, in the context of quantum density matrices, a trace of one is associated with a valid physical state. \n",
        "\n",
        "Please note that the sign of the trace of a matrix does not necessarily indicate that all elements, or even all eigenvalues, of the matrix are of the same sign. It's possible for a matrix to have both positive and negative eigenvalues but for their sum (i.e., the trace) to be positive, negative, or zero. Similarly, it's possible for a matrix to have positive and negative elements but for their diagonal sum (i.e., the trace) to be positive, negative, or zero."
      ],
      "metadata": {
        "id": "2LjS8JA23j-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Measurements*"
      ],
      "metadata": {
        "id": "dcFMdt_xthGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation Value and Backpropagation**\n",
        "\n",
        "The average value (expectation value) of the measurement result is given by the\n",
        "Born rule: **bold text**\n",
        "\n",
        "> $\\langle B\\rangle=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle$\n",
        "\n",
        "Just linear algebra! Every step is a matrix-vector or matrix-matrix multiplication\n",
        "\n",
        "Expectation values depend continuously on the gate parameters\n",
        "\n",
        "**Backpropagating Through Quantum Circuits**\n",
        "\n",
        "However, as long as we don't \"zoom in\" to what is happening in the quantum circuit, backpropagation can treat the quantum circuit as a single indivisible function\n",
        "\n",
        "The expectation value of a quantum circuit is a differentiable function\n",
        "\n",
        "> $\n",
        "f(\\theta)=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle=\\langle B\\rangle$\n",
        "\n",
        "Running on hardware and using the parameter-shift rule, we can provide both ingredients needed by backpropagation\n",
        "\n",
        "> $\n",
        "\\left(\\langle B\\rangle, \\frac{\\partial}{\\partial \\theta}\\langle B\\rangle\\right)\n",
        "$"
      ],
      "metadata": {
        "id": "NOFPaQ7LDw1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the cptp map ùëì(ùúå,ùëÇ) = tr(ùëÇE(ùúå))?**\n",
        "\n",
        "*Source: Learning to predict arbitrary quantum processes*\n",
        "\n",
        "The expression you've given, ùëì(ùúå,ùëÇ) = tr(ùëÇE(ùúå)), doesn't directly represent a completely positive trace-preserving (CPTP) map. Rather, it looks like the representation of an expectation value in the context of quantum mechanics or quantum information theory. \n",
        "\n",
        "In the given equation:\n",
        "- ùúå is a density operator representing the state of a quantum system.\n",
        "- ùëÇ is an observable, which is a Hermitian operator corresponding to a physical quantity that can be measured.\n",
        "- E(ùúå) is some operation being performed on the state ùúå; this could potentially represent a quantum channel or some other operation. The nature of this operation isn't specified by your question.\n",
        "- tr( ) denotes the trace, which for an operator is the sum of its eigenvalues.\n",
        "\n",
        "So, tr(ùëÇE(ùúå)) is the expected value (or mean value) of the measurement of the observable ùëÇ in the state resulting from the operation E(ùúå). \n",
        "\n",
        "A CPTP map, on the other hand, describes the evolution of quantum states when they interact with an environment. It is a map that takes a density operator to another density operator, and it has two properties: \n",
        "1. It's completely positive, meaning that it preserves the positive semi-definiteness of the quantum state (ensuring that the state remains a valid physical state).\n",
        "2. It's trace-preserving, which ensures that the total probability of all possible outcomes remains equal to 1.\n",
        "\n",
        "Given this, if E were to represent a CPTP map, then tr(ùëÇE(ùúå)) still wouldn't itself be a CPTP map, but instead the expectation value of an observable given a quantum state evolved by a CPTP map."
      ],
      "metadata": {
        "id": "bxF65qHK441R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting back to ùëì(ùúå,ùëÇ) = tr(ùëÇE(ùúå)) : why do I need to calculate the eigenvalues (trace)?**\n",
        "\n",
        "* Trace = sum of Eigenvalues (in this context the trace gives a weighted sum of all outcomes ‚Äì in other words, the average or expected value.)\n",
        "* Eigenvalues = each eigenvalue of an observable represents a possible outcome of a measurement\n",
        "\n",
        "In the expression ùëì(ùúå,ùëÇ) = tr(ùëÇE(ùúå)), the function tr denotes the trace of a matrix, not its eigenvalues. The trace of a matrix is defined as the sum of the elements on its main diagonal (from the top left to the bottom right). This is true for any square matrix, not just for those representing quantum states or operations.\n",
        "\n",
        "Now, for the specific case of this expression, the trace is used to calculate the expected value of an observable for a quantum state that has undergone some operation E. Here's why:\n",
        "\n",
        "In quantum mechanics, the expected (or average) value of an observable O for a state œÅ is given by the formula tr(ùëÇùúå). This comes from the Born rule, one of the key postulates of quantum mechanics, which relates the probabilities of measurement outcomes to the state of the system. The observable O is represented by a Hermitian operator, and the state œÅ is represented by a density operator.\n",
        "\n",
        "In your expression, E(ùúå) represents the state after some operation E has been applied. So tr(ùëÇE(ùúå)) is the expected value of the observable O for the state resulting from E(ùúå).\n",
        "\n",
        "The trace is crucial here because it ensures that we're summing up the probabilities of all possible outcomes. In a physical sense, it is analogous to summing over all possible states that the system might be in. **Because each eigenvalue of an observable represents a possible outcome of a measurement, and the corresponding element of the density matrix gives the probability of that outcome**, taking the trace in this context gives a weighted sum of all outcomes ‚Äì in other words, the average or expected value."
      ],
      "metadata": {
        "id": "bwYUoViG5BQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you accurately predict expectation values of Pauli observables in an unknown n-qubit quantum state œÅ?**\n",
        "\n",
        "In quantum mechanics, the expectation value of an observable represented by an operator A on a state œÅ can be calculated using the formula tr(AœÅ), where \"tr\" denotes the trace operation. \n",
        "\n",
        "The Pauli matrices (or Pauli operators), named after Wolfgang Pauli, are a set of 2 √ó 2 matrices which are fundamental in quantum mechanics. They are defined as:\n",
        "\n",
        "- Pauli-X: œÉ_x = [[0, 1], [1, 0]]\n",
        "- Pauli-Y: œÉ_y = [[0, -i], [i, 0]]\n",
        "- Pauli-Z: œÉ_z = [[1, 0], [0, -1]]\n",
        "\n",
        "These operators can be extended to n-qubit systems to form a complete basis for Hermitian operators on that system. A Pauli observable on an n-qubit system is a tensor product of Pauli matrices, one for each qubit. For example, Z ‚äó Z is a Pauli observable for a 2-qubit system that measures the parity of the two qubits.\n",
        "\n",
        "***In order to predict the expectation value of a Pauli observable in an unknown state œÅ, you would generally need to perform quantum state tomography***, which involves making many measurements on many identically-prepared quantum systems, and using the results to reconstruct the state œÅ. Once you have œÅ, you can use the formula tr(AœÅ) to calculate the expectation value of any observable A. \n",
        "\n",
        "However, ***full quantum state tomography is not always practical, especially for large systems, due to the number of measurements required***. There are other techniques like direct estimation of the expectation value via sampling, adaptive methods, or compressed sensing techniques which can estimate the expectation value with fewer measurements.\n",
        "\n",
        "In a practical quantum computing scenario, the expectation value of a Pauli observable would often be estimated by preparing many copies of the state, measuring each in the basis corresponding to the observable, and taking the average of the results. These measurements can be implemented with the appropriate rotations and then a computational basis measurement.\n",
        "\n",
        "Remember, this is all assuming you have physical access to the quantum state or quantum system. If you only have classical access to information about the system, you can't accurately predict the expectation values without additional assumptions or information. Quantum mechanics fundamentally limits what you can know about a quantum system without direct quantum access to the system."
      ],
      "metadata": {
        "id": "q345WRdUtkcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the difference between mixed state and product state?**\n",
        "\n",
        "A mixed state and a product state are two different types of quantum states that arise in quantum mechanics and quantum information theory, each with its own distinct properties and interpretations.\n",
        "\n",
        "1. **Mixed State**: A mixed state describes a statistical mixture of different quantum states, each with a certain probability. Mathematically, a mixed state is represented by a density matrix, which is a positive-semidefinite Hermitian operator with trace 1. If a system is in a mixed state, it means we have some classical uncertainty about which specific quantum state it's in. For example, a system might be in state |0‚ü© with probability 1/2 and in state |1‚ü© with probability 1/2. This would correspond to the mixed state represented by the density matrix œÅ = 1/2|0‚ü©‚ü®0| + 1/2|1‚ü©‚ü®1|.\n",
        "\n",
        "2. **Product State**: A product state refers to a state of a composite system that can be written as the tensor product of states of its subsystems. For example, if you have a two-qubit system and the first qubit is in state |0‚ü© and the second qubit is in state |1‚ü©, the overall state of the system is the product state |0‚ü©‚äó|1‚ü©, often written as |01‚ü©. A product state implies that there are no correlations between the subsystems, i.e., the subsystems are not entangled. If a state of a composite system cannot be written as a tensor product of states of its subsystems, it's called an entangled state.\n",
        "\n",
        "So, in summary, the difference lies in what each state represents: a mixed state represents classical uncertainty about which pure state a system is in, while a product state represents a separable (i.e., non-entangled) state of a composite system. Notably, these are not mutually exclusive: you can have a mixed state of product states, for example."
      ],
      "metadata": {
        "id": "Yyjr8c5B6SuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by incoherent and coherent measurements in sample complexity (for shadow tomography)?**\n",
        "\n",
        "In the context of quantum computing and quantum information theory, coherent and incoherent measurements refer to two different types of quantum measurements.\n",
        "\n",
        "1. **Coherent measurements**: These are the idealized measurements usually described in basic quantum mechanics where measuring a quantum state immediately collapses it into one of the eigenstates of the measured observable. Coherent measurements are unitary and reversible.\n",
        "\n",
        "2. **Incoherent measurements**: These measurements, on the other hand, correspond to a more generalized form of measurement, described by a set of positive-operator valued measures (POVMs). POVMs need not be unitary or reversible, and they can model a broader range of real-world measurement processes, including those where the quantum system interacts with an environment and decoheres. \n",
        "\n",
        "> The difference between coherent and incoherent measurements can become ***important when we consider the sample complexity of quantum state tomography, or in your specific case, shadow tomography.***\n",
        "\n",
        "Quantum state tomography is the process of determining the unknown state of a quantum system from the outcomes of measurements made on the system. The sample complexity of this process refers to the number of measurements needed to accurately reconstruct the quantum state. \n",
        "\n",
        "When performing quantum state tomography, incoherent measurements can sometimes offer advantages in terms of reduced sample complexity, since they can provide more information per measurement about the state of the system. However, they may also be more technically challenging to implement than coherent measurements, and they may introduce additional sources of error.\n",
        "\n",
        "The concept of \"shadow tomography\" you're referring to might involve using a small number of random measurements (or \"shadows\") to estimate properties of the quantum state, and it might have different sample complexity depending on whether coherent or incoherent measurements are used. The detailed analysis of this would depend on the specific shadow tomography protocol and the properties of the quantum state and the measurement process.\n"
      ],
      "metadata": {
        "id": "5923WPqg8sS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a POVM measurement?**\n",
        "\n",
        "A Positive Operator-Valued Measure (POVM) is a mathematical formalism used in quantum mechanics to describe the process of making a measurement on a quantum system. Unlike projective measurements (also called von Neumann measurements), which are limited to a specific set of states (the eigenstates of an observable), POVMs can represent more general kinds of measurements.\n",
        "\n",
        "A POVM is a collection of positive semi-definite operators {E_i} acting on the state space of a quantum system, which sum up to the identity operator, that is, ‚àëi E_i = I. These operators E_i are also called \"effects\". \n",
        "\n",
        "When you perform a measurement described by a POVM on a quantum system in a state represented by a density matrix œÅ, the probability of obtaining outcome i (associated with operator E_i) is given by tr(E_i œÅ), where tr() denotes the trace operation.\n",
        "\n",
        "The ***key aspect of POVMs is that they allow for the description of measurements that do not have a complete set of orthogonal eigenstates, which are typical in situations like indirect or incomplete measurements***, measurements involving ancillary systems, and measurements involving quantum entanglement. This makes POVMs an essential tool in quantum information theory and quantum computing."
      ],
      "metadata": {
        "id": "E8ZcEaFmtmWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a quantum state tomography?**\n",
        "\n",
        "Quantum state tomography is the process of determining the state of a quantum system based on the results of measurements. In essence, it's a method used to figure out the unknown quantum state of a system. It's called \"tomography\" because, like a CT scan in medicine, it combines many different measurements to construct a full picture of something you can't directly observe in one shot.\n",
        "\n",
        "For a quantum system, the state is typically represented by a density matrix. In the simplest case, for a single qubit, the density matrix is a 2x2 Hermitian matrix that is positive semi-definite and has trace 1. The generalization to multiple qubits is a matrix that is 2^n x 2^n (where n is the number of qubits), but with the same properties.\n",
        "\n",
        "Quantum state tomography involves making various measurements on many identically-prepared copies of the quantum system and then using the results to reconstruct the density matrix. The measurements you make are usually in different bases. For a single qubit, measurements are often made in the X, Y, and Z bases, which correspond to the Pauli matrices.\n",
        "\n",
        "However, as the number of qubits increases, the number of measurements you need to make grows exponentially, which makes full quantum state tomography unfeasible for large systems. For a system of n qubits, the density matrix contains 4^n - 1 independent real parameters, which is why the number of measurements needed increases so rapidly.\n",
        "\n",
        "Therefore, in practice, people use other methods to get partial information about the state or use assumptions to reduce the number of necessary measurements. There are also methods that attempt to directly estimate quantities of interest, like the expectation value of an observable, without fully reconstructing the state."
      ],
      "metadata": {
        "id": "-Bv3caiQuGsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are these alternative methods to quantum state tomography?**\n",
        "\n",
        "Indeed, full quantum state tomography is often impractical due to the exponential number of measurements required as the number of qubits increases. Here are some alternative methods that are commonly used in practice:\n",
        "\n",
        "1. **Quantum Process Tomography**: Instead of trying to determine the state of a quantum system, quantum process tomography aims to determine the quantum operation or quantum channel that describes a quantum process. This can be more practical in many experimental scenarios where you're more interested in characterizing a quantum gate or a quantum circuit.\n",
        "\n",
        "2. **Direct Estimation of Expectation Values**: Rather than attempting to reconstruct the full quantum state, you can directly estimate the expectation value of a specific observable by making many measurements in the basis corresponding to that observable and averaging the results.\n",
        "\n",
        "3. **Quantum State Estimation**: Quantum state estimation techniques use a set of projective measurements to estimate the state of a quantum system. This is similar to quantum state tomography, but instead of trying to reconstruct the full state, it only attempts to estimate it based on the results of projective measurements.\n",
        "\n",
        "4. **Quantum Compressed Sensing**: Compressed sensing is a method that uses the sparsity of a state in a certain basis to reduce the number of measurements required. It has been applied in the context of quantum state tomography, especially for states that are known to be sparse in a certain basis.\n",
        "\n",
        "5. **Machine Learning Techniques**: Machine learning methods, including neural networks and other techniques, have been increasingly used for quantum state tomography and characterization. These methods can often handle higher-dimensional data and can help reduce the number of measurements required.\n",
        "\n",
        "6. **Randomized Benchmarking**: This is a method for characterizing the performance of quantum gates by applying sequences of random gates followed by their inverse and seeing how often the system returns to the initial state.\n",
        "\n",
        "Remember, the choice of method depends on the specific task, the number of qubits, and the resources available. Each method has its strengths and limitations and might be suited to different experimental setups or goals.\n",
        "\n"
      ],
      "metadata": {
        "id": "AcHLAsw4uvoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by the parity of the two qubits?**\n",
        "\n",
        "The \"parity\" of two qubits refers to whether the number of qubits in the state |1‚ü© is even or odd.\n",
        "\n",
        "The parity operator for two qubits, which could be denoted as Z ‚äó Z in terms of Pauli matrices, acts on a two-qubit state |œà‚ü© = Œ±|00‚ü© + Œ≤|01‚ü© + Œ≥|10‚ü© + Œ¥|11‚ü© as follows:\n",
        "\n",
        "(Z ‚äó Z)|œà‚ü© = Œ±|00‚ü© - Œ≤|01‚ü© - Œ≥|10‚ü© + Œ¥|11‚ü©\n",
        "\n",
        "In this case, the eigenvalues of the parity operator are +1 and -1. States |00‚ü© and |11‚ü© have even parity (0 and 2 qubits are in the state |1‚ü©, respectively, both of which are even numbers), so the parity operator gives them a factor of +1. States |01‚ü© and |10‚ü© have odd parity (1 qubit is in the state |1‚ü©), so the parity operator gives them a factor of -1.\n",
        "\n",
        "Measuring the expectation value of the parity operator tells you about the relative likelihood of being in an even parity state versus an odd parity state. For example, if a two-qubit system is in the state |œà‚ü© = 1/‚àö2(|00‚ü© + |11‚ü©), then the expectation value of the parity operator would be +1, indicating that measurements of both qubits will always yield the same result, either both 0 or both 1. On the other hand, if the system is in the state |œà‚ü© = 1/‚àö2(|01‚ü© + |10‚ü©), then the expectation value of the parity operator would be -1, indicating that measurements of both qubits will always yield opposite results."
      ],
      "metadata": {
        "id": "n-lA7hp1t_Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a POVM measurement on the state E(|xi‚ü©‚ü®xi|)?**\n",
        "\n",
        "A Positive Operator-Valued Measure (POVM) is a type of generalized measurement used in quantum mechanics. When we want to make a measurement in quantum mechanics, we typically talk about observables, which are represented by Hermitian operators, and the associated eigenstates and eigenvalues. The eigenvalues are the possible outcomes of the measurement, and the state collapses into the corresponding eigenstate upon measurement.\n",
        "\n",
        "However, there are situations where a standard von Neumann measurement (which directly associates observables with Hermitian operators) is not sufficient. For example, if we want to consider more general types of measurements or if we're working with a subset of a larger system, we might need to use a POVM.\n",
        "\n",
        "A POVM is a set of positive semi-definite operators {E_i} that sum to the identity operator. Each E_i corresponds to a potential outcome of the measurement. The probability of each outcome i when measuring a state œÅ is given by tr(E_i œÅ).\n",
        "\n",
        "So if you have a state represented by E(|xi‚ü©‚ü®xi|) and a POVM {E_i}, the probability of getting the outcome corresponding to E_i would be tr(E_i E(|xi‚ü©‚ü®xi|)).\n",
        "\n",
        "The states E_i themselves don't necessarily need to be orthogonal or even pure states, which makes POVMs a very powerful and general measurement framework in quantum mechanics. They are often used in quantum information theory and quantum computation where complex measurement schemes are more common.\n"
      ],
      "metadata": {
        "id": "f70p5CAjtsMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a random Clifford measurement?**\n",
        "\n",
        "In the field of quantum computing, a Clifford measurement involves measuring a quantum state in a basis given by a Clifford operation. \n",
        "\n",
        "The Clifford group is a specific set of operations in quantum mechanics that map Pauli operators to other Pauli operators under conjugation. It includes operations like the Hadamard gate, phase gate, CNOT gate, and others. The Clifford group has important applications in quantum error correction and quantum cryptography.\n",
        "\n",
        "A random Clifford measurement is a measurement process where the measurement basis is chosen randomly from the set of all possible Clifford operations. Random Clifford operations can be used to create a form of \"scrambling\" that can be used to probe the properties of quantum systems, and they also find use in protocols for testing quantum computers, like the so-called randomized benchmarking procedure.\n",
        "\n",
        "When applied to quantum state tomography, random Clifford measurements can be used to extract information about the quantum state in a way that can be more efficient or robust against certain types of errors. For example, they might be used in a protocol for \"shadow tomography\", where a small number of random measurements are used to estimate properties of the quantum state. The specifics of how these measurements are used and their performance characteristics can depend on the details of the quantum system and the tomography protocol."
      ],
      "metadata": {
        "id": "EM6MX6sk-qUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a two-copy Bell basis measurement?**\n",
        "\n",
        "A two-copy Bell basis measurement is a measurement in the Bell basis on two copies of a quantum state. This is an important concept in quantum information theory and quantum computing, especially in the context of entanglement and quantum teleportation.\n",
        "\n",
        "The Bell basis consists of four maximally entangled two-qubit states, known as Bell states. These states are:\n",
        "\n",
        "1. |Œ¶+‚ü© = 1/‚àö2 (|00‚ü© + |11‚ü©)\n",
        "2. |Œ¶-‚ü© = 1/‚àö2 (|00‚ü© - |11‚ü©)\n",
        "3. |Œ®+‚ü© = 1/‚àö2 (|01‚ü© + |10‚ü©)\n",
        "4. |Œ®-‚ü© = 1/‚àö2 (|01‚ü© - |10‚ü©)\n",
        "\n",
        "When we talk about a \"two-copy Bell basis measurement\", we are referring to performing a Bell basis measurement on two copies of a quantum state. Let's consider a state œÅ. We have two copies of œÅ, let's say œÅ1 and œÅ2. The two-copy Bell basis measurement would involve performing a Bell basis measurement on the combined system of œÅ1 and œÅ2. \n",
        "\n",
        "This kind of measurement could be used to assess properties such as the fidelity of the state œÅ, or in protocols like entanglement distillation. It is also a key part of certain quantum error correction codes, where it's used to detect and correct errors without destroying the quantum information."
      ],
      "metadata": {
        "id": "LPat9lHG-_HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is meant by an observable with constant spectral norm?**\n",
        "\n",
        "An observable in quantum mechanics is represented by a Hermitian operator. **The spectral norm (or operator norm) of a Hermitian operator is equal to its largest eigenvalue in absolute value**. \n",
        "\n",
        "So, if an observable is described as having a \"constant spectral norm,\" it means that the maximum absolute value of its possible measurement outcomes (eigenvalues) is a constant. \n",
        "\n",
        "For example, the Pauli matrices that represent observables in a two-level quantum system (like a qubit) have a spectral norm of 1, because their eigenvalues are +1 and -1. So, these could be described as observables with constant spectral norm.\n",
        "\n",
        "In practice, this property could be useful because it gives a bound on the possible outcomes of measurements, which can simplify calculations or analyses. It also ensures that the observable is a bounded operator, which is an important property in functional analysis and quantum mechanics."
      ],
      "metadata": {
        "id": "_l8OqycaFhln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Classical (Statistical) and Quantum Information Theory (e.g. Bounds for sample complexity)*"
      ],
      "metadata": {
        "id": "PR_nHvw-wozW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Quantum_information"
      ],
      "metadata": {
        "id": "c_p4JnvZEr3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are Holevo‚Äôs theorem and VC Dimension related?**\n",
        "\n",
        "\n",
        "Holevo's theorem and VC dimension are fundamental concepts in two distinct fields of study: quantum information theory and statistical learning theory, respectively. Their domains of application are quite different, so they are not directly related.\n",
        "\n",
        "1. **Holevo's Theorem**: This theorem is a principle in quantum information theory. Named after Alexander Holevo, it sets an upper limit on the amount of classical information that can be extracted from a quantum system. Specifically, it states that n quantum bits (qubits) can carry no more than n classical bits of information.\n",
        "\n",
        "2. **VC Dimension**: Named after Vladimir Vapnik and Alexey Chervonenkis, the VC dimension is a measure of the complexity of a statistical model, specifically, the capacity of a statistical classification algorithm. It provides a bound on the amount of data the model can fit and is used in assessing overfitting and underfitting.\n",
        "\n",
        "Even though both concepts deal with limitations ‚Äî one in quantum information extraction and the other in statistical learning models' capacity ‚Äî they are not directly related because they apply to very different areas of study. However, both are part of a larger dialogue about information, learning, and complexity across different scientific domains.\n"
      ],
      "metadata": {
        "id": "S_J4cC_503zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ***Sample complexity of quantum state tomography (e.g. shadow tomography) important for difference between coherent and incoherent measurements***\n"
      ],
      "metadata": {
        "id": "2tcJVkBR9ZB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is Holevo‚Äôs theorem?**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Holevo%27s_theorem\n",
        "\n",
        "Holevo's theorem, also known as Holevo's bound, is a fundamental result in quantum information theory that places an upper limit on the amount of classical information that can be extracted from a quantum system.\n",
        "\n",
        "In more precise terms, the theorem states that if we have a quantum system prepared in one of several states œÅ_i, each with probability p_i, then the amount of classical information that can be obtained by a measurement on the system is bounded by the Holevo quantity œá(œÅ), which is given by:\n",
        "\n",
        "œá(œÅ) = S(‚àë p_i œÅ_i) - ‚àë p_i S(œÅ_i)\n",
        "\n",
        "where S(œÅ) denotes the von Neumann entropy of the state œÅ, and the sum ‚àë p_i œÅ_i represents the overall state of the system averaged over the probabilities p_i.\n",
        "\n",
        "This theorem has important implications for quantum communication. Despite the fact that a single qubit can exist in a superposition of states and appears to carry an infinite amount of information, Holevo's theorem tells us that when we measure that qubit, we can extract at most 1 bit of classical information from it. ***This makes clear the distinction between information capacity of quantum states and the accessible information that can be extracted via measurements.***"
      ],
      "metadata": {
        "id": "xXYTvnwRzBd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "how can you measure bounds for sample complexity?\n",
        "\n",
        "Sample complexity refers to the number of samples or data points that a learning algorithm needs in order to achieve a certain level of performance or accuracy. Measuring bounds for sample complexity is a key topic in the theory of machine learning and statistical learning theory.\n",
        "\n",
        "Here are a few key concepts that can be used to measure bounds for sample complexity:\n",
        "\n",
        "1. **VC Dimension**: This is a measure of the capacity or complexity of a class of functions, often used in binary classification problems. The VC Dimension can provide an upper bound on the sample complexity in terms of the size of the hypothesis class, the desired error rate, and the confidence level.\n",
        "\n",
        "2. **Rademacher Complexity**: This is another measure of complexity of a function class that can be used to provide an upper bound on sample complexity. Rademacher complexity measures the ability of functions in the class to fit to random noise and is used in deriving generalization bounds.\n",
        "\n",
        "3. **Covering Numbers and Fat-Shattering Dimension**: These are other measures of complexity of function classes that can be used to derive upper bounds on sample complexity.\n",
        "\n",
        "4. **Bias-Variance Tradeoff**: This is a key principle in statistical learning that helps to understand the tradeoff between model complexity and the risk of overfitting, which impacts the number of samples needed for learning.\n",
        "\n",
        "5. **PAC Learning**: The Probably Approximately Correct (PAC) learning framework provides a formal definition of learning and can be used to derive bounds on the sample complexity needed to learn a function to a given degree of accuracy with a certain probability.\n",
        "\n",
        "These are just a few examples of the types of tools and concepts used to measure bounds for sample complexity. The exact methods used can vary depending on the specific learning problem, the type of data, the learning algorithm, and the performance measure being used."
      ],
      "metadata": {
        "id": "BpGfMPlk7553"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How exactly can you use quantum cramer-rao bound for optimal strategies in quantum state estimation?**\n",
        "\n",
        "The quantum Cram√©r-Rao bound (QCRB) is a fundamental result in quantum estimation theory that provides a lower bound on the covariance matrix of any unbiased quantum estimator. It characterizes the best possible precision that can be achieved in estimating a parameter of interest from quantum measurements.\n",
        "\n",
        "To use the quantum Cram√©r-Rao bound for optimal strategies in quantum state estimation, you can follow these steps:\n",
        "\n",
        "1. Define the parameter of interest: Identify the parameter that you want to estimate. This parameter could be associated with a quantum state, such as its purity, coherence, or parameters describing a quantum gate.\n",
        "\n",
        "2. Choose an estimation strategy: Determine the measurement scheme or experiment that you will use to estimate the parameter. This involves selecting a set of measurement operators or observables that will be performed on the quantum state to extract information about the parameter.\n",
        "\n",
        "3. Compute the Fisher information matrix: Calculate the Fisher information matrix (FIM) associated with the chosen measurement strategy. The FIM quantifies the amount of information the measurements provide about the parameter and is related to the precision of the estimation.\n",
        "\n",
        "4. Evaluate the quantum Cram√©r-Rao bound: Use the FIM to compute the quantum Cram√©r-Rao bound. The QCRB sets a lower bound on the covariance matrix of any unbiased estimator of the parameter. The covariance matrix determines the uncertainty or precision of the estimated parameter.\n",
        "\n",
        "5. Optimize the estimation strategy: The quantum Cram√©r-Rao bound provides a benchmark for the best achievable precision. To find an optimal estimation strategy, you can vary the choice of measurement operators, measurement settings, or other parameters in the experiment to minimize the QCRB or approach its theoretical limit.\n",
        "\n",
        "6. Assess the estimation performance: Compare the obtained precision with the quantum Cram√©r-Rao bound to evaluate the performance of your estimation strategy. If the achieved precision approaches the QCRB, it indicates that your strategy is close to the optimal precision allowed by quantum mechanics.\n",
        "\n",
        "It's important to note that the QCRB assumes ideal measurement conditions and does not take into account practical limitations, such as noise, imperfections in the experimental setup, or systematic errors. These factors may degrade the actual estimation performance and deviate from the ideal precision predicted by the QCRB.\n",
        "\n",
        "Nevertheless, the quantum Cram√©r-Rao bound serves as a powerful tool for understanding the fundamental limits of precision in quantum state estimation. It guides the design of optimal estimation strategies and provides insights into the trade-offs between precision and experimental resources in quantum measurement scenarios.\n",
        "\n",
        "\n",
        "*How exactly:*\n",
        "\n",
        "Certainly! Here is a basic formula for calculating the quantum Cram√©r-Rao bound (QCRB) for parameter estimation:\n",
        "\n",
        "QCRB = Tr(F^{-1} M),\n",
        "\n",
        "where:\n",
        "- QCRB is the quantum Cram√©r-Rao bound, which provides a lower bound on the covariance matrix of any unbiased estimator.\n",
        "- F is the Fisher information matrix, which quantifies the amount of information provided by the measurements about the parameter of interest.\n",
        "- M is the symmetric, positive semidefinite matrix representing the measurement operators' influence on the parameter estimation.\n",
        "\n",
        "To calculate the QCRB, you'll need to obtain the Fisher information matrix (F) and the measurement matrix (M) for the chosen estimation strategy. The Fisher information matrix can be computed based on the measurement operators and the quantum state being measured.\n",
        "\n",
        "Below is a code snippet in Python that demonstrates how to calculate the QCRB for a simple estimation scenario involving a single-qubit state and Pauli measurement operators.\n",
        "\n",
        "Note that this code snippet assumes a simple scenario with a single-qubit state and Pauli measurement operators. You can customize it based on your specific estimation scenario by modifying the measurement operators and the true state.\n",
        "\n",
        "Keep in mind that this code snippet provides a basic framework for computing the QCRB, and in practice, you may need to adapt it to your specific estimation problem and measurement scheme."
      ],
      "metadata": {
        "id": "FDdoFk_8B_P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Pauli measurement operators\n",
        "M0 = np.array([[1, 0], [0, 0]])\n",
        "M1 = np.array([[0, 0], [0, 1]])\n",
        "M2 = np.array([[0, 1], [1, 0]])\n",
        "M3 = np.array([[1, 0], [0, 1]])\n",
        "\n",
        "# Create a list of measurement operators\n",
        "measurement_operators = [M0, M1, M2, M3]\n",
        "\n",
        "# Define the true quantum state (density matrix)\n",
        "true_state = np.array([[0.4, 0.3], [0.3, 0.4]])\n",
        "\n",
        "# Calculate the Fisher information matrix\n",
        "F = np.zeros((4, 4))\n",
        "for i, Mi in enumerate(measurement_operators):\n",
        "    F[i, i] = np.trace(true_state @ Mi @ true_state @ Mi)\n",
        "    for j, Mj in enumerate(measurement_operators[i+1:], start=i+1):\n",
        "        F[i, j] = F[j, i] = np.trace(true_state @ Mi @ true_state @ Mj)\n",
        "\n",
        "# Calculate the inverse of the Fisher information matrix\n",
        "F_inv = np.linalg.inv(F)\n",
        "\n",
        "# Define the measurement matrix\n",
        "M = np.eye(4)\n",
        "\n",
        "# Calculate the QCRB\n",
        "QCRB = np.trace(F_inv @ M)\n",
        "\n",
        "print(\"Quantum Cram√©r-Rao bound:\", QCRB)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EllvTRTB7vX",
        "outputId": "6c50f91f-6fe6-4290-9ece-8e5493ac8b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum Cram√©r-Rao bound: 5.4043195528446e+16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is QUANTUM CRAMER-RAO?**\n",
        "\n",
        "The Cram√©r-Rao bound is a concept from statistics that provides a lower bound on the variance of an unbiased estimator of a parameter. In other words, it gives a measure of the best possible precision that can be achieved when estimating that parameter from a given set of data.\n",
        "\n",
        "The quantum Cram√©r-Rao bound extends this concept to the realm of quantum mechanics. In a quantum mechanical context, the \"parameters\" to be estimated might be properties of a quantum state, such as its mean or variance. The \"data\" used to make these estimates come from the results of measurements performed on the quantum state.\n",
        "\n",
        "The quantum Cram√©r-Rao bound then provides a limit on the precision with which these properties can be estimated, given the inherent uncertainties of quantum mechanics. This can be particularly useful in quantum metrology, where the goal is to measure physical parameters as precisely as possible using quantum systems.\n",
        "\n",
        "The quantum Cram√©r-Rao bound is given by the inverse of the **Fisher information matrix**, which in the quantum case is calculated using the quantum state and the POVM (Positive Operator-Valued Measure) elements describing the measurements. It can provide insights into the optimal measurement strategies for estimating the parameters of a quantum state, and can help guide the design of quantum sensors and other quantum technologies."
      ],
      "metadata": {
        "id": "gGd2uJ8jjheq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference between shannon entropy and kolmorogov entropy?**\n",
        "\n",
        "Shannon entropy and Kolmogorov complexity, also known as Kolmogov entropy, are both measures of the randomness, disorder, or complexity in a set of data, but they come from different theoretical perspectives and are calculated in different ways.\n",
        "\n",
        "**Shannon entropy** is a concept from information theory. It quantifies the expected value of the information contained in a message. For a given probability distribution, Shannon entropy measures the average unpredictability or randomness in the data. Shannon entropy is typically computed using logarithms and probabilities, and it is usually applied in the context of a stochastic (random) process.\n",
        "\n",
        "On the other hand, **Kolmogov entropy or Kolmogorov complexity**, which comes from algorithmic information theory, is a measure of the computational resources needed to specify or reproduce a string, dataset, or other pieces of information. Specifically, the Kolmogov complexity of a string is the length of the shortest possible description of the string in some fixed universal description language. It's not computable in general, but it gives us a way to talk about the \"absolute\" complexity of a string independently of any specific probability distribution.\n",
        "\n",
        "To summarize, Shannon entropy is a probabilistic measure of average unpredictability in data, while Kolmogov complexity is a measure of the computational resources needed to reproduce a piece of data."
      ],
      "metadata": {
        "id": "FbsYvnvgXEzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the connection between the (quantum) Cram√©r-Rao bound and the Fisher information matrix?**\n",
        "\n",
        "The Cram√©r-Rao Bound (CRB), including its quantum version, is closely related to the Fisher Information Matrix (FIM). In classical statistics, the CRB is derived from the FIM, which measures the amount of information that an observable random variable carries about an unknown parameter that we are trying to estimate.\n",
        "\n",
        "The Fisher Information Matrix (FIM) is a matrix whose entries are the expected values of the squared derivatives of the log-likelihood with respect to the parameters. The inverse of the FIM gives the Cram√©r-Rao Bound, which provides a lower limit on the covariance of any unbiased estimator. In other words, the CRB derived from the FIM tells us the smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator.\n",
        "\n",
        "In the quantum context, the concept is extended to quantum states and quantum measurements. The Quantum Cram√©r-Rao Bound (QCRB) is defined in terms of the quantum Fisher information, which is a generalization of the classical Fisher information to quantum systems. The QCRB provides a lower bound on the variance of any unbiased estimator of a parameter that is encoded in a quantum state. Like in the classical case, the QCRB is derived from the quantum Fisher information matrix, providing a fundamental limit on precision in quantum parameter estimation."
      ],
      "metadata": {
        "id": "yx90205iwwdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How is the Cram√©r-Rao bound used in machine learning?**\n",
        "\n",
        "In machine learning, the Cram√©r-Rao Bound (CRB) is often used as a tool for understanding the fundamental limits of learning algorithms, particularly in the field of parameter estimation and model selection. Here are some specific applications:\n",
        "\n",
        "1. Variance Estimation: Similar to financial economics, the CRB provides a theoretical lower limit for the variance of an unbiased estimator. This can help understand how well a learning algorithm might perform in terms of parameter estimation, given a certain amount of data.\n",
        "\n",
        "2. Model Complexity: The CRB can provide insights into how model complexity affects estimation accuracy. A model with too many parameters may have a higher CRB (i.e., higher variance for the best possible estimator), indicating that it could be more prone to overfitting. \n",
        "\n",
        "3. Algorithm Evaluation: Researchers might use the CRB to evaluate and compare the performance of different learning algorithms. If an algorithm's performance is close to the CRB, it might be deemed near-optimal.\n",
        "\n",
        "4. Designing Neural Networks: In the design of neural networks, the CRB can be used to understand the limit of what the network can learn from the data, which can help in making decisions about network architecture and training strategies.\n",
        "\n",
        "Remember that the CRB provides a theoretical limit under certain conditions. These conditions might not always hold in practical machine learning applications, and so the CRB should be used as one tool among many for understanding and improving machine learning systems.\n"
      ],
      "metadata": {
        "id": "ADk2SS_gzCjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How is the Cram√©r-Rao bound used in financial economics?**\n",
        "\n",
        "The Cram√©r-Rao Bound (CRB) is a lower bound for the variance of an unbiased estimator. In financial economics, it's used in a few ways:\n",
        "\n",
        "1. Parameter Estimation: CRB provides a benchmark for how precisely we can estimate parameters, such as those in asset pricing models, given our data. It's often used in the context of maximum likelihood estimators, which are designed to attain the CRB under certain conditions.\n",
        "\n",
        "2. Efficiency: The CRB is used to determine the efficiency of an estimator. If an unbiased estimator reaches the Cram√©r-Rao lower bound, it is said to be efficient. This is important in areas like portfolio optimization and asset pricing where accuracy and efficiency of estimates are key.\n",
        "\n",
        "3. Model Comparison: By comparing the CRBs of different models or estimators, analysts can make inferences about which model might give more precise estimates given the data. This can aid in model selection and comparative analyses.\n",
        "\n",
        "4. Hypothesis Testing: The CRB also plays a role in statistical hypothesis testing, which is widely used in financial economics for tasks like testing the significance of factors in asset pricing models.\n",
        "\n",
        "Please note that while the Cram√©r-Rao Bound provides a theoretical lower bound on variance, it's based on a number of assumptions, and these assumptions might not always hold in real-world financial data.\n"
      ],
      "metadata": {
        "id": "1jySHlqkzHT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you explain quantum fisher information matrix in more detail please?**\n",
        "\n",
        "Quantum Fisher Information Matrix (QFIM) is an extension of the concept of Fisher Information Matrix from classical statistics to quantum mechanics. It measures the amount of information that a quantum state carries about an unknown parameter. This parameter might be related to some aspect of the quantum state or a transformation applied to it.\n",
        "\n",
        "The Fisher Information, in the classical sense, is a measure of how much information an observable random variable carries about an unknown parameter of the distribution. In the quantum case, the Quantum Fisher Information (QFI) is a measure of how much information a quantum state carries about an unknown parameter.\n",
        "\n",
        "Given a parameterized quantum state œÅ(Œ∏), where Œ∏ is the parameter vector we are interested in, the QFIM is defined in terms of the symmetric logarithmic derivative (SLD), which is a Hermitian operator L(Œ∏) that satisfies a particular equation related to the derivative of the state œÅ(Œ∏).\n",
        "\n",
        "The entries of the QFIM, denoted as H(Œ∏), are given by:\n",
        "\n",
        "H_ij(Œ∏) = 1/2 Tr [œÅ(Œ∏) {L_i(Œ∏), L_j(Œ∏)}]\n",
        "\n",
        "where L_i and L_j are the SLDs corresponding to the parameters Œ∏_i and Œ∏_j, respectively, and { , } denotes the anticommutator.\n",
        "\n",
        "Like the classical Fisher Information Matrix, the QFIM can be used to define a lower bound on the variance of an unbiased estimator for the parameters Œ∏. This is the Quantum Cram√©r-Rao Bound.\n",
        "\n",
        "In practical terms, the QFIM and Quantum Cram√©r-Rao Bound are used in quantum metrology to quantify the ultimate limit to precision that can be achieved in estimating parameters, such as phase shifts or magnetic fields, based on quantum mechanical measurements.\n"
      ],
      "metadata": {
        "id": "3jlngzN8w3Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fat-shattering dimension and VC dimension (Vapnik-Chervonenkis dimension)**\n",
        "\n",
        "The fat-shattering dimension is a complexity measure used in statistical learning theory. It's one way of quantifying the complexity or expressive power of a class of functions, and it's used to derive bounds on the generalization error of a learning algorithm. \n",
        "\n",
        "The formal definition of the fat-shattering dimension is a bit technical, but here's the general idea: suppose you have a class of functions, and you want to understand how complex this class is. One way of doing this is to look at how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity.\n",
        "\n",
        "Now, for the fat-shattering dimension, you add an additional constraint: you require a certain margin (the \"fatness\") between points that are labeled differently. The largest set of points that can be shattered with a given margin is used to define the fat-shattering dimension of the function class. \n",
        "\n",
        "The fat-shattering dimension is related to the VC dimension (Vapnik-Chervonenkis dimension), another complexity measure that is widely used in statistical learning theory. However, while the VC dimension only considers exact separation of points, the fat-shattering dimension takes into account this idea of a margin, which makes it more suitable for dealing with \"noisy\" or non-separable data.\n",
        "\n",
        "The concept of the fat-shattering dimension is often used in the analysis of machine learning algorithms, particularly those based on empirical risk minimization. It can help to understand the trade-off between the expressive power of a function class (which often corresponds to the complexity of a learning algorithm) and the ability of the algorithm to generalize well to unseen data."
      ],
      "metadata": {
        "id": "xN7dGTUE7aJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Complexity Theory*"
      ],
      "metadata": {
        "id": "PkHbbQ0YHcmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Complexity Theory*"
      ],
      "metadata": {
        "id": "SSGbKRPK34VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum complexity theory is a branch of theoretical computer science that studies the resources, like time and space, needed for quantum computers to solve problems. It's the quantum analog to classical complexity theory. Here are some key points to understand about quantum complexity theory:\n",
        "\n",
        "1. **Quantum Computation Models**: The most commonly studied model of quantum computation is the quantum circuit model, which is analogous to the classical circuit model. There's also the quantum Turing machine, a quantum analogue to the classical Turing machine, but it's used less frequently.\n",
        "\n",
        "2. **Complexity Classes**: Just like in classical complexity theory, problems in quantum complexity theory are grouped into complexity classes based on the resources needed to solve them. The most famous is probably BQP (Bounded-error Quantum Polynomial time), which consists of problems that can be solved efficiently (in polynomial time) by a quantum computer with an error probability of at most 1/3. Other important quantum complexity classes include QMA (Quantum Merlin Arthur), the quantum analogue of the classical class NP.\n",
        "\n",
        "3. **Quantum Speedup**: Some problems can be solved more efficiently on quantum computers than on classical computers. The most famous examples are factoring integers (Shor's algorithm) and unordered searching (Grover's algorithm), which offer a so-called \"quantum speedup\". However, for many problems, it's still an open question whether quantum computers offer a significant advantage over classical computers.\n",
        "\n",
        "4. **Quantum vs Classical Complexity**: One of the most important open problems in quantum complexity theory (and indeed in all of computer science) is whether BQP is equal to P (the class of problems that can be solved in polynomial time by a deterministic classical computer). In other words, it's still an open question whether there are problems that can be efficiently solved by a quantum computer but not by a classical computer. Most experts believe that BQP is larger than P (meaning that quantum computers are more powerful), but this hasn't been formally proven.\n",
        "\n",
        "5. **Quantum Error Correction**: Quantum error correction is a key consideration in the study of quantum complexity theory. Quantum computations are very susceptible to errors, and correcting these errors requires additional resources. Understanding the trade-off between error correction and computational efficiency is a major topic of study in quantum complexity theory.\n",
        "\n",
        "If you want to study quantum complexity theory, you'll need a solid background in both classical complexity theory and quantum mechanics, as well as comfort with advanced mathematical concepts like linear algebra, probability theory, and group theory."
      ],
      "metadata": {
        "id": "1QyxmVixDOTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Many Body problem as the basic challenge**\n",
        "\n",
        "\n",
        "* It is widely believed that it is exponentially difficult for classical computers to simulate quantum systems.\n",
        "\n",
        "* This problem is known as the [quantum many body problem](https://en.m.wikipedia.org/wiki/Many-body_problem).\n",
        "\n",
        "* However, quantum computers can simulate many (though not all) Hamiltonians in [polynomial time with bounded errors](https://en.m.wikipedia.org/wiki/BQP) (BQP), which is one of the main appeals of quantum computing.\n",
        "\n",
        "* This is applicable to chemical simulations, drug discovery, energy production, [climate modeling](https://en.m.wikipedia.org/wiki/Climate_model) and fertilizer production (e.g. [FeMoco](https://en.m.wikipedia.org/wiki/FeMoco)) as well. Because of this, quantum computers may be better than classical computers at aiding design of further quantum computers.\n",
        "\n",
        "*Source: https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem#Notes*\n",
        "\n",
        "\n",
        "* [Many-body problem](https://en.m.wikipedia.org/wiki/Many-body_problem) article with examples and aproaches from below\n",
        "\n",
        "* Examples:\n",
        "\n",
        "  * Condensed matter physics (solid-state physics, nanoscience, superconductivity)\n",
        "  * Bose‚ÄìEinstein condensation and Superfluids\n",
        "  * Quantum chemistry (computational chemistry, molecular physics)\n",
        "  * Atomic physics\n",
        "  * Molecular physics\n",
        "  * Nuclear physics (Nuclear structure, nuclear reactions, nuclear matter)\n",
        "  * Quantum chromodynamics (Lattice QCD, hadron spectroscopy, QCD matter, quark‚Äìgluon plasma)\n",
        "\n",
        "* Approaches:\n",
        "\n",
        "  * Mean-field theory and extensions (e.g. Hartree‚ÄìFock, Random phase approximation)\n",
        "  * Dynamical mean field theory\n",
        "  * Many-body perturbation theory and Green's function-based methods\n",
        "  * Configuration interaction\n",
        "  * Coupled cluster\n",
        "  * Various Monte-Carlo approaches\n",
        "  * Density functional theory\n",
        "  * Lattice gauge theory\n",
        "  * Matrix product state\n",
        "  * Neural network quantum states\n",
        "  * Numerical renormalization group\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rM91mt-54rmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computation and Consciousness**\n",
        "\n",
        "Integrated information theory\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Integrated_information_theory\n",
        "\n",
        "Pretty hard problem\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hard_problem_of_consciousness\n",
        "\n",
        "https://plato.stanford.edu/entries/zombies/\n",
        "\n",
        "https://forum.effectivealtruism.org/posts/Qiiiv9uJWLDptH2w6/the-pretty-hard-problem-of-consciousness\n",
        "\n",
        "Graph expansion\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Expander-Graph\n",
        "\n",
        "Is consciousness reducable to computation\n"
      ],
      "metadata": {
        "id": "NQjv96Zo5uag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lattice based crypto systems\n",
        "* linear cross entropy benchmark\n",
        "* Quantum pcp problem\n",
        "* busy beaver of 6: biggest number (incl. goldbach conjecture)\n",
        "* It from qubit\n",
        "* Violation Bell ineuqlity experiment in 2015. it closed Locatility loophole and detection loophole - physicao reality of entsnglemenr proven. open: superdeterminism.\n",
        "* Church turing thesis\n",
        "* Unknown laws of quantum gravity - uncomputable (eg with an oracle for a quantum problem)"
      ],
      "metadata": {
        "id": "mOZV3yu5Ewsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/\n",
        "\n",
        "What is actually quantum computing? While quantum computers look promising to tackle certain computational problems with a massive parallelization thanks to superposition, they are no more powerful than a Turing machine (in terms of the problems that can be solved)! Scott Aaronson published an excellent article\n",
        "\"The Limits of Quantum Computers\" in 2008 in\n",
        "Scientific American. Quantum computers may provide a massive speedup for problems in the BQP complexity class. Nothing more. Quantum computers are no out-of-control \"supercomputers\".\n",
        "\n",
        "https://foreignpolicy.com/2022/08/21/quantum-computing-artificial-intelligence-ai-technology-regulation/ and https://www.cs.virginia.edu/~robins/The_Limits_of_Quantum_Computers.pdf\n"
      ],
      "metadata": {
        "id": "xUDfsSEIq1xB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/mip-re-6e903720c82f\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/bqnp-the-quantum-analogue-of-np-486ed2469c1d"
      ],
      "metadata": {
        "id": "Zz1aptP1Rgnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complexity Classes*"
      ],
      "metadata": {
        "id": "jI0_l0el1j1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complexity Classes & Algorithmic Efficiency (Runtime)**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1274.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1273.png)\n",
        "\n",
        "\n",
        "* [Examples of runtime with big O](https://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly), and see examples of [Computational complexity of mathematical operations](https://en.m.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra), [Brilliant: Complexity Theory](https://brilliant.org/wiki/complexity-theory/), [Complexity class](https://en.m.wikipedia.org/wiki/Complexity_class), [Computational Complexity Theory](https://en.m.wikipedia.org/wiki/Computational_complexity_theory), [Quantum complexity theory](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory), [Quantum supremacy](https://en.m.wikipedia.org/wiki/Quantum_supremacy), [PH complexity](https://en.m.wikipedia.org/wiki/PH_(complexity)), [Finally, a Problem That Only Quantum Computers Will Ever Be Able to Solve](https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/),\n",
        "* [Theory of computation](https://en.m.wikipedia.org/wiki/Theory_of_computation), [Big O notation (Landau)](https://en.m.wikipedia.org/wiki/Big_O_notation): Big (O) worste case / Big Œ© (Omega) best case / Big Œ∏ (Theta) notation (Asymptotic Analysis of Algorithms) Omega=(O)\n",
        "* [Time complexity](https://en.m.wikipedia.org/wiki/Time_complexity), Space Complexity = Auxiliary Space + Space used for input values, [Space time tradeoff](https://en.m.wikipedia.org/wiki/Space‚Äìtime_tradeoff), [Garbage collection](https://de.m.wikipedia.org/wiki/Garbage_Collection), [Time and Space Complexity Analysis of Algorithm](https://afteracademy.com/blog/time-and-space-complexity-analysis-of-algorithm), [How to compute Time Complexity or Order of Growth of any program](https://www.rookieslab.com/posts/how-to-compute-time-complexity-order-of-growth-of-any-program)\n"
      ],
      "metadata": {
        "id": "xLp3ADPXlXzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems, that Quantum Computers can solve or not**\n",
        "\n",
        "* *See: [Quantum complexity theory](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory), [Quantum_supremacy](https://en.m.wikipedia.org/wiki/Quantum_supremacy), [Quantum algorithms](https://en.m.wikipedia.org/wiki/Quantum_algorithm), [Hamiltonian simulation](https://en.m.wikipedia.org/wiki/Hamiltonian_simulation), [Quantum information science](https://en.m.wikipedia.org/wiki/Quantum_information_science)*\n",
        "\n",
        "* Iterative improvements in classical algorithm have so far not paid off to transform a hard problem to an easy one. This is where quantum computing comes in: an ‚Äúeasy‚Äù problem that can be solved using a quantum computer in polynomial time is class BQP (Bounded-error Quantum Polynomial time), and a hard problem which can only be verified in polynomial time is class QMA (the playfully named Quantum Merlin Arthur, https://en.m.wikipedia.org/wiki/QMA). The hope in the field is that there is some overlap between the space of NP problems and BQP problems: that by leveraging quantum resources like superposition and entanglement, a hard problem can be transformed into an easy one.\n",
        "\n",
        "*Problems, that Quantum Computers can definitely solve efficiently*\n",
        "* [BQP](https://de.m.wikipedia.org/wiki/BQP) and [engl BQP](https://en.m.wikipedia.org/wiki/BQP), that are not in [BPP](https://de.m.wikipedia.org/wiki/BPP_(Komplexit√§tsklasse)) on a classical computer, like [Factorization](https://de.m.wikipedia.org/wiki/Faktorisierung) with [Shor's algorithm](https://de.m.wikipedia.org/wiki/Shor-Algorithmus)\n",
        "  * Currently, we know that $P \\subseteq B Q P$ (i.e. anything you can do with a classic computer you can do with a quantum computer). We don't know, strictly speaking, if that actually is a strict inequality! But many people will be very surprised if it turns out to not be one - not to mention the huge implications it would have. People think that $B Q P \\neq N P$ but that neither $B Q P \\subset N P$ nor $N P \\subset B Q P$ - but proving this is very hard and has not been done either way. [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "\n",
        "  * [Boson sampling](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Boson_sampling) under low enough error (quantum supremacy, but is it BQP class??)\n",
        "\n",
        "  * BQP = P - Dequantized algorithms for such problems ‚Äì high-rank matrix inversion, for example ‚Äì would imply that classical computers can efficiently simulate quantum computers, i.e., BQP = P, which is not currently considered to be likely. https://arxiv.org/abs/1905.10415\n",
        "\n",
        "  * https://www.quora.com/What-is-the-relationship-between-BQP-and-NP-1\n",
        "\n",
        "  * [Sampling the output distribution of random quantum circuits\n",
        "  ](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Sampling_the_output_distribution_of_random_quantum_circuits) - Google experiment (quantum supremacy, but is it BQP class??)\n",
        "\n",
        "* [QMA](https://en.m.wikipedia.org/wiki/QMA): many interesting classes are contained in QMA, such as P, BQP and NP, all problems in those classes are also in QMA. However, there are problems that are in QMA but not known to be in NP or BQP. A [list of known QMA-complete problems](https://arxiv.org/abs/1212.6312)\n",
        "  * Quantum circuit/channel property verification (V)\n",
        "  * Hamiltonian ground state estimation (H), icl. Quantum k-SAT (S)\n",
        "  * Density matrix consistency (C)\n",
        "\n",
        "* [QIP](https://en.m.wikipedia.org/wiki/QIP_(complexity))\n",
        "\n",
        "* **Contrary to decision problems that require yes or no answers, sampling problems ask for samples from probability distributions.** [Source](https://en.m.wikipedia.org/wiki/Quantum_supremacy), https://arxiv.org/abs/1702.03061\n",
        "\n",
        "* Further topics\n",
        "  * [How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html)\n",
        "  * a quantum computer can run quantum algorithms! It should be noted that we only know about a handful of quantum algorithms that perform exponentially faster than their best-known classical counterparts - and Grover is not one of them.\n",
        "  * if you solve any NP-complete problem, all other NP problems come as a 'freebie' (not just the NP-complete ones). In that sense, it would be a huge milestone. It is widely believed that quantum computers cannot solve NP-complete problems, but it has never been proven [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "\n",
        "*Problems, that Quantum Computers can maybe solve efficiently*\n",
        "* Electronic structure\n",
        "* P450, FeMoco (most definitely) and [quantum simulation in physics and chemistry](https://en.m.wikipedia.org/wiki/Quantum_simulator#Solving_physics_problems)\n",
        "\n",
        "*Problems, that Quantum Computers can not solve efficiently*\n",
        "* [np-complete](https://en.m.wikipedia.org/wiki/NP-completeness) (=the hardest of the problems to which solutions can be verified quickly), like [Halting problem](https://en.m.wikipedia.org/wiki/Halting_problem) or [3SAT](https://en.m.wikipedia.org/wiki/Boolean_satisfiability_problem) (except, one manages to create a reduction of Grovers algorithm on this NP-Complete algorithm)\n",
        "* [np-hard](https://en.m.wikipedia.org/wiki/NP-hardness), like [travelling salesman](https://en.m.wikipedia.org/wiki/Travelling_salesman_problem) (only annealing can approach better than classical computers) [Sign Problem](https://en.m.wikipedia.org/wiki/Numerical_sign_problem) is NP hard. https://arxiv.org/pdf/1805.08219.pdf\n",
        "* Big question in computer science: [P versus NP problem](https://en.m.wikipedia.org/wiki/P_versus_NP_problem), P vs NP videos: https://youtu.be/EHp4FPyajKQ and https://youtu.be/YX40hbAHx3s\n"
      ],
      "metadata": {
        "id": "p8UyNFNKszF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/FP_(Komplexit√§tsklasse)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Permanente\n",
        "\n",
        "Hartree‚ÄìFock method"
      ],
      "metadata": {
        "id": "CUkHL9bqzGKu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGOLSoAweEmL"
      },
      "source": [
        "**Example: Efficiency of Matrix Multiplication**\n",
        "\n",
        "* Matrix Multiplication was known to have cubic time complexity $O(n^3)$. Whether you manipulate the equations or take the matrix route, you‚Äôll end up performing the same total number of computational steps to solve the problem. That number is the cube of the number of variables in the system ($n^3$). In case we have three variables, so it takes 33, or 27, computational steps.\n",
        "\n",
        "  * Achtung: researchers measure the speed of matrix multiplication purely in terms of the number of multiplications required\n",
        "\n",
        "  * Generally, the number of additions is equal to the number of entries in the matrix, so four for the two-by-two matrices and 16 for the four-by-four matrices.\n",
        "\n",
        "  * Source: [Matrix Multiplication Inches Closer to Mythic Goal](https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/)\n",
        "\n",
        "* New research in matrix multiplication algorithms shows $O(n^{2.3728596})$. Via simple guessing and them use symmetry to compute just half ('Because the entries in the matrix are random, and coordination happens between them, the matrix itself ends up with certain symmetries. Those symmetries enable computational shortcuts. Just like with any highly symmetric object, you only need to know what one part of it looks like in order to deduce the whole.')\n",
        "\n",
        "  * [Wiki: Matrix Multiplication Algorithm](https://en.m.wikipedia.org/wiki/Matrix_multiplication_algorithm)\n",
        "\n",
        "  * [Wiki: Computational complexity of matrix multiplication](https://en.m.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication)\n",
        "\n",
        "  * [Quanta: New Algorithm Breaks Speed Limit for Solving Linear Equations](https://www.quantamagazine.org/new-algorithm-breaks-speed-limit-for-solving-linear-equations-20210308/)\n",
        "\n",
        "  * [Quanta: Matrix Multiplication Inches Closer to Mythic Goal](https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/)\n",
        "\n",
        "* Simple number multiplication: The traditional grade-school method for matrix multiplication requires $n^2$ steps (linear problem), where n is the number of digits of the numbers you‚Äôre multiplying. So three-digit numbers require nine multiplications, while 100-digit numbers require 10,000 multiplications.\n",
        "\n",
        "  * 1960: Karatsuba‚Äôs method made it possible to multiply numbers using only $n^{1.58}$ single-digit multiplications.\n",
        "\n",
        "  * 1971: Arnold Sch√∂nhage and Volker Strassen published a method capable of multiplying large numbers in n √ó log n √ó log(log n) multiplicative steps, where log n is the logarithm of n. For two 1-billion-digit numbers, Karatsuba‚Äôs method would require about 165 trillion additional steps. **It introduced the use of a technique from the field of signal processing called a fast Fourier transform.**\n",
        "\n",
        "  * 2007: Martin F√ºrer beat it and the floodgates opened. Over the past decade, mathematicians have found successively faster multiplication algorithms, each of which has inched closer to n √ó log n\n",
        "\n",
        "  * 2019: Harvey and van der Hoeven got to n √ó log n (via improved version of the fast Fourier transform and replace even more multiplications with additions and subtractions)\n",
        "\n",
        "  * Further improvements possible? Maybe. But: the design of computer hardware has changed. Two decades ago, computers performed addition much faster than multiplication. The speed gap between multiplication and addition has narrowed considerably over the past 20 years to the point where multiplication can be even faster than addition in some chip architectures. With some hardware, ‚Äúyou could actually do addition faster by telling the computer to do a multiplication problem, which is just insane,‚Äù Harvey said.\n",
        "\n",
        "  * [A New Approach to Multiplication Opens the Door to Better Quantum Computers](https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/))\n",
        "\n",
        "  * [Mathematicians Discover the Perfect Way to Multiply](https://www.quantamagazine.org/mathematicians-discover-the-perfect-way-to-multiply-20190411/)\n",
        "\n",
        "* Addition takes only 2n steps\n",
        "\n",
        "* Tang‚Äôs algorithm ran in polylogarithmic time ‚Äî meaning the computational time scaled with the logarithm of characteristics like the number of users and products in the data set ‚Äî and was exponentially faster than any previously known classical algorithm.\n",
        "\n",
        "  * [Major Quantum Computing Advance Made Obsolete by Teenager](https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Limits of Exponential Speedups*"
      ],
      "metadata": {
        "id": "0-Lu47J5MYMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewin Tang: [A quantum-inspired classical algorithm for recommendation systems](https://arxiv.org/abs/1807.04271)\n",
        "\n",
        "https://www.youtube.com/watch?v=P2Bucnq_ap0"
      ],
      "metadata": {
        "id": "dCqBW6oqOTUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it‚Äôs broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can‚Äôt express it easily n^80, you can‚Äôt write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don‚Äôt have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don‚Äôt need any edges, but n^1,5 you can‚Äôt have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "yloFCdAueEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1905.10415 with https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf\n",
        "\n",
        "* There are two ways to compute the average of many values. A first method is to add all of them together and divide the result by the total number of values. An alternative approach is to select a few values at random, then compute their average using the first method. Of course, in the second case we get only an approximation of the true average, but in many cases that is good enough and can be done in significantly less time.\n",
        "\n",
        "* **This is the strategy of quantum-inspired algorithms: the coefficients of the solution vector are inner products between vectors, which can be expressed as the average of a collection of numbers that depend on the entries of the vectors**. Instead of directly computing this average (which takes linear time) we select a few of the values at random and compute their average instead.\n",
        "\n",
        "* The question is, how many values do we need to sample to get a good approximation? It turns out that **the number of samples grows with the precision of the approximation, the rank, and the condition number of the input matrix.**\n",
        "\n",
        "* In practice, this estimation method is only faster than a direct calculation of the coefficients if (i) precision, rank, and condition number are small, and (ii) the input matrix has a colossal dimension.\n",
        "\n",
        "* (..)\n",
        "\n",
        "* In practice, it pays off to be slightly more sophisticated: we keep each number with likelihood proportional to its value, such that large numbers are kept with high probability and small numbers with low probability. This is the basis of a technique called [rejection sampling](https://en.m.wikipedia.org/wiki/Rejection_sampling), **which is employed in quantum-inspired algorithms to sample from the solution vectors**. Once the approximate singular value decomposition has been obtained and the coefficients have been estimated, it is possible to query any entry of the solution vector in sublinear time.\n",
        "\n",
        "https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n",
        "\n",
        "https://cstheory.stackexchange.com/questions/42338/list-of-quantum-inspired-algorithms\n",
        "\n",
        "https://github.com/XanaduAI/quantum-inspired-algorithms"
      ],
      "metadata": {
        "id": "-wa6Ap4QMXJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulated annealing (inspired) vs quantum annealing (with quantum tunneling):\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/quantum/optimization-overview-introduction\n"
      ],
      "metadata": {
        "id": "4E5NYRMXNrHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How exactly did Ewan Tang dequantize a quantum algorithm?**\n",
        "\n",
        "Ewin Tang, in her breakthrough work in 2018, presented a classical algorithm that can solve a problem almost as fast as the best quantum algorithm can. This came as a surprise because, until then, it was believed that quantum computers could significantly speed up the solution to such problems.\n",
        "\n",
        "Tang's focus was on a specific type of problem used in recommendation systems. For example, if you have a large group of users, each of whom has rated a subset of a large set of products, a recommendation system can predict how a user would rate products they have not yet rated. The core of this problem is to identify items that are similar to those the user has already shown a preference for. \n",
        "\n",
        "The most prominent quantum algorithm for recommendation systems was based on something called the Harrow-Hassidim-Lloyd (HHL) algorithm, a quantum algorithm used to solve systems of linear equations, which was believed to be exponentially faster than any classical algorithm.\n",
        "\n",
        "Ewin Tang's genius was to show a classical algorithm that could solve this recommendation problem almost as fast as the quantum version. The \"dequantization\" process involves using similar principles that the quantum algorithm uses, but translating them to a classical context. \n",
        "\n",
        "More technically, Tang‚Äôs algorithm builds on a classical data structure called a \"tree of singular vectors,\" which is used to approximate the product of the preference matrix with a vector that represents a new user's preferences. This approximation is good enough to recommend items in a way that's nearly as good as the quantum algorithm.\n",
        "\n",
        "It's important to note that the success of Tang's algorithm relies heavily on the specifics of the recommendation system problem - specifically, the fact that the user-product preference matrix has a certain property called \"low rank.\" Therefore, it doesn't mean that all quantum algorithms can be dequantized in the same way. It does, however, represent an important conceptual breakthrough, showing that the boundary between classical and quantum computing is not as clear-cut as it was once believed to be.\n",
        "\n",
        "other explanation:\n",
        "\n",
        "Ewin Tang made a breakthrough in computer science in 2018 by finding a classical algorithm that solves a problem just as quickly as the best-known quantum algorithm. This result came as a surprise because it was previously believed that quantum computers would be needed to achieve such a speed-up.\n",
        "\n",
        "Tang studied a problem known as recommendation systems, which is a task that lies at the heart of platforms like Netflix and Amazon. The quantum version of the recommendation system problem is known as the \"quantum singular value transformation,\" and was thought to give an exponential speed-up over any possible classical algorithm. Tang's work \"dequantizes\" this problem, meaning she derived a classical algorithm that achieves a similar performance to the quantum algorithm, thus closing what was seen as a \"quantum supremacy\" gap.\n",
        "\n",
        "In more technical terms, Tang's classical algorithm uses similar ideas and techniques to the quantum one but in a classical setting. The approach relies on a form of algorithm called \"randomized sampling algorithms\" and works by creating a probability distribution that closely matches the one that would have been created by a quantum computer. By cleverly sampling from this distribution, Tang's algorithm can estimate the answers to the recommendation problem almost as accurately as the quantum version.\n",
        "\n",
        "Tang's work has implications for the field of quantum computing. It highlights that the theoretical understanding of the boundaries between classical and quantum computation is still incomplete, and there may be more situations where seemingly quantum-exclusive speed-ups can be achieved by classical means. However, it also stresses that quantum supremacy might be more difficult to achieve than initially thought, and that claims of quantum advantage should be thoroughly scrutinized."
      ],
      "metadata": {
        "id": "w5EmlC2n2rN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do you mean exactly by: ‚Äûyou cannot read out all the entries in less than exponential time due to the ‚Äòno-cloning‚Äô theorem of quantum mechanics‚Äú?**\n",
        "\n",
        "In quantum computing, the no-cloning theorem is a fundamental principle which states that it is impossible to create an identical copy of an arbitrary unknown quantum state. This rule has some important implications for reading out the state of a quantum system.\n",
        "\n",
        "If you have a quantum system in a superposition of many states (let's say, for simplicity, that we're dealing with a string of qubits in a quantum computer), each individual state corresponds to a \"piece of information.\" In a perfect world, if you could clone this quantum state, you might imagine making many copies of it and measuring each one to read out all the possible states -- essentially reading out all the pieces of information.\n",
        "\n",
        "However, the no-cloning theorem forbids this. You can't make those copies. What's more, when you perform a measurement on a quantum system, it \"collapses\" into one of its possible states according to its probability distribution, giving you just one piece of information from your superposition. And after the measurement, the rest of the information in the original superposition is lost.\n",
        "\n",
        "So, if you want to know the full quantum state (i.e., all the entries), you would have to prepare identical copies of your original state and measure each one, hoping to eventually see every possible outcome. If your quantum state is a superposition of many possibilities, it would take an exponential amount of time to get a good chance of seeing every possibility. This is in stark contrast to a classical bit string where you could just read out every bit individually in a linear amount of time. \n",
        "\n",
        "That's why the no-cloning theorem limits our ability to fully read out the state of a quantum system: each measurement only gives you a tiny piece of the picture, and you would have to repeat the process an exponential number of times to see the full state."
      ],
      "metadata": {
        "id": "9p5C9WjH2vyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is an exponential speedup in quantum linear algebra so difficult?**\n",
        "\n",
        "Quantum computers use principles of quantum physics to perform computations, and while they promise significant computational speed-ups for certain problems, it's important to understand that this speed-up is not universal for all types of problems. The difficulty in achieving an exponential speed-up in quantum linear algebra (and many other problems) is due to several factors:\n",
        "\n",
        "1. **The nature of the problem**: Not all problems are amenable to quantum speed-up. Quantum algorithms are specifically efficient for problems where a superposition of solutions can be created and manipulated - this includes factoring integers (Shor's algorithm) and searching databases (Grover's algorithm). Linear algebra problems, especially in high dimensions, may not easily lend themselves to such superposition-based speed-ups.\n",
        "\n",
        "2. **Qubit limitations and error correction**: Quantum information is delicate and can be easily disrupted. Building a quantum computer with enough qubits to offer an exponential speedup for complex problems, and which also has enough error correction to prevent the computation from being disrupted, is a significant engineering challenge. As of my knowledge cutoff in September 2021, these challenges are still unsolved.\n",
        "\n",
        "3. **Output limitations**: Even if a quantum computer can perform a computation faster, it might not be able to provide the result in a way that's useful or can be checked in a reasonable time by a classical computer. This limitation is a crucial aspect in quantum linear algebra problems. For example, you can load a vector into a quantum state exponentially faster, but you cannot read out all the entries in less than exponential time due to the 'no-cloning' theorem of quantum mechanics.\n",
        "\n",
        "4. **Efficient classical algorithms**: In some cases, we already have very efficient classical algorithms for linear algebra problems. This means that a quantum algorithm would have to offer an extreme speed-up to be advantageous. Sometimes, we later find that classical algorithms can be further optimized, which narrows the gap between classical and quantum computation. This is what happened in Ewin Tang's dequantization of the recommendation systems problem.\n",
        "\n",
        "5. **Difficulty of programming and handling quantum algorithms**: Quantum algorithms are inherently more difficult to design and implement due to the intrinsic nature of quantum mechanics, which includes superposition, entanglement, and interference. Moreover, real-world quantum systems are often noisy and error-prone, making it even more challenging to implement quantum algorithms reliably.\n",
        "\n",
        "These challenges represent the forefront of current research in quantum computing and quantum information science. Progress on these fronts will continue to shape our understanding of the real power and limits of quantum computation.\n"
      ],
      "metadata": {
        "id": "VTCCOd3I23KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic Speedups**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Counting_problem_(complexity)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Polynomialzeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Komplexit√§tstheorie\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Speedup-Theorem\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Nondeterministic_Turing_machine\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Pushdown_automaton\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Quadratische_Optimierung\n",
        "\n",
        "Lineare Programmierung: immer quadratisch. Jedes lineare Optimierungsproblem ist ein quadratisches Optimierungsproblem. https://de.m.wikipedia.org/wiki/Quadratische_Optimierung\n",
        "\n",
        "Nichtlineare programmierung: ein teil ist quadratisch - Quadratic programming is a type of nonlinear programming. (https://en.m.wikipedia.org/wiki/Quadratic_programming)\n",
        "\n",
        "Conversely, any such constrained least squares program can be equivalently framed as a Quadratic programming - https://en.m.wikipedia.org/wiki/Quadratic_programming\n",
        "\n",
        "The use of a quadratic loss function is common, for example when using least squares techniques.  - https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_optimization_algorithms\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Optimization_problem\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Convex_optimization\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Mathematical_optimization"
      ],
      "metadata": {
        "id": "DBjGdloKeEmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Calculate complexity*"
      ],
      "metadata": {
        "id": "CtFnUPlheUKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import math\n",
        "import sympy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pu9OrL0VvvyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/space-and-time-complexity-in-computer-algorithms-a7fffe9e4683\n",
        "\n",
        "https://towardsdatascience.com/python-performance-and-gpus-1be860ffd58d\n",
        "\n",
        "https://urban-institute.medium.com/using-multiprocessing-to-make-python-code-faster-23ea5ef996ba\n",
        "\n",
        "> The VM used for Colaboratory appears to have 13GB RAM and 2 vCPU when checking using psutil (so a n1-highmem-2 instance)\n",
        "\n",
        "- 2-core Xeon 2.2GHz\n",
        "- 13GB RAM\n",
        "- 33GB HDD\n",
        "\n",
        "maximum lifetime of a VM is 12 hours. Idle VMs time out after 90m"
      ],
      "metadata": {
        "id": "DLubvtAOefxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display complexity of Fibonacci\n",
        "\n",
        "import time\n",
        "\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "\n",
        "# My input data for the function\n",
        "my_tuple = 5,10,15,20,25\n",
        "\n",
        "# Define empty bucket\n",
        "rows_list = []\n",
        "\n",
        "for n in my_tuple:\n",
        "\n",
        "    # Begin tracking CPU time\n",
        "    st = time.process_time(),\n",
        "\n",
        "    # Run algorithms\n",
        "    result = fibonacci(n),\n",
        "    #result = math.factorial(n)\n",
        "    #result = n**2\n",
        "\n",
        "    # Finish tracking CPU time\n",
        "    et = time.process_time(),\n",
        "\n",
        "    # Calculate performance (end - begin)\n",
        "    performance = [x - y for x, y in zip(list(et), list(st))]\n",
        "\n",
        "    # Collect results in a list\n",
        "    rows_list.append(performance)\n",
        "\n",
        "# Write results in a table\n",
        "df = pd.DataFrame(rows_list, index=my_tuple, columns=['CPU Performance in sec']) \n",
        "print(df)\n",
        "\n",
        "# Visualize results in a graph\n",
        "df = df.reset_index().rename(columns={\"index\": \"Datasize\"})\n",
        "df.plot(x ='Datasize', y='CPU Performance in sec', kind = 'bar')\t"
      ],
      "metadata": {
        "id": "AutzcBQJezFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.geeksforgeeks.org/running-python-script-on-gpu/\n",
        "\n",
        "* does it consider several CPUs? \n",
        "* how to check distribution of a process?\n",
        "* is it important to consider GPUs? \n",
        "\n",
        "**time.process_time()** function always returns the float value of time in seconds. \n",
        "* Return the value (in fractional seconds) of the sum of the system and user CPU time of the current process.\n",
        "* It does not include time elapsed during sleep. \n",
        "*The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid.\n",
        "\n",
        "Note: **process_time()** is very different from **pref_counter()**, as perf_counter() calculates the program time with sleep time and if any interrupt is there but process_counter only calculates the system and the CPU time during the process it not includes the sleep time.\n",
        "\n",
        "Advantages of **process_time()** :\n",
        "1. process_time() provides the system and user CPU time of the current process.\n",
        "2. We can calculate float and integer both values of time in seconds and nanoseconds.\n",
        "3. Used whenever there is a need to calculate the time taken by the CPU for the particular process."
      ],
      "metadata": {
        "id": "Nqs6OPRBe3KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**%time**\n",
        "\n",
        "* Time execution of a Python statement or expression.\n",
        "\n",
        "* The CPU and wall clock times are printed, and the value of the expression (if any) is returned. Note that under Win32, system time is always reported as 0, since it can not be measured.\n",
        "\n",
        "* This function can be used both as a line and cell magic:\n",
        "\n",
        "  * In line mode you can time a single-line statement (though multiple ones can be chained with using semicolons).\n",
        "\n",
        "  * In cell mode, you can time the cell body (a directly following statement raises an error).\n",
        "\n",
        "* This function provides very basic timing functionality. Use the timeit magic for more control over the measurement.\n",
        "\n",
        "https://ipython.readthedocs.io/en/stable/interactive/magics.html"
      ],
      "metadata": {
        "id": "-q2srP_AfBIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1217.png)\n"
      ],
      "metadata": {
        "id": "Lujstc4WfFpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways:\n",
        "\n",
        "* **time.process_time()** function with start and stop\n",
        "\n",
        "* **%time** magic function\n",
        "\n",
        "Sources: \n",
        "\n",
        "* https://www.geeksforgeeks.org/time-process_time-function-in-python/\n",
        "\n",
        "* https://www.askpython.com/python/examples/python-wait-for-a-specific-time\n",
        "\n",
        "* https://pynative.com/python-get-execution-time-of-program/"
      ],
      "metadata": {
        "id": "ZsxXpJeOfHtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gertingold.github.io/tools4scicomp/profiling.html\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Landauer-Prinzip\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Bremermann-Grenze\n",
        "\n",
        "https://www.hri.res.in/~qipa13/QIPA/docu/7.12.2013/morning%20session/2-pjoag-Quantum%20Algorithm%20for%20LP--2007.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2010.07852.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2203.15421.pdf\n",
        "\n",
        "https://en.wikipedia.org/wiki/Time_complexity\n",
        "\n",
        "https://en.wikipedia.org/wiki/Karp%27s_21_NP-complete_problems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Integer_programming\n",
        "\n",
        "https://en.wikipedia.org/wiki/COIN-OR\n",
        "\n",
        "Total CPU time vs Wallclock time:\n",
        "\n",
        "https://pythonspeed.com/articles/blocking-cpu-or-io/"
      ],
      "metadata": {
        "id": "3LpSP0JxfPi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kHnl5-823rO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Machine Learning*"
      ],
      "metadata": {
        "id": "p9cyxvqKIhGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Essentials*"
      ],
      "metadata": {
        "id": "t0wVu7UaIENk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Christophe-pere/Roadmap-to-QML/blob/main/Reviews/Reviews.md"
      ],
      "metadata": {
        "id": "wMuCBohWK9wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/Studien/QML/Quantum_Machine_Learning.pdf\n",
        "https://www.nature.com/articles/s41467-022-31679-5\n",
        "https://arxiv.org/abs/2208.06198"
      ],
      "metadata": {
        "id": "ITBaDwB3VfcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Machine Learning: what do you mean?**\n",
        "\n",
        "* Enhance classical or physics\n",
        "* Classical vs quantum native data\n",
        "* Discriminative vs generative approaches\n",
        "* NISQ hybrid (annealing, variational) vs error-corrected\n",
        "* Kernel-methods vs neural nets\n"
      ],
      "metadata": {
        "id": "Ye7VLc7kH_Tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open Questions in Quantum Machine Learning**\n",
        "* Are quantum circuits actually useful models? (can they help generalize?)\n",
        "* How can we benchmark them?\n",
        "* Is there a connection between quantum theory and deep learning?\n",
        "* How does noise impact applications?\n",
        "* What data domains is QML good for?\n",
        "* Do OML ideas scale?\n",
        "* What optimization strategies work for quantum circuits?\n",
        "* Which circuit architectures are good for ML?"
      ],
      "metadata": {
        "id": "GSB6Zq9ZQLmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems with traditional quantum computing applied to QML**\n",
        "\n",
        "* Check how to apply Grovers search to machine learning or hot to speed up a neural network.\n",
        "* First, it talks about runtime speedups, but doesn't care about practicability (100Mio years vs forever to train is a strong speedup, but useless in practical machine learning)\n",
        "* Second, too much focus on if something is provable: lots of things in ML are not provable. optimization training is np-hard. for quantum computing it's unsolvable. Most of what we do in machine learning is not provable, it just works.\n",
        "* Near time quantum computing is on the other side too empirical: this and that is faster than if we run it on a small classical neural network. applied experiments on small regimes that no one cares about (a few qubits)\n",
        "* third step: develop a theory about QML, taken from classical ML\n",
        "> **Goal of machine learning is generaliation, not speedup** But we theoretical need to understand what's generalization, because we can't just try, QCs are too small and noisy."
      ],
      "metadata": {
        "id": "tj__oF4_Wcfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differentiation**\n",
        "\n",
        "* **Data classical and algorithm classical**:\n",
        "  * using classical methods to understand quantum physics for classical machine learning\n",
        "  * using tensor networks (developed to understand and calculate with quantum states for neural networks)\n",
        "  * methods of using neural networks to represent quantum states\n",
        "* **Data quantum and algorithm classical**:\n",
        "  * data from quantum experiments\n",
        "  * quantum error correction with machine learning\n",
        "  * interpret or shortcut [quantum state tomography](https://en.m.wikipedia.org/wiki/Quantum_tomography) with machine learning\n",
        "* **Data quantum and algorithm quantum**:\n",
        "  * we get quantum data, but not in terms of measurements but actually the  states of quantum data\n",
        "  * you get a cryptography experiment and you get your photons out. and now you can send those photons into a photonic quantum computer and can process them into a quantum machine learning on the quantum data.\n",
        "* **Data classical and algorithm quantum**:\n",
        "  * how to use quantum computers to do machine learning.\n",
        "  * the most common understanding of quantum machine learning\n"
      ],
      "metadata": {
        "id": "qaVQzNVxQ3zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "8A77f2AkHlWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qhat Circuit should I use?**\n",
        "\n",
        "\n",
        "* at the moment we don't know what circuit would be the best in QML, no theoretical foundation\n",
        "* i.e. tensor networks\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1409.png)"
      ],
      "metadata": {
        "id": "5zMe2MdUJNnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Approach**\n",
        "\n",
        "* in classical ML pipeline what you can get from a quantum computer is information about the gradients, what are partial derivatives of node/ quantum computation with respect to its parameters?\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1411.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1412.png)\n",
        "\n",
        "https://arxiv.org/abs/1909.02108 Quantum Natural Gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "LvFX3r1INUJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pRIE-on-H6D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Podcasts*"
      ],
      "metadata": {
        "id": "z9buLeTdI1Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amira Abbas** \n",
        "\n",
        "https://youtu.be/qcB8tfISWHo\n",
        "\n",
        "https://youtu.be/aU8XBjG5tAw\n",
        "\n",
        "https://youtu.be/aU8XBjG5tAw\n",
        "\n",
        "https://youtu.be/ETxQNIR6dAg"
      ],
      "metadata": {
        "id": "6WZPCz6VcE9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maria Schuld**\n",
        "\n",
        "- What is meant by: In neural networks the differentiation rules that we use scale linearly with the number of parameters. In variational circuits the differentiation rules that we use scale quadratically with the number of parameters.\n",
        "- Automatic differentiation is based on recycling values of gradients, so that not for every parameter they have to run the whole network again, forwards and backwards. \n",
        "- Challenge: Just guessing an ansatz (expressive)\n",
        "- Adding more layers = Increase the frequency of the cosine kernel?? (Min 27 https://youtu.be/8bfUMdj0-x4), then just repeating these layers of encoding would be better. In many cases making an embedding and then repeating it makes the model class richer. \n",
        "- Quantum advantage for learning is currently still ill-posed. \n",
        "- What is meant by quantum speedup? Different concepts. It‚Äôs always relative to something. \n",
        "- And what do you mean by ‚Äòmore powerful‚Äô: learning or speedup? Etc.\n",
        "\n",
        "> How can I prove that my ansatz is classically intractable? versus **What is an ansatz design that allows gradient-descent to scale as efficient as it does when training neural networks?**\n",
        "\n",
        "> How can I show that QML is more powerful? versus **How can I understand what QML is doing?**\n",
        "\n",
        "https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1621s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1609.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1610.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "wFaHv5nhYDmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zoe Holmes**\n",
        "\n",
        "Translate priblem into a energy minimze / ground state\n",
        "\n",
        "Cost funcrion measurement on quantum computer, because maybe easy to solve there where hard on classical\n",
        "\n",
        "But cost landscape often complex / difficult with gradient descent\n",
        "\n",
        "Other optimiStiln methids than gradient?\n",
        "\n",
        "Shot noise to navigate quabtum landscapes\n",
        "\n",
        "Quqbtum benchmarking is not a scialable roite for proving quabtum supremacy\n",
        "\n",
        "A lot of back and forth between quantum & c√∂sssical. But good: Help understsnd quantum systems (& quabtum inspired.\n",
        "\n",
        "Qml in chemistry: you need chemical prexision with lots of shots\n",
        "\n",
        "Material scienxe and xondensed matter earlier than chemistry\n",
        "\n",
        "Qml in chemistry: less qml and mor eQuantum variational algorithms to learn electronic structure caclulations to compute ground states and excited states for molecular haniltonian. Use to compute reqction rates.\n",
        "\n",
        "There is no collapse. Everything is unizary >> leads to many world theory\n",
        "\n",
        "Source: https://youtu.be/zws9B66juBk\n"
      ],
      "metadata": {
        "id": "scZYC5ABJC4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 1: Backpropagation & Gradients*"
      ],
      "metadata": {
        "id": "QYjPeXk3YiDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation Value and Backpropagation**\n",
        "\n",
        "The average value (expectation value) of the measurement result is given by the\n",
        "Born rule: **bold text**\n",
        "\n",
        "> $\\langle B\\rangle=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle$\n",
        "\n",
        "Just linear algebra! Every step is a matrix-vector or matrix-matrix multiplication\n",
        "\n",
        "Expectation values depend continuously on the gate parameters\n",
        "\n",
        "**Backpropagating Through Quantum Circuits**\n",
        "\n",
        "However, as long as we don't \"zoom in\" to what is happening in the quantum circuit, backpropagation can treat the quantum circuit as a single indivisible function\n",
        "\n",
        "The expectation value of a quantum circuit is a differentiable function\n",
        "\n",
        "> $\n",
        "f(\\theta)=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle=\\langle B\\rangle$\n",
        "\n",
        "Running on hardware and using the parameter-shift rule, we can provide both ingredients needed by backpropagation\n",
        "\n",
        "> $\n",
        "\\left(\\langle B\\rangle, \\frac{\\partial}{\\partial \\theta}\\langle B\\rangle\\right)\n",
        "$\n",
        "\n",
        "[Automatic Differentiation of Quantum Circuits](https://youtu.be/McgBeSVIGus)\n",
        "\n",
        "[Variational Quantum Algorithms](https://youtu.be/YtepXvx5zdI)\n",
        "\n",
        "[Hybrid Quantum-Classical Machine Learning](https://youtu.be/t9ytqPTij7k)"
      ],
      "metadata": {
        "id": "1xA6TbS7EOSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paper: On quantum backpropagation, information reuse, and cheating measurement collapse**\n",
        "\n",
        "The proposed algorithm is presented in Figure 1 of the PDF file. It highlights the reduction in quantum resources due to the ability to reuse intermediate information and perform measurements in a way that minimizes disturbance to the quantum state. \n",
        "\n",
        "However, achieving backpropagation scaling is impossible without access to multiple copies of a state, which is enabled by recent developments in shadow tomography.\n",
        "\n",
        "* **The origin of the challenge lies within quantum measurement collapse and the inability to read out intermediate states while continuing a computation**, rather than the probabilistic formulation of the problem. \n",
        "\n",
        "* **In the classical setting, one is always promised to be in a computational basis state**, making it possible to do perfect measurements non-destructively at intermediate steps.\n",
        "\n",
        "* Although Proposition 3 presents a strict lower bound ruling out backpropagation in the quan- tum data case with single copies, **this leads one to wonder whether backpropagation scaling is possible when one has access to multiple copies**. Moreover, destructive quantum measurements are the inhibitor of backpropagation scaling in single copies, so perhaps there is some middle ground where one could perform measurements that are only partially destructive on multiple copies. This idea has led to breakthroughs in the shadow tomography problem, which we examine next in the context of backpropagation.\n",
        "\n",
        "* Interesting: \"If the difficulty to achieve an efficient scaling is due to inherently quantum properties, **perhaps backpropagation is not the correct method for optimization of quantum models, which seems to be a growing belief for classical models too, albeit for completely different reasons** (=Hinton 2022)\"\n",
        "\n",
        "*How can information reuse be leveraged to improve the efficiency of quantum neural network training?*\n",
        "\n",
        "* information reuse can be leveraged to improve the efficiency of quantum neural network training. By reusing intermediate information, backpropagation can facilitate training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters. \n",
        "\n",
        "* One expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. \n",
        "\n",
        "* Recent developments in shadow tomography have challenged this notion by assuming access to multiple copies of a quantum state. The paper investigates whether parameterized quantum models can train as efficiently as classical neural networks and shows that achieving backpropagation scaling is impossible without access to multiple copies of a state.\n",
        "\n",
        "*What is shadow tomography and how is it used in this context?*\n",
        "\n",
        "* Shadow tomography is a technique that assumes access to multiple copies of a quantum state and uses a hypothesis state model, often stored classically, to require a minimal number of destructive measurements.\n",
        "\n",
        "* In the context of quantum neural network training, recent developments in shadow tomography have challenged the notion that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. \n",
        "\n",
        "* By assuming access to multiple copies of a quantum state, a modification of existing shadow tomography routines enables backpropagation scaling if one restricts costs to the quantum overhead and ignores the classical cost incurred to implement known shadow tomography schemes.\n",
        "\n",
        "*What is shadow tomography and how is it used in the context of quantum computing?*\n",
        "\n",
        "* Shadow tomography, in the context of quantum computing, is a technique used to characterize an unknown quantum state by gathering statistical information about its measurement outcomes. It is an approach that aims to reconstruct the quantum state without directly accessing it.\n",
        "\n",
        "* In quantum computing, qubits represent the fundamental units of information and are typically manipulated in superposition states. However, obtaining complete knowledge about the quantum state of a qubit is challenging due to limitations such as noise, decoherence, and the no-cloning theorem. Shadow tomography offers a way to infer information about the state without requiring full access to it.\n",
        "\n",
        "* The technique involves performing a series of measurements on a large number of identically prepared quantum systems in the same unknown state. Each measurement provides partial information about the state, revealing statistical probabilities for the different measurement outcomes. By gathering a sufficiently large set of measurement statistics, it becomes possible to infer the likely quantum state.\n",
        "\n",
        "* The reconstruction of the quantum state from the collected statistics is achieved through mathematical techniques, such as maximum likelihood estimation or compressed sensing algorithms. These methods leverage the statistical patterns observed in the measurement outcomes to estimate the underlying quantum state that generated them.\n",
        "\n",
        "* Shadow tomography provides a means to characterize unknown quantum states in a non-invasive manner, making it particularly useful for quantum systems that are difficult or impossible to access directly. It plays a crucial role in quantum information processing tasks such as quantum error correction, state verification, and characterizing the performance of quantum devices.\n",
        "\n",
        "\n",
        "*What is gentle measurement in the context of quantum computing?*\n",
        "\n",
        "* Gentle measurement techniques often employ strategies such as weak measurements, quantum non-demolition measurements, or adaptive measurements. \n",
        "\n",
        "* Weak measurements involve extracting partial information about the system while causing minimal disturbance. \n",
        "\n",
        "* Quantum non-demolition measurements are designed to measure one property of the system without affecting other compatible observables. \n",
        "\n",
        "* Adaptive measurements dynamically adjust the measurement strategy based on the intermediate measurement results to minimize disturbance.\n",
        "\n",
        "* By using gentle measurement techniques, researchers can gather information about quantum states while preserving their fragile nature. This is particularly important in quantum computing, where maintaining the coherence of quantum bits (qubits) is crucial for performing reliable computations and preventing errors."
      ],
      "metadata": {
        "id": "froKIlJpmekh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the shadow tomography problem in quantum information theory?**\n",
        "\n",
        "In quantum information theory, the shadow tomography problem refers to the task of characterizing an unknown quantum state by gathering statistical information about its measurement outcomes. It addresses the challenge of reconstructing a quantum state without directly accessing it.\n",
        "\n",
        "The shadow tomography problem arises due to the limitations of directly measuring quantum states. In quantum systems, the act of measurement can disturb the system and potentially cause the collapse of its quantum state. Additionally, some quantum states may be inaccessible or difficult to measure directly due to experimental constraints or limitations.\n",
        "\n",
        "To overcome these challenges, shadow tomography offers a non-invasive approach to infer information about the unknown quantum state. Instead of directly measuring the state, it focuses on collecting statistical data by repeatedly measuring identically prepared copies of the state and recording the outcomes.\n",
        "\n",
        "The goal of the shadow tomography problem is to use the collected statistical data to reconstruct an estimate of the quantum state that is consistent with the observed measurement outcomes. This estimation process involves mathematical techniques such as maximum likelihood estimation, compressed sensing, or machine learning algorithms.\n",
        "\n",
        "Solving the shadow tomography problem is important in various quantum information tasks, including quantum state verification, characterization of quantum devices, and quantum error correction. By obtaining accurate reconstructions of unknown quantum states, researchers can gain insights into the properties of quantum systems and develop techniques to mitigate errors and improve quantum information processing tasks.\n",
        "\n",
        "**what is the exponential classical cost within the shadow tomography problem?**\n",
        "\n",
        "The term \"exponential classical cost\" in the context of the shadow tomography problem refers to the computational complexity associated with classical methods of characterizing or reconstructing an unknown quantum state. It signifies that the amount of computational resources required grows exponentially with the size of the quantum system.\n",
        "\n",
        "In shadow tomography, the goal is to estimate the quantum state by collecting measurement statistics. The challenge lies in finding an efficient algorithm to reconstruct the state from this statistical data. Classical methods typically involve searching through a large space of possible quantum states and optimizing a likelihood function or another measure of agreement with the observed statistics.\n",
        "\n",
        "However, for large quantum systems, the number of possible quantum states grows exponentially with the number of qubits. As a result, exhaustive classical methods that explore the entire state space become computationally infeasible as the system size increases. This exponential growth in the number of possible states leads to an exponential increase in the computational cost required for reconstruction.\n",
        "\n",
        "The exponential classical cost in shadow tomography highlights the need for alternative approaches, such as leveraging quantum computers or developing more efficient reconstruction algorithms. Quantum computers can potentially provide speedup by harnessing the principles of quantum mechanics, allowing for more efficient exploration of the state space or utilizing specialized quantum algorithms tailored for state reconstruction.\n",
        "\n",
        "Reducing the classical cost of shadow tomography is an active area of research, and various techniques, including compressed sensing, machine learning, and adaptive measurement strategies, have been explored to mitigate the computational challenges associated with large-scale quantum systems."
      ],
      "metadata": {
        "id": "8QKE7d8OvYR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is backpropagation scaling?**\n",
        "\n",
        "\"Backpropagation scaling\" usually refers to techniques that manage the magnitudes of gradients during the process of backpropagation, which is used to train neural networks. Proper scaling is important because it can impact the speed and effectiveness of learning.\n",
        "\n",
        "Here are two common scaling issues that may arise during backpropagation:\n",
        "\n",
        "1. **Vanishing gradients:** When deep neural networks are trained, gradients of the loss function can become very small. As a result, weight updates during training become insignificant, and the network takes a very long time to learn, if it can learn at all. This problem is particularly common with activation functions like the sigmoid or hyperbolic tangent, which squish a large input space into a small output range.\n",
        "\n",
        "2. **Exploding gradients:** Conversely, gradients can also become very large, leading to large updates to the weights and causing the model to oscillate around the optimal solution, or even to diverge entirely. This is often a problem in recurrent neural networks (RNNs).\n",
        "\n",
        "Various techniques have been proposed to mitigate these issues:\n",
        "\n",
        "1. **Gradient clipping:** This is a common technique to prevent exploding gradients. If the norm of the gradient exceeds a certain threshold, we scale it back to prevent it from getting too large.\n",
        "\n",
        "2. **Weight initialization:** Properly initializing the weights can prevent gradients from vanishing or exploding too quickly. For example, Xavier/Glorot and He initialization are strategies that consider the sizes of the input and output layers.\n",
        "\n",
        "3. **Choice of activation function:** The choice of activation function can help alleviate the vanishing gradients problem. For example, ReLU (Rectified Linear Unit) and its variants (like Leaky ReLU and Parametric ReLU) are commonly used because they do not saturate for positive inputs.\n",
        "\n",
        "4. **Batch normalization:** This technique normalizes the activations of each layer to prevent the distribution of inputs to each layer from changing too much during training, which can help mitigate both vanishing and exploding gradients.\n",
        "\n",
        "5. **Use of optimizers:** Certain optimization algorithms like RMSProp, Adam, and Nadam can adaptively scale learning rates to mitigate both exploding and vanishing gradients.\n",
        "\n",
        "In summary, backpropagation scaling techniques are important to ensure that the magnitude of updates during training is appropriate and to prevent issues related to vanishing or exploding gradients."
      ],
      "metadata": {
        "id": "1kN_2ppZzdXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are proposals to ensure backpropagation scaling in quantum machine learning?**\n",
        "\n",
        "As of my knowledge cutoff in September 2021, ensuring proper backpropagation scaling in quantum machine learning is an active area of research. A few of the techniques that have been suggested for improving gradient scaling and thus learning in quantum machine learning systems include:\n",
        "\n",
        "1. **Variational Quantum Algorithms:** In the field of quantum machine learning, Variational Quantum Algorithms (VQAs) like the Variational Quantum Eigensolver (VQE) or Quantum Approximate Optimization Algorithm (QAOA) are commonly used. These algorithms employ a hybrid quantum-classical approach, which allows classical optimization techniques to be used in the learning process. This means classical techniques for managing gradient scaling, like batch normalization or gradient clipping, can still be applicable in this quantum setting.\n",
        "\n",
        "2. **Parameter Shift Rule:** The parameter-shift rule is a method for computing gradients in quantum circuits, which is particularly important for variational quantum algorithms. The rule ensures that for certain types of quantum gates (those that generate rotations), the gradient of the expectation value of a quantum circuit with respect to a parameter can be computed exactly, regardless of the number of qubits or the complexity of the circuit.\n",
        "\n",
        "3. **Randomized Layerwise Training:** Another strategy suggested for training deep quantum circuits involves training one layer at a time with a random initialization for the rest of the circuit, a technique inspired by classical machine learning strategies. This approach can help to alleviate barren plateaus -- regions in the cost function landscape where the variance of the gradients vanishes exponentially with increasing system size.\n",
        "\n",
        "4. **Natural Gradient Descent:** There have been some initial studies into quantum natural gradient descent, which is an analogue to the classical natural gradient descent algorithm and is believed to be more robust to issues with gradient scaling.\n",
        "\n",
        "It's important to note that the quantum machine learning field is quite young, and there is still much research to be done on these topics. Strategies that work well in practice for classical machine learning don't always have direct analogues in the quantum setting due to the fundamental differences between classical and quantum computations, and new techniques will likely need to be developed as the field progresses."
      ],
      "metadata": {
        "id": "hF07FqRizvp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why can backpropagation not easily scale in quantum machine learning?**\n",
        "\n",
        "Backpropagation, the algorithm used to optimize the parameters of a neural network in classical machine learning, relies on efficiently computing gradients of a cost function with respect to the network parameters. This is done by propagating errors backwards through the network, layer by layer, hence the name \"backpropagation\". \n",
        "\n",
        "When it comes to quantum machine learning, where we deal with quantum data and quantum systems, the concept of backpropagation doesn't translate straightforwardly due to fundamental differences between classical and quantum systems.\n",
        "\n",
        "Here are some reasons why backpropagation may not easily scale in quantum machine learning:\n",
        "\n",
        "1. **Non-commutativity of operators**: Quantum operations, unlike classical ones, are represented by operators that generally do not commute, meaning the order in which they are applied matters. This non-commutativity makes the computation of gradients more complex than in the classical case.\n",
        "\n",
        "2. **Measurement**: In quantum mechanics, obtaining information about the state of a system involves measurement, which is a probabilistic process that collapses the state of the system. The inherent randomness of measurement makes the direct application of backpropagation problematic.\n",
        "\n",
        "3. **Complexity of quantum states**: Quantum states live in a complex vector space and can exist in a superposition of multiple states simultaneously, further complicating the process of backpropagation. \n",
        "\n",
        "4. **Barren plateaus**: In the context of variational quantum algorithms, it's been found that the cost function landscape often suffers from the problem of \"barren plateaus\", where the function is flat almost everywhere. This makes it difficult for gradient-based methods like backpropagation to find a direction to move in to improve the function.\n",
        "\n",
        "To address these challenges, researchers have developed quantum versions of backpropagation, such as the parameter-shift rule and the adjoint method, which are used to compute gradients of quantum circuits. However, these methods often require additional quantum resources, making it difficult to scale them up to larger quantum systems. New methods and strategies to overcome these issues are an active area of research in quantum machine learning."
      ],
      "metadata": {
        "id": "VXWcAsxVYr3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What ate exactly the parameter-shift rule and the adjoint method, which are used to compute gradients of quantum circuits?\n",
        "\n",
        "The parameter-shift rule and the adjoint method are techniques used in quantum machine learning to compute gradients of quantum circuits. Let's break down each one.\n",
        "\n",
        "1. **Parameter-shift rule**: This rule allows the derivative of a quantum circuit output with respect to its parameters to be computed in terms of circuit evaluations. Given a parameterized quantum gate (say, a rotation gate), the derivative of the expectation value of an observable with respect to the parameter can be computed as the difference between the expectation values of the observable for two slightly different values of the parameter. These \"shifted\" parameter values are typically chosen to be a small positive or negative shift from the original parameter value.\n",
        "\n",
        "    The parameter-shift rule is powerful because it allows us to compute exact gradients using only additional evaluations of the quantum circuit, with no need for complex computations involving the inner workings of the quantum operations. This makes it well-suited to hardware implementations, as the additional circuit evaluations can be run on the quantum device.\n",
        "\n",
        "2. **Adjoint method**: The adjoint method, also known as the reverse-mode differentiation, is a technique borrowed from classical automatic differentiation, generalized to the context of quantum circuits. It involves running the quantum circuit forward, storing the state at each step, and then running a modified version of the circuit backward to calculate the derivatives.\n",
        "\n",
        "   This method is efficient in terms of the number of quantum operations required, especially for circuits with many parameters but a single output (as is common in quantum machine learning models). However, it requires the ability to run quantum operations in reverse, as well as the ability to store quantum states, which can be challenging to implement on near-term quantum devices.\n",
        "\n",
        "Both methods are valuable tools for training variational quantum algorithms, where we seek to optimize the parameters of a quantum circuit to minimize a certain cost function. The choice between the two methods depends on factors like the specific structure of the quantum circuit, the capabilities of the quantum hardware, and the number of parameters versus the number of outputs."
      ],
      "metadata": {
        "id": "HUdO0UsEYosh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In collaboration with researchers at Google and Caltech, we have released a new research paper demonstrating a core problem in current quantum machine learning proposals: optimization using gradients will NOT scale as efficiently as deep neural networks equipped with backpropagation.\n",
        "\n",
        "These results illustrate that, unless better quantum models or optimization methods are found, quantum machine learning and variational approaches in general, have a large hurdle to overcome in order to practically compete with state-of-the-art deep learning models. \n",
        "\n",
        "We propose a way forward by reducing the task of estimating gradients from shadow tomography - a well studied problem in quantum information theory. Using ideas from shadow tomography, we develop a \"quantum-efficient\" protocol for computing gradients, but highlight the unavoidable classical costs involved.\n",
        "\n",
        "https://arxiv.org/abs/2305.13362 "
      ],
      "metadata": {
        "id": "TlSI56Ya4jpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 2: Barren Plateaus*"
      ],
      "metadata": {
        "id": "wdOoLdM0Fp_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical methods to overcome vanishing gradient:\n",
        "\n",
        "* Better initialization\n",
        "* ResNet networks with skip connections\n",
        "* ReLU activation function"
      ],
      "metadata": {
        "id": "KFrb7_MvrXtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pennylane.ai/qml/demos/tutorial_barren_plateaus.html"
      ],
      "metadata": {
        "id": "5twlzWBHIQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic solution to solve barren Plateaus**\n",
        "\n",
        "Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - Overparamerization (too much capacity) arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren‚Äôt needed anymore. Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g.\n",
        "\n",
        "\n",
        "From: [QHack 2022: Marco Cerezo ‚ÄîBarren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "Exponentiate Lie algebra to get lie groups:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)"
      ],
      "metadata": {
        "id": "Vljfs_A8Fr9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use unbounded objective function**\n",
        "\n",
        "* Other barren plateaus also don't apply for unbounded objective function. Almost all of QML uses bounded operators.\n",
        "* KL divergence in classical, would be quantum relativ entropy, but that's too hard to compute. better: Maximal Quantum R√©nyi Divergence\n",
        "* compute with Extended swap test (Generalizes swap test and Hadamard test)\n",
        "* Learning thermal states: Generative algorithm to thermal state\n",
        "learning,Access to LCU decomposition of the Hamiltonian\n",
        "\n",
        "Video: [Maria Kieferova - Training quantum neural networks with an unbounded loss function - IPAM at UCLA](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "Abstract: Quantum neural networks (QNNs) are a framework for creating quantum algorithms that promises to combine the speedups of quantum computation with the widespread successes of machine learning. A major challenge in QNN development is a concentration of measure phenomenon known as a barren plateau that leads to exponentially small gradients for a range of QNNs models. In this work, we examine the assumptions that give rise to barren plateaus and show that an unbounded loss function can circumvent the existing no-go results. We propose a training algorithm that minimizes the maximal Renyi divergence of order two and present techniques for gradient computation. We compute the closed form of the gradients for Unitary QNNs and Quantum Boltzmann Machines and provide sufficient conditions for the absence of barren plateaus in these models. We demonstrate our approach in two use cases: thermal state learning and Hamiltonian learning. In our numerical experiments, we observed rapid convergence of our training loss function and frequently archived a 99% average fidelity in fewer than 100 epochs.\n"
      ],
      "metadata": {
        "id": "9PEgOiqS8-lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 3: Expressibility (Capacity) vs Trainability*"
      ],
      "metadata": {
        "id": "UA8Zmuh7FRwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expressibility & trainability**\n",
        "\n",
        "Zoe holmes: https://youtu.be/RO3g7B0-IKA"
      ],
      "metadata": {
        "id": "2ZFI23s1EgeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expressibility of circuits: Learning an unknown unitary**\n",
        "\n",
        "* Expressibility & trainability is a tradeoff!!\n",
        "\n",
        "* Zoe Holmes: reduce expressibility to increase trainability. Find ansatz that fits the use case problem.\n",
        "\n",
        "* https://pennylane.ai/qml/demos/tutorial_haar_measure.html\n",
        "\n",
        "* Barren plateaus, and hence expressibility issues, are not so much in classical ML, because vanishing cost gradients are not so much of an issue in classical ML because we don‚Äòt have precision limitations in the same way.  You don‚Äòt have to evauluate your cost function using many many shots, or is so resource intensive to get gradients.\n",
        "* In quantum ML: you can use ideas from control theory to try to assess whether or not your ansatz is gonna be trainable or not in advance. Thats an important strategy.\n",
        "* The other approach: use symmetries of your problem / you gonna have to use physics to come up with whats a good ansatz. ZB: use VGQ for some system with various (particle number conserving) translational symmetries, you want to build all of those symmetries into your ansatz and hence reduce expressibility while capturing some of the solution space.\n",
        "\n",
        "* Maria schuld paper: how expressive are circuits? You can distribute circuits and how flexible are they? - identity gate maps to one point only. If you have a couple of more gates it maps to more points.\n",
        "\n",
        "https://arxiv.org/abs/1905.10876: Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1408.png)"
      ],
      "metadata": {
        "id": "kuNW1Ap_Kzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is expressivity even important? (maria schuld) https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1384s"
      ],
      "metadata": {
        "id": "C-45mVQCEF8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model expressivity is different from circuit expressivity (maria schuld) https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1384s"
      ],
      "metadata": {
        "id": "8Xv84AR_EJed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is model expressivity the same as model expressibility?**\n",
        "\n",
        "The terms \"expressivity\" and \"expressibility\" are often used interchangeably in the context of machine learning, deep learning, and artificial intelligence. **Both terms refer to a model's ability to represent a range of functions or hypotheses.**\n",
        "\n",
        "**Expressivity, more commonly used in computational fields**, describes how well a model can approximate a wide variety of functions. For example, a neural network with a higher number of layers and neurons would be considered more expressive than a network with fewer layers and neurons because it can theoretically approximate a greater variety of functions.\n",
        "\n",
        "Similarly, \"expressibility\" can refer to the richness of a model's representational space or the complexity of the functions it can represent. It also implies the ability of the model to explain or represent complex relationships in the data.\n",
        "\n",
        "However, a highly expressive or highly expressible model isn't always the best. Models that are too expressive may overfit to the training data, capturing noise and failing to generalize to unseen data. This concept is known as overfitting, and it's a common challenge in machine learning.\n",
        "\n",
        "In conclusion, while the terms expressivity and expressibility are often used interchangeably, you should always consider the context in which they are used, as some communities or researchers may assign slightly different meanings to them. As of my knowledge cutoff in September 2021, both terms generally refer to a model's capacity to represent a wide range of functions or hypotheses.\n"
      ],
      "metadata": {
        "id": "chqBt-sfEtEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are model capacity and model trainability the same?**\n",
        "\n",
        "\n",
        "No, model capacity and model trainability are not the same; they are two distinct concepts in machine learning.\n",
        "\n",
        "Model capacity refers to the complexity of the patterns a model is capable of learning from the data. It's often associated with the size or complexity of the model ‚Äî a model with more parameters (like a deep neural network) has a higher capacity than one with fewer parameters (like a simple linear regression). A higher capacity model is theoretically able to learn more complex relationships in the data, but this also opens up the risk of overfitting, where the model learns the noise or specific quirks of the training data instead of the general underlying patterns.\n",
        "\n",
        "Model trainability, on the other hand, refers to the ease with which a model can be trained. This is influenced by factors such as the design of the learning algorithm, the optimization techniques used, the presence of an appropriate loss function, the quality and relevance of the training data, and so on. A model might have high capacity (i.e., it could theoretically learn complex patterns), but if it's not easily trainable, it may not reach its potential.\n",
        "\n",
        "So, in summary, capacity and trainability are two sides of the machine learning coin: the former about what the model could potentially learn, and the latter about how efficiently or effectively it can learn it."
      ],
      "metadata": {
        "id": "FbnTHf0uzjDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means expressibility of a learning circuit?**\n",
        "\n",
        "The expressibility of a learning circuit, whether classical or quantum, refers to the variety or complexity of functions, mappings, or patterns that the circuit can represent or learn given a set of parameters.\n",
        "\n",
        "For instance, in a neural network, which can be viewed as a kind of classical learning circuit, the expressibility is related to the network's ability to approximate complex functions. A neural network with a single layer can approximate any continuous function given enough neurons (this is known as the universal approximation theorem), but it may require a very large number of neurons to do so. Adding more layers to the network (making it a deep network) can increase its expressibility, allowing it to approximate complex functions more efficiently.\n",
        "\n",
        "Similarly, in a quantum circuit used for machine learning (a variational quantum circuit), the expressibility refers to the range of quantum states that the circuit can generate or the set of unitary transformations it can perform. The expressibility of a quantum learning circuit can affect its ability to learn complex patterns in quantum data.\n",
        "\n",
        "In both cases, having high expressibility can be beneficial because it allows the learning circuit to represent more complex patterns. However, it can also make the circuit more difficult to train, because the optimization landscape that needs to be navigated to find the best set of parameters can become more complex and may include more local minima. In the quantum case, there is also the phenomenon of \"barren plateaus\" where the optimization landscape becomes flat, making it difficult for gradient-based methods to find a good solution."
      ],
      "metadata": {
        "id": "7rkSsM1mawsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What means expressibility of a quantum circuit?**\n",
        "\n",
        "In the context of quantum computing, the expressibility of a quantum circuit refers to the variety or range of quantum states that the circuit can generate or the set of unitary operations it can perform, given a set of parameters.\n",
        "\n",
        "Expressibility is an important property to consider when designing quantum circuits for quantum algorithms, especially variational quantum algorithms. A more expressive quantum circuit has the potential to represent a wider range of quantum states or unitary transformations, which may make it more powerful or versatile for certain tasks.\n",
        "\n",
        "However, there is a trade-off between expressibility and other properties of the circuit:\n",
        "\n",
        "1. **Trainability**: While having a highly expressive quantum circuit may sound beneficial, it can actually make the circuit harder to train. This is due to the so-called \"Barren Plateaus\" phenomenon, where the cost function landscape that you're trying to optimize becomes flat in all directions for highly expressive circuits, making it difficult for gradient-based optimization methods to find a good solution.\n",
        "\n",
        "2. **Noise sensitivity**: More expressive quantum circuits typically involve a greater number of quantum gates, which can make the circuit more susceptible to noise and errors in a real quantum device.\n",
        "\n",
        "3. **Computational cost**: More expressive quantum circuits can require a larger number of qubits and quantum operations, which increases the computational cost of running and simulating the circuit.\n",
        "\n",
        "So, while the expressibility of a quantum circuit is an important factor in quantum algorithm design, it's only one piece of the puzzle. Other factors like trainability, noise sensitivity, and computational cost also need to be taken into account."
      ],
      "metadata": {
        "id": "zMDOBUV2ajgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are expressibility and model capacity related?**\n",
        "\n",
        "Yes, expressibility and model capacity are closely related concepts in the context of machine learning, whether classical or quantum.\n",
        "\n",
        "Model capacity refers to the complexity of the functions or relationships that a model can represent. In the context of neural networks, for example, model capacity is influenced by the architecture of the network, including the number of layers, the number of nodes in each layer, and the types of activation functions used.\n",
        "\n",
        "Expressibility, on the other hand, often refers to a similar concept but from the perspective of the functions or states that can be achieved or represented given a specific set of parameters. For instance, the expressibility of a quantum circuit refers to the variety of quantum states or transformations that can be generated by the circuit.\n",
        "\n",
        "In general, a model with higher capacity has higher expressibility because it can represent a wider variety of functions or states. However, having high capacity or high expressibility doesn't always lead to better performance on a specific task, because it can also make the model more prone to overfitting, harder to train, and more computationally expensive. Hence, finding the right balance between model capacity and expressibility and the complexity of the task at hand is an important part of machine learning model design and training."
      ],
      "metadata": {
        "id": "BzhafAwvbFfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are there cases where expressibility is high and capacity is small and vice versa, or not?**\n",
        "\n",
        "> You can have High expressibility, small capacity (shallow neural nets), but unlikely to have Low expressibility, high capacity. \n",
        "\n",
        "\n",
        "Expressibility and capacity are both measures of the complexity of functions a model can represent. They're closely related and usually increase or decrease together. However, there may be subtle differences depending on the specific definitions used, the model architecture, and the context.\n",
        "\n",
        "1. **High expressibility, small capacity**: This could potentially occur in situations where a model has a wide range of different functions it can represent (high expressibility) but is limited in the complexity of those functions (small capacity). For example, a shallow neural network (only a few layers deep) can represent a wide variety of different functions, but it may struggle to accurately represent highly complex functions or patterns (like those present in high-dimensional data or complex tasks). This is because it lacks the depth necessary for creating intricate compositional representations.\n",
        "\n",
        "2. **Low expressibility, high capacity**: This situation might be harder to come by, but one could imagine a scenario where a model has the potential to represent very complex functions (high capacity) but is restricted in the variety of functions it can actually express due to constraints on its parameters (low expressibility). An example could be a deep neural network with high capacity but with parameters constrained in such a way that it can only express a narrow range of functions. However, such a situation might be considered somewhat artificial.\n",
        "\n",
        "It's important to note that the relationship between capacity and expressibility can also depend on how we define these terms and on the specific context or model architecture. Moreover, the goal in machine learning is often to find a balance between these properties to achieve good generalization: we want a model that is flexible enough to represent the complexity of the task (sufficient capacity and expressibility) but not so flexible that it overfits to the training data."
      ],
      "metadata": {
        "id": "kJOtl185birX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 4: Measuring Model Capacity (Generalization / The Power of a Machine Learning Model)*"
      ],
      "metadata": {
        "id": "ncQDxd2nxffx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is meant by the power of a machine learning model?**\n",
        "\n",
        "\n",
        "The \"power\" of a machine learning model can be interpreted in a few different ways, depending on the context, but it generally refers to the model's ability to learn from data and make accurate predictions. Here are a few specific interpretations:\n",
        "\n",
        "1. **Expressive Power**: This relates to the complexity of the functions or relationships a model can learn. For example, a linear regression model has less expressive power than a neural network because it can only model linear relationships between inputs and outputs, while neural networks can model complex, non-linear relationships.\n",
        "\n",
        "2. **Predictive Power**: This refers to the model's ability to make accurate predictions on unseen data. A powerful model in this sense would have a high predictive accuracy. However, it's important to be aware of overfitting, where a model might have high predictive power on training data but performs poorly on new, unseen data.\n",
        "\n",
        "3. **Generalization Power**: This refers to the model's ability to perform well on unseen data, which has not been used during the training process. It's a key factor in creating a machine learning model because we usually want a model that not only performs well on the training data, but also generalizes well to new data.\n",
        "\n",
        "4. **Capacity**: The model's capacity can also be considered its power. A model with a large number of parameters might have high capacity (and therefore, potentially, more \"power\"), as it can fit a wide range of functions. But again, models with high capacity risk overfitting the training data.\n",
        "\n",
        "5. **Robustness**: A powerful model is often expected to be robust, meaning it can handle noise in the data, outliers, or slight variations in the input, and still produce reliable and accurate results.\n",
        "\n",
        "Remember that increasing a model's power isn't always beneficial. While powerful models can learn complex patterns, they can also overfit to the training data and fail to generalize to new data, leading to poor real-world performance. This tradeoff between fitting the training data well and generalizing to new data is a central challenge in machine learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrgZEDXNqaMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you measure the power of a machine learning model?**\n",
        "\n",
        "The power of a machine learning model can be measured in several ways, each addressing a different aspect of the model's performance or capabilities:\n",
        "\n",
        "1. **Accuracy**: This is one of the most direct measures of a model's predictive power. It's the proportion of correct predictions made out of all predictions. However, it might not be a good measure if the classes are imbalanced. \n",
        "\n",
        "2. **Precision, Recall, and F1 Score**: Precision measures the percentage of true positive predictions among all positive predictions, while recall (or sensitivity) measures the percentage of true positive predictions among all actual positives. The F1 score is the harmonic mean of precision and recall, providing a balance between the two. These metrics are particularly useful when dealing with imbalanced datasets.\n",
        "\n",
        "3. **AUC-ROC (Area Under the Receiver Operating Characteristic Curve)**: This metric measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). It provides an aggregate measure of performance across all possible classification thresholds. This metric is also robust against imbalanced classes.\n",
        "\n",
        "4. **Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)**: These are popular metrics for regression tasks, which measure the average magnitude of the prediction errors.\n",
        "\n",
        "5. **Cross-Validation**: This is a technique for assessing how well the model will generalize to an independent dataset. The model is trained on a subset of the data and validated on a separate subset. This is typically repeated several times with different subsets.\n",
        "\n",
        "6. **Learning Curves**: These plots can show whether a model is overfitting or underfitting. A learning curve plots the model's performance on the training set and the validation set as a function of the training set size or the training iteration. \n",
        "\n",
        "7. **Model Complexity Graphs**: These graphs show the effect of varying the model complexity: training and validation scores are plotted as a function of model complexity, giving insight into the model's learning and its potential for overfitting or underfitting.\n",
        "\n",
        "It's important to note that the choice of metrics or techniques can depend heavily on the problem at hand, the nature of the data, and the specific type of machine learning model being used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yM-m3iwhql_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computational Learning Theory**\n",
        "\n",
        "* Statistical Learning Theory (SLT): Formal study of learning algorithms.\n",
        "\n",
        "* Computational Learning Theory (CoLT): Formal study of learning tasks.\n",
        "\n",
        "  * PAC Learning (Theory of Learning Problems): PAC learning seeks to quantify the difficulty of a learning task and might be considered the premier sub-field of computational learning theory. Consider that in supervised learning, we are trying to approximate an unknown underlying mapping function from inputs to outputs. We don‚Äôt know what this mapping function looks like, but we suspect it exists, and we have examples of data produced by the function. PAC learning is concerned with how much computational effort is required to find a hypothesis (fit model) that is a close match for the unknown target function.\n",
        "\n",
        "  * VC Dimension (Theory of Learning Algorithms): The VC dimension estimates the capability or capacity of a classification machine learning algorithm for a specific dataset (number and dimensionality of examples). Formally, the VC dimension is the largest number of examples from the training dataset that the space of hypothesis from the algorithm can ‚Äúshatter.‚Äù The Vapnik-Chervonenkis dimension, VC(H), of hypothesis space H defined over instance space X is the size of the largest finite subset of X shattered by H.\n",
        "\n",
        "* https://machinelearningmastery.com/introduction-to-computational-learning-theory/\n",
        "\n",
        "* More:\n",
        "\n",
        "  * [Statistical learning theory, Wikipedia.](https://en.wikipedia.org/wiki/Statistical_learning_theory)\n",
        "  * [Computational learning theory, Wikipedia.](https://en.wikipedia.org/wiki/Computational_learning_theory)\n",
        "  * [Probably approximately correct learning, Wikipedia.](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning)\n",
        "  * [Vapnik‚ÄìChervonenkis theory, Wikipedia.](https://en.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_theory)\n",
        "  * [Vapnik‚ÄìChervonenkis dimension, Wikipedia.](https://en.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_dimension)\n",
        "\n"
      ],
      "metadata": {
        "id": "MaV5A-AMyTkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Vapnik‚ÄìChervonenkis (VC) dimension](https://en.m.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_dimension)\n",
        "\n",
        "* [Neural network capacity](https://en.m.wikipedia.org/wiki/Artificial_neural_network#Capacity): A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity."
      ],
      "metadata": {
        "id": "DU3cn6tJxprr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a relationship between Generalization and capacity (and the max capacity is not necessarily what we're looking for):\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1399.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1397.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1398.png)\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=fDIGmkq9xNE&t=2067s"
      ],
      "metadata": {
        "id": "937MJYN_zNKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1602.png)\n",
        "\n",
        "*Source: [QML Meetup: Dr David Sutter (IBM Research, Zurich), The power of quantum neural networks](https://www.youtube.com/watch?v=ETxQNIR6dAg&t=684s)*"
      ],
      "metadata": {
        "id": "IDnTOjYsYETG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 5: Data Properties bzw. Problems where we have Quantum Advantage*"
      ],
      "metadata": {
        "id": "kYZ-u0aXqvlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://www.ipam.ucla.edu/programs/long-programs/mathematical-and-computational-challenges-in-quantum-computing/?tab=application"
      ],
      "metadata": {
        "id": "gqPzu4RmqL77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Video: [Quantum advantage in learning from experiments](https://www.youtube.com/watch?v=HY7IhKN03Vk&list=WL&index=3)\n",
        "\n",
        "* Paper [Quantum advantage in learning from experiments](https://arxiv.org/abs/2112.00778)\n",
        "\n",
        "* When can a quantum computer be replaced by a dataset?\n",
        "\n",
        "* See also: [The Power of data in quantum machine learning](https://arxiv.org/abs/2011.01938)\n",
        "\n",
        "* What kinds of problems are learnable from a little data?: [Provably efficient machine learning for quantum many-body problems](https://arxiv.org/abs/2106.12627): for interesting problems like learning ground states, in some cases a few pieces of data are sufficient to make interesting predictions about  ground state properties. - Is the fate of quantum computers to provide training data for classical models?\n",
        "\n",
        "* [Quantum Algorithmic Measurement](https://arxiv.org/abs/2101.04634) and [Information-theoretic bounds on quantum advantage in machine learning](https://arxiv.org/abs/2101.02464): quantum computers interfacing with the quantum parts of our world. you can't subsitute that classical computation in the same way that you can perhaps do in other places.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cty3agl-qx23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning problems where quantum learning algorithms can achieve a provable exponential speedup \n",
        "\n",
        "One of the key challenges of the quantum machine learning field is identifying learning problems where quantum learning algorithms can achieve a provable exponential speedup over classical learning algorithms. The currently known examples are all contrived, and all rely on cryptographic methods to make learning hard for a classical learner. However, the broadly shared intuition is that learning separations should in particular be most apparent when the learning task involves data from genuinely quantum sources, which are very different from cryptographic problems. In this talk I will discuss this apparent discrepancy and present a series of new results showing various types of advatnages that can be obtained. In particular, we show how many complex physical systems have associated learning problems which exhibit exponential separations, subject to plausible assumptions in complexity theory, giving a confirmation of the intuition."
      ],
      "metadata": {
        "id": "ck9nCScsCrZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Challenge 6: Neural Networks or Kernel Methods?*"
      ],
      "metadata": {
        "id": "ZJsisvkNIxrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[QML Meetup: Dr Maria Schuld, Taking stock of quantum machine learning - a critical perspective](https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1384s)\n",
        "\n",
        "* Neural networks are composable, efficiently differentiable functions that transform inputs by chains of linear & non-linear operations.\n",
        "\n",
        "* Variational circuits give rise to composable, differentiable functions that map inputs to quantum states and perform linear operations on these states >> sounds much closer to a support vector machine!"
      ],
      "metadata": {
        "id": "y4dZVaHpI5DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1608.png)"
      ],
      "metadata": {
        "id": "ZKajH_4RJ9_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *QML Applications in Natural Sciences*"
      ],
      "metadata": {
        "id": "bIoYM5RYq5BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finance: https://www.mckinsey.com/industries/financial-services/our-insights/how-quantum-computing-could-change-financial-services"
      ],
      "metadata": {
        "id": "_RPhkZrsri3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is quantum data?\n",
        "2. Use cases\n",
        "3. Quantum sensing\n",
        "4. Quantum agent vs classical agent (quantum advantage in learning from experiments)"
      ],
      "metadata": {
        "id": "yNuWNGCTBi8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A review on **classical machine learning methods to characterize quantum systems**, stemmed from a wide collaboration of experts in the field, is now out in its final revision on Nat Rev Phys! Very proud to have been part of the team :)\n",
        "\n",
        "https://arxiv.org/pdf/2207.00298.pdf\n",
        "\n",
        "https://www.nature.com/articles/s42254-022-00552-1"
      ],
      "metadata": {
        "id": "ATMKteAxUvFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications: **Detecting dark matter with quantum computers**\n",
        "\n",
        "* When dark matter particles traverse a strong magnetic field, they may produce photons that Chou and his team can measure with superconducting qubits inside aluminum photon cavities. Because the qubits have been shielded from all other outside disturbances, when scientists detect a disturbance from a photon, they can infer that it was the result of dark matter flying through the protective layers.\n",
        "\n",
        "* ‚ÄúThese disturbances manifest as errors where you didn‚Äôt load any information into the computer, but somehow information appeared, like zeroes that flip into ones from particles flying through the device,‚Äù he said.\n",
        "\n",
        "https://news.fnal.gov/2022/12/detecting-dark-matter-with-quantum-computers/"
      ],
      "metadata": {
        "id": "f_u86pL8IBsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications: **Generative Machine Learning with Classical Data**\n",
        "\n",
        "* Quantum Advantage with QML Models:\n",
        "\n",
        "* GANs (famous for collapsing) and VAE (issue of memorizing data and overfitting)\n",
        "\n",
        "* Probabilistic generative models seems a more prominent candidate for quantum advantage.\n",
        "\n",
        "* https://www.youtube.com/watch?v=uGpbP1tRDyQ&list=WL&index=3&t=465s\n",
        "\n",
        "* https://arxiv.org/abs/1708.09757\n",
        "\n",
        "* problem: lack of general metric (also in classical ML), for example \"inception score\" only works for images\n",
        "\n",
        "* Many of the evaluation techniques, e.g., divergences, favor memorization\n",
        "  - SOA models, e.g., VAE and GAN don't possess tractable likelihood functions.\n",
        "  - Scales badly in high dimensions\n",
        "Obscures distinct mode of failure into a single (uninterpretable) number\n",
        "\n",
        "* Possible solution: https://arxiv.org/abs/2201.08770 Evaluating Generalization in Classical and Quantum Generative Models\n",
        "\n",
        "* https://learnopencv.com/variational-autoencoder-in-tensorflow/\n",
        "\n",
        "* VAE in Astrophysics: We propose to use generative models based on deep neural networks, namely variational autoencoders (VAE), to learn probabilistic models directly from data. We train a VAE on images of centred, isolated galaxies, which we reuse, as a prior, in a second VAE-like neural network in charge of deblending galaxies. https://academic.oup.com/mnras/article/500/1/531/5919458"
      ],
      "metadata": {
        "id": "i03C7gyqLwfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Variational Eigensolver*"
      ],
      "metadata": {
        "id": "rwNM4NMnXEfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational Quantum Eigensolver (VQE) algorithms, like all quantum algorithms, come with their unique set of challenges. Here are some of the main challenges faced when using VQE algorithms:\n",
        "\n",
        "1. **Noise**: VQE algorithms, like all quantum algorithms, are sensitive to noise. This includes noise from imperfect quantum gates, decoherence, and readout errors, among other sources. This noise can lead to errors in the estimated energy eigenvalues. Error mitigation techniques and quantum error correction are still active areas of research.\n",
        "\n",
        "2. **Circuit Depth**: VQE algorithms often require deep circuits, meaning circuits with many quantum gates applied in sequence. The longer the sequence, the more time there is for errors to accumulate, leading to less accurate results. This is particularly a challenge for near-term quantum computers (\"noisy intermediate-scale quantum\" or NISQ devices), which have limited coherence times.\n",
        "\n",
        "3. **Classical Optimization**: VQE is a hybrid quantum-classical algorithm, meaning it involves both a quantum computation (to evaluate the energy of a quantum state) and a classical computation (to optimize the parameters of the quantum state based on the results of the quantum computation). The classical optimization part can be challenging, especially in cases where the optimization landscape is non-convex and includes many local minima. This can cause the optimization routine to get stuck in suboptimal solutions.\n",
        "\n",
        "4. **Ansatz Selection**: The efficiency and effectiveness of VQE are highly dependent on the choice of the ansatz, which is the form of the quantum state that we're optimizing. The ansatz needs to be expressive enough to represent the ground state of the system accurately, but simple enough to be implementable on a quantum computer. Finding the right balance is a non-trivial task.\n",
        "\n",
        "5. **Measurement**: VQE requires the measurement of the expectation value of the Hamiltonian, which often involves decomposing the Hamiltonian into a sum of simpler terms that can be measured independently. This can sometimes require a large number of measurements, especially for systems with many qubits and complex Hamiltonians.\n",
        "\n",
        "6. **Scalability**: As with any quantum algorithm, there's a question of how well VQE can scale with the size of the system it's being used to simulate. The complexity of the Hamiltonian, and thus the depth and complexity of the quantum circuit, often increases rapidly with the size of the system, making it challenging to simulate large systems with VQE.\n",
        "\n",
        "7. **Barren Plateaus**: This is a problem where the landscape of the cost function that you are trying to optimize becomes exceedingly flat as the number of qubits (or circuit depth) increases. This makes it hard for the classical optimizer to find the global minimum because gradients provide almost no useful direction in these \"barren plateau\" regions. \n",
        "\n",
        "Despite these challenges, VQE algorithms are an active area of research due to their promise for performing useful quantum computations with near-term quantum devices. Improving VQE algorithms and developing strategies to mitigate these challenges is a key focus for many researchers in quantum computing.\n"
      ],
      "metadata": {
        "id": "DOBbBkHNjVkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overhead cost of the quantum variational eigensolver (VQE) and quantum phase estimation (QPE) algorithms originates out of many iterations to find the ground state, the lowest eigenvalue. \n",
        "\n",
        "However, a trick published by Artur Izmaylov and co-authors can reduce the computational overhead by lowering the norm of the Hamiltonian. \n",
        "\n",
        "How to increase the efficiency quantum phase estimation for quantum chemistry - lowering the norm of the electronic Hamiltonian by a new symmetry subtraction trick, check out our new submission for details :  \n",
        "\n",
        "https://arxiv.org/abs/2304.13772"
      ],
      "metadata": {
        "id": "bnbNyH3IXH33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Algorithms*"
      ],
      "metadata": {
        "id": "dddUNyIsyJEO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMSa4Pt2dvys"
      },
      "source": [
        "###### *Quantum Fourier Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Space and Position Space with Quantum Fourier Transform \n",
        "\n",
        "https://youtu.be/W8QZ-yxebFA"
      ],
      "metadata": {
        "id": "9wrzQ3norW5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Fourier Transform is the change from one basis (computational) to another (Fourier basis)**\n",
        "\n",
        "* Quantum Fourier Transform is the inverse Discrete Fourier Transform)\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_047.png)\n",
        "\n",
        "**General formula**\n",
        "\n",
        "* Remember: <font color=\"blue\">$e^{2\\pi i}$ = 1</font> (identity operation), and see why $e^{\\pi i}$ = -1 in [this video](https://youtu.be/-AyE1Wpgo3Q) \n",
        "\n",
        "\n",
        "* In QFT we change the <font color=\"blue\">$\\theta$ = phase in $e^{2\\pi i \\theta}$</font> = Eigenvalue of Oracle function $U$ associated with an eigenvector |u‚ü©\n",
        "\n",
        "* The phase $\\theta$ is expressed as: <font color=\"blue\">$\\theta$ = $\\frac{x_n}{2^{k_n}}$</font> with:\n",
        "\n",
        "  * <font color=\"blue\">$x_n$ = 0 or 1</font> state\n",
        "  \n",
        "  * <font color=\"blue\">$k_n$</font> number of Qubits\n",
        "\n",
        "* This is expressed in a so-called \"controlled-R quantum gate\" that **applies a relative phase change to |1>**\n",
        "\n",
        "* The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i \\frac{x_n}{ 2^{k_n}}}\\end{array}\\right)$</font>"
      ],
      "metadata": {
        "id": "I_dkpT8Cdvyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits: Introduction*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1, $k_2$ = 2, $k_3$ = 3</font>\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^1}+\\frac{x_{2}}{2^2}+\\frac{x_{3}}{2^3}\\right)}|1\\rangle\\right)$  = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_1}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 90¬∞ S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 45¬∞ T-rotation of $\\frac{\\pi}{4}$ radians = between 1 and i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_2}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^{k_1}}+\\frac{x_3}{2^{k_2}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^1}+\\frac{x_3}{2^2}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2}+\\frac{x_3}{4}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 90¬∞ S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If both $\\tilde{x_2}$ and $\\tilde{x_3}$ are activated, then it is a 180¬∞ + 90¬∞ = 170¬∞ rotation of $\\pi + \\frac{\\pi}{2}$ radians = -i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_3}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^1}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{\\pi \\mathrm{i}x_3}|1\\rangle\\right)$\n",
        "\n",
        "* If $\\tilde{x_3}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n"
      ],
      "metadata": {
        "id": "xOBGPt6Pdvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)\n",
        "\n",
        "*Here including the 8x8 matrix form for the complete operator:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0801.png)"
      ],
      "metadata": {
        "id": "J8n3fjsedvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 1 Qubit*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1</font>\n",
        "\n",
        "\n",
        "*Linear transformation of a qubit in the computational basis 0 and 1 each separately to the Fourier basis:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0813.png)\n",
        "\n",
        "**Computational Basis in $|0\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 0 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$  $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i} 0}$  = $\\mathrm{e}^{0}$ = 1 (no rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0810.png)\n",
        "\n",
        "**Computational Basis in $|1\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 1 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$ $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}\\right)}$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0811.png)"
      ],
      "metadata": {
        "id": "hBqueHp-dvyu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4Vdy6sdvyv"
      },
      "source": [
        "*Quantum Fourier Transform with 1 Qubit is a Hadamard transform!*\n",
        "\n",
        "**One qubit QFT matrix**: $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & \\mathrm{e}^{\\pi i}\\end{array}\\right)$, where $\\mathrm{e}^{\\pi \\mathrm{i}}$ = -1. So it is: <font color=\"blue\"> QFT f√ºr x=1 = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**Compare with Hadamard transform matrix:** \n",
        "\n",
        "In quantum computing, the Hadamard gate is a one-qubit rotation, mapping the qubitbasis states $|0\\rangle$ and $|1\\rangle$ to two **superposition** states with **equal weight of the computational basis** states $|0\\rangle$ and $|1\\rangle$. Usually the phases are chosen so that\n",
        "\n",
        ">$\n",
        "H=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}\\langle 0|+\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}\\langle 1|\n",
        "$\n",
        "\n",
        "in Dirac notation. This corresponds to the transformation matrix\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "H_{1}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "1 & -1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "in the $|0\\rangle,|1\\rangle$ basis, also known as the computational basis. The states $\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}$ and $\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}$ are known as $|+\\rangle$ and $|-\\rangle$ respectively, and together constitute the polar basis in quantum computing.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_073.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJhiyX4dvyw"
      },
      "source": [
        "**Why Hadamard transform is exactly a 1 qubit Quantum Fourier Transform:** (see result of + for 0 state and - for 1 state) - Matrix-Vector-Multiplication (Single Qubit)\n",
        "\n",
        "> <font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "> <font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_045.png)\n",
        "\n",
        "2 im denominator verschwindet hier. 2^n f√ºr n=1 qubit. mit 2 oben und unten verschwinden beide."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|001\\rangle$*\n",
        "\n",
        "**Computational Basis in $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0804.png)\n",
        "\n",
        "**Fourier Basis for $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0805.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 0</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{0}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$  = <font color=\"blue\">$\\frac{\\pi i}{4}$</font> (45¬∞ T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = <font color=\"blue\">$\\frac{\\pi i}{2}$</font> (90¬∞ S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)"
      ],
      "metadata": {
        "id": "XH-LUgBzdvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is not activated because $x_2$ = 0:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "tAr5mMp3dvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|111\\rangle$*\n",
        "\n",
        "**Computational Basis in $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0802.png)\n",
        "\n",
        "**Fourier Basis for $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0803.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$ = $\\mathrm{e}^{2 \\pi i 0.875} = \\mathrm{e}^{\\pi i 1.75}$ (180¬∞ Z-rotation + 90¬∞ S-rotation + 45¬∞ T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = $e^{\\pi i 1.5} =$ <font color=\"blue\">$-i$</font> (180¬∞ Z-rotation + 90¬∞ S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)"
      ],
      "metadata": {
        "id": "qXRBmM0advy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "HYY1eloedvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Cirq Code for Quantum Fourier Transform*\n",
        "\n",
        "*Compare the code above with the circuit activations below (how a circuits computes the results):* \n",
        "\n",
        "* $H$ gate = bring qubit in superposition. \n",
        "\n",
        "  * *For $x=0$, no further rotation*\n",
        "  \n",
        "  * *For $x=1$, then appy additional *$Z$ gate = 180¬∞ rotation = $\\pi$**\n",
        "\n",
        "* *$S$ gate = 90¬∞ rotation = $\\frac{\\pi}{2}$*\n",
        "\n",
        "* *$T$ gate = 45¬∞ rotation = $\\frac{\\pi}{4}$*\n",
        "\n",
        "$C R_{j}=C Z^{1 / 2^{j-1}}$\n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse) \n",
        "\n",
        "* $S$ entspricht $\\frac{\\pi}{2}$, also wenn qubit 1 = 1, dann bei qubit 0 das $S$ transform anwenden (0,5)\n",
        "\n",
        "  * S: The square root of Z gate, equivalent to cirq.Z ** 0.5\n",
        "\n",
        "  * See: [Cirq Gates](https://quantumai.google/cirq/gates)\n",
        "\n",
        "* $T$ entspricht $\\frac{\\pi}{4}$"
      ],
      "metadata": {
        "id": "lWJ2Oz2Ydvy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q\n",
        "import cirq\n",
        "\n",
        "def make_qft(qubits):\n",
        "\n",
        "    # Generate list of qubits\n",
        "    qreg = list(qubits)\n",
        "    \n",
        "    # Make sure list is longer than 0 qubits:\n",
        "    while len(qreg) > 0:\n",
        "    \n",
        "    # Remove first qubit from list and return its value (set as head-qubit):\n",
        "        q_head = qreg.pop(0)\n",
        "    \n",
        "    # Apply Hadamard superposition to this head-qubit\n",
        "        yield cirq.H(q_head)\n",
        "\n",
        "    # Enumerate through list with i (index position) and corresponding qubit value (0 or 1)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "\n",
        "    # Apply Controlled-Z * Theta-Phase-Shift on target ('q-head') if control-qubit ('qubit') is in state 1\n",
        "            yield (cirq.CZ ** (1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "    # Do the inverse QFT as subroutine in quantum phase estimation\n",
        "    #        yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Use inverse QFT as subroutine in quantum phase estimation\n",
        "# phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Iterating through until \"while len(qreg) = 0\", then processes stops\n",
        "\n",
        "\"\"\"Visually check the QFT circuit.\"\"\"\n",
        "qubits = cirq.LineQubit.range(17)\n",
        "qft = cirq.Circuit(make_qft(qubits))\n",
        "print(qft)"
      ],
      "metadata": {
        "id": "myOgAQXddvy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0815.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "mzHIk-hJdvy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTg0Uyudvy6"
      },
      "source": [
        "*Inverse Quantum Fourier Transform ('QFT Dagger' - Dagger is a complex conjugate operation!)*\n",
        "\n",
        "Reminder of QFT:\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$\n",
        "\n",
        "**Remember: Dagger is a complex conjugate operation!**\n",
        "\n",
        "QFT inverse (see -2 turning i in -i which is a complex conjugate operation):\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$ \n",
        "\n",
        "\n",
        "The operator is then (\n",
        "We have already seen that the Hadamard gate is self-inverse, and the same is clearly true for the SWAP gate; the inverse of the rotations gate $R_k$ is given by):\n",
        "\n",
        "> The matrix form of inverse QFT operator is: <font color=\"blue\">${R^{\\dagger}}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{-2 \\pi i / 2^{k}}\\end{array}\\right)$</font> and compare with QFT operator:  <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "https://www.cl.cam.ac.uk/teaching/1920/QuantComp/Quantum_Computing_Lecture_9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Phase Estimation*"
      ],
      "metadata": {
        "id": "Op-cit2CFEiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNOT Gate and Phase Kickback** \n",
        "\n",
        "<font color=\"blue\">*CNOT-Gate applied to the computational basis 0 and 1*\n",
        "\n",
        "* https://qiskit.org/textbook/ch-gates/phase-kickback.html\n",
        "\n",
        "* Main article about Phase Kickback: https://towardsdatascience.com/quantum-phase-kickback-bb83d976a448\n",
        "\n",
        "The CNOT-gate is a two-qubit gate. Thus, it transforms qubit states whose state we represent by a four-dimensional vector.\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\alpha|0\\rangle|0\\rangle+\\beta|0\\rangle|1\\rangle+\\gamma|1\\rangle|0\\rangle+\\delta|1\\rangle|1\\rangle=\\left[\\begin{array}{c}\n",
        "\\alpha \\\\\n",
        "\\beta \\\\\n",
        "\\gamma \\\\\n",
        "\\delta\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Remember Vector-Vector-Multiplikation (Kronecker / tensor product):\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$. \n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$ \n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Accordingly, the CNOT-gate has a $4 \\times 4$ transformation matrix.\n",
        "\n",
        ">$\n",
        "C N O T=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "There is no effect if the control qubit (at the left-hand position in the Dirac notation) is in state |0‚ü©, as in states |00‚ü© and |01‚ü©.\n",
        "\n",
        "> CNOT $\\cdot|00\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=|00\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|01\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=|01\\rangle$\n",
        "\n",
        "<font color=\"blue\">But if the control qubit is in state |1‚ü©, then the controlled (target) qubit switches from |0‚ü© to |1‚ü© and vice versa.</font>\n",
        "\n",
        "> CNOT $\\cdot|10\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=|11\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|11\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=|10\\rangle$\n",
        "\n",
        "When we describe the quantum states and operations in terms of mathematical formulae, we use the vectors |0‚ü© and |1‚ü© as a basis. |0‚ü© and |1‚ü© denote the standard or computational basis states. These states correspond to the possible measurements we might obtain when looking at the qubit. We measure a qubit in state |0‚ü© as 0 with absolute certainty. And, we measure a qubit in state |1‚ü© as 1, accordingly. While the basis {|0‚ü©,|1‚ü©} is convenient to work with mathematically, it is just a representation of the underlying physics.\n"
      ],
      "metadata": {
        "id": "-0AI_GVFZk0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">*CNOT-Gate applied to the superposition basis + and -*\n",
        "\n",
        "The mathematical basis we chose leads to a specific representation of the CNOT-transformation. But this is not the only possible representation. In fact, there are infinitely many other possible choices. Our qubits are not limited to these two states. Qubits can be in a superposition of both states. For instance, there are the states that result from applying the Hadamard-gate on the basis states:\n",
        "\n",
        "> $|+\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ and $|-\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}}\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "Now apply Hadamard gate on a qubit that is in the |1> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "Mathematically, the following matrix represents the application of Hadamard gates on each of the two qubits.\n",
        "\n",
        "> $H \\otimes H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}H & H \\\\ H & -H\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]$\n",
        "\n",
        "So, if we apply this matrix on two qubits in state |00‚ü©, they end up in state |++‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|00\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |01‚ü© results in state |+‚àí‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|01\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |10‚ü© results in state |‚àí+‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|10\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "Finally, if we apply this transformation on two qubits in state |11‚ü©, we put them into state |‚àí‚àí‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|11\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$"
      ],
      "metadata": {
        "id": "7s9KUHDdV_5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let‚Äôs apply the CNOT-gate on qubits in superposition. We can calculate the overall transformation matrix by multiplying the matrices of the CNOT-gate and the H‚äóH transformation. The CNOT-gate switches the second and fourth columns of the H‚äóH-matrix.\n",
        "\n",
        "> $\\operatorname{CNOT}(H \\otimes H)=\\left[\\begin{array}{cccc}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot \\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right]$\n",
        "\n",
        "* And now, we apply this transformation to the four combinations of basis states.\n",
        "\n",
        "<font color=\"blue\">If the target qubit (at the right-hand side) is in state |1‚ü©, the state of the control qubit (at the left-hand side) flips from |+‚ü© to |‚àí‚ü© and vice versa:\n",
        "\n",
        "> \n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|00\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|01\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|10\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|11\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "In short, we can say:\n",
        "\n",
        "> $\\operatorname{CNOT}(|++\\rangle)=|++\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|+-\\rangle)=|--\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|-+\\rangle)=|-+\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|--\\rangle)=|+-\\rangle$\n",
        "\n",
        "The two states |+‚ü© and |‚àí‚ü© have the same measurement probabilities of |0‚ü© and |1‚ü©. They result in either value with a probability of 0.5. **So, the CNOT-gate does not have any directly measurable implications**. <font color=\"blue\">However, the control qubit switches its phase. It takes on the phase of the controlled (target) qubit.</font>\n",
        "\n",
        "> For the phase of the target qubit is kicked up to the control qubit, we call this phenomenon phase kickback.\n",
        "\n",
        "We learned the CNOT-gate is not a one-sided operation. It clearly has the potential to affect the state of the control qubit. Even though the phase is not directly measurable, there are ways to exploit differences in the phase between states. In fact, prominent algorithms, such as Grover‚Äôs search algorithm, exploit this effect.\n"
      ],
      "metadata": {
        "id": "Tg8PpY31YMh_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwuxaKm4lF2"
      },
      "source": [
        "**Quantum Phase Estimation** \n",
        "\n",
        "* algorithm for determining the eigenvalues of a unitary operator\n",
        "\n",
        "* the [quantum phase estimation algorithm](https://en.m.wikipedia.org/wiki/Quantum_phase_estimation_algorithm) (also referred to as quantum eigenvalue estimation algorithm), is a quantum algorithm to estimate the phase (or eigenvalue) of an eigenvector of a unitary operator. \n",
        "\n",
        "* More precisely, given a unitary matrix $U$ and a quantum state $|\\psi\\rangle$ such that $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$, the algorithm estimates the value of $\\theta$ with high probability within additive error $\\varepsilon$, using $O(\\log (1 / \\varepsilon))$ qubits (without counting the ones used to encode the eigenvector state) and $O(1 / \\varepsilon)$ controlled- $U$ operations. \n",
        "\n",
        "* The algorithm was initially introduced by Alexei Kitaev in 1995.\n",
        "\n",
        "* Phase estimation is frequently used as a subroutine in other quantum algorithms, such as Shor's algorithm and the quantum algorithm for linear systems of equations.\n",
        "\n",
        "<font color=\"blue\">*One Qubit Phase Estimation (with Hadamard Gate):*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_117.png)\n",
        "\n",
        "<font color=\"blue\">*Multi-Qubit Phase Estimation (with inverse Quantum Fourier Transform):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/a/a5/PhaseCircuit-crop.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7UKWz2y03L"
      },
      "source": [
        "Remember in **Quantum Fourier Transform**: \n",
        "\n",
        "\n",
        "> x1 = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ \n",
        "\n",
        "* <font color=\"blue\">$e^{2\\pi i}$ = 1 = identity</font>\n",
        "\n",
        "* In Quantum Fourier Transform we change the phase <font color=\"blue\">$\\theta$ in $e^{2\\pi i}$</font> <font color=\"red\">$^{\\theta}$</font>\n",
        "\n",
        "  * <font color=\"red\">= Eigenvalue of Oracle function $U$ associated with an eigenvector |u‚ü©</font>\n",
        "\n",
        "* Phase <font color=\"blue\">$\\theta$ is $\\frac{x_n}{2^{k}}$ with $x_n$ 0 or 1</font> state and $k$ number of Qubits.\n",
        "\n",
        "* A controlled-R quantum gate applies a relative phase change to |1>. The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "**Now in Phase Estimation**: \n",
        "\n",
        "> In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$ mit dem Operator: $U^{2^n} = \\phi$\n",
        "\n",
        "Quantum phase estimation addresses the following problem:\n",
        "* We have a $n$-qubit oracle function $U$, encoded in the form of a controlled- $U$ unitary.\n",
        "* **$U$ has an eigenvalue $e^{2 \\pi i \\phi}$, associated with an eigenvector $|u\\rangle$ which we can prepare.**\n",
        "* <font color=\"red\">**We wish to estimate the phase, $\\phi$, of the eigenvalue to $t$ bits of precision.**\n",
        "\n",
        "> <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$** $\\quad$ (based on Eigenvalue equation)</font>\n",
        "\n",
        "* Here $|\\psi\\rangle$ is an eigenvector / eigenstate and $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue. \n",
        "\n",
        "* <font color=\"red\">For example: the eigenvalues of X are ‚àí1 and 1 and have the eigenvectors |‚àí‚ü© and |+‚ü© respectively.*</font>\n",
        "\n",
        "*Since $U$ is unitary, all of its eigenvalues have a norm of 1.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-6ThVrEdXJ"
      },
      "source": [
        "Reminder: QFT\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_084.png)\n",
        "\n",
        ">**See below: <font color=\"red\">Remember that a unitary matrix has eigenvalues of the form $e^{i \\theta_{\\psi}}$ (ohne $2 \\pi$ wie oben bei QFT) and that it has eigenvectors that form an orthonormal basis**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_083.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p-zzXyW-tU6"
      },
      "source": [
        "The problem: in both cases the probability is 0,5, just differs by the phase added: $=e^{\\frac{i \\pi}{2}}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_078.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSvCLdgF_YQC"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_079.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rHCBrjo_ZWr"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_080.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scRiAADhBR3t"
      },
      "source": [
        "**The probability of measuring 0 and 1 is each 0,5, but there is a small factor that makes them differ from 0,5, depending on the phase (angle):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0oYz_yZBCoU"
      },
      "source": [
        "> **In the different between the probability of measuring 0 or 1, you've encoded that phase! (In other words: you've taken that phase information and turned it into and amplitude that you can measure.**\n",
        "\n",
        "* How to do this experimentally: you do a million shots of the experiment, collect statistics and check what the statistics say. How many times did I get zero? How many times did I get one? The hope is that the difference between the statistics of zero and one would allow us to back out theta\n",
        "\n",
        "* Next level: now getting more precision with more qubits: (there is another circuit to prepare Psi yet, which is assumed to be given here)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_082.png)\n",
        "\n",
        "writing out the calculation:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_085.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1qf44PHLJJ"
      },
      "source": [
        "**Comparing QPE with QFT (QPE is the same as QFT with a different phase):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_086.png)\n",
        "\n",
        "It's like applying a QFT of something (of a special phase $\\frac{\\theta_{\\psi}}{2^{n}} 2 \\pi$, the green box above!), and in order to get back to the original state you need to apply an inverse QFT at the end:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_087.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 1: Set up the unitary and number of bits to use in phase estimation*\n",
        "\n",
        "<font color=\"blue\">*Let's take as an example the T-gate, and use Quantum Phase Estimation to estimate its phase.*\n",
        "\n",
        "You will remember that the $T$-gate adds a phase of $e^{\\frac{i \\pi}{4}}$ to the state $|1\\rangle$ :\n",
        "\n",
        "$\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$\n",
        "\n",
        "Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$\n",
        "\n",
        "<font color=\"red\">We expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "We first perform a Hadamard gate on the first qubit to get the state \n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "  \n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "  \n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "  * <font color=\"blue\">this part above is the rule from tensor products: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition: $\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "</font>\n",
        "\n",
        "    * The Hadamard states ‚à£+‚ü© and ‚à£‚àí‚ü© are considered superposition states because they are a combination of the two computational states:\n",
        "\n",
        "    * State: $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$ so for + it is: $|\\+\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "  * we have intentionally omitted the normalization factor of 1/‚àö2 for clarity\n",
        "\n",
        "> $|+\\rangle \\otimes|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] \\otimes\\left[\\begin{array}{l}\\psi\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\, [\\psi] \\\\ 1 \\, [\\psi]\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $|+\\rangle$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ =  $\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$"
      ],
      "metadata": {
        "id": "mV4nEZ1l2VMu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zxsy6Cc5Wrx"
      },
      "source": [
        "# Value of Œ∏ which appears in the definition of the unitary U above.\n",
        "# Try different values.\n",
        "theta = 0.125\n",
        "\n",
        "# Define the unitary U-Gate:\n",
        "U = cirq.Z ** (2 * theta)\n",
        "\n",
        "# Accuracy of the estimate for theta. Try different values.\n",
        "n_bits = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8u3NsIlTNsO"
      },
      "source": [
        "Here details about unitary U-Gate: \n",
        "\n",
        "$U$ = $Z^{2^{n-n}}$ \n",
        "\n",
        "Z = $e^{\\pi}$ \n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse) \n",
        "\n",
        "\n",
        "then:\n",
        "\n",
        "> <font color=\"blue\">$U$ = $e^{\\pi * 2^{n-n}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2: Build the first part of the circuit for phase estimation with controlled U-gate (Phase Kickback)*\n",
        "\n",
        "We then perform a controlled U operation, which we have written as $U^{2^0}$. Here applies the **Phase Kickback!**\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+|1\\rangle$ <font color=\"red\">$U$</font> $|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle) \\otimes|\\psi\\rangle$\n",
        "\n",
        "* Here are 2 things very important: \n",
        "  \n",
        "    * The second qubit register containing |œà‚ü© hasn‚Äôt changed. We shouldn‚Äôt expect it to, **since |œà‚ü© is an eigenstate of U (Remember: <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$ based on the Eigenvalue equation**</font>). Thus, no matter how many times we apply U to this register, nothing happens to |œà‚ü©**. But if we apply it more often it will 'amplify' the phase (Not in the sense of amplitude amplification) - we amplify it with adding more qubits and hence more $\\phi$ to get more precision\n",
        "    \n",
        "    * what‚Äôs the point of applying U then? The effect was that **it wrote some information about the eigenvalue into the relative phase of the first qubit**. Namely, the entire effect was to\n",
        "map: $|0\\rangle+|1\\rangle \\mapsto|0\\rangle+e^{2 \\pi i 0. \\phi_{1}}|1\\rangle$"
      ],
      "metadata": {
        "id": "iQE_Unsl2YXo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMG3fjxw5aXU",
        "outputId": "bc0de073-cb3a-4fa4-c04e-009bff44710a"
      },
      "source": [
        "# Get qubits for the phase estimation circuit.\n",
        "qubits = cirq.LineQubit.range(n_bits)\n",
        "u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "# Build the first part of the phase estimation circuit.\n",
        "phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "# Set the input state of the eigenvalue register: Add gate to change initial state to |1>\n",
        "phase_estimator.insert(0, cirq.X(u_bit))\n",
        "\n",
        "# bit = cirq.LineQubit\n",
        "for i, bit in enumerate(qubits):\n",
        "    phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - i - 1)))\n",
        "    # explanation: U-rot control aktiviert wenn entsprechendes qubit in state 1 (??)\n",
        "    # dann aktiviere formel: U^2^(n-1) ...U^2^(n-2) ...U^2^(n-n)\n",
        "\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ\n",
            "1: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ\n",
            "2: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ   ‚îÇ\n",
            "u: ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄZ‚îÄ‚îÄ‚îÄS‚îÄ‚îÄ‚îÄT‚îÄ‚îÄ‚îÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoxXF9-W9orx"
      },
      "source": [
        "> <font color=\"blue\">$U$ = $Z^{2^{n-n}}$ = $e^{\\pi * 2^{n-n}}$ fur das erste Gate: = $e^{\\pi * (-0.128)}$ ????\n",
        "\n",
        "\n",
        "*Why are we adding Pauli-X? The initial state for u_bit is the  state, but the phase for this state is trivial with the operator we chose. Inserting a Pauli  operator at the begining of the circuit changes this to the  state, which has the nontrivial  phase.*\n",
        "\n",
        "*The controlled u gate*:\n",
        "\n",
        "$|00\\rangle \\mapsto|00\\rangle$\n",
        "\n",
        "$|01\\rangle \\mapsto|01\\rangle$\n",
        "\n",
        "$|10\\rangle \\mapsto|1\\rangle \\otimes U|0\\rangle=|1\\rangle \\otimes\\left(u_{00}|0\\rangle+u_{10}|1\\rangle\\right)$\n",
        "\n",
        "$|11\\rangle \\mapsto|1\\rangle \\otimes U|1\\rangle=|1\\rangle \\otimes\\left(u_{01}|0\\rangle+u_{11}|1\\rangle\\right)$\n",
        "\n",
        "The matrix representing the controlled $U$ is\n",
        "\n",
        ">$\n",
        "\\mathrm{C} U=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & u_{00} & u_{01} \\\\\n",
        "0 & 0 & u_{10} & u_{11}\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "**When U is one of the Pauli operators, X,Y, Z, the respective terms \"controlled-X\", \"controlled-Y\", or \"controlled-Z\" are sometimes used**. \n",
        "Sometimes this is shortened to just CX, CY and CZ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti0MmESYOnrd"
      },
      "source": [
        "<font color=\"blue\">*Why should we use more than one control Qubit?*\n",
        "\n",
        "**Remember from Eigenvalue problem: Ax = Œªx in our case with the unitary operator: Ux = Œªx**\n",
        "\n",
        "> $Ux =$ <font color=\"red\">$e^{2œÄi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$\n",
        "\n",
        "> Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2œÄi*0}$ = Œª = 1, so dass Ux = 1x. Damit ist Œª = 1 ist der Eigenwert von f.\n",
        "\n",
        "* Since |Œª| = 1, we can write it without loss of generality as Œª = $e^{2œÄiœÜ}$, where <font color=\"red\">$e^{2œÄi}$ = 1 (= identity, if you insert 2*œÄ*i into exponent at random, you will not change the result. Sometimes it can be a useful identity [Source](https://www.physicsforums.com/threads/e-2-pi-i-where-from.430393/), from Euler identity)</font> and **0 ‚â§ œÜ ‚â§ 1 is called the phase. This is what we want to estimate!**\n",
        "\n",
        "* We saw that in QFT, œÜ being between 0 and 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}$\n",
        "\n",
        "> **The term ‚Äúestimation‚Äù comes about not from the fact that quantum computation is probabilistic, but rather in the degree of precision that we are going to compute, or estimate, the phase to.**\n",
        "\n",
        "* The phase œÜ is going to be between zero and one, so we can write it as a decimal in binary notation as follows: $œÜ = 0.œÜ_1 œÜ_2 ¬∑¬∑¬∑œÜ_n$, where each œÜi is either zero or one\n",
        "\n",
        "  * The expression $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n}$ is equivalent to $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n} \\Longleftrightarrow \\phi=\\sum_{k=1}^{n} \\phi_{k} 2^{-k}$. Some numbers as binary decimals: \n",
        "  \n",
        "    * <font color=\"blue\">The number 0.5 in decimal is 0.1 in binary, since 0.1 ‚â° (1) ¬∑ $2^{‚àí1}$ = 1/2 = 0.5. So: $0.5_{10} = 0.1_2$. Note that 0.1 is the same as 0.100000....</font>\n",
        "\n",
        "    * <font color=\"blue\">The number 0.75 in decimal is 0.11 in binary, since 0.11 ‚â° (1)¬∑$2^{‚àí1}$ +1¬∑$2^{‚àí2}$ = 1/2+1/4 = 3/4 = 0.75. To get this we need 2 Qubits. So we get more precision with more qubits</font>\n",
        "\n",
        "    * 0.111 = 0.875\n",
        "\n",
        "    * 0.1111 = 0.9375 in decimal, because: $0 \\cdot 2^{0}+1 \\cdot 2^{-1}+1 \\cdot 2^{-2}+1 \\cdot 2^{-3}+1 \\cdot 2^{-4}=0 \\cdot 1+1 \\cdot 0.5+1 \\cdot 0.25+1 \\cdot 0.125+1 \\cdot 0.0625=0+0.5+0.25+0.125+0.0625=0.937510$\n",
        "\n",
        "  * Check also what is the value of the infinitely repeating binary decimal 0.1111111...\n",
        "  \n",
        "  * If it needed to be proved, the above exercise proves that 0 ‚â§ 0.œÜ1œÜ2 ¬∑ ¬∑ ¬∑ ‚â§ 1\n",
        "\n",
        "\n",
        "*Operator $U^{2^n}$ in QPE*\n",
        "\n",
        "* $U^{2^0}$: 1 (decimal) = 00001\n",
        "\n",
        "* $U^{2^1}$: 2 (decimal) = 00010\n",
        "\n",
        "* $U^{2^2}$: 4 (decimal) = 00100\n",
        "\n",
        "* $U^{2^3}$: 8 (decimal) = 01000\n",
        "\n",
        "* $U^{2^4}$: 16 (decimal) = 10000\n",
        "\n",
        "\n",
        "**So for falls die Phase 0.111 ist, wuerde bei 3 Qubits QPE berechnen:**\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "  * $2^0$ = 1 in decimal = 001 in binary\n",
        "\n",
        "  * $2^1$ = 2 in decimal = 010 in binary\n",
        "\n",
        "  * $2^2$ = 4 in decimal = 100 in binary\n",
        "\n",
        "* in this case the phase $\\theta$ = 0.111\n",
        "\n",
        "\n",
        "**Compare that with Quantum Fourier Transform:**\n",
        "\n",
        "* In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$ \n",
        "\n",
        "* Let's say all $x_1, x_2$ and $x_3$ = 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{{1}}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i}(0.5+0.25+0.125)}$ and in binary form: <font color=\"red\">$\\mathrm{e}^{2 \\pi \\mathrm{i}(0.100+0.010+0.001)}$</font>\n",
        "\n",
        "* **We see that in QFT and QPE it's the same (both in red)!**\n",
        "\n",
        "<font color=\"red\">Jedes $U^{2^n}$ wird immer dann aktiviert, wenn im Control-Qubit oben eine 1 gemessen wird (siehe Bild hier unten):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzz61dyQB_6X"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_118.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et3XRupiWg0n"
      },
      "source": [
        "*Step 3: Perform the inverse QFT on the estimation qubits and measure them*\n",
        "\n",
        "\n",
        "How can we read out this information from the quantum state? Consider the effect of applying another Hadamard transformation on the first qubit (without another H we will always measure 50/50 % a 0 or 1), which will produce (ignoring the normalization factor of 1/2): \n",
        "\n",
        "  * $H(|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0 \\cdot \\phi_{1}}$</font> $|1\\rangle)=$ $(1+$<font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font>$)|0\\rangle$ + $(1-$<font color=\"red\">$e^{2 \\pi i 0 . \\phi_{1}}$</font>$)|1\\rangle$\n",
        "\n",
        "  * this shares the phase with the first Qubit and allows us to read it out\n",
        "\n",
        "  * Now, $\\phi_{1}$ can only be zero or one. In the case that $\\phi_{1}=0, e^{2 \\pi i 0 . \\phi_{1}}=1$, hence the state is exactly $|0\\rangle$: $(\\frac{1}{2}\\left(1+e^{2 \\pi i 0 . 0}\\right)|0\\rangle+\\frac{1}{2}\\left(1-e^{2 \\pi i 0 . 0}\\right)|1\\rangle$ = $\\frac{1}{2}\\left(1+1\\right)|0\\rangle+\\frac{1}{2}\\left(1-1\\right)|1\\rangle$ = $|0\\rangle$\n",
        "\n",
        "  * these values in front of $|0\\rangle$ and $|1\\rangle$ are probabilities (here 0 has probability of being measured = 1, but small differences her reveal the phase and hence the Eigenvalue in other cases. See here:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)\n",
        "\n",
        "* **For 1 Qubit we can use a Hadamard Gate, and for more than 1 Qubit we use the inverse Fourier Transform**: on Quantum Phase Estimation $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ then the inverse Quantum Fourier transform:  <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font> so that: $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font>\n",
        "\n",
        "  * inverse QFT for 1 Qubit is: $ \\frac{1}{2^{\\frac{1}{2}}} \\sum_{x=0}^{2^{1}-1} e^{\\frac{-2 \\pi i k x}{2^{1}}}|x\\rangle$ = $\\frac{1}{\\sqrt{2}} e^{-1 \\pi i k x}$ fur $k$ = $\\varphi$ = 0 and $x$ = 0. --> somehting is not right here yet!\n",
        "\n",
        "Thus, we measure with certainty (i.e., not probabilistically) a state that tells us exactly what the phase,\n",
        "and hence the eigenvalue, is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv_xQnY__HDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9860e9b5-1f82-4d04-b9a6-696742dfb4d0"
      },
      "source": [
        "def make_qft_inverse(qubits):\n",
        "    \"\"\"Generator for the inverse QFT on a list of qubits.\"\"\"\n",
        "    qreg = list(qubits)[::-1]\n",
        "    while len(qreg) > 0:\n",
        "        q_head = qreg.pop(0)\n",
        "        yield cirq.H(q_head)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "            yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Do the inverse QFT\n",
        "phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "# Add measurements to the end of the circuit\n",
        "phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "0: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM('m')‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ       ‚îÇ         ‚îÇ                        ‚îÇ\n",
            "1: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ@^-0.5‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ             ‚îÇ           ‚îÇ            ‚îÇ\n",
            "2: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@^-0.25‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@^-0.5‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ   ‚îÇ\n",
            "u: ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄZ‚îÄ‚îÄ‚îÄS‚îÄ‚îÄ‚îÄT‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax explanation for list(qubits)[::-1]: list[<start>:<stop>:<step>]\n",
        "# So, when you do a[::-1], it starts from the end towards the first taking each element. \n",
        "# So it reverses a. This is applicable for lists/tuples as well.\n",
        "# Example: >>> a = '1234' >>> a[::-1] will get you: '4321'"
      ],
      "metadata": {
        "id": "4GAmFOoFH6wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 4: Simulate the circuit and convert from measured bit values to estimated Œ∏ values*"
      ],
      "metadata": {
        "id": "oJVEbP4OGB6p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPBFj8JyDYCd",
        "outputId": "91c04675-5f7f-4ad4-eb45-6c639f31a23a"
      },
      "source": [
        "# Simulate the circuit.\n",
        "sim = cirq.Simulator()\n",
        "result = sim.run(phase_estimator, repetitions=10)\n",
        "\n",
        "# Convert from output bitstrings to estimate Œ∏ values.\n",
        "theta_estimates = np.sum(2 ** np.arange(n_bits) * result.measurements['m'], axis=1) / 2**n_bits\n",
        "print(theta_estimates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(theta_estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of trials\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "7kifm9aPIcJd",
        "outputId": "da8725fb-30f2-48fa-8708-a6193bc8b7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iff49oDyK9"
      },
      "source": [
        "def phase_estimation(theta, n_bits, n_reps=10, prepare_eigenstate_gate=cirq.X):\n",
        "    # Define qubit registers.\n",
        "    qubits = cirq.LineQubit.range(n_bits)\n",
        "    u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "    # Define the unitary U.\n",
        "    U = cirq.Z ** (2 * theta)\n",
        "\n",
        "    # Start with Hadamards on every qubit.\n",
        "    phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "    # Do the controlled powers of the unitary U.\n",
        "    for i, bit in enumerate(qubits):\n",
        "        phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - 1 - i)))\n",
        "\n",
        "    # Do the inverse QFT.\n",
        "    phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Add measurements.\n",
        "    phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "\n",
        "    # Gate to choose initial state for the u_bit. Placing X here chooses the |1> state.\n",
        "    phase_estimator.insert(0, prepare_eigenstate_gate.on(u_bit))\n",
        "\n",
        "    # Code to simulate measurements\n",
        "    sim = cirq.Simulator()\n",
        "    result = sim.run(phase_estimator, repetitions=n_reps)\n",
        "\n",
        "    # Convert measurements into estimates of theta\n",
        "    theta_estimates = np.sum(2**np.arange(n_bits)*result.measurements['m'], axis=1)/2**n_bits\n",
        "\n",
        "    return theta_estimates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Analyze convergence vs n_bits.\"\"\"\n",
        "# Set the value of theta. Try different values.\n",
        "theta = 0.123456\n",
        "\n",
        "max_nvals = 16\n",
        "nvals = np.arange(1, max_nvals, step=1)\n",
        "\n",
        "# Get the estimates at each value of n.\n",
        "estimates = []\n",
        "for n in nvals:\n",
        "    estimate = phase_estimation(theta=theta, n_bits=n, n_reps=1)[0]\n",
        "    estimates.append(estimate)\n",
        "\n",
        "print(theta_estimates)\n",
        "print(estimates)\n",
        "\n",
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(nvals, estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of bits\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "id": "N5ZJqQs63tbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 5: Compute the Eigenvalues from the theta value*\n",
        "\n",
        "Eigenvalue: $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue, so for $\\theta$ = 0.125 $\\rightarrow$ $e^{2 * \\pi * i * 0.125}$ = <font color=\"blue\">0.707106781 + 0.707106781 i (Eigenvalue of T-gate)</font>\n",
        "\n",
        "* Verification: $\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$ $\\rightarrow$ <font color=\"blue\">$e^{\\frac{i \\pi}{4}}$ is the same as $e^{2 * \\pi * i * 0.125}$ bzw. $e^{\\frac{2 * \\pi * i}{8}}$</font>\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "* ps: $e^{2 \\pi i}$ =  1 (identity) - ohne theta, die phase\n",
        "\n",
        "* From Eigenvalue euqation: $Ux =$ <font color=\"red\">$e^{2œÄi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$. Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2œÄi*0}$ = Œª = 1, so dass Ux = 1x. Damit ist Œª = 1 ist der Eigenwert von f.\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/9577/how-to-find-eigenvalues-and-eigenvector-for-a-quantum-gate\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_115.png)\n"
      ],
      "metadata": {
        "id": "DvJzmiWfKQi0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiqOEUrSLR_5"
      },
      "source": [
        "###### *Shor's Algorithm*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumai.google/cirq/experiments/shor"
      ],
      "metadata": {
        "id": "1S_wdVn7P6_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Shor-Algorithmus l√§sst sich am besten f√ºr die Primfaktorzerlegung erkl√§ren. Damit Quantencomputer diese Aufgabe meistern k√∂nnen, muss man das Problem allerdings etwas umformulieren. Denn der Quantenalgorithmus st√ºtzt sich auf eine Anleitung zum Faktorisieren von Zahlen, die aus den 1970er Jahren stammt. Damals fand man heraus, dass nur vier Schritte n√∂tig sind, um die Primfaktoren p und q einer Zahl N = p¬∑q zu berechnen.\n",
        "\n",
        "1. W√§hle eine zuf√§llige Zahl a < N.\n",
        "\n",
        "2. Finde die Periodenl√§nge r von a Modulo N.\n",
        "\n",
        "3. Stelle sicher, dass r eine gerade Zahl ist und dass (a^(r/2) + 1) nicht durch N teilbar ist.\n",
        "\n",
        "4. Dann ist p der gr√∂√üte gemeinsame Teiler von (a^(r/2) ‚àí 1) und N. Der andere Primteiler q ist entsprechend der gr√∂√üte gemeinsame Teiler von (a^(r/2) + 1) und N.\n",
        "\n",
        "https://www.spektrum.de/kolumne/shor-algorithmus-wie-quantencomputer-verschluesselungen-knacken/2133048\n"
      ],
      "metadata": {
        "id": "cbvNcG-vOM6M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6yeVZAuzrMk"
      },
      "source": [
        "**Classical Calculation**\n",
        "\n",
        "* <font color=\"blue\">Factoring is equivalent to finding a nontrivial squareroot of 1 mod N.\n",
        "\n",
        "* all we need to do is find this nontrivial squareroot of unity, and we can factor whatever number we need. As promised, we can do this with period finding, specifically by computing the order of a random integer\n",
        "\n",
        "* The order of some integer x modulo N is the smallest integer r such that $x^r$ = 1 mod N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCxHYDSsb6-y"
      },
      "source": [
        "*Modular Arithmetic*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_088.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_089.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axJsmDe1lF5Y"
      },
      "source": [
        "**<font color=\"blue\">Step 1: Pick coprime of N**\n",
        "\n",
        "*Drei m√∂gliche Verfahren zur Berechnung des ggT :*\n",
        "\n",
        "Erstes Verfahren: Euklidischer Algorithmus \n",
        "* 15\t:\t13\t  = \t1\t  Rest  \t2.\t  Also ist ggT (15,13)= ggT (13,2)\n",
        "* 13\t:\t2\t  = \t6\t  Rest  \t1.\t  Also ist ggT (13,2)= ggT (2,1)\n",
        "* 2\t:\t1\t  = \t2\t  Rest  \t0.\t  Also ist ggT (2,1)= ggT (1,0)\n",
        "* Ergebnis: Der ggT von 15 und 13 ist 1.\n",
        "\n",
        "Zweites Verfahren: Vergleichen der Teilermengen .\n",
        "* Die Teilermenge von 15 lautet: {1,3,5,15}.\n",
        "* Die Teilermenge von 13 lautet: {1,13}.\n",
        "* Die gr√∂√üte in beiden Teilermengen vorkommende Zahl ist 1. Also ist 1 der ggT von 15 und 13. \n",
        "\n",
        "Dritte M√∂glichkeit: Vergleichen der Primfaktorzerlegung\n",
        "* Die Primfaktorzerlegung von 15 lautet: 15= 3¬∑5.\n",
        "* Die Primfaktorzerlegung von 13 lautet: 13= 13.\n",
        "* Die gemeinsamen Primfaktoren sind: 1.\n",
        "* Also ist 1 der ggT.\n",
        "\n",
        "*Modulo (kurz: mod) berechnet den Rest einer Division zweier Zahlen. In Mathematischen Formeln wird modulo mit mod abgek√ºrzt, beispielsweise: 23 mod 8 = 7. Bei dieser Rechnung kommt 7 heraus, weil die 8 zweimal in die 23 passt und dann 7 √ºbrig bleiben.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48daYc8to4oz",
        "outputId": "0e23b039-edab-4720-c3ec-36000abf9910"
      },
      "source": [
        "# Product of two prime numbers (to check later if result is correct)\n",
        "N=5 * 3\n",
        "\n",
        "# Pick coprime (!) number to N to factorize N into primes\n",
        "a=13\n",
        "\n",
        "# Code Example to understand periodicity in the context of factoring prime numbers:\n",
        "\n",
        "import math\n",
        "# Compute greated common divisor between a and N\n",
        "math.gcd(a, N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqnmPhvoTQw"
      },
      "source": [
        "**<font color=\"blue\">Step 2: Find the period of $a^r$ $\\equiv$ 1 $(modN)$:**\n",
        "\n",
        "* <font color=\"blue\">the order of x is just the period of the function f(i) = $x^i$ mod N. \n",
        "\n",
        "* <font color=\"blue\">In quantum computing you use QFT in order to determine the period !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "3SoazOyVpdiU",
        "outputId": "90753159-c874-45b6-f12b-f94bf1bcbb72"
      },
      "source": [
        "import matplotlib. pyplot as plotter\n",
        "sns.set(rc={'figure.figsize':(12, 5), \"lines.linewidth\": 1.5})\n",
        "\n",
        "r = list(range(N))\n",
        "y= [a**r0 % N for r0 in r]\n",
        "\n",
        "plotter.plot (r, y)\n",
        "plotter.xlabel('r')\n",
        "plotter.ylabel('Rest:' f'{a}^r (mod{N})')\n",
        "plotter.title('Periode der Restwerte (aus den Multiples von r)')\n",
        "plotter.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qyWTdoZDtL"
      },
      "source": [
        "<font color=\"red\">**Beispiel: Choose any number $a$ and takes its multiple $r$ so many times, until the rest in modulo is 1, (except r=0)**</font>\n",
        "\n",
        "> $13^0$ (mod 15) = 1 (mod 15) = 1\n",
        "\n",
        "> $13^1$ (mod 15) = 13 \n",
        "\n",
        "> $13^2$ (mod 15) = 169 (mod 15) = 4\n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 11 = 165, bis zur 169 verbleibt ein Rest 4*\n",
        "\n",
        "> $13^3$ (mod 15) = 2197 (mod 15) = 7 \n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 146 = 2190, bis zur 2197 verbleibt ein Rest 7*\n",
        "\n",
        "> $13^4$ (mod 15) = 28561 (mod 15) = 1 (<font color=\"blue\"><u>hier started die Periode wieder, that's the r we are looking for!</u>)\n",
        "\n",
        "> usw.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dblciep4qxfM",
        "outputId": "3be061ad-66e6-4c88-8020-117f8c632318"
      },
      "source": [
        "r= r[y[1:].index(1)+1]\n",
        "print(f'r = {r}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HLzUiXj6AR"
      },
      "source": [
        "**<font color=\"blue\">Step 3: Bestimme $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$**. Mindestens einer der beiden Primfaktoren von N={p,q} is beinhalted in gcd(x+1, N) bzw. gcd(x-1, N)\n",
        "\n",
        "*In this case with a=13, N=15 and r=4:*\n",
        "\n",
        "* $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$\n",
        "\n",
        "* $x \\equiv 13^{\\frac{4}{2}}(\\operatorname{mod} 15)$\n",
        "\n",
        "* x = 169 (mod 15) = 4\n",
        "\n",
        "  * gcd(x-1, N) = 3 = p\n",
        "\n",
        "  * gcd(x+1, N) = 5 = q\n",
        "\n",
        "Achtung: in einem anderen Beispiel: N=11*7 (Primzahlen), a=18, ergibt x=43. \n",
        "\n",
        "* Davon x-1=42 und x+1=44. \n",
        "* Das sind naturlich keine Primzahlen, \n",
        "* Aber deren Faktoren sind: 44 = 2 * 2 * 11 und 42 = 2 * 3 * 7\n",
        "* das heisst, x-1 und x+1 kann auch die Primzahlen indirekt enthalten!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8MwnRiUrqhR",
        "outputId": "799b2778-511f-4484-dfa9-ff854e1d871e"
      },
      "source": [
        "if r % 2 == 0:\n",
        "  x = (a**(r/2.)) % N\n",
        "  print(f'x = {x}')\n",
        "  if ((x + 1) % N) != 0:\n",
        "    print(math.gcd((int(x)+1), N), math.gcd((int(x)-1), N))\n",
        "  else:\n",
        "      print (\"x + 1 is 0 (mod N)\")\n",
        "else:\n",
        "  print (f'r = {r} is odd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 4.0\n",
            "5 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rj2GI7_cC86"
      },
      "source": [
        "**Shor's Algorithm**\n",
        "\n",
        "* When finding order using the period finding algorithm, it is important to use enough qubits. A sensible rule is that you need to use m qubits so that $2^m$ >> $N^2$, where N is the number we are trying to factor, because the order of a random number might be as large as N\n",
        "\n",
        "* Example: Lets factor N=119. Suppose we pick the number 16 to start with. Wie viele Qubits m sollten wir mindestens nehmen? $N^2$ = $119^2$ =14.161 und $2^m$ muss deutlich grosser sein, also mindestens = $2^{14}$ = 16.384. Wir brauchen also mindestens 14 Qubits, um 119 zu faktorisieren.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_090.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrrdCnywczMT"
      },
      "source": [
        "* Because we know that the order of x will be even and $x^{s/2}$ will be a nontrivial square root with probability at least 1/2, we can be confident that we will be able to factor N in just a few runs of the algorithm. Because the time it takes to find the period grows as a polynomial in the number of bits, and the number of bits grows like 2logN(by the above requirement), we expect the time it takes to factor N to grow as a polynomial in logN.\n",
        "\n",
        "* Here is the circuit for Shor‚Äôs Algorithm. It relies heavily on period finding, and so the circuit looks a lot like the circuit for period finding. The key difference is that we are finding the period of f(i) = xi, and the number of bits we need to input is very large.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_091.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQthOfm_JfT"
      },
      "source": [
        "**How does it work in the quantum circuit?**\n",
        "\n",
        "That's the function in $U$: given an $x$, the $U$ will compute:\n",
        "\n",
        "> $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_092.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_093.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mApRUwSPydz"
      },
      "source": [
        "**<font color=\"blue\">Shor's Algorithm: Step by Step**\n",
        "\n",
        "**Beispiel: a=13 und N=15, was macht Shor's Algorithm genau im Circuit an der Stelle $U_{f_{(a,N)}}$ und $QFT^{\\dagger}$?**\n",
        "\n",
        "ps: a muss ein Coprime von N sein. Wenn es kein Coprime ist, muessen wir nicht durch Shor's Algorithm gehen, weil a dann einen Faktor mit N teilt :) Aber es ist very unlikely to find a coprime of a large number N.\n",
        "\n",
        "**First let's divide it into steps. 1-5:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrEvCMunLBeC"
      },
      "source": [
        "**Step 1**: Get Qubits in state 0 and apply Hadamard Superposition\n",
        "\n",
        "We start with 4 Qubits all in zeros, mit den Registers x und w, und jedes 4 Mal Tensorproduct multipliziert, weil wir 4 Qubits haben: \n",
        "\n",
        "> $|0\\rangle_{x}^{\\otimes 4}$ $|0\\rangle_{w}^{\\otimes 4}$\n",
        "\n",
        "All Hamadard Gates are applied to top 4 Qubits (x register), and right part (w register) gets nothing applied to it:\n",
        "\n",
        "> $[H^{\\otimes 4}|0\\rangle] \\,\\, |0\\rangle^{\\otimes^{4}}$\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle+|1\\rangle+|2\\rangle+\\cdots+|15\\rangle]$ $|0\\rangle$\n",
        "\n",
        "* Reminder 1: Multiplikation mit $\\frac{1}{4}$, weil 4 Qubits in Hadamard-Superposition\n",
        "\n",
        "* Reminder 2: this is the 4 bit representation of the decimal number, so for example 15 in binary = 1111. Daher kann man auch die 4 angeben als Erinnerung der Bit representation:\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle_4+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $|0\\rangle_4$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKOyXX5bG_i1"
      },
      "source": [
        "**Step 2**: Compute $U$ with $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$ - Was passiert genau in der Box mit $U_{f_{(a,N)}}$?\n",
        "\n",
        "**Given an $x$, the $U$ will compute: <font color=\"red\">$f_{a, N}(x) \\equiv a^{x}(\\bmod N)$</font>**\n",
        "\n",
        "Schauen wir nochmal im vorherigen Schritt und markieren eine Komponente:\n",
        "\n",
        "> = $\\frac{1}{4}[$ <font color=\"red\">$|0\\rangle_4$</font> $+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $\\,$ $|0\\rangle_4$\n",
        "\n",
        "<font color=\"red\">$U_{f_{(a,N)}}$</font> macht dann folgendes:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|  0 \\bigoplus 13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|0 \\bigoplus 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|0 \\bigoplus 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|0 \\bigoplus 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Remember: $\\bigoplus$ means \"addition modular 2\" bzw. \"XOR\". Anything XORs with 0, is thing itself: 0 $\\bigoplus$ Z = Z. damit ergibt sich folgende Rechnung:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|   13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left| 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left| 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left| 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "\n",
        "Aus der Modulo-Rechnung ergeben sich die Restwerte:\n",
        "\n",
        "* <font color=\"red\">$13^{0}(\\bmod 15)$ = 1</font>\n",
        "\n",
        "* $13^{1}(\\bmod 15)$ = 13\n",
        "\n",
        "* $13^{2}(\\bmod 15)$ = 4\n",
        "\n",
        "* $13^{3}(\\bmod 15)$ = 7\n",
        "\n",
        "* $13^{4}(\\bmod 15)$ = 1\n",
        "\n",
        "* usw..\n",
        "\n",
        "Since it's periodic, it will repeat, with the x and w register:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\,\\left|1\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|13\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|4\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|7\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Hier nochmal untereinander mit denselben Restwerten zur besseren Visualisierung:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_094.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-QZylnP7nv"
      },
      "source": [
        "**Step 3: Measurement of the w register / bottom 4 Qubits**\n",
        "\n",
        "* the outputs of the w-register measurements are either 1, 13, 4 or 7 (die Restwerte) with equal probability\n",
        "\n",
        "* let's say we measure 7, what happens to x? X becomes either 3, 7, 11 or 15 (the value in front of the qubit with 7!) with equal probability:\n",
        "\n",
        "  * after $|\\omega\\rangle$ = $|7\\rangle_4$ , $|x\\rangle$ becomes:\n",
        "\n",
        "  * <font color=\"blue\">$|x\\rangle$ $|\\omega\\rangle$ = $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$ $\\otimes |7\\rangle_4$\n",
        "\n",
        "  * Normalization has changed: before we had 16 combinations mit 1/4, here we have only 4 combinations with 1/2 (=one over square root of 4)\n",
        "\n",
        "* **For the next step 4, the Restwert doesn't matter anymore, here: $\\otimes |7\\rangle_4$. We can ignore it. Because it step 4 we apply the measured $|x\\rangle$ in the $QFT^{\\dagger}$, and don't care about $|\\omega\\rangle$ anymore**. And $|x\\rangle$ is in this case: $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exkurs: Eine komplexe Zahl $z=a+b i$ und die zu ihr konjugiert komplexe Zahl $\\bar{z}=a-b i$*:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Komplexe_konjugation.svg/294px-Komplexe_konjugation.svg.png)\n",
        "\n",
        "√Ñndert man das Vorzeichen des Imagin√§rteils $b$ einer komplexen Zahl \n",
        "\n",
        "> $z=a+b \\mathrm{i}$\n",
        "\n",
        "so erh√§lt man die zu $z$ konjugiert komplexe Zahl \n",
        "\n",
        "> $\\bar{z}=a-b \\mathrm{i}$ \n",
        "\n",
        "(manchmal auch $z^{*}$ geschrieben).\n",
        "\n",
        "Die Konjugation $\\mathbb{C} \\rightarrow \\mathbb{C}, z \\mapsto \\bar{z}$ ist ein (involutorischer) K√∂rperautomorphismus, da sie mit Addition und Multiplikation vertr√§glich ist, d. h., f√ºr alle $y, z \\in \\mathbb{C}$ gilt\n",
        "\n",
        ">$\n",
        "\\overline{y+z}=\\bar{y}+\\bar{z}, \\quad \\overline{y \\cdot z}=\\bar{y} \\cdot \\bar{z}\n",
        "$\n",
        "\n",
        "In der Polardarstellung hat die konjugiert komplexe Zahl $\\bar{z}$ bei unver√§ndertem Betrag gerade den negativen Winkel von $z$. \n",
        "\n",
        "* **Man kann die Konjugation in der komplexen Zahlenebene also als die Spiegelung an der reellen Achse interpretieren**. \n",
        "\n",
        "* <font color=\"blue\">**Insbesondere werden unter der Konjugation genau die reellen Zahlen auf sich selbst abgebildet**.\n",
        "\n",
        "Das Produkt aus einer komplexen Zahl $z=a+b$ i und ihrer komplex Konjugierten $\\bar{z}$ ergibt das Quadrat ihres Betrages:\n",
        "\n",
        "> $\n",
        "z \\cdot \\bar{z}=(a+b i)(a-b i)=a^{2}+b^{2}=|z|^{2}\n",
        "$\n",
        "\n",
        "Die komplexen Zahlen bilden damit ein triviales Beispiel einer [C*-Algebra](https://de.m.wikipedia.org/wiki/C*-Algebra)."
      ],
      "metadata": {
        "id": "imQLT4Zc7er9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_7uaW02Wyvg"
      },
      "source": [
        "**Step 4**: Apply inverse $QFT^{\\dagger}$ on the $|x\\rangle$ register\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$ (Reminder!)\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$ (see -2 turning i in -i which is a **complex conjugate operation**)\n",
        "\n",
        "* We want to know what QFT dagger is doing to (it is $\\frac{1}{\\sqrt{16}}$ because we have 4 Qubits)\n",
        "\n",
        "  * $QFT^{\\dagger}|3\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 3 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|7\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 7 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|11\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 11 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|15\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 15 y}{16}}|y\\rangle$\n",
        "\n",
        "Alltogether:\n",
        "\n",
        "  * $QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}} \\sum_{y=0}^{15}$ [ $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$] $|y\\rangle$\n",
        "\n",
        "    * with: $e^{-i\\frac{ 3 \\pi}{8}y}$ = $\\cos \\left(\\frac{3 \\pi}{8} y\\right)-i \\sin \\left(\\frac{3 \\pi}{8} y\\right)$ (und aquivalent fur alle anderen drei)\n",
        "\n",
        "    * siehe coding rechnung unten was genau passiert hier!\n",
        "  \n",
        "  * <font color=\"blue\">$QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}}$ [ $4|0\\rangle_4$ + $4i|4\\rangle_4$ $-4|8\\rangle_4$ $-4i|12\\rangle_4$ ]</font>\n",
        "  \n",
        "  * Remember we had a sum before: $\\frac{1}{{8}} \\sum_{y=0}^{15}$. And notice how all the other terms now vanished to zero, because you had equal contributions of plus and minus.\n",
        "\n",
        "    * **This is exactly what it means when people tell you that quantum computers take advantage of interference!! = when a lot of the terms vanish, and the answer only converges to the terms that we care about.**\n",
        "\n",
        "    * here is the calculation what happened, you see many zeros:\n",
        "\n",
        "<font color=\"red\">Hier Beispielrechnung fur y=1, um vanishing components zu verstehen</font>. Unten im Code die Ergebnisse, zum Beispiel fur y=1 als Ergebnis = 0, $QFT^{\\dagger}|x\\rangle$ fur y = 1: \n",
        "    \n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$ =\n",
        "    \n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}1}$ + $e^{-i\\frac{ 7 \\pi}{8}1}$ + $e^{-i\\frac{ 11 \\pi}{8}1}$ + $e^{-i\\frac{ 15 \\pi}{8}1}$ =\n",
        "\n",
        "    * $e^{-i\\frac{ 3 \\pi}{8}1}$ = <font color=\"green\">0,382683432 - 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 7 \\pi}{8}1}$ = <font color=\"orange\">-0,923879533 - 0,382683432 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 11 \\pi}{8}1}$ = <font color=\"green\">-0,382683432 + 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 15 \\pi}{8}1}$ = <font color=\"orange\">0,923879533 + 0,382683432 i</font>\n",
        "\n",
        "  * Wie man sieht canceln sich die Terme aus (in gleicher Farbe), weshalb als Ergebnis fur y=1 Null entsteht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWn_Jmu_OMSJ",
        "outputId": "20ecb4a6-6407-4ebd-abb1-14bece962df0"
      },
      "source": [
        "# Hier Beispiel fur y=1 und den ersten e-Term:\n",
        "y = 1\n",
        "pi = np.pi\n",
        "coeff = np.exp(-1j*3*pi/8 * y)\n",
        "if abs(coeff) < 1e-10: coeff= 0\n",
        "print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 (0.38268343236508984-0.9238795325112867j)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iERBMPR2y9hm",
        "outputId": "5ffcb615-a899-4d10-f474-8398f25e98f9"
      },
      "source": [
        "# Hier die komplette Rechnung fur alle y und alle 4 e-Terme:\n",
        "import numpy as np\n",
        "\n",
        "pi = np.pi\n",
        "for y in range (15) :\n",
        "  coeff = np.exp(-1j*3*pi/8 * y) + \\\n",
        "          np.exp(-1j*7*pi/8 * y) + \\\n",
        "          np.exp(-1j*11*pi/8* y) + \\\n",
        "          np.exp(-1j*15*pi/8* y)\n",
        "  if abs(coeff) < 1e-10: coeff= 0\n",
        "  print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (4+0j)\n",
            "1 0\n",
            "2 0\n",
            "3 0\n",
            "4 (-5.757077917265737e-15+4j)\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 (-4-1.1514155834531474e-14j)\n",
            "9 0\n",
            "10 0\n",
            "11 0\n",
            "12 (2.2600304269997962e-14-4j)\n",
            "13 0\n",
            "14 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXrcIqo14n0"
      },
      "source": [
        "**Step 5: Measure the |x> register**\n",
        "\n",
        "* You get either 0 or 4 or 8 or 12 with equal probability\n",
        "\n",
        "* Remaining steps are classical post-processing\n",
        "\n",
        "* You can already see the periodicity in the result: the difference is always 4\n",
        "\n",
        "* Analyse what happens for each outcome: **The measurement results peak near $j\\frac{N}{r}$ for same integer j $\\in Z$. And r is the period that we are looking for. N = $2^n$ Qubits!**\n",
        "\n",
        "  * if we measure |4>$_4$: $j\\frac{16}{r}$ = 4, true if j=1 and r=4\n",
        "\n",
        "  * there are multiple values that would work, but this is the lowest one\n",
        "\n",
        "* now check our protocoll for r=4:\n",
        "\n",
        "  * Is r even? yes!\n",
        "\n",
        "  * $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{4 / 2}(\\bmod 15)$ = 4\n",
        "\n",
        "  * x+1 = 5 and x-1 = 3\n",
        "\n",
        "* This looks good, now check:\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(5,15)=5$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "What do you do if r = 8 ?\n",
        "\n",
        "* |8>$_4$: $j\\frac{16}{r}$ = 8, true if j=1 and r=2 AND j=2 and r=4\n",
        "\n",
        "* if r=4 we are back in the case before\n",
        "\n",
        "* if r=2 then $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{2 / 2}(\\bmod 15)$ = 2, which brings x+1 = 3 and x-1 = 1\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(1,15)=1$\n",
        "\n",
        "* This leads you to a partial solution. Now you can back out the other solution, with checking 3 divides into 15\n",
        "\n",
        "* If we get r=0, then we need to do the experiment again\n",
        "\n",
        "Hier die Faktorisierungsergebnisse fur verschiedene QC-Ausgaben r. Mit r=0 geht es nicht, also kann man in 3 von 4 Faellen faktorisieren (und mit r=8 bekommt man eine partial solution, kann aber immer noch faktorisieren).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_097.png)\n",
        "\n",
        "Aus dem 2001 Paper von IBM, Faktorisierung von 15 auf einem Quantum Computer:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_096.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8beuwX8zVd_"
      },
      "source": [
        "**Appendix: What is the Gate structure in $U$?**\n",
        "\n",
        "* $a^{x_1}$, $a^{x_2}$, $a^{x_n}$ tells you this is a controlled operation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_098.png)\n",
        "\n",
        "* look now how the exponent doesn't contain $x_1$, $x_2$, .. $x_n$ anymore\n",
        "\n",
        "* this is done by implementing it by doing these controls\n",
        "\n",
        "* this is exactly like quantum phase estimation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_099.png)\n",
        "\n",
        "**Der linke Term stammt aus QPE, der rechte Term ist der Teil $U$ aus Shor's Algorithms:**\n",
        "\n",
        "> <font color=\"blue\">$U^{2^{x}}=a^{2^{x}}(\\bmod N)$</font>\n",
        "\n",
        "continue: https://youtu.be/IFmkzWF-S2k?t=1181"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0aBln0tpWG"
      },
      "source": [
        "###### *Grover Search*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://web.archive.org/web/20171221161408/http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm\n",
        "\n",
        "https://www.lesswrong.com/posts/5vZD32EynD9n94dhr/configurations-and-amplitude"
      ],
      "metadata": {
        "id": "pLwQheEKWqG1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgjkcQ3gZhD"
      },
      "source": [
        "> **[Grover‚Äôs algorithm](https://en.m.wikipedia.org/wiki/Grover's_algorithm) teaches us how we can search for an item in an unsorted list without needing to look at each item one by one but by looking at them all at once.**\n",
        "\n",
        "It accomplishes that using two techniques:\n",
        "\n",
        "  * First, it uses a quantum oracle to mark the searched state. \n",
        "  \n",
        "  * Second, it uses a diffuser that amplifies the amplitude of the marked state to increase its measurement probability.\n",
        "\n",
        "Grover's Algorithm : Suche in grossen Datenbanken (Squared speedup: get result in the square root of time that on classical computers)\n",
        "\n",
        "* Auf einem klassischen Computer ist der prinzipiell schnellstm√∂gliche Suchalgorithmus in einer unsortierten Datenbank die [lineare Suche](https://de.m.wikipedia.org/wiki/Lineare_Suche), die ${\\mathcal {O}}\\left(N\\right)$ Rechenschritte erfordert (Der Suchaufwand w√§chst linear mit der Anzahl der Elemente in der Liste.)\n",
        "\n",
        "* Die effizientere [Bin√§re Suche](https://de.m.wikipedia.org/wiki/Bin√§re_Suche) kann nur bei geordneten Listen benutzt werden. Die Bin√§re Suche ist deutlich schneller als die lineare Suche, welche allerdings den Vorteil hat, auch in unsortierten Feldern zu funktionieren. In Spezialf√§llen kann die [Interpolationssuche](https://de.m.wikipedia.org/wiki/Interpolationssuche) schneller sein als die bin√§re Suche.\n",
        "\n",
        "* Makes use of Amplitude Amplification, Quantum Walk & Quantum Counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBmtO9AqRfgC"
      },
      "source": [
        "> Grover's Algorithm uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states (=Phase Flip of Desired Outcome + Probability Amplitudes Inversion about the Mean to Amplify)\n",
        "\n",
        "Grover‚Äôs algorithm solves oracles that **add a negative phase to the solution states**. I.e. for any state |x‚ü© in the computational basis:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=\\left\\{\\begin{aligned}|x\\rangle & \\text { if } x \\neq \\omega \\\\-|x\\rangle & \\text { if } x=\\omega \\end{aligned}\\right.$\n",
        "\n",
        "We create a function $f$ that takes a proposed solution $x$, and returns \n",
        "\n",
        "* $f(x)=0$ if $x$ is not a solution ( $x \\neq \\omega)$ \n",
        "\n",
        "* $f(x)=1$ for a valid solution $(x=\\omega)$.\n",
        "\n",
        "The oracle can then be described as:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=(-1)^{f(x)}|x\\rangle$\n",
        "\n",
        "* you can see this is an Eigenvalue equation\n",
        "\n",
        "The oracle's matrix will be a diagonal matrix of the form:\n",
        "\n",
        "> $U_{\\omega}=\\left[\\begin{array}{cccc}(-1)^{f(0)} & 0 & \\cdots & 0 \\\\ 0 & (-1)^{f(1)} & \\cdots & 0 \\\\ \\vdots & 0 & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & (-1)^{f\\left(2^{n}-1\\right)}\\end{array}\\right]$\n",
        "\n",
        "*Source: https://qiskit.org/textbook/ch-algorithms/grover.html*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEw1xzY4XrR"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_105.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9OKn5QL4ZEt"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_106.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9O3gEWD4ayN"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_107.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJDLETO4cKX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_108.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrowEYzZ4Ptk"
      },
      "source": [
        "*Step 1: Hadamard-Operator for Superposition + Assign a Phase -1 to the desired outcome*\n",
        "\n",
        "Start with a balanced superposition, and assign a phase of -1 to the chosen ket, 111). Assigning -1 means applying a Pauli-Z-operator in the superposition to this one !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_102.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_101.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuU4yrjT4NB"
      },
      "source": [
        "**<font color=\"blue\">More about Amplitude Amplification**\n",
        "\n",
        "* [Amplitude amplification](https://en.m.wikipedia.org/wiki/Amplitude_amplification) is a technique in quantum computing which **generalizes the idea behind the Grover's search algorithm**, and gives rise to a family of quantum algorithms.\n",
        "\n",
        "* In a quantum computer, amplitude amplification can be used to **obtain a quadratic speedup over several classical algorithms**.\n",
        "\n",
        "1. If there are $G$ good entries in the database in total, then we can find them by initializing a quantum register $|\\psi\\rangle$ with $n$ qubits where $2^{n}=N$ into a uniform superposition of all the database elements $N$ such that\n",
        "\n",
        ">$| \\psi \\rangle=\\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1}|k\\rangle\n",
        "$\n",
        "\n",
        "2. and running the above algorithm. In this case the overlap of the initial state with the good subspace is equal to the square root of the frequency of the good entries in the database, $\\sin (\\theta)=|P| \\psi\\rangle \\mid=\\sqrt{G / N}$. If $\\sin (\\theta) \\ll 1$, \n",
        "\n",
        "3. we can\n",
        "approximate the number of required iterations as\n",
        "\n",
        ">$\n",
        "n=\\left\\lfloor\\frac{\\pi}{4 \\theta}\\right\\rfloor \\approx\\left\\lfloor\\frac{\\pi}{4 \\sin (\\theta)}\\right\\rfloor=\\left\\lfloor\\frac{\\pi}{4} \\sqrt{\\frac{N}{G}}\\right\\rfloor=O(\\sqrt{N})\n",
        "$\n",
        "\n",
        "Measuring the state will now give one of the good entries with high probability.\n",
        "\n",
        "Since each application of $S_{P}$ requires a single oracle query (assuming that the oracle is implemented as a quantum gate), we can find a good entry with just $O(\\sqrt{N})$ oracle queries, thus obtaining a quadratic speedup over the best possible classical algorithm. (The classical method for searching the database would be to perform the query for every $e \\in\\{0,1, \\ldots, N-1\\}$ until a solution is found, thus costing $O(N)$ queries.) Moreover, we can find all $G$ solutions using $O(\\sqrt{G N})$ queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoynUeh6XX0t"
      },
      "source": [
        "* Now, let‚Äôs say four qubits are enough and Mr. Grover is known as |0010‚ü©. The oracle uses the specific characteristic of this state to identifying it. That is the state has a |1‚ü© at the third position and |0‚ü© otherwise.\n",
        "\n",
        "* Since the quantum oracle takes all qubits as input, it can easily apply a transformation of this exact state. It doesn‚Äôt matter whether we use four qubits or 33. The oracle identifies Mr. Grover in a single turn.\n",
        "\n",
        "* The transformation the oracle applies to the searched state is an inversion of the amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_109.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_110.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkLvP9cX-_N"
      },
      "source": [
        "* In the representation above representation (the dark one) of the amplitudes, we can clearly see a difference between the searched state and all the other states. We could prematurely declare the search is over.\n",
        "\n",
        "* The only difference is in the sign of the amplitude. For the measurement probability results from the amplitude‚Äô absolute square, the sign does not matter at all.\n",
        "\n",
        "* The amplitude originates from the concept that every quantum entity may be described not only as a particle but also as a wave. The main characteristic of a wave is that it goes up and down as it moves. The amplitude is the distance between the center and the crest of the wave.\n",
        "\n",
        "* If we invert the amplitude of a wave at all positions, the result is the same wave shifted by half of its wavelength.\n",
        "\n",
        "* These two waves differ only in their relative position. This is the phase of the wave. For the outside world, the phase of a wave is not observable. Observed individually, the two waves appear identical. So, the problem is we can‚Äôt tell the difference between these two waves.\n",
        "\n",
        "> As a consequence, the system does not appear any different from the outside. Even though the oracle marked the searched state and it, therefore, differs from the other states, all states still have the same measurement probability.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_111.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XWXZNDvSt2X"
      },
      "source": [
        "*Step 2: Invert all probability amplitudes about the mean + Measure*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WSsCfvzaSx-"
      },
      "source": [
        "* We need to turn the difference into something measurable. We need to increase the measurement probability of the marked state. This is the task of the diffuser. The diffuser applies an inversion about the mean amplitude.\n",
        "\n",
        "* Let‚Äôs have a look at the average amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_112.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63cW7pbbQVL"
      },
      "source": [
        "* With four qubits, we have 16 different states. Each state has an amplitude of 1/sqrt(16)=1/4. In fact, each but one state ‚Äî the searched state has this amplitude. The searched state has an amplitude of ‚àí1/4. Thus, the average is (15‚àó1/4‚àí1/4)/16=0.21875.\n",
        "\n",
        "* The average is a little less than the amplitude of all states we did not mark. If we invert these amplitudes by this mean, they end up a little lower than the average at 0.1875.\n",
        "\n",
        "* For the amplitude of the marked state is negative, it is quite far away from the average. The inversion about the mean has a greater effect. It flips the amplitude from ‚àí0.25 by 2‚àó(0.25+0.21875) to 0.6875 (bzw: =0,21875-(-0,25-0,21875)).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_113.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZMnRRIbolE"
      },
      "source": [
        "* The inversion about the mean works well if we search for a single or a few negative amplitudes among many positive amplitudes. Then, this operation increases the negative amplitudes we know are the correct ones. And this operation decreases the positive amplitudes, we know are wrong.\n",
        "\n",
        "> This operation increases the negative amplitude by a large amount while decreasing the positive amplitudes by a small amount.\n",
        "\n",
        "* But the more states we have, the lower the overall effect will be. In our example, we calculated the new amplitude of the searched state as 0.6875. The corresponding measurement probability is 0.6875^2=0.47265625. Accordingly, we measure this system only about every other time in the state we are looking for. Otherwise, we measure it in any other case.\n",
        "\n",
        "* Of course, we could now measure the system many times and see our searched state as the most probable one. But running the algorithm so many times would give away any advantage we gained from not searching all the states.\n",
        "\n",
        "> **Instead, we repeat the algorithm. We use the same oracle to negate the amplitude of the searched state. Then we invert all the amplitudes around the mean, again**.\n",
        "\n",
        "However, we must not repeat this process too many times. There is an optimal number of times of repeating this process to get the greatest chance of measuring the correct answer. \n",
        "\n",
        "* The probability of obtaining the correct result grows until we reach about œÄ/4*sqrt(N) with N is the number of states of the quantum system. Beyond this number, the probability of measuring the correct result decreases again.\n",
        "\n",
        "* In our example with four qubits and N=16 states, the optimum number of iterations is 3.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_104.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuIq27h5ge43"
      },
      "source": [
        "https://towardsdatascience.com/towards-understanding-grovers-search-algorithm-2cdc4e885660"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWV-r4APgANP"
      },
      "source": [
        "*Grover Search: Simple Examples (Z-Gate and I-Gate as amplifiers. with H as Diffuser)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77PyMqZgnEg"
      },
      "source": [
        "**Example 1: Z Gate to switch from 0 to 1**\n",
        "\n",
        "* Let‚Äôs say the state |1‚ü© depicts the favorable state we want to find. Then, the oracle consists of the Z-gate that switches the amplitude when the corresponding qubit is in state |1‚ü©.\n",
        "\n",
        "* As a result, we see the amplitude changed for state |1‚ü©. The qubit is now in state |‚àí‚ü©. Its two states |0‚ü© and |1‚ü© are in two different phases, now.\n",
        "\n",
        "* In other words, we flipped the amplitude of state |1‚ü© from positive to negative.\n",
        "\n",
        "* Both states still have a measurement probability of 0.5. It is the task of the diffuser to magnify the amplitude to favor the searched state.\n",
        "\n",
        "* **The diffuser in a single-qubit circuit is quite simple. It is another H-gate**. This circuit results in state |1‚ü© with absolute certainty.\n",
        "\n",
        "We apply an important sequence on the qubit, the HZH-circuit. This circuit is known as an identity to the NOT-gate (X-gate) that turns state |0‚ü© into |1‚ü© and vice versa.\n",
        "\n",
        "The following equation proves this identity.\n",
        "\n",
        "> $H Z H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=X$\n",
        "\n",
        "Then, why would we use the HZH-sequence? If it is similar to the NOT-gate, why don‚Äôt we use that instead?\n",
        "\n",
        "<font color=\"red\">Simply put, the HZH-sequence is more flexible. It is the simplest form of Grover‚Äôs search algorithm.</font> It starts with all states being equal (the first H-gate). It applies an oracle (Z-gate). \n",
        "\n",
        "And, <font color=\"blue\">**it uses a diffuser that amplifies the amplitude of the selected state |1‚ü© (the second H-gate)**.\n",
        "\n",
        "> While we could rewrite these two circuits more succinctly, the circuit identities of HZH=X and HIH=I let us use the general structure of Grover‚Äôs algorithm. Simply by changing the oracle, we can mark and amplify different states. We don‚Äôt need to come up with a new algorithm for each possible state we want to select out of a list. But we only need to find an appropriate oracle. This ability comes in handy the more states our quantum system has.\n",
        "\n",
        "> The search for one of two possible states does not even deserve to be called a search. But the general structure of Grover‚Äôs algorithm is not different from this very simple example. **It uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bMTt-MVijqL"
      },
      "source": [
        "**Quantum Counting**\n",
        "\n",
        "> **In quantum counting, we simply use the quantum phase estimation algorithm to find an eigenvalue of a Grover search iteration.**\n",
        "\n",
        "* how many solutions exist?\n",
        "\n",
        "* does there any solution exist?\n",
        "\n",
        "\n",
        "The percentage number of solutions in our search space affects the difference between $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$. For example, if there are not many solutions, $|s\\rangle$ will be very close to $\\left|s^{\\prime}\\right\\rangle$ and $\\theta$ will be very small. It turns out that the eigenvalues of the Grover iterator are $e^{\\pm i \\theta}$, and **we can extract this using quantum phase estimation (QPE) to estimate the number of solutions $(M)$**.\n",
        "\n",
        "**First we want to get $\\theta$ from measured_int. (phase estimation)** \n",
        "\n",
        "* You will remember that $\\mathrm{QPE}$ gives us a measured value $=2^{n} \\phi$ from the eigenvalue $e^{2 \\pi i \\phi}$, \n",
        "\n",
        "* so to get $\\theta$ we need to do:\n",
        "\n",
        "> $\n",
        "\\theta=\\text { value } \\times \\frac{2 \\pi}{2^{t}}\n",
        "$\n",
        "\n",
        "**Second, we calculate the inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle:$**\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\cos \\frac{\\theta}{2}\n",
        "$\n",
        "\n",
        "* And that $|s\\rangle$ (a uniform superposition of computational basis states) can be written in terms of $|\\omega\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ as:\n",
        "\n",
        "> $\n",
        "|s\\rangle=\\sqrt{\\frac{M}{N}}|\\omega\\rangle+\\sqrt{\\frac{N-M}{N}}\\left|s^{\\prime}\\right\\rangle\n",
        "$\n",
        "\n",
        "* The inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ is:\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\sqrt{\\frac{N-M}{N}}=\\cos \\frac{\\theta}{2}$\n",
        "\n",
        "* From this, we can use some trigonometry and algebra to show:\n",
        "\n",
        "> $\n",
        "N \\sin ^{2} \\frac{\\theta}{2}=M\n",
        "$\n",
        "\n",
        "**Third, calculate number of solutions**\n",
        "\n",
        "* From the Grover's algorithm chapter, you will remember that a common way to create a diffusion operator, $U_{s}$, is actually to implement $-U_{s}$. \n",
        "\n",
        "* This implementation is used in the Grover iteration provided in this chapter. In a normal Grover search, this phase is global and can be ignored, but now we are controlling our Grover iterations, this phase does have an effect. \n",
        "\n",
        "* The result is that we have effectively searched for the states that are not solutions, and our quantum counting algorithm will tell us how m√¢ny states are not solutions. To fix this, we simply calculate \n",
        "\n",
        "> $N-M$\n",
        "\n",
        "The ability to perform quantum counting efficiently is needed in order to use Grover's search algorithm (because running Grover's search algorithm requires knowing how many solutions exist). Moreover, this algorithm solves the quantum existence problem (namely, deciding whether any solution exists) as a special case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kEC0TTtwdt"
      },
      "source": [
        "###### *Harrow-Hassidim-Lloyd Algorithm (HHL)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/using-quantum-algorithms-to-compute-discrete-logs-bad2c94f6df6\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/quantum-neural-networks-7b5bc469d984\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/hhl-solving-linear-systems-of-equations-with-quantum-computing-efb07eb32f74"
      ],
      "metadata": {
        "id": "If0SFdAZRoiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Solving a system of linear equations with a quantum computer (HHL)**\n",
        "\n",
        "* **Given a matrix $A \\in \\mathbb{C}^{N \\times N}$ and a vector $\\vec{b} \\in \\mathbb{C}^{N}$, find $\\vec{x} \\in \\mathbb{C}^{N}$ satisfying $A \\vec{x}=\\vec{b}$**\n",
        "\n",
        "* The spectrum of $A$ is given by: $A\\left|v_{j}\\right\\rangle=\\lambda_{j}\\left|v_{j}\\right\\rangle, 1 \\geq\\left|\\lambda_{j}\\right| \\geq 1 / \\kappa$\n",
        "\n",
        "\n",
        "**Objective: We want to solve a system of linear equations by finding $\\vec{x}$**\n",
        "\n",
        "* Familiar methods of solutions: Substitution method, Graphical method, Matrix method, Cramer's rule, Gaussian elimination\n",
        "\n",
        "* The classical algorithm returns the full solution, while the HHL can only approximate functions of the solution vector.\n",
        "\n",
        "\n",
        "> $A \\vec{x} = \\vec{b}$\n",
        "\n",
        "Classically you would take the inverse of $A$ (via spectral decomposition / eigendecomposition):\n",
        "\n",
        "> $\\vec{x} = A^{-1} \\vec{b}$\n",
        "\n",
        "The first step towards solving a system of linear equations with a quantum computer is to encode the problem in the quantum language. \n",
        "\n",
        "* By rescaling the system, we can assume $\\vec{b}$ and $\\vec{x}$ to be normalised and map them to the respective quantum states $|b\\rangle$ and $|x\\rangle$. \n",
        "\n",
        "* Usually the mapping used is such that $i^{\\text {th }}$ component of $\\vec{b}$ (resp. $\\vec{x}$ ) corresponds to the amplitude of the $i^{\\text {th }}$ basis state of the quantum state $|b\\rangle$ (resp. $|x\\rangle$ ). \n",
        "\n",
        "From now on, we will focus on the rescaled problem\n",
        "\n",
        "><font color=\"blue\">$A|x\\rangle=|b\\rangle\n",
        "$</font> $\\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        "><font color=\"blue\">$|x\\rangle=A^{-1}|b\\rangle$</font> $\\quad$ (the solution is: $|x\\rangle = \\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle$)\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        " \n",
        ">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        ">$\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$\n",
        "\n",
        "and the right hand side of the system can be written in the eigenbasis of $A$ as\n",
        "\n",
        ">$\n",
        "|b\\rangle=\\sum_{j=0}^{N-1} b_{j}\\left|u_{j}\\right\\rangle, \\quad b_{j} \\in \\mathbb{C}\n",
        "$\n",
        "\n",
        "It is useful to keep in mind that the goal of the HHL is to exit the algorithm with the readout register in the state\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle\n",
        "$\n",
        "\n",
        "Note that here we already have an implicit normalisation constant since we are talking about a quantum state."
      ],
      "metadata": {
        "id": "M6rk5CuVYtO6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij_dB1BOtFEY"
      },
      "source": [
        "**HHL-Algorithm**\n",
        "\n",
        "*Main Subroutines in HHL: Hamiltonian simulation, Phase estimation (newer: linear combination of unitaries) and (Variable-time) amplitude amplification*\n",
        "\n",
        "1. Prepare the initial state $|b\\rangle$. Note that $|b\\rangle=\\sum_{j} c_{j}\\left|v_{j}\\right\\rangle$.\n",
        "\n",
        "2. Use the so-called phase estimation algorithm to perform the map\n",
        "$|b\\rangle \\rightarrow \\sum_{j} c_{j}\\left|v_{j}\\right\\rangle\\left|\\tilde{\\lambda}_{j}\\right\\rangle$\n",
        "\n",
        "* $|\\tilde{\\lambda}_{j}\\rangle$ -> This register contains the eigenvalue estimates.\n",
        "\n",
        "3. Apply a one-qubit conditional rotation to perform the map\n",
        "$|0\\rangle \\rightarrow \\frac{1}{\\kappa \\tilde{\\lambda}_{j}}|0\\rangle+\\sqrt{1-\\frac{1}{\\kappa^{2} \\tilde{\\lambda}_{j}^{2}}}|1\\rangle$\n",
        "\n",
        "4. Undo step 2 - apply the inverse of phase estimation\n",
        "$\\sum_{j} \\frac{c_{j}}{\\kappa \\tilde{\\lambda}_{j}}\\left|v_{j}\\right\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle \\approx \\frac{1}{\\kappa A}|b\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle$\n",
        "\n",
        "5. Use amplitude amplification to get rid of the ‚Äûbad‚Äú part of the state with |1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YObfeUgOQfyO"
      },
      "source": [
        "**Applications of HHL**\n",
        "\n",
        "* Systems of linear equations arise naturally in many real-life applications in a wide range of areas, such as in the solution of Partial Differential Equations, the calibration of financial models, fluid simulation or numerical field calculation. \n",
        "\n",
        "* Used in many quantum machine learning algorithms as a building block\n",
        "\n",
        "\n",
        "* The quantum algorithm for linear systems of equations has been applied to a support vector machine, which is an optimized linear or non-linear binary classifier (https://arxiv.org/abs/1307.0471v2)\n",
        "\n",
        "* for Least-squares fitting (https://arxiv.org/abs/1204.5242)\n",
        "\n",
        "* for finite-element-methods (https://arxiv.org/abs/1512.05903) (but only for higher problems which include solutions with higher-order derivatives and large spatial dimensions. For example, problems in many-body dynamics require the solution of equations containing derivatives on orders scaling with the number of bodies, and some problems in computational finance, such as Black-Scholes models, require large spatial dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfhXMt-vq9J"
      },
      "source": [
        "**Promise**: \n",
        "\n",
        "* Solving 10,000 linear equation: a classical computer needs in best case 10,000 steps. HHL just 13. The [quantum algorithm for linear systems of equations](https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd: Provided the linear system is sparse and has a low condition number $\\kappa_{1}$ and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of $O\\left(\\log (N) \\kappa^{2}\\right)$, where $N$ is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in $O(N \\kappa)$ (or $O(N \\sqrt{\\kappa})$ for positive semidefinite matrices).\n",
        "\n",
        "* Unlike the classical solutions to the Deutsch-Jozsa and search problems, most of our classical methods for matrix manipulation do work in polynomial time. However, as data analysis becomes more and more powerful (and more and more demanding on today‚Äôs computers), the size of these matrices can make even polynomial time too long.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* solution vector is not yielded (rather it prepares a quantum state that is proportional to the solution): Actually reading out the solution vector would take O(N)time, so we can only maintain the logarithmic runtime by sampling the solution vector like ‚ü®x|M|x‚ü©, where M is a quantum-mechanical operator. Therefore, **HHL is useful mainly in applications where only samples from the solution vector are needed**. \n",
        "\n",
        "* Entries of matrix have to be sparse: Additionally, although HHL is exponentially faster than Conjugate Gradient in N, it is polynomially slower in s and ùúÖ, so HHL is restricted to only those matrices that are sparse and have low condition numbers.\n",
        "\n",
        "* Must satisfy robust invertibility (means that entries of matrix must all approx. of same size)\n",
        "\n",
        "* Preparation of input vector is complicated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-28aw9k0W5g"
      },
      "source": [
        "**Sources:**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution\n",
        "\n",
        "* [Peter Witteg: Quantum Machine Learning - 37 - Overview of the HHL Algorithm](https://m.youtube.com/watch?v=hQpdPM-6wtU)\n",
        "\n",
        "* https://qiskit.org/textbook/ch-applications/hhl_tutorial.html\n",
        "\n",
        "* https://github.com/quantumlib/Cirq/blob/master/examples/hhl.py\n",
        "\n",
        "* [Google Quantum: Quantum Algorithms for Systems of Linear Equations (Quantum Summer Symposium 2020)](https://m.youtube.com/watch?v=Xvp56xeNZo4)\n",
        "\n",
        "* https://www.quantamagazine.org/new-quantum-algorithms-finally-crack-nonlinear-equations-20210105/\n",
        "\n",
        "* [A New Approach to Multiplication Opens the Door to Better Quantum Computers](https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/)\n",
        "\n",
        "* https://www.quantamagazine.org/new-algorithm-breaks-speed-limit-for-solving-linear-equations-20210308/\n",
        "\n",
        "* https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/\n",
        "\n",
        "* https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/\n",
        "\n",
        "* https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical result\n",
        "from numpy import linalg as LA\n",
        "w, v = LA.eigh(A)\n",
        "w"
      ],
      "metadata": {
        "id": "rd6dnJ3G6OTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=wrong-or-nonexistent-copyright-notice\n",
        "\"\"\"Demonstrates the algorithm for solving linear systems by Harrow, Hassidim, Lloyd (HHL).\n",
        "\n",
        "The HHL algorithm solves a system of linear equations, specifically equations of the form Ax = b,\n",
        "where A is a Hermitian matrix, b is a known vector, and x is the unknown vector. To solve on a\n",
        "quantum system, b must be rescaled to have magnitude 1, and the equation becomes:\n",
        "\n",
        "|x> = A**-1 |b> / || A**-1 |b> ||\n",
        "\n",
        "The algorithm uses 3 sets of qubits: a single ancilla qubit, a register (to store eigenvalues of\n",
        "A), and memory qubits (to store |b> and |x>). The following are performed in order:\n",
        "1) Quantum phase estimation to extract eigenvalues of A\n",
        "2) Controlled rotations of ancilla qubit\n",
        "3) Uncomputation with inverse quantum phase estimation\n",
        "\n",
        "For details about the algorithm, please refer to papers in the REFERENCE section below. The\n",
        "following description uses variables defined in the HHL paper.\n",
        "\n",
        "This example is an implementation of the HHL algorithm for arbitrary 2x2 Hermitian matrices. The\n",
        "output of the algorithm are the expectation values of Pauli observables of |x>. Note that the\n",
        "accuracy of the result depends on the following factors:\n",
        "* Register size\n",
        "* Choice of parameters C and t\n",
        "\n",
        "The result is perfect if\n",
        "* Each eigenvalue of the matrix is in the form\n",
        "\n",
        "  2œÄ/t * k/N,\n",
        "\n",
        "  where 0‚â§k<N, and N=2^n, where n is the register size. In other words, k is a value that can be\n",
        "  represented exactly by the register.\n",
        "* C ‚â§ 2œÄ/t * 1/N, the smallest eigenvalue that can be stored in the circuit.\n",
        "\n",
        "The result is good if the register size is large enough such that for every pair of eigenvalues,\n",
        "the ratio can be approximated by a pair of possible register values. Let s be the scaling factor\n",
        "from possible register values to eigenvalues. One way to set t is\n",
        "\n",
        "t = 2œÄ/(sN)\n",
        "\n",
        "For arbitrary matrices, because properties of their eigenvalues are typically unknown, parameters C\n",
        "and t are fine-tuned based on their condition number.\n",
        "\n",
        "\n",
        "=== REFERENCE ===\n",
        "Harrow, Aram W. et al. Quantum algorithm for solving linear systems of\n",
        "equations (the HHL paper)\n",
        "https://arxiv.org/abs/0811.3171\n",
        "\n",
        "Coles, Eidenbenz et al. Quantum Algorithm Implementations for Beginners\n",
        "https://arxiv.org/abs/1804.03719\n",
        "\n",
        "=== CIRCUIT ===\n",
        "Example of circuit with 2 register qubits.\n",
        "\n",
        "(0, 0): ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄRy(Œ∏‚ÇÑ)‚îÄRy(Œ∏‚ÇÅ)‚îÄRy(Œ∏‚ÇÇ)‚îÄRy(Œ∏‚ÇÉ)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ\n",
        "                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îê\n",
        "(1, 0): ‚îÄH‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ      ‚îÇ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÇ   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "           ‚îÇ         ‚îÇQFT^-1‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ ‚îÇQFT‚îÇ         ‚îÇ\n",
        "(2, 0): ‚îÄH‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÇ      ‚îÇ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÇ   ‚îÇ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "           ‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îÇ       ‚îÇ\n",
        "(3, 0): ‚îÄ‚îÄ‚îÄe^iAt‚îÄe^2iAt‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄe^-2iAt‚îÄe^-iAt‚îÄ\n",
        "\n",
        "Note: QFT in the above diagram omits swaps, which are included implicitly by\n",
        "reversing qubit order for phase kickbacks.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import sympy\n",
        "import cirq\n",
        "\n",
        "\n",
        "class PhaseEstimation(cirq.Gate):\n",
        "    \"\"\"A gate for Quantum Phase Estimation.\n",
        "\n",
        "    The last qubit stores the eigenvector; all other qubits store the estimated phase,\n",
        "    in big-endian.\n",
        "\n",
        "    Args:\n",
        "        num_qubits: The number of qubits of the unitary.\n",
        "        unitary: The unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        yield cirq.H.on_each(*qubits[:-1])\n",
        "        yield PhaseKickback(self.num_qubits(), self.U)(*qubits)\n",
        "        yield cirq.qft(*qubits[:-1], without_reverse=True) ** -1\n",
        "\n",
        "\n",
        "class HamiltonianSimulation(cirq.EigenGate):\n",
        "    \"\"\"A gate that represents e^iAt.\n",
        "\n",
        "    This EigenGate + np.linalg.eigh() implementation is used here purely for demonstrative\n",
        "    purposes. If a large matrix is used, the circuit should implement actual Hamiltonian\n",
        "    simulation, by using the linear operators framework in Cirq, for example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, A, t, exponent=1.0):\n",
        "        cirq.EigenGate.__init__(self, exponent=exponent)\n",
        "        self.A = A\n",
        "        self.t = t\n",
        "        ws, vs = np.linalg.eigh(A)\n",
        "        self.eigen_components = []\n",
        "        for w, v in zip(ws, vs.T):\n",
        "            theta = w * t / math.pi\n",
        "            P = np.outer(v, np.conj(v))\n",
        "            self.eigen_components.append((theta, P))\n",
        "\n",
        "    def _num_qubits_(self) -> int:\n",
        "        return 1\n",
        "\n",
        "    def _with_exponent(self, exponent):\n",
        "        return HamiltonianSimulation(self.A, self.t, exponent)\n",
        "\n",
        "    def _eigen_components(self):\n",
        "        return self.eigen_components\n",
        "\n",
        "\n",
        "class PhaseKickback(cirq.Gate):\n",
        "    \"\"\"A gate for the phase kickback stage of Quantum Phase Estimation.\n",
        "\n",
        "    It consists of a series of controlled e^iAt gates with the memory qubit as the target and\n",
        "    each register qubit as the control, raised to the power of 2 based on the qubit index.\n",
        "    unitary is the unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        super(PhaseKickback, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        memory = qubits.pop()\n",
        "        for i, qubit in enumerate(qubits):\n",
        "            yield cirq.ControlledGate(self.U ** (2**i))(qubit, memory)\n",
        "\n",
        "\n",
        "class EigenRotation(cirq.Gate):\n",
        "    \"\"\"Perform a rotation on an ancilla equivalent to division by eigenvalues of a matrix.\n",
        "\n",
        "    EigenRotation performs the set of rotation on the ancilla qubit equivalent to division on the\n",
        "    memory register by each eigenvalue of the matrix. The last qubit is the ancilla qubit; all\n",
        "    remaining qubits are the register, assumed to be big-endian.\n",
        "\n",
        "    It consists of a controlled ancilla qubit rotation for each possible value that can be\n",
        "    represented by the register. Each rotation is a Ry gate where the angle is calculated from\n",
        "    the eigenvalue corresponding to the register value, up to a normalization factor C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, C, t):\n",
        "        super(EigenRotation, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.C = C\n",
        "        self.t = t\n",
        "        self.N = 2 ** (num_qubits - 1)\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        for k in range(self.N):\n",
        "            kGate = self._ancilla_rotation(k)\n",
        "\n",
        "            # xor's 1 bits correspond to X gate positions.\n",
        "            xor = k ^ (k - 1)\n",
        "\n",
        "            for q in qubits[-2::-1]:\n",
        "                # Place X gates\n",
        "                if xor % 2 == 1:\n",
        "                    yield cirq.X(q)\n",
        "                xor >>= 1\n",
        "\n",
        "                # Build controlled ancilla rotation\n",
        "                kGate = cirq.ControlledGate(kGate)\n",
        "\n",
        "            yield kGate(*qubits)\n",
        "\n",
        "    def _ancilla_rotation(self, k):\n",
        "        if k == 0:\n",
        "            k = self.N\n",
        "        theta = 2 * math.asin(self.C * self.N * self.t / (2 * math.pi * k))\n",
        "        return cirq.ry(theta)\n",
        "\n",
        "\n",
        "def hhl_circuit(A, C, t, register_size, *input_prep_gates):\n",
        "    \"\"\"Constructs the HHL circuit.\n",
        "\n",
        "    Args:\n",
        "        A: The input Hermitian matrix.\n",
        "        C: Algorithm parameter, see above.\n",
        "        t: Algorithm parameter, see above.\n",
        "        register_size: The size of the eigenvalue register.\n",
        "        *input_prep_gates: A list of gates to be applied to |0> to generate the desired input\n",
        "            state |b>.\n",
        "\n",
        "    Returns:\n",
        "        The HHL circuit. The ancilla measurement has key 'a' and the memory measurement is in key\n",
        "        'm'.  There are two parameters in the circuit, `exponent` and `phase_exponent` corresponding\n",
        "        to a possible rotation  applied before the measurement on the memory with a\n",
        "        `cirq.PhasedXPowGate`.\n",
        "    \"\"\"\n",
        "\n",
        "    ancilla = cirq.LineQubit(0)\n",
        "    # to store eigenvalues of the matrix\n",
        "    register = [cirq.LineQubit(i + 1) for i in range(register_size)]\n",
        "    # to store input and output vectors\n",
        "    memory = cirq.LineQubit(register_size + 1)\n",
        "\n",
        "    c = cirq.Circuit()\n",
        "    hs = HamiltonianSimulation(A, t)\n",
        "    pe = PhaseEstimation(register_size + 1, hs)\n",
        "    c.append([gate(memory) for gate in input_prep_gates])\n",
        "    c.append(\n",
        "        [\n",
        "            pe(*(register + [memory])),\n",
        "            EigenRotation(register_size + 1, C, t)(*(register + [ancilla])),\n",
        "            pe(*(register + [memory])) ** -1,\n",
        "            cirq.measure(ancilla, key='a'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    c.append(\n",
        "        [\n",
        "            cirq.PhasedXPowGate(\n",
        "                exponent=sympy.Symbol('exponent'), phase_exponent=sympy.Symbol('phase_exponent')\n",
        "            )(memory),\n",
        "            cirq.measure(memory, key='m'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "def simulate(circuit):\n",
        "    simulator = cirq.Simulator()\n",
        "\n",
        "    # Cases for measuring X, Y, and Z (respectively) on the memory qubit.\n",
        "    params = [\n",
        "        {'exponent': 0.5, 'phase_exponent': -0.5},\n",
        "        {'exponent': 0.5, 'phase_exponent': 0},\n",
        "        {'exponent': 0, 'phase_exponent': 0},\n",
        "    ]\n",
        "\n",
        "    results = simulator.run_sweep(circuit, params, repetitions=5000)\n",
        "\n",
        "    for label, result in zip(('X', 'Y', 'Z'), list(results)):\n",
        "        # Only select cases where the ancilla is 1.\n",
        "        # TODO: optimize using amplitude amplification algorithm.\n",
        "        # Github issue: https://github.com/quantumlib/Cirq/issues/2216\n",
        "        expectation = 1 - 2 * np.mean(result.measurements['m'][result.measurements['a'] == 1])\n",
        "        print(f'{label} = {expectation}')\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main program loop.\n",
        "\n",
        "    Simulates HHL with matrix input, and outputs Pauli observables of the resulting qubit state |x>.\n",
        "    Expected observables are calculated from the expected solution |x>.\n",
        "    \"\"\"\n",
        "\n",
        "    # Eigendecomposition:\n",
        "    #   (4.537, [-0.971555, -0.0578339+0.229643j])\n",
        "    #   (0.349, [-0.236813, 0.237270-0.942137j])\n",
        "    # |b> = (0.64510-0.47848j, 0.35490-0.47848j)\n",
        "    # |x> = (-0.0662724-0.214548j, 0.784392-0.578192j)\n",
        "    A = np.array(\n",
        "        [\n",
        "            [4.30213466 - 6.01593490e-08j, 0.23531802 + 9.34386156e-01j],\n",
        "            [0.23531882 - 9.34388383e-01j, 0.58386534 + 6.01593489e-08j],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    t = 0.358166 * math.pi\n",
        "    register_size = 4\n",
        "    input_prep_gates = [cirq.rx(1.276359), cirq.rz(1.276359)]\n",
        "    expected = (0.144130, 0.413217, -0.899154)\n",
        "\n",
        "    # Set C to be the smallest eigenvalue that can be represented by the\n",
        "    # circuit.\n",
        "    C = 2 * math.pi / (2**register_size * t)\n",
        "\n",
        "    # Simulate circuit.\n",
        "    print(\"Expected observable outputs:\")\n",
        "    print(\"X =\", expected[0])\n",
        "    print(\"Y =\", expected[1])\n",
        "    print(\"Z =\", expected[2])\n",
        "    print(\"Actual: \")\n",
        "    simulate(hhl_circuit(A, C, t, register_size, *input_prep_gates))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "FWww_qXA6LeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hidden Subgroup Problem*"
      ],
      "metadata": {
        "id": "kY-gbnwtW_Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hidden subgroup problem**\n",
        "\n",
        "The dihedral hidden subgroup problem https://arxiv.org/abs/2106.09907\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem"
      ],
      "metadata": {
        "id": "K5mtRvKj1f07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Examples of HSP over non-Abelian groups](https://www.youtube.com/watch?v=D-pfWjGqENg)\n",
        "\n",
        "* We describe the HSP over symmetric and dihedral groups and their interesting applications.\n",
        "\n",
        "* example 2 graphs A and B: it's not sufficient if they have same number of edges and vertices\n",
        "\n",
        "* it's also important to know if they have the same connectivity (ismorphic): edge 5 and 10 in graph A are connected as well as edge 5 and 10 in graph B\n",
        "\n",
        "* compelixity of graph ismorphism is NP: if you have the solution, it's easy to check if it's true.\n",
        "\n",
        "* Graph automorpphism: you relabel the graph and it looks the same (non trivial besides identity map)\n"
      ],
      "metadata": {
        "id": "idQjSalFVKtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hidden Subgroup Problem](https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem) (HSP) generalizes many problems on which quantum computers offer a potential exponential speed-up over classical computers and derives its significance from two important and related questions. First, the study of the problem sheds some light on what it is that quantum computers are really good at. Second, the ability to solve HSP for non-Abelian groups in polynomial time would enable us to extend the set of problems on which quantum computers offer substantial performance advantage. In this introductory talk, I will recap essential concepts from group theory before defining the Hidden Subgroup Problem and describing the quantum algorithm that solves it quickly. I will end by showing how to express a number of well-known problems, such as Simon's problem, discrete logarithm and graph isomorphism, as instances of HSP.\n"
      ],
      "metadata": {
        "id": "X1R28Zo3W95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hidden Subgroup Problem (HSP) is a computational problem in the field of quantum computing and cryptography. It is a generalization of the well-known problem called the Discrete Logarithm Problem (DLP) and the Integer Factorization Problem (IFP), which are both important in modern cryptography.\n",
        "\n",
        "In the Hidden Subgroup Problem, the input is a group G and a function f: G -> X, where G is a finite group and X is a finite set. The goal is to find a subgroup H of G such that f(x) is constant for all elements x in the same coset of H. In simpler terms, the problem involves finding a hidden structure within a group based on the information provided by the function.\n",
        "\n",
        "The Hidden Subgroup Problem has important implications in cryptography because certain mathematical problems, such as factoring large numbers, can be reduced to it. If an efficient quantum algorithm were developed to solve the Hidden Subgroup Problem, it would have significant consequences for public-key cryptosystems like RSA and elliptic curve cryptography, which rely on the difficulty of factoring large numbers or solving the DLP.\n",
        "\n",
        "Efforts have been made to find efficient quantum algorithms for solving the Hidden Subgroup Problem, particularly using quantum Fourier transform-based algorithms like Shor's algorithm. However, no general efficient quantum algorithm has been discovered yet for arbitrary groups, although there are some specific cases where efficient algorithms have been developed.\n",
        "\n",
        "The study of the Hidden Subgroup Problem and its related algorithms is an active area of research in quantum computing and cryptography, with the aim of better understanding the limits and possibilities of quantum computing's impact on cryptographic systems."
      ],
      "metadata": {
        "id": "Se1miG3Q2qH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the challenge in the hidden subgroup problem? \n",
        "\n",
        "The main challenge in the Hidden Subgroup Problem (HSP) is to efficiently find the hidden subgroup within a given group based on the information provided by a function. The difficulty lies in identifying the underlying structure or pattern that the function reveals about the subgroup.\n",
        "\n",
        "One of the key aspects that makes the HSP challenging is that the function only provides partial information about the subgroup. The function maps elements of the group to a set of values, but it does not directly reveal the subgroup itself. Instead, the function is designed in such a way that it distinguishes elements of different cosets of the hidden subgroup while keeping elements within the same coset indistinguishable. This limited information makes it difficult to directly extract the subgroup.\n",
        "\n",
        "Another challenge is that the HSP encompasses a wide range of group structures. While some specific cases of the problem have been efficiently solved using quantum algorithms, finding a general solution that works for arbitrary groups remains an open question. The difficulty varies depending on the properties and structure of the group involved.\n",
        "\n",
        "Additionally, the HSP is of particular interest in the field of quantum computing because it has the potential to break certain cryptographic protocols. Efficiently solving the HSP for specific groups could lead to the development of quantum algorithms that can break widely used encryption schemes based on problems like factoring large numbers or solving the Discrete Logarithm Problem.\n",
        "\n",
        "Despite efforts to develop quantum algorithms for the HSP, no general solution has been found yet. The challenge lies in understanding the underlying mathematical structures, devising efficient algorithms, and leveraging the power of quantum computation to solve the problem in a scalable and practical manner."
      ],
      "metadata": {
        "id": "r6qrUX10Wh-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you have a concrete example for a hidden subgroup?\n",
        "\n",
        "\n",
        "Certainly! One of the well-known examples of the Hidden Subgroup Problem involves the symmetric group, denoted as S<sub>n</sub>. The symmetric group consists of all possible permutations of n elements. In this case, the hidden subgroup problem involves finding the hidden subgroup based on the information provided by a function.\n",
        "\n",
        "Let's consider an example with n = 3, where the elements of the symmetric group S<sub>3</sub> are permutations of three objects, let's say {1, 2, 3}. The hidden subgroup problem would involve finding a hidden subgroup within S<sub>3</sub> based on the function f: S<sub>3</sub> -> X, where X is some set.\n",
        "\n",
        "In this scenario, the function f could be defined as follows:\n",
        "f(1, 2, 3) = 'A'\n",
        "f(1, 3, 2) = 'A'\n",
        "f(2, 1, 3) = 'B'\n",
        "f(2, 3, 1) = 'B'\n",
        "f(3, 1, 2) = 'C'\n",
        "f(3, 2, 1) = 'C'\n",
        "\n",
        "Here, the function assigns labels 'A', 'B', and 'C' to certain permutations in S<sub>3</sub>. The goal of the Hidden Subgroup Problem would be to determine the hidden subgroup, which, in this case, corresponds to the permutations that are labeled with the same letter.\n",
        "\n",
        "In this particular example, the hidden subgroup consists of the identity permutation (1, 2, 3), the permutation (2, 1, 3), and the permutation (3, 2, 1). The function f labels these permutations with the letter 'A'. So, the hidden subgroup in this case is { (1, 2, 3), (2, 1, 3), (3, 2, 1) }.\n",
        "\n",
        "The challenge is to find the hidden subgroup based on the given function, which may involve clever algorithms and techniques to deduce the structure of the subgroup from the function's output."
      ],
      "metadata": {
        "id": "8XCw9wUqWk3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would be a potential algorithm in this case?\n",
        "\n",
        "In the specific case of the Hidden Subgroup Problem (HSP) for the symmetric group S<sub>3</sub> with the given function f: S<sub>3</sub> -> X, there is a known efficient algorithm called the Coset Sampling Algorithm. This algorithm allows us to find the hidden subgroup by exploiting the structure of the symmetric group.\n",
        "\n",
        "The Coset Sampling Algorithm for S<sub>3</sub> proceeds as follows:\n",
        "\n",
        "1. Start with an initial candidate subgroup H = {identity permutation}.\n",
        "2. For each element g in S<sub>3</sub> - H:\n",
        "   a. Compute the coset gH.\n",
        "   b. Apply the function f to each element in the coset gH and record the results.\n",
        "3. Analyze the recorded results to determine if there is a consistent pattern.\n",
        "   a. If the results show that all elements in a coset map to the same value, update H to include the elements of that coset.\n",
        "   b. If the results do not exhibit a consistent pattern, return to step 2 with the next element in S<sub>3</sub> - H.\n",
        "4. Repeat steps 2 and 3 until no new cosets are added to H.\n",
        "5. The final H obtained is the hidden subgroup.\n",
        "\n",
        "In the case of S<sub>3</sub>, the Coset Sampling Algorithm will correctly identify the hidden subgroup { (1, 2, 3), (2, 1, 3), (3, 2, 1) } based on the given function f.\n",
        "\n",
        "It's important to note that the Coset Sampling Algorithm is specific to the symmetric group S<sub>3</sub> and may not generalize to other groups. The Hidden Subgroup Problem for other groups often requires different algorithms and techniques tailored to the specific group's structure."
      ],
      "metadata": {
        "id": "Y38g40y_WosB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you write this as a python function?\n",
        "\n",
        "Certainly! Here's an example of a Python function that implements the Coset Sampling Algorithm for the Hidden Subgroup Problem in the case of the symmetric group S<sub>3</sub>:\n",
        "\n",
        "In this example, the hidden_subgroup_algorithm_S3 function implements the Coset Sampling Algorithm for S<sub>3</sub>. The f parameter represents the given function that maps permutations to certain values. The algorithm iterates through the elements of S<sub>3</sub> and updates the candidate subgroup H based on the function's output until no new cosets are added to H. Finally, the function returns the discovered hidden subgroup.\n",
        "\n",
        "You can run this code to find the hidden subgroup in the provided example function example_function. Feel free to modify the function example_function or provide your own function f to experiment with different scenarios."
      ],
      "metadata": {
        "id": "nixfuFWvWrEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "\n",
        "def hidden_subgroup_algorithm_S3(f):\n",
        "    n = 3  # Symmetric group S3\n",
        "    group = list(permutations(range(1, n+1)))  # Generate all permutations of n elements\n",
        "    H = [(1, 2, 3)]  # Initial candidate subgroup H\n",
        "\n",
        "    while True:\n",
        "        updated = False\n",
        "        for g in group:\n",
        "            if g in H:\n",
        "                continue\n",
        "\n",
        "            coset = [tuple([g[i] for i in perm]) for perm in H]  # Compute the coset gH\n",
        "            results = [f(perm) for perm in coset]  # Apply the function f to each element in the coset\n",
        "\n",
        "            if all(result == results[0] for result in results):  # Check if all elements in the coset map to the same value\n",
        "                H.extend(coset)  # Update H to include the elements of the coset\n",
        "                updated = True\n",
        "                break\n",
        "\n",
        "        if not updated:\n",
        "            break\n",
        "\n",
        "    return H\n",
        "\n",
        "# Example function f for S3\n",
        "def example_function(perm):\n",
        "    if perm == (1, 2, 3) or perm == (2, 1, 3) or perm == (3, 2, 1):\n",
        "        return 'A'\n",
        "    elif perm == (1, 3, 2) or perm == (3, 1, 2):\n",
        "        return 'B'\n",
        "    else:\n",
        "        return 'C'\n",
        "\n",
        "# Usage example\n",
        "hidden_subgroup = hidden_subgroup_algorithm_S3(example_function)\n",
        "print(\"Hidden subgroup:\", hidden_subgroup)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "ZXfjqs1WWxP_",
        "outputId": "e427cf00-71ac-4838-d27e-df3e583c5959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2b4788abb065>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mhidden_subgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_subgroup_algorithm_S3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hidden subgroup:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_subgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2b4788abb065>\u001b[0m in \u001b[0;36mhidden_subgroup_algorithm_S3\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mcoset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Compute the coset gH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoset\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Apply the function f to each element in the coset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2b4788abb065>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mcoset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Compute the coset gH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoset\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Apply the function f to each element in the coset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2b4788abb065>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mcoset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Compute the coset gH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoset\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Apply the function f to each element in the coset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Boson Sampling*"
      ],
      "metadata": {
        "id": "5iVvtH_jA15X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boson Sampling is a computational problem related to quantum computing. It was proposed by Scott Aaronson and Alex Arkhipov in 2011 and is intended to demonstrate the potential power of quantum computers over classical ones.\n",
        "\n",
        "The basic problem is this: Given a linear optical network (a device that takes in multiple photons, passes them through a series of beam splitters and phase shifters, and then outputs them), where each photon is initially sent into a different input mode, the task is to sample from the probability distribution of photon count outcomes at the network's outputs. \n",
        "\n",
        "In more detail, you start with an arrangement of beam splitters and phase shifters that form a linear optical network. You inject a number of indistinguishable single photons into this network. The photons then interact with the network and with each other due to quantum interference and finally are detected at the output. The task is to compute the probabilities of different detection patterns.\n",
        "\n",
        "Boson Sampling is believed to be hard for classical computers, yet could be solved efficiently with a relatively rudimentary type of quantum computer - one that can prepare certain states of multiple photons and feed them into a linear optics network. \n",
        "\n",
        "The importance of Boson Sampling lies in its implications for the question of whether quantum computers can do things that classical computers cannot. If it's proven that Boson Sampling can't be efficiently simulated on a classical computer (which is believed to be the case but hasn't been formally proven as of my knowledge cutoff in September 2021), then this would be strong evidence of the power of quantum computation. \n",
        "\n",
        "Furthermore, Boson Sampling experiments can be performed with current technology, and in fact, several experimental groups have demonstrated small-scale Boson Sampling. A significant milestone was reached in 2020 when a team from the University of Science and Technology of China reported results from a 50-photon Boson Sampling experiment, which would be infeasible to simulate exactly with a classical computer. However, the demonstration of \"quantum supremacy\" via Boson Sampling remains a topic of active debate and research.\n"
      ],
      "metadata": {
        "id": "v4KP_9oz2J9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Boson_sampling\n",
        "\n",
        "Moreover, while not universal, the boson sampling scheme is strongly believed to implement computing tasks which are hard to implement with classical computers by using far fewer physical resources than a full linear-optical quantum computing setup. This advantage makes it an ideal candidate for demonstrating the power of quantum computation in the near term."
      ],
      "metadata": {
        "id": "KPeh9pIDyeyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Problem**\n",
        "\n",
        "https://arxiv.org/abs/2111.03011\n",
        "\n",
        "https://en.wikipedia.org/wiki/Boson_sampling"
      ],
      "metadata": {
        "id": "remd7NZ3sK0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum-Inspired Algorithms*"
      ],
      "metadata": {
        "id": "IcQxYCTe27bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum-inspired Algorithms**\n",
        "\n",
        "* quantum computers still exhibit an exponential speedup over all known classical algorithms for sparse, full-rank matrix problems, including the quantum Fourier transform, eigenvector and eigenvalue analysis, linear systems, and others https://arxiv.org/abs/1905.10415\n",
        "* Quantum algorithms for linear algebra are a flagship application of quantum computing, partic- ularly due to their relevance in machine learning. These algorithms typically [scale polylogarithmically](https://en.wikipedia.org/wiki/Polylogarithmic_function) with dimension, which, at the time they were reported, implied an asymptotic exponential speedup compared to state-of-the-art classical methods. For this reason, significant interest has been generated in the dequantization approach that led to breakthrough quantum- inspired classical algorithms for linear algebra problems with sublinear complexity https://arxiv.org/abs/1905.10415\n",
        "\n",
        "* https://crypto.stackexchange.com/questions/48638/whats-the-difference-between-polylogarithmic-and-logarithmic\n",
        "\n",
        "* For example, matrix chain ordering can be solved in polylogarithmic time on a Parallel Random Access Machine."
      ],
      "metadata": {
        "id": "mUzaGhCi1Y26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can the Quantum Approximate Optimization Algorithm (QAOA) be translated into a quantum-inspired version to solve combinatorial optimization problems?**\n",
        "\n",
        "\n",
        "The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm, and as such it is inherently designed to run on quantum computers. It uses principles of quantum mechanics, such as superposition and interference, to find approximate solutions to combinatorial optimization problems. \n",
        "\n",
        "However, the key idea behind QAOA can be translated into a classical, or \"quantum-inspired\", algorithm. The central concept of QAOA is the idea of using a quantum system to explore a large solution space more effectively than classical algorithms. This concept can be mimicked on a classical computer, albeit with some limitations.\n",
        "\n",
        "Here is a simplified overview of how the quantum-inspired version of QAOA might work:\n",
        "\n",
        "1. **Initial State:** Similar to QAOA, a quantum-inspired algorithm would start with an initial state that is a superposition of all possible solutions. On a classical computer, this might be represented as a probability distribution over all possible solutions.\n",
        "\n",
        "2. **Cost and Mixer Functions:** QAOA uses two types of quantum operations (unitary transformations) in its procedure: a cost function and a mixer function. The cost function encodes the problem's objective, and the mixer function helps to explore the solution space. In a quantum-inspired algorithm, these could be represented as operations on the probability distribution that adjust the probabilities based on the cost function and allow exploration of the solution space.\n",
        "\n",
        "3. **Iterative Process:** Like QAOA, a quantum-inspired algorithm would iteratively apply the cost and mixer functions. This iterative process is intended to gradually \"steer\" the system towards better solutions.\n",
        "\n",
        "4. **Measurement:** Finally, in QAOA, a measurement is performed on the quantum state to obtain a solution. In a quantum-inspired algorithm, a solution might be selected based on the final probability distribution.\n",
        "\n",
        "It should be noted that while quantum-inspired algorithms can provide speed-ups for certain problems, they do not offer the same potential exponential speed-ups that true quantum algorithms might provide on quantum computers. The simulation of quantum behaviors on classical computers is inherently limited by the classical computational resources, and the quantum-inspired algorithms typically run in exponential time complexity. However, they may still offer advantages over traditional classical algorithms for certain tasks.\n"
      ],
      "metadata": {
        "id": "EOHWd4H_2-iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How are tensor networks applied in quantum-inspired algorithms?** \n",
        "\n",
        "Tensor networks are a mathematical tool used in many areas of physics, including quantum mechanics and quantum computing. They are particularly useful for modeling quantum systems with many particles, which are generally hard to simulate on classical computers due to the exponential growth in complexity. \n",
        "\n",
        "In a tensor network, each tensor is a mathematical object that can represent quantum states, and the network captures the relationships and interactions between these states. When used in quantum-inspired algorithms, tensor networks allow for the efficient representation and manipulation of quantum states, which can lead to significant speedup in the solution of certain problems.\n",
        "\n",
        "Here are a few ways tensor networks are applied in quantum-inspired algorithms:\n",
        "\n",
        "1. **Quantum Simulation:** Tensor networks can be used to simulate the behavior of quantum systems. This is often used in condensed matter physics but can also be applied to quantum computing. By using tensor networks, we can represent the states of a quantum system in a way that allows us to perform calculations more efficiently than we would be able to with a naive representation.\n",
        "\n",
        "2. **Machine Learning and Data Compression:** Tensor networks can be used to compress high-dimensional data into a lower-dimensional form, without losing too much information. This is particularly useful in machine learning, where high-dimensional data is the norm. Quantum-inspired machine learning algorithms can leverage this property to handle large datasets efficiently.\n",
        "\n",
        "3. **Quantum Circuit Simulation:** Tensor networks can be used to simulate quantum circuits, which is a crucial component of quantum computing. By using tensor networks, we can simulate the operation of a quantum circuit more efficiently, especially when dealing with large, complex circuits.\n",
        "\n",
        "4. **Solving Optimization Problems:** Tensor networks can also be used to solve optimization problems. By mapping the problem onto a tensor network, we can use techniques like the Density Matrix Renormalization Group (DMRG) to find the optimal solution. This is particularly useful for problems where the number of variables is large, as is often the case in optimization problems.\n",
        "\n",
        "5. **Quantum Error Correction:** Tensor networks have also been used in the study and implementation of quantum error correction codes. These codes are crucial for dealing with the inherent noise and instability in quantum systems.\n",
        "\n",
        "In all these cases, the key advantage of tensor networks is their ability to represent and manipulate complex, high-dimensional quantum states in a computationally efficient manner. However, it's important to note that while tensor networks provide a powerful tool for quantum-inspired algorithms, they are not a magic bullet. They work best for certain classes of problems and quantum states, particularly those with low entanglement.\n"
      ],
      "metadata": {
        "id": "f9bCFWsI3f-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Algorithm for Nonlinear Differential Equations*"
      ],
      "metadata": {
        "id": "5gEmqW10EUY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "well its linear PDEs we have well established quantum algorithms for and CFD is highly nonlinear. That didn't stop airbus from pretending that quantum algorithms for non-linear PDEs existed, for like 5 years before they did, and even making a competition about it (which nobody won). \n",
        "\n",
        "But it turns out that more recently there has been a bit of progress in developing quantum algorithms for nonlinear PDEs when you have bounded nonlinearities. I am still a bit skeptical these will end applying to the type of problems airbus is interested in but it is not totally crazy to think there might be something here for certain CFD problems. Bill on our team has recently started a project that aims to look into this. \n",
        "\n",
        "https://www.pnas.org/doi/10.1073/pnas.2026805118\n",
        "\n",
        "https://arxiv.org/abs/2303.16550 "
      ],
      "metadata": {
        "id": "66_x1rEJEXeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Algorithms Components*"
      ],
      "metadata": {
        "id": "UnzMHYy2j-mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Krylov algorithms*"
      ],
      "metadata": {
        "id": "Drnam8uix3L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Krylov_subspace\n",
        "\n",
        "\n",
        "https://quantum-journal.org/papers/q-2023-05-23-1018/\n",
        "\n",
        "We present an algorithm that uses block encoding on a quantum computer to exactly construct a Krylov space, which can be used as the basis for the Lanczos method to estimate extremal eigenvalues of Hamiltonians. While the classical Lanczos method has exponential cost in the system size to represent the Krylov states for quantum systems, our efficient quantum algorithm achieves this in polynomial time and memory. The construction presented is exact in the sense that the resulting Krylov space is identical to that of the Lanczos method, so the only approximation with respect to the exact method is due to finite sample noise. This is possible because, unlike previous quantum Krylov methods, our algorithm does not require simulating real or imaginary time evolution. We provide an explicit error bound for the resulting ground state energy estimate in the presence of noise. \n",
        "\n",
        "https://medium.com/qiskit/a-mainstay-of-classical-ground-state-algorithms-gets-a-quantum-speedup-8003687dbb0d"
      ],
      "metadata": {
        "id": "z_W9d3IIx7NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Shadow Tomography of Quantum States*"
      ],
      "metadata": {
        "id": "S523jdTu5665"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1711.01053"
      ],
      "metadata": {
        "id": "-AzCvKg-59TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Quantum_tomography\n",
        "\n",
        "https://en.wikipedia.org/wiki/Classical_shadow\n",
        "\n",
        "Scott Aaronson: Shadow Tomography https://arxiv.org/abs/1711.01053\n",
        "\n",
        "https://www.spektrum.de/news/quantennetzwerke-zeigen-schwarmintelligenz/2116689"
      ],
      "metadata": {
        "id": "ZstkVSpb2UOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shadow tomography, also known as X-ray phase-contrast imaging or phase tomography, is a technique used to create detailed three-dimensional images of an object's internal structure. It is primarily employed in the field of medical imaging but also finds applications in materials science and other areas.\n",
        "\n",
        "Traditional X-ray imaging relies on the attenuation of X-rays as they pass through different tissues or materials, resulting in a two-dimensional projection image that shows variations in X-ray absorption. However, some tissues or materials have similar absorption properties, making it challenging to distinguish between them based solely on their attenuation.\n",
        "\n",
        "Shadow tomography overcomes this limitation by exploiting the phase shift of X-rays as they pass through an object. When X-rays encounter a boundary between two materials with different refractive indices, they undergo a phase shift. This phase shift contains valuable information about the object's internal structure.\n",
        "\n",
        "To perform shadow tomography, a coherent X-ray source, such as a synchrotron or an X-ray tube, is used. The X-rays pass through the object and are collected by a detector placed behind it. The collected X-rays contain both intensity and phase information.\n",
        "\n",
        "The key to shadow tomography lies in extracting the phase information from the intensity measurements. This is typically achieved by using specialized techniques such as interferometry or grating-based methods. These methods create interference patterns that encode the phase information, which can then be computationally reconstructed into a three-dimensional image.\n",
        "\n",
        "The resulting shadow tomography image provides not only information about the X-ray absorption but also about the object's refractive index distribution and other structural features. This enables visualization of fine details, such as boundaries between tissues or subtle density variations, which may not be easily distinguishable in traditional X-ray images.\n",
        "\n",
        "Shadow tomography has the potential to improve diagnostic accuracy and enhance our understanding of complex biological and materials systems. However, it currently requires advanced X-ray sources and specialized techniques, limiting its widespread availability in clinical settings. Nevertheless, ongoing research and technological advancements aim to make shadow tomography more accessible and applicable in various fields."
      ],
      "metadata": {
        "id": "iMNjAZqA5-s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the confusion in my previous response. Shadow tomography can indeed be related to the characterization of quantum states, often referred to as \"quantum state tomography.\"\n",
        "\n",
        "In quantum mechanics, the state of a quantum system is described by a mathematical object known as a quantum state. Quantum state tomography aims to reconstruct or characterize the complete quantum state of a system by performing measurements on the system in different bases or using various observables.\n",
        "\n",
        "Shadow tomography of quantum states is a specific technique used for quantum state reconstruction. It involves measuring the shadows or projections of the quantum state onto a set of measurement operators or observables. These measurements provide partial information about the quantum state, similar to the way shadows reveal information about the object casting them.\n",
        "\n",
        "The procedure for shadow tomography of quantum states typically involves preparing the system in different known initial states, applying a set of measurements or observables to the system, and then collecting the measurement outcomes. By repeating this process for a sufficiently large number of initial states and measurements, it becomes possible to reconstruct the complete quantum state of the system through computational methods.\n",
        "\n",
        "The reconstructed quantum state provides valuable information about the system's properties, such as its purity, entanglement, coherence, and other relevant characteristics. This information is crucial for understanding and characterizing quantum systems, as well as for verifying the performance of quantum devices and protocols.\n",
        "\n",
        "Quantum state tomography, including shadow tomography, plays a vital role in the development, verification, and optimization of quantum algorithms, quantum error correction, quantum communication, and other applications of quantum information science.\n",
        "\n",
        "It's important to note that quantum state tomography can be a challenging task due to various factors, such as noise, imperfections in measurements, and the exponential growth of the state space with the number of quantum bits (qubits). Researchers continually work on improving the efficiency and accuracy of quantum state tomography techniques to overcome these challenges."
      ],
      "metadata": {
        "id": "bI5xQd9s6Atw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Trotterization*"
      ],
      "metadata": {
        "id": "Olk_ZtHZxxvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trotterization**\n",
        "\n",
        "https://vtomole.com/blog/2019/04/07/trotter"
      ],
      "metadata": {
        "id": "iOtuSjOEybKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several quantum Krylov algorithms have been proposed over the last few years, with the majority focusing on creating the Krylov space from time-evolved states with the given Hamiltonian. These methods are especially appealing for existing quantum computers, since one can obtain crude approximations of time evolutions using low-depth quantum circuits (i.e., Trotterization).\n",
        "\n",
        "https://medium.com/qiskit/a-mainstay-of-classical-ground-state-algorithms-gets-a-quantum-speedup-8003687dbb0d"
      ],
      "metadata": {
        "id": "tLlG13IGx14c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Block Encoding (Gibbs-Sampling)*"
      ],
      "metadata": {
        "id": "BLIqNf8wLAzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.wikipedia.org/wiki/Gibbs-Sampling"
      ],
      "metadata": {
        "id": "Sd2SdVyZPSrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "Arxiv: [Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics](https://arxiv.org/abs/1806.01838)\n",
        "\n",
        "In HHL f(x) = 1/x. Use Singular Value Transformation to approximate it! https://www.youtube.com/watch?v=L40UUDxPEbE&list=WL&index=3&t=215s\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18197/in-the-context-of-block-encoding-what-does-0-rangle-otimes-i-represent\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "7XFH8tKPLEJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Imaginary time evolution*"
      ],
      "metadata": {
        "id": "Z7Xj6gTt3FEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system\n",
        "\n",
        "Imaginary time is a concept derived from quantum mechanics and statistical mechanics. It introduces the idea of replacing \"real time\" with \"imaginary time\" by a Wick rotation in the complex plane. That is, the time variable 't' is replaced with an imaginary number 'it', where 'i' is the imaginary unit.\n",
        "\n",
        "Imaginary time evolution plays a significant role in several areas of physics, including quantum field theory, statistical mechanics, and quantum computing.\n",
        "\n",
        "1. **Quantum Field Theory and Statistical Mechanics**: In these fields, the use of imaginary time is often a mathematical trick that simplifies calculations. By transforming to imaginary time, the calculations of quantum mechanics often become calculations in statistical mechanics. This is utilized in the technique called \"path integral formulation,\" where the evolution of a system in imaginary time makes the system go to its lowest energy state or the ground state.\n",
        "\n",
        "2. **Quantum Computing**: In quantum computing, the idea of imaginary time evolution can be used to design quantum algorithms for tasks such as finding the ground state of a system. This is used in the Quantum Approximate Optimization Algorithm (QAOA) and the Quantum Imaginary Time Evolution (QITE) algorithm.\n",
        "\n",
        "Remember that \"imaginary time\" is not about time in the sense that we experience it. Instead, it's a mathematical construct that physicists use to solve certain types of problems."
      ],
      "metadata": {
        "id": "yO5fzJsOye7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Qubitization (Hamiltonian Simulation)*"
      ],
      "metadata": {
        "id": "GuoalDqRyRyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qubitization**\n",
        "\n",
        "\"Qubitization\" is a term related to quantum computing. It is a procedure to convert a certain operation into a quantum operation that can be performed on a quantum computer. Qubitization offers an approach for performing certain computational tasks with a significant reduction in resource requirements compared to other quantum algorithms.\n",
        "\n",
        "A primary example of the use of qubitization is in simulating Hamiltonians, a fundamental problem in quantum physics, with an algorithm known as the \"qubitization of Hamiltonian.\" This algorithm allows the efficient estimation of quantities such as ground state energy, which are of significant interest in quantum chemistry and condensed matter physics.\n",
        "\n",
        "The key advantage of qubitization over other quantum simulation algorithms is that it allows a more efficient use of quantum resources. However, the technique typically requires a more complex set of quantum gates and, as of my knowledge cut-off in September 2021, is a subject of active research and development in the field of quantum computing.\n",
        "\n",
        "Please note that the understanding and implementation of qubitization may have evolved beyond my last update. For the most accurate information, it's recommended to refer to the latest literature in the field of quantum computing.\n",
        "\n",
        "https://arxiv.org/abs/1610.06546"
      ],
      "metadata": {
        "id": "Ph9a0fu1y9Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *ZX-calculus*"
      ],
      "metadata": {
        "id": "cZPcI0kqrh4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZX Calculus: Hopf rule\n",
        "\n",
        "Reasoning with connectivity using diagrammatic reasoning\n",
        "\n",
        "ZX Calculus: Hadamard rule\n",
        "\n",
        "Evaluating quantum circuits using diagrammatic reasoning."
      ],
      "metadata": {
        "id": "5gLhGntqCW37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/ZX-calculus\n",
        "\n",
        "https://en.wikipedia.org/wiki/Penrose_graphical_notation\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_network_theory?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Unified_field_theory\n",
        "\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_product_state?wprov=sfti1\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Density_matrix_renormalization_group?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Categorical_quantum_mechanics?wprov=sfti1\n",
        "\n",
        "And finally I'll learn about fault-tolerance cause now it's in a language that I can understand, cause I co-invented it with Ross Duncan!\n",
        "\n",
        "https://lnkd.in/dX2YDzfF\n",
        "\n",
        "But the ‚Äúnewly introduced ZX-instruments‚Äù want to be the bastard spiders of the dodo-book. They go back a long time, even before ZX calculus itself, and published here:\n",
        "\n",
        "https://lnkd.in/e4VvrGHA\n",
        "\n",
        "Quantum In Pictures also has that stuff, and so does this paper:\n",
        "\n",
        "https://lnkd.in/eSwFWkHZ"
      ],
      "metadata": {
        "id": "KbDcHaTjrqmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Data Encoding (Embedding)*"
      ],
      "metadata": {
        "id": "0nJWHdt9gbt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "xYRmG4v9vwCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10> \n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "Fxjw-h8ox4Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "F2UqpWiN0Dv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features \n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value \n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "GbtZMigG2mij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "T8NOeUKI5dOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "BtmFtmBU6JFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *QRAM*"
      ],
      "metadata": {
        "id": "yp-FcBnVLeFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, it points out that high performance parallel systems like TPUs tend to keep the ratio of CPUs to memory in a range as opposed to going all memory or all CPU. And the same kinds of tradeoffs apply to quantum memory, especially if it's error corrected which requires CPUs. It's just sort of agglomerating all these arguments we have against QRAM as a physical device that doesn't use error correction.\n",
        "\n",
        "https://arxiv.org/abs/2305.10310"
      ],
      "metadata": {
        "id": "yC3RiKjfLh1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Networks*"
      ],
      "metadata": {
        "id": "vUaRXgHIgW2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**are Tensor Networks a form of quantum-inspired algorithm or not?**\n",
        "\n",
        "Tensor networks are a mathematical tool widely used in many-body quantum physics and quantum information theory. They provide a way to efficiently represent and manipulate high-dimensional tensors (or high-dimensional arrays of numbers), which frequently occur in these fields.\n",
        "\n",
        "Quantum many-body systems, which involve multiple interacting quantum particles, give rise to high-dimensional tensors. When these interactions are local, it turns out that the resulting state of the system can be represented more efficiently with a tensor network, allowing for computational tractability.\n",
        "\n",
        "However, tensor networks are not inherently quantum-inspired algorithms. They are more of a framework or a technique for representing complex multi-dimensional data structures, which can be used in the development of algorithms for quantum and classical computations.\n",
        "\n",
        "Where the quantum inspiration comes in is their application: tensor networks are extensively used to study quantum systems. For instance, they are used in algorithms for simulating quantum systems, understanding quantum entanglement, quantum error correction, and so on. They have also found applications beyond quantum physics, such as machine learning and artificial intelligence.\n",
        "\n",
        "In summary, tensor networks themselves are not quantum-inspired algorithms, but they are a critical tool in the development and application of algorithms (including quantum-inspired ones) used in quantum physics and quantum computing."
      ],
      "metadata": {
        "id": "Zn8-X-V23faS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tensor networks are mathematical constructs that are used to represent and manipulate quantum states, particularly those with many parts or \"qubits\".\n",
        "\n",
        "* In the context of quantum computing, each number in the tensor can represent an amplitude of a quantum state.\n",
        "\n",
        "* A tensor network is a particular way of organizing and manipulating these tensors. In a tensor network, each tensor is represented as a node, and the indices of the tensor are represented as edges connecting the nodes. By organizing the tensors in this way, **certain computations can be performed more efficiently, particularly those involving high-dimensional tensors**.\n",
        "\n",
        "* The real power of tensor networks comes from their ability to efficiently represent and manipulate certain types of quantum states, particularly those that exhibit some form of entanglement. **In many physical systems, and particularly in systems that are near their ground state, the quantum state of the system can be well approximated by a tensor network with a relatively simple structure**. \n",
        "\n",
        "* This makes **tensor networks a very useful tool for simulating such systems on a classical computer**.\n",
        "\n",
        "* For instance, the class of states that can be efficiently represented by a tensor network includes many states that arise in condensed matter physics and quantum chemistry, as well as some states that are used in quantum error correction and quantum computing.\n",
        "\n",
        "* One common type of tensor network is the [Matrix Product State](https://en.m.wikipedia.org/wiki/Matrix_product_state) (MPS), which is used in the [Density Matrix Renormalization Group](https://en.m.wikipedia.org/wiki/Density_matrix_renormalization_group) (DMRG) method, a powerful method for simulating one-dimensional quantum systems. \n",
        "\n",
        "  * The density matrix renormalization group (DMRG) is a numerical variational technique devised to obtain the [low-energy physics](https://en.m.wikipedia.org/wiki/Macroscopic_scale) of quantum many-body systems with high accuracy. As a variational method, DMRG is an efficient algorithm that attempts to find the lowest-energy matrix product state wavefunction of a Hamiltonian.\n",
        "\n",
        "( Other types of tensor networks include the Projected Entangled Pair State (PEPS), the Multi-scale Entanglement Renormalization Ansatz (MERA), and others."
      ],
      "metadata": {
        "id": "vtMw-E_BgcNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Mechanics*"
      ],
      "metadata": {
        "id": "iQFAjnERdvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Mathematical Formulation & Postulates of Quantum Mechanics*"
      ],
      "metadata": {
        "id": "iptu2LVks6If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2023-01-scientists-quantum-harmonic-oscillator-room.html"
      ],
      "metadata": {
        "id": "6VMJ5J0fVOgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video: [Map of Quantum Physics](https://youtu.be/gAFAj3pzvAA)"
      ],
      "metadata": {
        "id": "OEoP77S6dgsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"There are (at least) two possible ways to formulate precisely (i.e. mathematically) elementary\n",
        "QM. The eldest one, historically speaking, is due to von Neumann in essence, and is formulated using the language of Hilbert spaces and the spectral theory of unbounded operators. A more recent and mature formulation was developed by several authors in the attempt to solve quantum field theory problems in mathematical physics. It relies on the theory of abstract algebras (*-algebras and C* -algebras) that are built mimicking the operator algebras defined and studied, again, by von Neumann (nowadays known as W* -algebras or von Neumann algebras), but freed from the Hilbert-space structure. The core result is the celebrated GNS theorem (after Gelfand, Najmark and Segal), that we will prove in Chap. 14. The newer formulation can be considered an extension of the former one, in a very precise sense that we shall not go into here, also by virtue of the novel physical context it introduces and by the possibility of treating physical systems with infinitely many degrees of freedom, i.e. quantum fields. In particular, this second formulation makes precise sense of the demand for locality and covariance of relativistic quantum field theories, and allows to extend quantum field theories to a curved spacetime.\"\n",
        "-Valter Moretti"
      ],
      "metadata": {
        "id": "VNk-lacMqzQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2022-11-common-misconceptions-quantum-physics.html\n",
        "\n",
        "https://www.derstandard.de/story/2000140294674/wie-die-quantenphysik-mit-unserer-vorstellung-von-realitaet-aufraeumt\n",
        "\n",
        "https://physicsworld.com/a/how-the-stern-gerlach-experiment-made-physicists-believe-in-quantum-mechanics/"
      ],
      "metadata": {
        "id": "eT_D-OhRlWtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Korrespondenzprinzip](https://de.m.wikipedia.org/wiki/Korrespondenzprinzip): Klassische Gr√∂√üen werden durch Operatoren ersetzt. Die quantenmechanische Aufenthaltswahrscheinlichkeitsdichte eines Teilchens ist proportional zum Quadrat der Wellenfunktion der Materiewelle an jener Stelle. F√ºr gro√üe Quantenzahlen geht die quantenmechanische Wahrscheinlichkeitsdichte asymptotisch in die klassische √ºber.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Korrespondenzprinzip.svg/640px-Korrespondenzprinzip.svg.png)\n"
      ],
      "metadata": {
        "id": "MSQgI9GjQAv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKKG0DSv8FhZ"
      },
      "source": [
        "*Postulate der Quantenmechanik (Kopenhagener Interpretation)*\n",
        "\n",
        "1. **Zustand**: Der Zustand eines physikalischen Systems zu einem Zeitpunkt $t_{0}$ wird durch die Angabe eines zum Zustandsraum $\\mathcal{H}$ geh√∂renden komplexen Zustandsvektors $\\left|\\psi\\left(t_{0}\\right)\\right\\rangle$ definiert. Vektoren, die sich nur um einen von 0 verschiedenen Faktor $c \\in \\mathbb{C}$ unterscheiden, beschreiben denselben Zustand. Der Zustandsraum des Systems ist ein Hilbertraum.\n",
        "\n",
        "2. **Observable**: Jede Gr√∂√üe $A$, die physikalisch , gemessen\" werden kann, ist durch einen im Zustandsraum wirkenden hermiteschen Operator $\\hat{A}$ beschrieben. Dieser Operator wird als Observable bezeichnet und hat ein reelles Spektrum mit einer vollst√§ndigen sogenannten Spektralschar, bestehend aus einem , diskreten\" Anteil mit Eigenvektoren und Eigenwerten (Punktspektrum) und aus einem Kontinuum.\n",
        "\n",
        "3. **Messresultat**: Resultat der Messung einer physikalischen Gr√∂√üe $A$ kann nur einer der Eigenwerte der entsprechenden Observablen $\\hat{A}$ sein oder bei kontinuierlichem Spektrum des Operators eine messbare Menge aus dem Kontinuum.\n",
        "\n",
        "4. **Messwahrscheinlichkeit im Fall eines diskreten nichtentarteten Spektrums**: Wenn die physikalische Gr√∂√üe $A$ an einem System im Zustand $|\\psi\\rangle$ gemessen wird, ist die Wahrscheinlichkeit $P\\left(a_{n}\\right)$, den nichtentarteten Eigenwert $a_{n}$ der entsprechenden Observable $\\hat{A}$ zu erhalten (mit dem zugeh√∂rigen Eigenvektor $\\left|u_{n}\\right\\rangle$ ) $P\\left(a_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$. Dabei seien $\\psi$ und $u_{n}$ normiert.\n",
        "\n",
        "5. **Die Zeitentwicklung des Zustandsvektors** $|\\psi(t)\\rangle$ ist gegeben durch die folgende Schr√∂dingergleichung, wobei $\\hat{H}(t)$ die der totalen Energie des Systems zugeordnete Observable ist:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t}|\\psi(t)\\rangle=\\hat{H}(t)|\\psi(t)\\rangle$\n",
        "\n",
        "http://vergil.chemistry.gatech.edu/notes/quantrev/node20.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Mathematical_formulation_of_quantum_mechanics](https://en.m.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics)\n",
        "* [C*-algebra](https://en.m.wikipedia.org/wiki/C*-algebra)\n",
        "* [Quantum_geometry](https://en.m.wikipedia.org/wiki/Quantum_geometry)\n",
        "* [Noncommutative_geometry](https://en.m.wikipedia.org/wiki/Noncommutative_geometry)\n",
        "* [Geometric_quantization](https://en.m.wikipedia.org/wiki/Geometric_quantization)"
      ],
      "metadata": {
        "id": "wGmG22vunC0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_209.png)\n"
      ],
      "metadata": {
        "id": "q8MuXUSiqRrJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE--P8idzoP-"
      },
      "source": [
        "**Video: [Crash course in density matrices](https://www.youtube.com/watch?v=1tserF6VGqI)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWlXYxSBzPS3"
      },
      "source": [
        "**Single spin one half particle, focus on spin degrees of freedom & Pauli-matrices**\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad |\\downarrow\\rangle=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvOyiyJtzRtd"
      },
      "source": [
        "**States we are used to use in quantum mechanics are called 'pure states'.**\n",
        "\n",
        "* Any state can be written as the linear combination of the up and down vectors in the z-direction\n",
        "\n",
        "> $|\\psi\\rangle=a|\\uparrow\\rangle+b|\\downarrow\\rangle$\n",
        "\n",
        "* a and b are complex numbers whose modulus squared sum to 1\n",
        "\n",
        "> $\\langle\\psi \\mid \\psi\\rangle=|a|^{2}+|b|^{2}$\n",
        "\n",
        "* a and b can be referred to as the probability amplitudes, where their modulus squared are the probabilities that the system is in a particular configuration.\n",
        "\n",
        "* If we perform an ensemble of measurements on the system, we will find that the mean or expected value, for example of the Pauli-Z matrix, will be given by:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZoOoevVzUPa"
      },
      "source": [
        "**Dynamics: if we want to study how a quantum system changes in time, we usually refer to the Schroedinger equation.**\n",
        "\n",
        "> $i \\frac{d}{d t}|\\psi(t)\\rangle=\\hat{H}|\\psi\\rangle$\n",
        "\n",
        "* the operator $\\hat{H}$ is called the Hamiltonian and it tells us about the total energy in a system, and how things in a system interact with each other.\n",
        "\n",
        "* the easiest way to solve this problem is to first solve the Eigenvalue problem, which involves finding the Eigenvectors and the Eigenvalues of the hamiltonian. We are able the different values of the Eigenvalues and Eigenvectors with the label k:\n",
        "\n",
        "> $\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* This allows us to know that the Eigenvectors, when plugged into the Schroedinger equation ...\n",
        "\n",
        "> $i \\frac{d}{d t}\\left|E_{k}\\right\\rangle=\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* ... just pick up a phase in time depending on the energy they correspond to:\n",
        "\n",
        "> $\\left|E_{k}(t)\\right\\rangle=e^{-i E_{k} t}\\left|E_{k}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitINkMkzWk-"
      },
      "source": [
        "**Now to sum more general situation with some generic state $\\psi$**\n",
        "\n",
        "* we can decompose $\\psi$ in terms of energy Eigenbasis:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* where $c_{m}$ is given by the inner product of the energy Eigenvector labelled by m and $\\psi$ itself:\n",
        "\n",
        "> $c_{m}=\\left\\langle E_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* We can then time-evolve the state by simply time-evolving the energy Eigen-Kets:\n",
        "\n",
        "> $|\\psi(t)\\rangle=\\sum_{m} c_{m} e^{-i E_{m} t}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* Tracking the expectation value of an observable is quite easy: Simply applying the state vector in time to both sides of the matrix gives the following equation:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\sum_{m, n} \\bar{c}_{n} c_{m} A_{n, m} e^{i\\left(E_{n}-E_{m}\\right) t}$\n",
        "\n",
        "* Where we have labelled the matrix entries of a by the energy Eigenbasis in the following way:\n",
        "\n",
        "> $A_{n, m}=\\left\\langle E_{n}|A| E_{m}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utZdbq6QzZD9"
      },
      "source": [
        "**Propeties of the trace of a matrix**\n",
        "\n",
        "* We have a basis of states labeled by j $|j\\rangle, j=1,2$.. N that form a complete orthonormal basis. Then I write the trace of a matrix in the following way:\n",
        "\n",
        "> $\\operatorname{Tr}(A)=\\sum_{j}\\langle j|A| j\\rangle$\n",
        "\n",
        "* the trace is a linear mapping which tells us that we can separate the trace of the sum of two matrices apart like this, and we can also pull scalar multiples outside of the trace:\n",
        "\n",
        "> $\\operatorname{Tr}(A+B)=\\operatorname{Tr} A+\\operatorname{Tr} B$ with $\\operatorname{Tr}(c A)=c \\operatorname{Tr} A$\n",
        "\n",
        "* When we take the trace of two matrices multiplied by each other, the trace is invariant under swapping the two matrices inside of the trace.\n",
        "\n",
        "> $\\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Om09eqlza4O"
      },
      "source": [
        "**Density Matrices**\n",
        "\n",
        "* Going from a pure state represented by a Ket we can introduce the density matrix which is a completely equivalent way to represent the state of a quantum system\n",
        "\n",
        "* the density matrix in this context is just the outer product of the state with itself:\n",
        "\n",
        "> $\\rho=|\\psi\\rangle\\langle\\psi|$\n",
        "\n",
        "* the expectation value is rewritten in terms of a trace, but it is mathematically equivalent to the earlier expression:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\operatorname{Tr}\\left(\\rho \\sigma^{2}\\right)=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$\n",
        "\n",
        "* the density matrix has two fundamental properties: it's trace is 1 in the context of a pure state:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1$\n",
        "\n",
        "> $\\operatorname{Tr}|\\psi\\rangle\\left\\langle\\left.\\psi|=|\\langle\\psi \\mid \\psi\\rangle\\right|^{2}=1\\right.$\n",
        "\n",
        "* and it's a positive operator:\n",
        "\n",
        "> $\\rho \\geq 0$\n",
        "\n",
        "* If I take any state vector our state space and perform the following operation, then the result is always greater than or equal to zero:\n",
        "\n",
        "> $\\langle\\phi|\\rho| \\phi\\rangle=|\\langle\\phi \\mid \\psi\\rangle|^{2} \\geq 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuLO_P1u6FBJ"
      },
      "source": [
        "**Dynamics: von Neumann equation of time evolution**\n",
        "\n",
        "* show from the Schroedinger equation we can derive the von Neumann equation of time evolution:\n",
        "\n",
        "> $i \\frac{\\partial \\rho}{\\partial t}=[\\hat{H}, \\rho]$\n",
        "\n",
        "* It features the commutation relationship between the Hamiltonian and the density matrix itself\n",
        "\n",
        "* then the expectation value for an observable in time can be rewritten in the following way:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\operatorname{Tr}(\\rho(t) A)$\n",
        "\n",
        "* where the density matrix $\\rho$ evolves with the Hamiltonian being applied to it in time:\n",
        "\n",
        "> $\\rho(t)=e^{-i \\hat{H} t} \\rho e^{i \\hat{H} t}$\n",
        "\n",
        "*  similarly re-expressed in the basis of the energy Eigenvalues\n",
        "\n",
        "> $\\rho(t)=\\sum_{m, n} \\rho_{m, n} e^{-i\\left(E_{m}-E_{n}\\right) t}\\left|E_{m}\\right\\rangle\\left\\langle E_{n}\\right|$\n",
        "\n",
        "* where $\\rho_{m,n}$ can be written in the following way, where we have re-expressed its entries in terms of the energy Eigenbasis:\n",
        "\n",
        "> $\\rho_{m, n}=\\left\\langle E_{m}|\\rho| E_{n}\\right\\rangle$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmo42wRj8v6C"
      },
      "source": [
        "**Mixed States**\n",
        "\n",
        "* what if we are unsure of what pure state our system is in? (We are somehow ignorant / unwissend to what pure state we are in)\n",
        "\n",
        "* then we can describe a statistical ensemble of pure states which we call a mixed states\n",
        "\n",
        "> $\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|$\n",
        "\n",
        "* the ensemble is written as a sum of pure states with probabilities $p_j$ and the probabilities are greater than zero and sum up to one:\n",
        "\n",
        "> $p_{j} \\geq 0 \\quad \\sum_{j} p_{j}=1$\n",
        "\n",
        "* Mixed states are also trace 1 and are positive operators:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1, \\quad \\rho \\geq 0$\n",
        "\n",
        "* Dynamics and the expectation values work in an identical way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUFROLA7BPGU"
      },
      "source": [
        "**Non-uniqueness of mixed states decomposition**\n",
        "\n",
        "* Interestingly it's not completely correct to interpret $p_j$ as the probability of being in a particular state labeled by j.\n",
        "\n",
        "* To see this consider the following example let $\\rho$ be a mixed state written in the following way:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* we say the system is in state down with probability 4/5 and in state up with 1/5.\n",
        "\n",
        "* What if I prepared two other states with the following new state vectors a and b with the following probability amplitudes of being in down or up state:\n",
        "\n",
        "> $|a\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle+\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "> $|b\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle-\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "* Then if we prepare these states with probability one-half:\n",
        "\n",
        "> $\\rho=\\frac{1}{2}|a\\rangle\\left\\langle a\\left|+\\frac{1}{2}\\right| b\\right\\rangle\\langle b|$\n",
        "\n",
        "* We see that we get the same density matrix as before working this out expanding the definitions of a and b allows us to arrive at our original density matrix:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* Therefore two different ensembles of pure states can give rise to the same mixed state and we must be careful when we interpret $p_j$ as strictly probabilities of a particular system being strictly in its associated pure state in the sum.\n",
        "\n",
        "* Finally, if you get a matrix how could you tell if it's pure or mixed? Mathematically quite simple way to check: square the matrix and take its trace. If the trace is still 1, we say that the density matrix is pure if it is less than one then we say it's mixed. This is actually independent of the time evolution.\n",
        "\n",
        "> $\\operatorname{Tr}\\left(\\rho^{2}\\right) \\leq 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket $|\\psi\\rangle$ - State Space $V$(Vector)*** *- also: Wave Function (Pure State, Postulate I)*"
      ],
      "metadata": {
        "id": "jBh1Tr-vn_4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcIVsSgCSld"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate I of Quantum Mechanics</u>: The state of a physical system is characterized by a state vector that belongs to a complex vector space $\\mathcal{V}$, called the state space of the system**\n",
        "\n",
        "* State Space Formalism: unification of Schrodinger wave mechanics and matrix mechanics (Heisenberg, Born, Jordan), both are equivalent.\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin). Then it reduces to the rules of simple matrix multiplication**. Kronecker delta\n",
        "\n",
        "* **Schrodinger wave mechanics: for continuous basis (like position)** Dirac delta function\n",
        "\n",
        "* State Space: the vector space in which quantum systems live\n",
        "\n",
        "  * Euclidean space: classical physics, 3D, real, inner product (Hilbert Space)\n",
        "\n",
        "  * State space: quantum physics, infinite dimensions, complex numbers, inner product (Hilbert Space)\n",
        "\n",
        "Video: [Dirac notation: state space and dual space](https://www.youtube.com/watch?v=hJoWM9jf0gU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlC5y6Hpsj_E"
      },
      "source": [
        "<font color=\"blue\">*Ket Algebra - Quantum State Vector*\n",
        "\n",
        "**A Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state', <u>represents</u> the wave function of a quantum system (\"Psi\")**, consisting of **probability amplitudes**. This Quantum state is a **stochastic vector**, written as a column vector.\n",
        "\n",
        "> **A Ket is technically a pure quantum state**.\n",
        "\n",
        "* Wave function notation to describe superposition of Pure States.\n",
        "\n",
        "So, you have an (orthonormal) basis with 3 vectors and coefficients, to describe a vector in space (in Hilbert space):\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$2$</font>$\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)$<font color='blue'>$+3$</font>$\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)$<font color='blue'>$+0$</font>$\\left(\\begin{array}{l}0 \\\\ 0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "The basis vectors will all entries in 0 and only one with 1 correspond to possibe measurement outcomes (spin up or down).\n",
        "\n",
        "The coefficients can be collected in one vector and are complex numbers:\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$\\left(\\begin{array}{l}2 \\\\ 3 \\\\ 0\\end{array}\\right)$</font>\n",
        "\n",
        "An arbitrary state for a qubit can be written as a linear combination of the Pauli matrices, which provide a basis for $2 \\times 2$ self-adjoint matrices:\n",
        "\n",
        "> $\n",
        "\\rho=\\frac{1}{2}\\left(I+r_{x} \\sigma_{x}+r_{y} \\sigma_{y}+r_{z} \\sigma_{z}\\right)\n",
        "$\n",
        "\n",
        "* where the real numbers $\\left(r_{x}, r_{y}, r_{z}\\right)$ are the coordinates of a point within the unit ball and\n",
        "\n",
        "> $\n",
        "\\sigma_{x}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{y}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{z}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siehe auch: [Projective Hilbert Space](https://en.m.wikipedia.org/wiki/Projective_Hilbert_space) with rays or projective rays"
      ],
      "metadata": {
        "id": "_sFD45vxEg5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSzA4IwURCu"
      },
      "source": [
        "<font color=\"blue\">*Multiply Ket with a Scalar: Vector-Scalar-Multiplication*\n",
        "\n",
        "Vector-Scalar:\n",
        "\n",
        "$\\left[\\begin{array}{l}x_{0} \\\\ x_{1}\\end{array}\\right] \\otimes\\left[y_{0}\\right]=\\left[\\begin{array}{l}x_{0}\\left[y_{0}\\right] \\\\ x_{1}\\left[y_{0}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}x_{0} y_{0} \\\\ x_{1} y_{0}\\end{array}\\right]$\n",
        "\n",
        "\n",
        "<font color=\"blue\">*Multiply Ket with another Ket: Ket - Tensor Product (Vector-Vector-Multiplication, Kronecker Product)*\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "*Zur Kombination von Quantum States:*\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$.\n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH8JeO4mW4q6"
      },
      "source": [
        "**How do we represent Ket's in a particular basis?**\n",
        "\n",
        "> <font color=\"blue\">$|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle \\quad$ where: $c_{i}=\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "Video: [Representations in quantum mechanics](https://www.youtube.com/watch?v=rp2k2oR5ZQ8)\n",
        "\n",
        "\n",
        "> $\\left\\{c_{i}\\right\\}$ are the representation of $|\\psi\\rangle$ in the $\\left\\{\\left|u_{i}\\right\\rangle\\right\\}$ basis\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_210.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_211.png)\n",
        "\n",
        "* a und b coefficients in euclidean space sind wie $u_i$ coefficients in quantum state space. (difference is bra-ket, wo bra das conjugate complex ist als dot product mit ket: es ist im erstem argument antilinear!).\n",
        "\n",
        "* expansion coefficients c are given by projection of the Ket onto the basis states u:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_212.png)\n",
        "\n",
        "* Here we can take out $|\\Psi\\rangle$ because it doesnt explicitely depend on $_i$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_213.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xfgmufRt0W"
      },
      "source": [
        "**Additional Ket-Algebra**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_202.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_203.png)\n",
        "\n",
        "For euclidean space: the scalar product is linear both in first and second argument:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_204.png)\n",
        "\n",
        "For state space: the scalar product is only linear in the second argument (and antilinear in the first argument):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_205.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra $\\langle \\psi |$ - Dual Space $V^{*}$ (Covector)***"
      ],
      "metadata": {
        "id": "cqdowsM5oHEP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOxv3yFtjD1"
      },
      "source": [
        "**How do we represent Bra's in a particular basis?**\n",
        "\n",
        "* Bra $\\langle \\Psi|$ is an element of the dual vector space $V^*$\n",
        "\n",
        "* Bra Psi times the identity operator ($\\langle \\Psi| * \\mathbb{I}$), and then write the identity operator out (its resolution in the u basis)\n",
        "\n",
        "* psi doesnt explicitylt depend on i, so we can write it into the summation, which brings us to this expression\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi \\mid u_{i}\\right\\rangle\\left\\langle u_{i}\\right|$\n",
        "\n",
        "* This is an expression for the bra psi in terms of the basis bra's u, and then the expansion coefficients are the brackets between psi and u $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$\n",
        "\n",
        "\n",
        "If we look at these expansion coefficients $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$, we can use the conjugation property of the scalar product to rewrite them like this:\n",
        "\n",
        "> $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$ = $\\left\\langle u_{i} \\mid \\psi \\right\\rangle^*$ = $c_i^*$\n",
        "\n",
        "**The expansion coefficients are the complex conjugates of the expansion coefficients of the Ket**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_214.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Changing Basis*"
      ],
      "metadata": {
        "id": "fdcELPZDs-X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB17Qq-GLGBT"
      },
      "source": [
        "**Changing Basis**\n",
        "\n",
        "* <font color=\"blue\">**For example: A particles position can be expressed as the superposition of momentum states**</font>\n",
        "\n",
        "* Goal: choose a 'good' basis that makes the maths as simple as possible\n",
        "\n",
        "Video: [Changing basis in quantum mechanics](https://www.youtube.com/watch?v=CDmXvPDMIFs)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_252.png)\n",
        "\n",
        "If we go from one representation to another: we need to calculate the overlaps between the corresponding basis states (with overlap matrix):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_253.png)\n",
        "\n",
        "How do we get back from the new to the old basis?\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_254.png)\n",
        "\n",
        "How we do transform the representation of operators between basis? (resolve identities in the u basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_255.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_256.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra-Ket $\\langle \\psi |\\psi\\rangle = c$ - Linear Form (Covector-Vector)*** *- also: Projective Measurement*"
      ],
      "metadata": {
        "id": "1fAAZcgWoiJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A6siRC6YyVY"
      },
      "source": [
        "> **Bra-Ket - Projective Measurement (Kovector-Vector-Multiplication, Born Rule)**\n",
        "\n",
        "* Dirac delta function (a distribution!) = quantum measurement\n",
        "\n",
        "* See also [wiki: Bra‚Äìket notation](https://en.m.wikipedia.org/wiki/Bra‚Äìket_notation)\n",
        "\n",
        "* From 'Exterior algebra' $\\rightarrow$ 'Multilinear Forms':\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti‚Äìisomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra‚Äìket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© = 1.\n",
        "\n",
        "*See also: [dot product between a covector and a vector is a scalar, after all!](https://cafephysics35698708.wordpress.com/2017/12/23/moving-away-from-ortho-land-vectors-covectors-and-all-that/) but read also why [There are a couple ways to view a dot product as a linear map by changing your view slightly](https://math.stackexchange.com/questions/2856198/is-dot-product-a-kind-of-linear-transformation)*\n",
        "\n",
        "* Inner Product / Bra-Ket, conjugate transpose of Ket.\n",
        "You get a scalar as output.\n",
        "\n",
        "* The Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$  represents the inner product in the Hilbert space\n",
        "\n",
        "* Zur Messung von Zustaenden in einer Basis (zB ich will die Probability wissen, mit der man den State=1 erh√§lt)\n",
        "\n",
        "* Quantum mechanical systems are represented by Hilbert spaces, which are anti‚Äìisomorphic to their own dual spaces.\n",
        "\n",
        "* **A state of a quantum mechanical system can be identified with a linear functional**.\n",
        "\n",
        "> $\\mathbf{u}^{\\top} \\mathbf{v}=\\left[\\begin{array}{llll}u_{1} & u_{2} & \\cdots & u_{n}\\end{array}\\right]\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n}\\end{array}\\right]=\\left[u_{1} v_{1}+u_{2} v_{2}+\\cdots+u_{n} v_{n}\\right]$ = scalar\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_248.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 1: Inner product of two basis vectors**:\n",
        "\n",
        "\n",
        "* ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), im Detail fur $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$\n",
        "\n",
        "* ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© ergeben inner product 1 (mit sich selbst multipliziert / die beiden Berechnungsbasisvektoren sind orthonormal)\n",
        "\n",
        "**Case 2: Berechne Wahrscheinlichkeit fur Messung eines Eigenstates (Born's Rule)**\n",
        "\n",
        "* Die orthonormalen Eigenschaften sind im folgenden Beispiel n√ºtzlich\n",
        "\n",
        "* Es gibt einen Zustand mit der Wave Function / Wahrscheinlichkeit fur beide Zustaende $|\\psi\\rangle=\\frac{3}{5}|1\\rangle+\\frac{4}{5}|0\\rangle$\n",
        "\n",
        "* die Wahrscheinlichkeit des Messens 1 ist dann (1 ist hierbei die basis, weil ich es messen will) [Source](https://docs.microsoft.com/de-de/azure/quantum/concepts-dirac-notation):\n",
        "\n",
        ">$\n",
        "|\\langle 1 \\mid \\psi\\rangle|^{2}=\\left|\\frac{3}{5}\\langle 1 \\mid 1\\rangle+\\frac{4}{5}\\langle 1 \\mid 0\\rangle\\right|^{2}=\\frac{9}{25}\n",
        "$\n",
        "\n",
        "* weil $\\langle 1 \\mid 0\\rangle=0$ faellt der zweite Term weg: $\\frac{4}{5}\\langle 1 \\mid 0\\rangle$ = $\\frac{4}{5} * 0$\n",
        "\n",
        "* Another example: $\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left(\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]\\right)=2 x+1 y$. **The covector [2, 1] can be thought of as a function, rather than a row vector, that acts on another vector.**\n",
        "\n",
        "> <font color=\"red\">*The probability of a particular measurement is then the absolute square of the scalar product with the basis vector that corresponds to the outcome (probability of measuring $X$ is). This is <u>Born's Rule</u>. This scalar product of the wave function with a basis vector is also sometimes called a <u>projection</u> on that basis vector:*\n",
        "\n",
        "> $|\\langle X \\mid \\Psi\\rangle|^{2}=a_{1} a_{1}^{*}$\n",
        "\n",
        "* where $\\mid \\Psi\\rangle$ is the wavefunction of the probability superposition and $\\langle X \\mid$ is the basis vector of one outcome.\n",
        "\n",
        "**Case 3: Sum over all basis vectors**\n",
        "\n",
        "> $\\langle\\Psi^* |\\Psi\\rangle$ = $ (a{_1}^*, a{_2}^*, a{_3}^*) \\left(\\begin{array}{l}a_{1} \\\\ a_{2} \\\\ a_{3}\\end{array}\\right)$ = $(a{_1}^*a_1 + a{_2}^*a_2 + a{_2}^*a_2)$\n",
        "\n",
        "**The probability to get ANY measurement outcome is equal to one, which means that the sum over the squared scalar products with all basis vectors has to be one. Which is just the length of the vector (all wave functions have length 1)**:\n",
        "\n",
        "> $1=a_{1} a_{1}^{*}+a_{2} a_{2}^{*}+a_{3} a_{3}^{*}=|\\langle \\Psi \\mid \\Psi\\rangle|^{2}$\n",
        "\n",
        "* With this bra-ket notation it's now very easy to write dot products = the **inner product between Bra and Ket which is $\\langle\\psi \\mid \\psi\\rangle$ = 1**, and it is normalized the result is 1(it's a particular way of writing the 2-norm when using complex inputs).\n",
        "\n",
        "* With the dot product of bra and ket you will get a **scalar as a result**, like total probability is 1, here for the coefficients (probability amplitudes):\n",
        "\n",
        "> $\\langle\\psi^* \\mid \\psi\\rangle = \\left|a_{0}\\right|^{2}+\\left|a_{1}\\right|^{2}= 1$\n",
        "\n",
        "* **We expand the Ket $\\psi$ in the basis u**, where the expansion coefficients c are given by the Braket between u and psi:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle = c_{1} * u_{1} + c_{2} * u_{2} .. + c_{i}* u_{i} = c_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)+{c_{2}}\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)+.. c_{i}\\left(\\begin{array}{l}0 \\\\ 0 \\\\ i\\end{array}\\right) \\quad =\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">This c cofficients are what we call **representation of the Ket psi in the u basis**.\n",
        "\n",
        "> $\\left(\\begin{array}{c}\\langle u_{1} \\mid \\psi\\rangle \\\\ \\left\\langle u_{2} \\mid \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{i} \\mid \\psi\\right\\rangle \\\\ \\vdots\\end{array}\\right)=\\left(\\begin{array}{c}c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{i} \\\\ \\vdots \\\\ \\end{array}\\right)$"
      ],
      "metadata": {
        "id": "JhSZ3kxevCQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Measurements from an Observable (via its Operator) - Use Case: What is the most likely Eigenvalue of an Observable like Momentum or Position?**\n",
        "\n",
        "Video: [Measurements in quantum mechanics || Concepts](https://www.youtube.com/watch?v=u1R3kRWh1ek)\n",
        "\n",
        "> **$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "* Physical quantity $\\mathcal{A} \\longrightarrow$ Observable $\\hat{A}$\n",
        "\n",
        "* To understand what it means to measure $\\hat{A}$ in quantum mechanics, the key equation to consider is the Eigenvalue equation of the operator $\\hat{A}$ here:\n",
        "\n",
        "> $\n",
        "\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle\n",
        "$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ When you apply a measurement operator in an Eigenstate, you will measure /get the Eigenvalue of this Eigenstate (postulate III of quantum mechanics). Challenge: we just dont know in which Eigenstate our system $|\\Psi\\rangle$ is.*</font>\n",
        "\n",
        "* with $\\lambda_{n}$ Eigenvalues and $u_{n}$ Eigenstates\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "\n",
        "* This means: when we want to measure property $\\hat{A}$ we first need to solve the corresponding Eigenvalue equation, which allows us to find all Eigenvalues $\\lambda_{1}$, $\\lambda_{2}$, .. $\\lambda_{n}$\n",
        "\n",
        "* So the operator $\\hat{A}$ encodes all the possible outcomes of the measurement, irrespective of what the state of the system is.\n",
        "\n",
        "* **So the question is: if we measure $\\hat{A}$ in state $|\\Psi\\rangle$: which Eigenvalue will we get?** (Postulate III will tell us we will get one of the Eigenvalues $\\lambda_{n}$, but not which one)\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate VI of quantum mechanics</u>: The measurement of $\\mathcal{A}$ in a system in normalized state $|\\psi\\rangle$ gives eigenvalue $\\lambda_{n}$ with probability:**\n",
        "\n",
        "\n",
        "> $\n",
        "P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\n",
        "$ $\\quad (= \\left|c_{n}\\right|^{2})$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ $u_n$ are the possible Eigenstates to which we project our actual system state $|\\Psi\\rangle$. We square its norm and then see which one has the highest value = highest probability that the state is in this Eigenstate (with a givne Eigenvalue $\\lambda_{n}$). For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$*</font>\n",
        "\n",
        "\n",
        "* This means: for an operator $\\hat{A}$ we have a list of Eigenvalues, where each of them corresponds to an Eigenstate:\n",
        "\n",
        "  * $\\lambda_{1}$ for $|u_1\\rangle$,\n",
        "\n",
        "  * $\\lambda_{2}$ for $|u_2\\rangle$,\n",
        "\n",
        "  * ...\n",
        "\n",
        "  * $\\lambda_{n}$ for $|u_n\\rangle$\n",
        "\n",
        "* these relations and values come from the Eigenvalue equation of the specific operator $\\mathcal{A}$ and are independent of the state of our system.\n",
        "\n",
        "* **But when we measure $\\mathcal{A}$, we measure it in a specific state $|\\Psi\\rangle$**. We will get $\\lambda_{1}$ with $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* So what we get as result depends (1) on the intrinsic properties $\\mathcal{A}$ with its Eigenvalues and Eigenstates,  and (b) on the specific state $|\\Psi\\rangle$ in which our system is.\n",
        "\n",
        "* So with postulate 4: rather then telling us the precise outcome of a measurement it tells us the probability associated with any given outcome\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "\n",
        "**How does the state $|\\Psi\\rangle$ of a system encodes the possible outcomes of a measurement?**\n",
        "\n",
        "> <font color =\"blue\">$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We can write the state $|\\Psi\\rangle$ in a complete basis of our state space and the Eigenstates $u_n$ over Hermitian operator like $\\hat{A}$ provide such a basis\n",
        "\n",
        "* this means we can write $|\\Psi\\rangle$ in the u-Basis (Eigenstates) like this:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{n} c_{n}\\left|u_{n}\\right\\rangle$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ $\\left|c_{m}\\right|^{2}$ is the probability and hence $c_n$ the squared root of the probability of a given Eigenvalue for each Eigenstate $\\left|u_{n}\\right\\rangle$ from this part here above:\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "* and these expansion coefficients c which we call **the representation of $|\\Psi\\rangle$ in the u-Basis**, are given by the projection of $|\\Psi\\rangle$ onto the u-Basis states:\n",
        "\n",
        "> $c_{n}=\\left\\langle u_{n} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* This means we can rewrite the probability p of measuring Eigenvalue $\\lambda_n$ as also equal to absolute value of $c_n$ squared:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{m}\\right|^{2}$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "**What does this mean?**\n",
        "\n",
        "* imagine our system is in state $\\Psi\\rangle$ and we want to measure a property $|\\Psi\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow \\hat{A} \\longrightarrow ?$\n",
        "\n",
        "* Then what we do is to write the state $|\\Psi\\rangle$ in the basis of Eigenstates of $\\hat{A}$, and <font color =\"blue\">the expansion coefficient $c_n$ tell us the relative contribution of Eigenstate $|u_n\\rangle$ to state $|\\Psi\\rangle$, which in turn tell us how likely it is to measure the associated Eigenvalue $\\lambda_n$</font>\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} \\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">here you can re-arrange and get an outer product (is this true???)\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} |u_{m}\\rangle \\langle u_{m}| \\psi\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "Passt zu weiter unten (video prof m mit projection operators):\n",
        "\n",
        "> $(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_199.png)\n",
        "\n",
        "Imagine we have n copies of the state $|\\Psi\\rangle$ and measure each. We get an Eigenvalue from the Eigenvalue equation, but with different probabilities. As p approaches $\\infty$ with N copies (= $p_n$), we will reach the most probable Eigenvalue P($\\lambda_m$) (postulates 4):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_200.png)\n",
        "\n",
        "**A special case if our system with state $|\\Psi\\rangle$ is in an Eigenstate of the property that we are measuring (the operator $\\hat{A}$, for example momentum, positon etc), say $|u_m\\rangle$**:\n",
        "\n",
        "> $|\\psi\\rangle=\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* This corresponds to the expansion coefficient $c_m$ = 1, while all other coefficients vanish.\n",
        "\n",
        "* In this particular case we do know with absolute certainty what the outcome of the measurement will be. Probability = 1:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|c_{m}\\right|^{2}=1$\n",
        "\n",
        "**This means: If the system is in an Eigenstate of the property that we are measuring (=momentum, position etc, measured by an operator), then the outcome of the measurement is the associated Eigenvalue with probability 1.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_287.png)"
      ],
      "metadata": {
        "id": "nKVQn725YOeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi | = P$ - Linear Map (Vector-Covector) - Projection Operator***"
      ],
      "metadata": {
        "id": "cyrUknMLo8bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case 1: Projection Operator (Outer Product)**"
      ],
      "metadata": {
        "id": "vmUk0nAHzPCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A measurement outcome is actually a projection (to the first basis vector)**\n",
        "\n",
        "* So if we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$> bzw. | 0 > < 0 | $\\Phi$ >\n",
        "\n",
        "* this is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "> The probability of measuring a value with probability amplitude $\\phi$ is $1 \\geq|\\phi|^{2} \\geq 0$, where $|\\cdot|$ is the [modulus](https://en.m.wikipedia.org/wiki/Absolute_value#Complex_numbers).\n",
        "\n",
        "**Ket-Bra (also: Projection Operator, Outer Product or Density Matrix)**\n",
        "\n",
        "* Use Case 1: used for mixed states\n",
        "\n",
        "* Use Case 2: used when I want to measure a specific Eigenstate / Eigenvector to get its probability.\n",
        "\n",
        "* Is a pair: Vector-Covector\n",
        "\n",
        "> $\\mathbf{u v}^{T}=\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right]\\left[\\begin{array}{llll}v_{1} & v_{2} & \\cdots & v_{n}\\end{array}\\right]$= $\\left[\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{1}\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\ldots \\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\right]$ = $\\left[\\begin{array}{cccc}u_{1} v_{1} & u_{1} v_{2} & \\cdots & u_{1} v_{n} \\\\ u_{2} v_{1} & u_{2} v_{2} & \\cdots & u_{2} v_{n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ u_{n} v_{1} & u_{n} v_{2} & \\cdots & u_{n} v_{n}\\end{array}\\right]$\n",
        "\n",
        "> $|0\\rangle\\langle 0| =$ $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$\n",
        "\n",
        "* A measurement outcome is actually a projection (to the first basis vector)\n",
        "\n",
        "* If we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$>. This is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_307.jpg)\n",
        "\n",
        "From this video: https://youtu.be/U6fn5LvevEE\n",
        "\n",
        "**Example: if you want to measure with which probability the quantum system is in state 0, you apply $|0\\rangle \\langle0|$ operator which is $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$, because: $|0\\rangle\\langle 0|=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$**\n",
        "\n",
        "* \"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since **we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix**. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "https://physics.stackexchange.com/questions/394258/what-is-the-standard-definition-of-projector-projection-and-projection-ope\n",
        "\n",
        "The density matrix is a representation of a linear operator called the density operator. The density matrix is obtained from the density operator by choice of basis in the underlying space. In practice, the terms density matrix and density operator are often used interchangeably.\n",
        "\n",
        "In operator language, a density operator for a system is a positive semidefinite, Hermitian operator of trace one acting on the Hilbert space of the system. This definition can be motivated by considering a situation where a pure state $\\left|\\psi_{j}\\right\\rangle$ is prepared with probability $p_{j}$, known as an ensemble. The probability of obtaining projective measurement result $m$ when using projectors $\\Pi_{m}$ is given by\n",
        "\n",
        "> $\n",
        "p(m)=\\sum_{j} p_{j}\\left\\langle\\psi_{j}\\left|\\Pi_{m}\\right| \\psi_{j}\\right\\rangle=\\operatorname{tr}\\left[\\Pi_{m}\\left(\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\\right)\\right]\n",
        "$\n",
        "\n",
        "which makes the density operator, defined as\n",
        "\n",
        "> <font color=\"red\">$\n",
        "\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\n",
        "$\n",
        "\n",
        "= probability of getting a state |0> for example with outer product / density matrix $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$ PLUS probability of getting state |1> etc."
      ],
      "metadata": {
        "id": "5bWbx2kRFnsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">*Projection Operator (Outer Product) & Eigenvalues in Systems of Linear Equations (HHL)*\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "> $A|x\\rangle=|b\\rangle \\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle \\quad \\text { (the solution is: }|x\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle \\text { ) }\n",
        "$\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "(if I want to know get the Eigenvector from a measurement, I create the outer product / density matrix of the Eigenvector like above. this is like a measurement)\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        "> $\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$"
      ],
      "metadata": {
        "id": "MXGA0EYlF9iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Case 1 - Bra-Ket (Inner Product)**: *I want to know the Eigenvalue of an observable (but I don't know which is the most probable Eigenvector)? Then I apply an operator to the quantum state (position operator, momentum operator etc). I will get the most probable Eigenvector / Eigenstate with an according Eigenvalue*\n",
        "\n",
        "> $\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$\n",
        "\n",
        "= apply a certain operator (=observable) on a quantum state and you get the Eigenvalue of this state (here u can also be considered the eigenstate / eigenvector)\n",
        "\n",
        "<font color=\"blue\">**Case 2 - Ket-Bra (Outer Product)**: *I want to measure a specific Eigenstate / Eigenvector to get its probability? Then I take the outer product / density matrix of the desired outcome and make a measurement on the quantum state / system (=project it onto the desired Eigenvector)*\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "*  mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m}\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* here you can re-arrange and get an outer product (true?):\n",
        "\n",
        "> $\n",
        "|\\psi\\rangle=\\sum_{m}\\left|u_{m}\\right\\rangle\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "\n",
        "* Passt zu weiter unten (video prof $m$ mit projection operators):\n",
        "\n",
        "> $\n",
        "(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle\n",
        "$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_250.png)\n",
        "\n",
        "Video: [Projection operators in quantum mechanics](https://www.youtube.com/watch?v=M9V4hhqyrKQ)\n",
        "\n",
        "> **Projection Operators project one quantum state on another or onto a subspace of the state space**\n",
        "\n",
        "* for example: when we measure a property of a quantum particle, then the state of the particle collapses onto a different state\n",
        "\n",
        "* the projection operator mathematically describes this collapse\n",
        "\n",
        "> **projection operator associated with a Ket Psi: $| \\Psi \\rangle$ is the outer product of psi with itself: $|\\Psi\\rangle \\langle \\Psi|$**\n",
        "\n",
        "* then we check what it does on an arbirary state phi\n",
        "\n",
        "* the projection operator projects an arbitrary state (here Phi) onto the reference state (here Psi).\n",
        "\n",
        "* the proportionality constant C is given by the overlap between the initial state phi and the state psi that defines the projection operator\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_264.png)\n",
        "\n",
        "Properties of the projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_265.png)\n",
        "\n",
        "Eigenvalues and Eigenstates of Projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_266.png)\n",
        "\n",
        "The Projection operator allows us to write any Ket as a sum of two other Kets:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_267.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_268.png)\n",
        "\n",
        "One common way in which the projection operator is used in quantum mechanics is to project onto a subspace of the whole state space.\n",
        "\n",
        "* Most convenient way to write this down is in terms of basis states of our state space.\n",
        "\n",
        "* For a basis u we consider a subset of basis states u1, to un which soon an n-dimensional subspace of the full state space.\n",
        "\n",
        "* We then write the projection operator onto this n-dimensional subspace as pn\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_269.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_270.png)\n",
        "\n",
        "**Projective measurement (PVM - Projection-valued measure)**\n",
        "\n",
        "* See [this article](https://en.m.wikipedia.org/wiki/Measurement_in_quantum_mechanics#Projective_measurement) and longer article on [PVM](https://en.m.wikipedia.org/wiki/Projection-valued_measure)\n",
        "\n",
        "* The eigenvectors of a von Neumann observable form an orthonormal basis for the Hilbert space, and each possible outcome of that measurement corresponds to one of the vectors comprising the basis. A density operator is a positive-semidefinite operator on the Hilbert space whose trace is equal to 1.\n",
        "\n",
        "> <font color =\"red\">**For each measurement that can be defined, the probability distribution over the outcomes of that measurement can be computed from the density operator. The procedure for doing so is the Born rule, which states that**</font>\n",
        "\n",
        "> $\n",
        "P\\left(x_{i}\\right)=\\operatorname{tr}\\left(\\Pi_{i} \\rho\\right)\n",
        "$\n",
        "\n",
        "* where $\\rho$ is the density operator, and $\\Pi_{i}$ is the [projection operator](https://en.m.wikipedia.org/wiki/Projection_(linear_algebra)) onto the basis vector corresponding to the measurement outcome $x_{i}$.\n",
        "\n",
        "* The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the [expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)) of that observable. For an observable $A$, the expectation value given a quantum state $\\rho$ is\n",
        "\n",
        ">$\n",
        "\\langle A\\rangle=\\operatorname{tr}(A \\rho)\n",
        "$\n",
        "\n",
        "* A density operator that is a rank-1 projection is known as a pure quantum state, and all quantum states that are not pure are designated mixed. Pure states are also known as wavefunctions."
      ],
      "metadata": {
        "id": "qy0sjtMGU9XY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kD-s4t82ez"
      },
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi |$ - Mixed States (Density Matrix)***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Use Case 2: Mixed States (Density Matrix)***\n",
        "\n",
        "Die [Quantenstatistik](https://de.m.wikipedia.org/wiki/Quantenstatistik) wendet zur Untersuchung makroskopischer Systeme die Methoden und Begriffe der klassischen statistischen Physik an und ber√ºcksichtigt zus√§tzlich die quantenmechanischen Besonderheiten im Verhalten der Teilchen.\n",
        "\n",
        "Wie die Quantenmechanik ber√ºcksichtigt auch die Quantenstatistik die folgende doppelte Unkenntnis:\n",
        "\n",
        "> 1. Kennt man den Zustand eines Systems genau ‚Äì liegt also ein **reiner Zustand (pure state)** vor ‚Äì und ist dieser kein Eigenzustand der Observablen, so kann man den Messwert einer Einzelmessung dennoch nicht exakt vorhersagen.\n",
        "\n",
        "* A pure state can be written in terms of a Ket state $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$\n",
        "\n",
        "  * <font color =\"blue\">$|\\psi\\rangle$</font> $=\\sum_{i} c_{i}\\left|\\varphi_{i}\\right\\rangle$\n",
        "\n",
        "  * Spin up or spin down = **pure states**, can be written as a single wave function $\\mid \\psi\\rangle$.\n",
        "\n",
        "> 2. Kennt man den Zustand des Systems nicht genau, so muss von einem **gemischten Zustand (mixed state)** ausgegangen werden. Ebenso: when one wants to describe a physical system which is entangled with another, as its state can not be described by a pure state. For mixed states with noise in qubits.\n",
        "\n",
        "* A mixed state can be described with a density matrix:\n",
        "\n",
        "  * <font color =\"blue\">$\\rho$ </font>$=\\sum_{i} p_{i}\\left|\\varphi_{i} \\times \\varphi_{i}\\right|$\n",
        "  * Sum over probabilities times the corresponding projection operators onto certain basis states $\\varphi_{i}$\n",
        "\n",
        "  * It allows for the calculation of the probabilities of the outcomes of any measurement performed upon this system, using the Born rule.\n",
        "\n",
        "**There are two more ways to distinguish a pure state from a mixed state:**\n",
        "\n",
        "1. Take the trace of the square of the density matrix:\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]=1 \\rightarrow$ Pure state (like Summe der Eigenwerte entlang der Spur: 1-0-0-0..)\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]< 1 \\rightarrow$ Mixed state. <font color=\"red\">But in the examples below the trace is always = 1? (and some off-diagonal elements)</font>\n",
        "\n",
        "2. Geometricaly, pure states lie on the surface of the Blochsphere. Mixed states are confined within the Bloch sphere.\n",
        "\n",
        "> $\n",
        "\\begin{array}{c|c|c}\n",
        "& \\begin{array}{c}\n",
        "\\text { Pure } \\\\\n",
        "\\text { State }\n",
        "\\end{array} & \\begin{array}{c}\n",
        "\\text { Mixed } \\\\\n",
        "\\text { State }\n",
        "\\end{array} \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Density } \\\\\n",
        "\\text { Matrix } \\\\\n",
        "|\\psi\\rangle\\langle\\psi|\n",
        "\\end{array} & \\color{green} ‚úî & \\color{green}‚úî \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Wave } \\\\\n",
        "\\text { Function } \\\\\n",
        "|\\Psi\\rangle\n",
        "\\end{array} & \\color{green}‚úî & \\color{red} ‚ùå\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Why do we need this alternative formalism, these density matrices?**\n",
        "\n",
        "* To illustrate the difference, think about this Ket $|\\psi\\rangle$, which is the equal superposition of 0 and 1. We can write out the vector form $\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right]$. And if we write the corresponding $\\rho$, it will have 0.5 for every element in the matrix.\n",
        "\n",
        "> $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+(1\\rangle)=\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right] \\rightarrow $$\\rho=\\left[\\begin{array}{ll}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* On the other hand if we create the uniform distribution over the density matrix corresponding to the 0-Ket and the density matrix corresponding to the 1-Ket, then this density matrix will be different:\n",
        "\n",
        "> $\\rho^{\\prime}=\\frac{1}{2}(|0 \\times 0|+|1 \\times 1|)=\\left[\\begin{array}{ll}0.5 & 0 \\\\ 0 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* It doesn't have off-diagonal elements. These off-diagonal elements are critical for many quantum operations, sometimes also called 'coherences'. This is then called a maximally mixed state = equivalent of a uniform distribution in classical probability theory. This means we have absolutely no predictive power of what's going to happen next. Entropy of the state is maximal.\n",
        "\n",
        "* Ideally we want quantum states with a high coherence, but in reality noise affects, and these coherences disappear.\n",
        "\n",
        "https://www.youtube.com/watch?v=BE8RxAESx5I&list=PLBn8lN0Dcvpla6a6omBni1rjyQJ4CssTP&index=47\n"
      ],
      "metadata": {
        "id": "XKh8o95dlsKS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiVIYzwp82e1"
      },
      "source": [
        "**Explanation 1: Density Matrix for Mixed States**\n",
        "\n",
        "*Source here point 2.1: https://qiskit.org/textbook/ch-quantum-hardware/density-matrix.html*\n",
        "\n",
        "* Let's consider the case where we initialize a qubit in the |0‚ü© state, and then apply a Hadamard gate.\n",
        "\n",
        "* **Now, unlike the scenario we described for pure states, this Hadamard gate is not ideal: Due to errors in the quantum-computer hardware, only 80% of the times the state is prepared**, this Hadamard gate produces the desired state:\n",
        "\n",
        "> $\\left|\\psi_{1}\\right\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "* The remaining $20 \\%$ of the times, the pulse applied to rotate the state is either too short or too long by $\\frac{\\pi}{6}$ radians about the $x$-axis. This means that when we use this Hadamard gate, we could end up with following two undesired outcome states:\n",
        "\n",
        ">$\n",
        "\\left|\\psi_{2}\\right\\rangle=\\frac{\\sqrt{3}}{2}|0\\rangle+\\frac{1}{2}|1\\rangle, \\quad\\left|\\psi_{3}\\right\\rangle=\\frac{1}{2}|0\\rangle+\\frac{\\sqrt{3}}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "* The figure below shows the Bloch representation for the three possible states our qubit could take if the short pulse happens 10% of the time, and the long pulse the remaining 10% of the time:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_198.png)\n",
        "\n",
        "* Since we do not know the outcome of our qubit everytime we prepare it, we can represent it as a mixed state of the form:\n",
        "\n",
        "**Step 1: how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:**\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "in this case:\n",
        "\n",
        "> $\n",
        "\\rho_{H}=\\frac{4}{5}\\left|\\psi_{1}\\right\\rangle\\left\\langle\\psi_{1}\\left|+\\frac{1}{10}\\right| \\psi_{2}\\right\\rangle\\left\\langle\\psi_{2}\\left|+\\frac{1}{10}\\right| \\psi_{3}\\right\\rangle\\left\\langle\\psi_{3}\\right|$\n",
        "\n",
        "* Here, the factors $\\frac{4}{5}, \\frac{1}{10}$ and $\\frac{1}{10}$ correspond to the classical probabilities of obtaining the states $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle$ and $\\left|\\psi_{3}\\right\\rangle$, respectively.\n",
        "\n",
        "**Step 2: Add the matrices together**: By replacing each of these three possible state vectors into  $\\rho$ , we can find the density matrix that represents this mixture:\n",
        "\n",
        "> $\\rho_{H}=\\frac{4}{5}\\left[\\begin{array}{ll}\\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4}\\end{array}\\right]$\n",
        "$\\rho_{H}=\\left[\\begin{array}{cc}\\frac{1}{2} & \\frac{\\sqrt{3}}{20}+\\frac{2}{5} \\\\ \\frac{\\sqrt{3}}{20}+\\frac{2}{5} & \\frac{1}{2}\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWPEnYF82e2"
      },
      "source": [
        "**Explanation 2: Density Matrix for Mixed States**\n",
        "\n",
        "*Source: Parth G: https://www.youtube.com/watch?v=ZAOc4eMTQiw*\n",
        "\n",
        "* **Pain Point**:  when we see this notation for two states spin up and spin down: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$ we think of it as being in a superposition of both states. But sometimes we don't know if the system is actually in this superposition, or not already collapsed to one.\n",
        "\n",
        "* So, in practice before we measure a system, it can be in following three states, because we lack information about it. This is a mixed state:\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\uparrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\downarrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$\n",
        "\n",
        "* **Mixed state**: lack of information about the system, hence for example giving equal probability to each possible outcome.\n",
        "\n",
        "* **When dealing with a mixed state we need to represent it with what's known as a density operator or density matrix**.\n",
        "\n",
        "* Wave function notation $|\\psi\\rangle$ can only be used to describe pure states. But density matrices $|\\psi\\rangle\\langle\\psi|$ can be used to describe mixed and pure states.\n",
        "\n",
        "* and how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "On top of that you can even add several mixed states to get one final mixed states. Mixed state: our knowledge of the system is limited. It could be either one of multiple psi states.\n",
        "\n",
        "* Image 1: We have to add up the density matrices of each possible spin state whilst making sure that we weight it with the probability of it being in that psi state\n",
        "\n",
        "* Image 2: That would give us the final density matrix, which is the mixture in the mixed state.\n",
        "\n",
        "* Image 3: it is for this reason that there is no way to write a mixed state as a single wave function or a single vector. We have to deal with matrices that represent the different pure states in which our system could be. And each of these density matrices has to be weighted by how likely it is that our system is on that pure that.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_006a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXuS8c94Iay"
      },
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi |$ - Expectation Values***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An observable in quantum mechanics is represented by a Hermitian operator. The spectral norm (or operator norm) of a Hermitian operator is equal to its largest eigenvalue in absolute value."
      ],
      "metadata": {
        "id": "5cVUHgYvHoiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Density matrix formalism</u> to calculate Expectation Values**\n",
        "\n",
        "\n",
        "* [Expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)): The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the expectation value of that observable. What is the average value of quantum measurement outcomes?\n",
        "\n",
        "* Large number of particle interactions when we are trying to measure a total effect from an aggregation of subatomic particles: run an experiment involving the observation of energy levels in a type of atom N times under fixed conditions. The expectation value e of the experiment turns out to be an illegitimate energy level for that atom, but E = Ne will give the correct total energy assuming N is large enough.\n",
        "\n",
        "* Expectation value of an observable $Q$ (like the momentum of a particle) in a (normalized) state $|\\Psi\\rangle$ gives the average of all possible values (weighted by their corresponding probabilities) that one may expect to observe in an experiment designed to measure $Q$ in state $|\\Psi\\rangle$ over many experiments where the average of all values of $Q$ will approach the expectation value $\\langle\\Psi|Q| \\Psi\\rangle$ (one set of measurement perturbs the system)"
      ],
      "metadata": {
        "id": "NWbNfdo0aS0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**First: The expectation value of an operator $Q$ is given by $\\langle Q\\rangle=\\langle\\psi|Q| \\psi\\rangle$ = $\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$(spectral decomposition)**</font>\n",
        "\n",
        "* Given a state vector for a pure state $|s\\rangle$, with its complex-conjugate transpose is $\\langle s|$, the operator whose expectation value you are evaluating could be written $|A|$ and $|s\\rangle$ has a component which is a $\\lambda$-eigenstate for $A$.\n",
        "\n",
        "\n",
        "* A mixture of states is a weighted average of such operators, and its expectation value is the weighted average of the expectation values of the pure states.\n",
        "\n",
        "* Suppose, $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle, \\ldots$ be the orthonormal basis of eigenstates of observable $Q$ with corresponding eigenvalues $\\lambda_{1}, \\lambda_{2}, \\ldots$ respectively. One can expand the given (normalized) state $|\\Psi\\rangle$ in the above basis as\n",
        "\n",
        "> $\n",
        "|\\Psi\\rangle=\\alpha_{1}\\left|\\psi_{1}\\right\\rangle+\\alpha_{2}\\left|\\psi_{2}\\right\\rangle+\\cdots+\\alpha_{n}\\left|\\psi_{n}\\right\\rangle+\\ldots\n",
        "$\n",
        "\n",
        "* According to a basic postulate of Quantum mechanics the above expansion implies that upon measuring $Q$ in state $|\\Psi\\rangle$ the outcome of the experiment will be eigenvalue $\\lambda_{1}$ with probability $\\left|\\alpha_{1}\\right|^{2}$, eigenvalue $\\lambda_{2}$ with probability $\\left|\\alpha_{2}\\right|^{2}$ and so on. The (weighted) average of all possible values of $Q$ that can be observed in state $|\\Psi\\rangle$ will be the expectation value of $Q$ in state $|\\Psi\\rangle$:\n",
        "\n",
        "> <font color=\"blue\">$\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$ = $\\left|\\alpha_{1}\\right|^{2} \\lambda_{1}+\\left|\\alpha_{2}\\right|^{2} \\lambda_{2}+\\ldots =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots| \\, | \\lambda_{1} \\alpha_{1} \\psi_{1}+\\lambda_{2} \\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots|Q| \\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\langle\\Psi|Q| \\Psi\\rangle$\n",
        "\n",
        "* This is because operator $Q$ can be written as a sum over its Eigenvalues $a_i$ times projection operators onto its Eigenstates.\n",
        "\n",
        "> $Q=\\sum_{i} \\lambda_{n} \\left| u_{n} \\rangle \\langle \\ u_{n}\\right|$ $\\quad$ with $Q\\left| u_{n}\\right\\rangle= \\lambda_{n}\\left| u_{n}\\right\\rangle$\n",
        "\n",
        "* By using decomposition in the definition of the expectation value, we get this result:\n",
        "\n",
        "> $\\langle Q\\rangle_{\\psi}=\\langle\\psi|Q| \\psi\\rangle=$ $\\sum_{i} \\lambda_{n}\\langle\\psi| u_{n} \\rangle $ <font color =\"orange\">$\\langle u_{u}| \\psi \\rangle$</font> $=\\sum_{i} \\lambda_{n}$ <font color =\"blue\">$\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We sum over all Eigenvalues, which are the possible outcomes of a measurement, weighted by the probabilities that this particular Eigenvalue occurs if we start in the state $\\Psi$.\n",
        "\n",
        "  * <font color =\"orange\">$\\langle u_{n} | \\psi\\rangle$</font> - this is called the **transition amplitude**\n",
        "\n",
        "  * <font color =\"blue\">$|\\langle u_{n} | \\psi\\rangle|^{2}$</font> - this is called the **transition probability** (absolute square of the transition amplitude)\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ and since: $P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$:\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|c_{n}\\right|^{2}$</font>\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow\\langle\\hat{A}\\rangle_{\\psi}=\\langle\\hat{A}\\rangle=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ <font color =\"blue\">$=\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* <font color =\"blue\">$\\rightarrow$ die Summe der Eigenwerte $\\lambda_{n}$ jeweils multipliziert mit der Wahrscheinlichkeit $P\\left(\\lambda_{n}\\right) = \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$, den jeweiligen Eigenwert zu erhalten = Expectation Value</font>\n",
        "\n",
        "* Beispielrechnung mit interessantem Ergebnis: Eigenwert -1 mit Amplitude 0.25 und Eigenwert +1 mit Amplitude 0.25\n",
        "\n",
        "  * = $(-1) * (0.25)^2$ + $(+1) * (0.25)^2$\n",
        "\n",
        "  * = $(-1) * (0,5)$ + $(+1) * (0.5)$ = -0.5 + 0.5\n",
        "\n",
        "  * = 0 $\\rightarrow$ Expectation value, aber kein erlaubtes physikalisches Ergebnis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Im0HXIwpZSfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Second: The expectation value $\\langle \\psi|A| \\psi\\rangle$ can be evaluated as the trace of $|\\psi\\rangle\\langle \\psi| A$.**</font>\n",
        "\n",
        "* The operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1, and the operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1.\n",
        "\n",
        "* $\\rho = |\\psi\\rangle \\langle \\psi|$ is an operator of trace=1. We can calculate the expectation value of $A$ by taking the trace over $\\rho$ times $A$:\n",
        "\n",
        "> **$\\langle A\\rangle_{\\rho}:= \\langle \\psi|A| \\psi\\rangle = \\operatorname{Tr}[\\rho A] =\\operatorname{Tr}[\\, |\\psi\\rangle \\langle \\psi| \\, A]$**</font>\n",
        "\n",
        "* Taking the trace over an operator means to sum over the matrix elements of that operator between states of a complete basis $\\operatorname{Tr}[B]=\\sum_{i}\\left\\langle\\eta_{i}|B| \\eta_{i}\\right\\rangle$. We are adding $\\rho = |\\psi\\rangle \\langle \\psi|$ and move the two complex numbers $\\eta$, since they are not operators:\n",
        "\n",
        "> $\\langle A\\rangle_{\\rho}:=\\operatorname{Tr}[\\rho A]$ = $\\sum_{i} {\\left\\langle\\eta_{i}\\right| \\psi \\rangle}{\\left\\langle\\psi|A| \\eta_{i}\\right\\rangle}$\n",
        "\n",
        "* We sum over the basis states. Since $| \\eta_{i} \\rangle \\langle \\eta_{i} |$ = 1, we get $\\langle\\psi|A| \\psi\\rangle$, which is the same result from working with density matrices:\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi|A| \\eta_{i} \\rangle \\langle \\eta_{i} | \\psi\\right\\rangle$ = $\\langle\\psi|A| \\psi\\rangle=\\langle A\\rangle_{\\psi}$\n"
      ],
      "metadata": {
        "id": "taewaWhtaMqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_195.png)"
      ],
      "metadata": {
        "id": "lX-bFe39aNwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_201.png)"
      ],
      "metadata": {
        "id": "RyU5akyieNvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Eigenvalues, Observables & Operators*"
      ],
      "metadata": {
        "id": "gXZXzSSsrrUL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpO0HLIBkdmU"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "Video: [Eigenvalues and eigenstates in quantum mechanics](https://www.youtube.com/watch?v=p1zg-c1nvwQ)\n",
        "\n",
        "> We consider the action of $\\hat{A}$ on a special Ket $|\\Psi\\rangle$ such that the only way in which $\\hat{A}$ changes $|\\Psi\\rangle$ is by scaling it by a constant and we obtain $\\lambda |\\Psi\\rangle$\n",
        "\n",
        "The Eigenvector of an operator are those special directions in the vector space which the operator doesn't change.\n",
        "\n",
        "The probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_257.png)\n",
        "\n",
        "> [C$*$-algebras](https://en.m.wikipedia.org/wiki/C*-algebra) were first considered primarily for their use in quantum mechanics **to model algebras of physical observables**. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. See also [Quantum Group](https://en.m.wikipedia.org/wiki/Quantum_group).\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_251.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Anti-)Commutators, Quantum Numbers und \"Vollst√§ndiger Satz kommutierender Observablen\"**\n",
        "\n",
        "Um einen quantenmechanischen Zustand eindeutig zu charakterisieren, sind oft mehrere Observablen notwendig. Beispielsweise ist es beim Wasserstoffatom nicht ausreichend, nur die Energie anzugeben (mittels der Hauptquantenzahl n), sondern es sind zwei weitere Observablen notwendig: der Betrag des Drehimpulses (Quantenzahl l) und die z-Komponente des Drehimpuls (Quantenzahl m). Diese drei Gr√∂√üen bilden dann einen vollst√§ndigen Satz kommutierender Observablen.\n",
        "Eine Menge von Observablen A, B, C,... bildet einen v.S.k.O., wenn eine orthonormale Basis des Zustandsraums aus gemeinsamen Eigenvektoren der Observablen existiert, und diese Basis (bis auf einen Phasenfaktor) eindeutig ist.\n",
        "\n",
        "Solch ein Verhalten ist in der Quantenmechanik allerdings eher die Ausnahme. Die meisten Paare von Observablen lassen sich nicht gleichzeitig beliebig genau messen, was eine Konsequenz aus der heisenbergschen Unsch√§rferelation ist. Man spricht dann auch von komplement√§ren Observablen.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Satz_kommutierender_Observablen"
      ],
      "metadata": {
        "id": "rRJWyBvzCSAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc9HeAnZvFPv"
      },
      "source": [
        "**Solving: How to find Eigenvalues and Eigenvectors?**\n",
        "\n",
        "* For an arbitrary operator (except the identity operator) it is generally not possible to figure out the eigenvalues and eigenstates simply by inspection of how the operator acts\n",
        "\n",
        "* More general approach needed:\n",
        "\n",
        "\t* consider basis u that is orthonormal.\n",
        "\n",
        "\t* Then we write the Eigenvalue equation in the u basis (just project both sides of the equation onto the basis states u).\n",
        "\n",
        "\t* next we insert the identity operator after the A operator on left and side\n",
        "\n",
        "\t* then write the resolution of the identity in the u basis\n",
        "\n",
        "\t* then take sum on the beginning\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_260.png)\n",
        "\n",
        "* the equation $\\sum_{j}\\left(A_{i j}-\\lambda \\delta_{i j}\\right) c_{j}=0$ is equivalent to the original Eigenvalue equation $\\hat{A}|\\psi\\rangle=\\lambda|\\psi\\rangle$, but now written in the u representation\n",
        "\n",
        "* finding the eigenvalues and eigenvectors now becomes finding the lambda and c in the new equation = \"matrix diagonalization\"\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_261.png)\n",
        "\n",
        "> **All we really need to do quantum mechanics is to get enough practice in diagonalizing matrices**\n",
        "\n",
        "* we have following operator A and state Psi\n",
        "\n",
        "* we want to find the eigenvalues and eigenvectors of the operator A\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYnM169ZFHeZ"
      },
      "source": [
        "<font color=\"blue\">**Operators acting on Quantum State Vectors: Matrix-Vector-Multiplication (Single Qubit)**\n",
        "\n",
        "*Applying a Hadamard gate to a single qubit (matrix-vector multiplication) - Simple dot product! You get a vector out.*\n",
        "\n",
        "$H |0\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 1 + 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right] = |+\\rangle$\n",
        "\n",
        "$H |1\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}0 + 1 \\\\ 0 -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right] = |-\\rangle$\n",
        "\n",
        "*Diese Zust√§nde k√∂nnen auch mithilfe der Dirac-Notation als Summen von |0‚ü© und |1‚ü© erweitert werden:*\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "*Zusammenfassend, das ist alles das gleiche, sieht aber komplett anders aus:*\n",
        "\n",
        "<font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "<font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*$|0\\rangle$ und $|1\\rangle$ stellen hier die Basis dar, in der die Quantenzustaende berechnet werden.* ***Man kann allerdings auch eine andere Basis waehlen, zB $|+\\rangle$ und $|-\\rangle$:***\n",
        "\n",
        "$|0\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle+|-\\rangle)$ vergleiche: <font color=\"blue\">$|+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "$|1\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle-|-\\rangle)$ vergleiche: <font color=\"blue\">$|-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*Applying an Identity Operator:*\n",
        "\n",
        "$I |0\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 0\\end{array}\\right]=\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "$I |1\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\left[\\begin{array}{ll}0 + 0 \\\\ 0 + 1\\end{array}\\right]=\\left[\\begin{array}{c}0 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "*Applying a Pauli-X Operator:*\n",
        "\n",
        "$X|0\\rangle=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 + 0 \\\\ 1 + 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTK-u1Q4nrGv"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Vector-Multiplication (Multi Qubit)**\n",
        "\n",
        "> $A \\mathbf{x}=\\left[\\begin{array}{cccc}a_{11} & a_{12} & \\ldots & a_{1 n} \\\\ a_{21} & a_{22} & \\ldots & a_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m 1} & a_{m 2} & \\ldots & a_{m n}\\end{array}\\right]\\left[\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right]=\\left[\\begin{array}{c}a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n} \\\\ a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n} \\\\ \\vdots \\\\ a_{m 1} x_{1}+a_{m 2} x_{2}+\\cdots+a_{m n} x_{n}\\end{array}\\right]$\n",
        "\n",
        "**First create the tensor product of two operators**: For construction of the desired two-qubit gate, you need the same tensor product operation as you used for the vectors. Here where $H_1$ is a one-qubit Hadamard gate in the two-qubit space $(\\hat{H} \\otimes \\mathbf{I})$, where Hadamard is applied only to one Qubit:\n",
        "\n",
        "> $H_{1} \\equiv H_{0} \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "**Second, apply the new 'tensored' operator on the tensor product of two state vectors**:\n",
        "\n",
        "See the tensor product of two vectors in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Now applying $H_1$ you mix up the first qubit states and keep the second qubit state unchanged:\n",
        "\n",
        "> $H_{1}(|0\\rangle \\otimes|0\\rangle)=H_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$= <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right) \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right)$</font>= $\\frac{1}{\\sqrt{2}}(|0\\rangle \\otimes|0\\rangle+|1\\rangle \\otimes|0\\rangle)$\n",
        "\n",
        "**Another way of writing this (=apply H to one qubit only in a 1 qubit in a 2 qubit system) for a joint state of two initialized qubits $\n",
        "|0\\rangle \\otimes|0\\rangle\n",
        "$ is:**\n",
        "\n",
        "> <font color=\"orange\">$\\hat{H}\\left|q_{0}\\right\\rangle \\otimes\\left|q_{1}\\right\\rangle$</font> = $\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "The tensor product is distributive, which in this case means it acts much like multiplication:\n",
        "\n",
        ">$\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\otimes|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "See [Tensor_product](https://en.wikipedia.org/wiki/Tensor_product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z7cDFlh_eL1"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Matrix-Multiplication (Kronecker / Tensor)**\n",
        "\n",
        "Zur Kombination von Operators in einem Timestep (zB H bei qubit 1 und I bei Qubit 2). Two systems being described as a joint system.\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right]=\\left[\\begin{array}{lll}a\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & b\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] \\\\ c\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & d\\left[\\begin{array}{llll}e & f \\\\ g & h\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{llll}a e & a f & \\text { be } & b f \\\\ a g & a h & b g & b h \\\\ c e & c f & d e & d f \\\\ c g & c h & d g & d h\\end{array}\\right]$\n",
        "\n",
        "$H \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\otimes\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{rrrr}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "$I \\otimes H=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\otimes \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\\\ 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1\\end{array}\\right)$\n",
        "\n",
        "$Y \\otimes X=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & -i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\\\ i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & 0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{cccc}0 & 0 & 0 & -i \\\\ 0 & 0 & -i & 0 \\\\ 0 & i & 0 & 0 \\\\ i & 0 & 0 & 0\\end{array}\\right]$\n",
        "\n",
        "$X \\otimes H=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\otimes \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] \\\\ 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cccc}0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0\\end{array}\\right] =\\left[\\begin{array}{cc}\n",
        "0 & H \\\\\n",
        "H & 0\n",
        "\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnohitquaGyf"
      },
      "source": [
        "**<font color=\"blue\">Matrix-Matrix-Multiplication (Usual)**\n",
        "\n",
        "> Used in serially wired gates (so NOT gates in one time step - here we use tensor product to combine them, but serial gates!)\n",
        "\n",
        "A line in the circuit is considered as a quantum wire and basically represents a single qubit. The product of operators keeps the same dimension.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Serially_wired_quantum_logic_gates.png/500px-Serially_wired_quantum_logic_gates.png)\n",
        "\n",
        "For example, putting the Pauli X gate after the Pauli Y gate, both of which act on a single qubit, can be described as a single combined gate C:\n",
        "\n",
        ">$\n",
        "C=X \\cdot Y=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right] \\cdot\\left[\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
        "i & 0 \\\\\n",
        "0 & -i\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "X \\cdot X=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89sYf-N0_6u"
      },
      "source": [
        "<font color=\"blue\">**Hamiltonian Operator $\\mathcal{H}$**\n",
        "\n",
        "> <font color=\"red\">**The sum of the possible outcomes of kinetic and potential energy of this entire system in quantum mechanics is referred to the Hamiltonian $\\mathcal{H}$ (to calculate the lowest total energy of a two atom system)**\n",
        "\n",
        "* der [Hamiltonoperator](https://de.m.wikipedia.org/wiki/Hamiltonoperator) (Energieoperator) ist in der Quantenmechanik ein Operator, **der (m√∂gliche) Energiemesswerte und die Zeitentwicklung angibt = describes the total energy of a system or particle**. <font color=\"red\">Er liefert beispielsweise die Energieniveaus des Elektrons im Wasserstoffatom.</font>\n",
        "\n",
        "* In der Quantenmechanik wird jeder Zustand des betrachteten physikalischen Systems durch einen zugeh√∂rigen Vektor $\\psi$ im Hilbertraum angegeben. Seine Zeitentwicklung wird nach der Schr√∂dingergleichung durch den Hamiltonoperator $\\hat{H}$ bestimmt:\n",
        "\n",
        ">$\n",
        "\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\psi(t)=\\hat{H} \\psi(t)\n",
        "$\n",
        "\n",
        "* **For every problem there is a different Hamiltonian and a different corresponding Eigenspectrum**. Spektrum: Bereich der m√∂glichen Messwerte. Eigenvalues = stabile Energielevel = Zustande, die die Elektronen in den Orbitalen beschreiben. Siehe auch [Hamilton-Funktion](https://de.m.wikipedia.org/wiki/Hamilton-Funktion).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwSGedvzNov"
      },
      "source": [
        "<font color=\"blue\">**Time Evolution Operator $\\mathcal{U}$ bzw. $\\mathcal{T}$**\n",
        "\n",
        "* Der [Zeitentwicklungsoperator](https://de.m.wikipedia.org/wiki/Zeitentwicklungsoperator) $\\mathcal{U}$ bzw. $\\mathcal{T}$ ist ein quantenmechanischer Operator, mit dem sich die zeitliche Entwicklung eines physikalischen Systems berechnen l√§sst. Siehe auch [Time evolution](https://en.m.wikipedia.org/wiki/Time_evolution)\n",
        "\n",
        "* Der quantenmechanische Operator ist eng verwandt mit dem [Propagator](https://de.m.wikipedia.org/wiki/Propagator) in der Quantenfeld- oder Vielteilchentheorie. √úblicherweise wird er als $U\\left(t, t_{0}\\right)$ geschrieben und bezeichnet die Entwicklung des Systems vom Zeitpunkt $t_{0}$ zum Zeitpunkt $t$.\n",
        "\n",
        "Der Zeitentwicklungsoperator $U\\left(t, t_{0}\\right)$ wird definiert √ºber die Zeitentwicklung eines beliebigen Zustandes $|\\psi\\rangle$ zu einem Zeitpunkt $t_{0}$ bis zum Zeitpunkt $t$ :\n",
        "\n",
        ">$\n",
        "|\\psi(t)\\rangle=U\\left(t, t_{0}\\right)\\left|\\psi\\left(t_{0}\\right)\\right\\rangle \\quad \\forall|\\psi\\rangle\n",
        "$\n",
        "\n",
        "Einsetzen in die Schr√∂dingergleichung liefert einen Satz gew√∂hnlicher Differentialgleichungen 1. Ordnung:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} U\\left(t, t_{0}\\right)=H(t) U\\left(t, t_{0}\\right)$\n",
        "\n",
        "Diese Gleichungen sind zur Schr√∂dingergleichung insofern √§quivalent, als sie die Erweiterung des Zeitentwicklungsoperators um einen infinitesimalen Zeitschritt $\\delta t$ beschreiben:\n",
        "\n",
        ">$\n",
        "U\\left(t+\\delta t, t_{0}\\right)=\\left(1-\\frac{i}{\\hbar} H(t) \\delta t\\right) U\\left(t, t_{0}\\right)+O\\left(\\delta t^{2}\\right)\n",
        "$\n",
        "\n",
        "mit dem Hamiltonoperator $H$, der den Erzeuger der Zeitentwicklungen darstellt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBWmyGiyVUq"
      },
      "source": [
        "<font color=\"blue\">**Position Operator $\\hat{x}$**\n",
        "\n",
        "Der [Ortsoperator](https://de.m.wikipedia.org/wiki/Ortsoperator) geh√∂rt in der Quantenmechanik zur Ortsmessung von Teilchen.\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch gegeben durch den zugeh√∂rigen Vektor eines Hilbertraumes $\\mathrm{H}$.\n",
        "\n",
        "* Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathrm{H}$ dargestellt.\n",
        "\n",
        "Speziell ist der Ortsoperator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{x}}=\\left(\\hat{x}_{1}, \\hat{x}_{2}, \\hat{x}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{x}_{j}\\right)=\\left\\langle\\hat{x}_{j} \\Psi, \\Psi\\right\\rangle_{\\mathrm{H}}, \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der j-ten Ortskoordinate des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_j3K_Pef4GA"
      },
      "source": [
        "<font color=\"blue\">**Momentum Operator $\\hat{p}$**\n",
        "\n",
        "Video: [Why Momentum in Quantum Physics is Complex](https://www.youtube.com/watch?v=kG-iihrYCG4&list=WL&index=22)\n",
        "\n",
        "Classical Momentum\n",
        "\n",
        "> p = m * v\n",
        "\n",
        "Quantum Mechanics: apply a measurement operator on wave function (eigenvalue equation:)\n",
        "\n",
        "> $\\hat{p}|\\Psi\\rangle=\\lambda|\\Psi\\rangle$\n",
        "\n",
        "Momentum measurement operator\n",
        "\n",
        "> $\\hat{p}=-i \\hbar \\frac{\\partial}{\\partial x}$ $\\quad$ with: $\\, i=\\sqrt{-1}$\n",
        "\n",
        "Der [Impulsoperator](https://de.m.wikipedia.org/wiki/Impulsoperator) $\\hat{p}$ ist in der Quantenmechanik der Operator zur Impulsmessung von Teilchen. In der Ortsdarstellung ist der Impulsoperator in einer Dimension gegeben durch (mit $\\frac{\\partial}{\\partial x}$ die partielle Ableitung in Richtung der Ortskoordinate $x$):\n",
        "\n",
        ">$\n",
        "\\hat{p}_{x}=-\\mathrm{i} \\hbar \\frac{\\partial}{\\partial x}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x}\n",
        "$\n",
        "\n",
        "Mit dem Nabla-Operator $\\nabla$ erh√§lt man in drei Dimensionen den Vektor:\n",
        "\n",
        ">$\n",
        "\\hat{\\mathbf{p}}=-\\mathrm{i} \\hbar \\nabla\n",
        "$\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch durch einen zugeh√∂rigen Vektor eines Hilbertraumes $\\mathcal{H}$ gegeben. Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathcal{H}$ dargestellt. Speziell ist der Impuls-Operator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{p}}=\\left(\\hat{p}_{1}, \\hat{p}_{2}, \\hat{p}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{p}_{j}\\right)=\\left\\langle\\Psi\\left|\\hat{p}_{j}\\right| \\Psi\\right\\rangle \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der $j$ -ten Komponente des Impulses des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Translation Operator $\\hat{T}$**\n",
        "\n",
        "* the translation operator is the operator that allows us to move quantum states from one point to another\n",
        "\n",
        "* it allows us to understand many properties of wave functions:\n",
        "\n",
        "> **the wave function in real space is related to the wave function in momentum space by a Fourier transform**\n",
        "\n",
        "> $[\\hat{x}, \\hat{p}]=i \\hbar$\n",
        "\n",
        "* position $\\hat{x}$ and momentum $\\hat{p}$ operators. Their most important property is their commutator which is equal to $i \\hbar$\n",
        "\n",
        "* **Translation operator**:\n",
        "\n",
        "> $\\hat{T}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} \\quad \\alpha \\in \\mathbb{R}$\n",
        "\n",
        "* this is an operator that translates by an amount $\\alpha$\n",
        "\n",
        "* What does it mean to have a **function of an operator**, like the exponential function here: the function of an operator is defined by its Taylor expansion\n",
        "\n",
        "* adjoint of an operator: tells us about what the operator looks like in the dual space (NOT hermitian as you can see):\n",
        "\n",
        "> $\\begin{aligned} \\hat{T}^{t}(\\alpha)=& e^{i \\alpha \\hat{r}^{\\dagger} / \\hbar}=e^{i \\alpha \\hat{p} / \\hbar}=e^{-i(-\\alpha) \\hat{p} / \\hbar}=\\hat{T}(-\\alpha) \\\\ & \\hat{p}^{+}=\\hat{p} \\end{aligned}$\n",
        "\n",
        "* Let's look at the action of T dagger alpha on T alpha:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=\\mathbb{1}$\n",
        "\n",
        "> $[\\hat{p}, \\hat{p}]=0$\n",
        "\n",
        "* remember: in general we cannot combine exponents of operators like if they were numbers, but here we can because the two exponents commute because the P operator commutes with itself\n",
        "\n",
        "* An operator whose adjoint is equal to its inverse is called a unitary operator\n",
        "\n",
        "> $\\left.\\begin{array}{l}\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=I \\\\ {[\\hat{p}, \\hat{p}]=0} \\\\ \\hat{T}(\\alpha) \\hat{T}^{\\dagger}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} e^{i \\alpha \\hat{p} / \\hbar}=I\\end{array}\\right\\} \\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)$\n",
        "\n",
        "* Overall:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)=\\hat{T}(-\\alpha)$"
      ],
      "metadata": {
        "id": "kHsMw4tA4gRv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscthBMzOLUM"
      },
      "source": [
        "###### *Schr√∂dinger Equation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIrMWsUSPKj2"
      },
      "source": [
        "> for fast moving electrons or electrons in electro-magnetic fields, the Schroedinger equation gives the wrong answers (see quantum field theory, spinor field)\n",
        "\n",
        "<font color=\"blue\">**From Newton's 2nd Law of Motion to the Schr√∂dinger Equation**\n",
        "\n",
        "**Step 1: Starting with Newton's 2nd law of motion**\n",
        "\n",
        "* let's assume a particle is moving along an x-axis and we apply several forces on it $\\overrightarrow{F}_1$, $\\overrightarrow{F}_2$, .. $\\overrightarrow{F}_n$\n",
        "\n",
        "* these forces depend on the position $x$ of the particle and the time elapsed $t$\n",
        "\n",
        "Then we can find the particle's position $x$ as a function of time using **Newton's 2nd law**:\n",
        "\n",
        "> $\\vec{F}_{net}=\\sum \\vec{F}_{i}(x, t)=m \\vec{a}  \\quad(\\text { Newton's 2nd law })$\n",
        "\n",
        "But the law of acceleration $\\vec{a}$ can be also written as the second time derivative of position, so we end up with a [governing equation](https://en.wikipedia.org/wiki/Governing_equation) like this, the **Equation of Motion**:\n",
        "\n",
        "> $m \\frac{d^{2} x}{d t^{2}}=\\sum_{i=1}^{n} F_{i}(x, t) \\quad(\\text { Equation of Motion })$\n",
        "\n",
        "* (another way of writing it is: $m \\ddot x = -kx$, see Colab 'Variationsrechnung')\n",
        "\n",
        "* Once we solve this equation for the particle's position, we could infer many things about the particle's state, such as its velocity, kinetic energy etc.\n",
        "\n",
        "* *Exkurs: The governing equations of a mathematical model describe how the values of the unknown variables (i.e. the dependent variables) change when one or more of the known (i.e. independent) variables change*\n",
        "\n",
        "**Step 2: Schr√∂dinger Equation & Operator /Functionals**\n",
        "* The goal of quantum mechanics is to solve the Schr√∂dinger Equation, which is very similar conceptually\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "\n",
        "* in contrast to classical equation of motion, where we solve for position and then get velocity, kinetic energy etc about particle's state, **in the Schrodinger equation we solve for wavefunction** $\\psi$! (because that is kind of it's position :)\n",
        "\n",
        "  * $\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}$ this is the Kinetic energy operator $L$ on the wavefunction $\\psi$\n",
        "\n",
        "  * $V$ represents the potential energy operator\n",
        "\n",
        "<font color=\"red\">**In quantum mechanics we use the term operator because you generally don't get fixed numerical values for kinetic and potential energy.**</font>\n",
        "\n",
        "* Instead you need to perform operations on the wavefunction to extract those kinetic and potential energy values. That's what these operators do.\n",
        "\n",
        "* You can think of Schrodinger equation as a statement of energy conservation: kinetic energy + potential energy = total energy (which is on the left side of the v)\n",
        "\n",
        "**Step 3: Wavefunction & (Dirac) Delta Function**\n",
        "\n",
        "* the wavefunction represents the state of a system - related to the probability of finding a particle at a particular region in the domain which it occupies\n",
        "\n",
        "* the square of the norm of the wavefunction gives the probability density function of a particle.\n",
        "\n",
        "* if you integrate this norm squared over the entire domain you will get 1\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$\n",
        "\n",
        "* you can't tell exactly where the position of a particle is before measurement, just a probability, same for velocity, momentum, kinetic energy etc.\n",
        "\n",
        "* But if you **apply the measurement operator, you change the wavefunction**! after one measurement, or further measurements will always get you the same result\n",
        "\n",
        "**So instead of being a probability distribution that covers multiple values, <font color=\"red\">by taking a measurement I change the wavefunction to a [delta function](https://de.wikipedia.org/wiki/Delta-Distribution) with one spike at what my measurement gave me. If I take more measurement on the same system, the delta function doesnt change.</font> This is called the wavefunction collapse.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_174.png)\n",
        "\n",
        "* However, if I let the system settle so that it eventually occupies its original wavefunction it had and then if I take my measurement, I might get something different according to the probability distribution corresponding to my original wavefunction\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_175.png)\n",
        "\n",
        "**Step 4: Partial Differential Equation**\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "* when solving a partial differential equation we also need auxiliary conditions in order to determine the unknown constants that you get from the integration process that's inherent in solving a differential equation\n",
        "\n",
        "* auxiliary conditions = initial conditions and boundary conditions\n",
        "\n",
        "* however the Schrodinger equation doesn't come with your typical boundary conditions that you might be used to seeing\n",
        "\n",
        "**Instead,the auxiliary condition we have on our solution is the normalization constraint: $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$**\n",
        "\n",
        "* Solutions to the Schrodinger equation have two be normalizable because they are wavefunctions, and in a way they represent probability density functions\n",
        "\n",
        "\t* if the solutions to Schrodinger's equation are not normalizable, we can't use them to represent a physical system\n",
        "\n",
        "\t* the trivial solution $\\psi$ (x,t) = 0 is not normalizable, because its integral from negative infinity to infinity will always be 0, it can never be 1, which is why $\\psi$ (x,t) = 0 is an unphysical solution, because the particle has to be somewhere\n",
        "\n",
        "**Step 5: Solving Schrodinger's Equation under the normalization condition**\n",
        "\n",
        "Let's say we solve Schrodinger's equation and we get following solution:\n",
        "\n",
        "> $\\Psi (x,t) = A f(x,t)$ with $A$ being an arbitrary constant\n",
        "\n",
        "* The process of normalization is to find the value of the constant $A$ so that the solution obeys the normalization condition:\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=\\int_{-\\infty}^{\\infty}|A f|^{2} d x = 1$\n",
        "\n",
        "* we might end up with a difficult task if the wavefunction is dependent on time, which means it has a different shape for different times, then wouldn't the normalization constant change with time as well?\n",
        "\n",
        "* the answer is NO!\n",
        "\n",
        "**Theorem**: If you normalize the wavefunction once then you don't need to normalize for other times = the normalization stays preserved !\n",
        "\n",
        "**Proof**:\n",
        "\n",
        "* the norm squared of a wavefunction is just the complex conjugate of that wavefunction time the wavefunction: $|\\psi|^{2}=(\\psi)^{*} \\psi$\n",
        "\n",
        "* if we go back to the Schrodinger equation we would see that the complex conjugate of the equation would be something like this with all they size turned into their conjugates and all the imaginary terms with their signs switched:\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "> $-i \\hbar \\frac{\\partial \\psi^{*}}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi^{*}}{\\partial x^{2}}+V \\psi^{*}$\n",
        "\n",
        "* the goal of the proof is to show that this normalization integral $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=$ const doesn't change with time, it stays the same:\n",
        "\n",
        "* the way to do this is to take the time derivate: if we can show that the time derivative of the normalization integral is 0 then our proof is complete $\\frac{d}{a t}\\left[\\int_{-\\infty}^{\\infty}|\\varphi|^{2} d x\\right]=0$\n",
        "\n",
        "* see complete proof in [video](https://www.youtube.com/watch?v=kUm4q0UIpio&list=WL&index=72&t=651s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HCh0gYIJv8x"
      },
      "source": [
        "<font color=\"blue\">**Schr√∂dinger Equation (Time Independent / Eigenvalue Equation)**</font>\n",
        "\n",
        "Time independent = Total energy of a system does NOT change with time\n",
        "\n",
        "**The Schroedinger equation can be written as a type of Eigenvalue equation**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle= -i \\hbar \\frac{d}{d t}|\\psi\\rangle =\\frac{\\hbar}{i} \\frac{d}{d t}|\\psi\\rangle$\n",
        "\n",
        "**Simplified (when system is not changing over time: time-independent Schroedinger equation):**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle=E |\\psi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_151.png)\n",
        "\n",
        "> $E \\Psi(x)=\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}+V \\Psi(x)$\n",
        "\n",
        "* E = **Energy the electron** is allowed to have\n",
        "\n",
        "* $\\Psi$ = **Wavefunction** (most likely position of an electron)\n",
        "\n",
        "* **Kinetic energy**: $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}$ (klassische Form: $K E=\\frac{1}{2} m v^{2}$)\n",
        "\n",
        "* **Potential energy**: $V \\Psi(x)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ELVzEm879_7"
      },
      "source": [
        "*What would a typical Schroedinger solution look like? - All the solutions to the wave function take these two forms:*\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$ when $n=1,3,5 \\ldots$ (is odd $)$\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\sin \\left(\\frac{\\pi n x}{L}\\right)$ when $n=2,4,6 \\ldots$ (is even)\n",
        "\n",
        "*Now looking at $\\psi$, the probable position of an electron:*\n",
        "\n",
        "* central question: where is the electron?\n",
        "\n",
        "* n is the energy state / level of an electron (look above at quantum numbers)\n",
        "\n",
        "* When an electron is state n=1 (its first energy state) we apply the first formula: $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$\n",
        "\n",
        "* then we get wave function for the electron that is in a given box in this case:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_147.png)\n",
        "\n",
        "* And if we square it, we get the probability distribution (the probable position of an electron):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_148.png)\n",
        "\n",
        "\n",
        "* And here some wave functions and probability densities for other energy states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_149.png)\n",
        "\n",
        "*And the solution that popped out was this:*\n",
        "\n",
        "> $E=\\frac{\\hbar^{2} n^{2} \\pi^{2}}{2 m L^{2}}$\n",
        "\n",
        "* Everything is a constant ($\\hbar$, $\\pi$, 2, m, L) or a whole number (here: n, which stands for the different states of an electron)\n",
        "\n",
        "* which means that energy E can ony have certain discrete (=quantum) values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIv5tk_B7h9Q"
      },
      "source": [
        "<font color=\"blue\">**How to solve the Schr√∂dinger Equation (Time Independent)**\n",
        "\n",
        "Assumption: particle is moving on one line between 0 and a and limited by infinite blocks left and right, where its probability is 0 and the potential energy infinite = this means that the particle can only be found on the line between 0 and a.\n",
        "\n",
        "[SOLVING the SCHRODINGER EQUATION | Quantum Physics by Parth G](https://www.youtube.com/watch?v=sPZWtZ8vt1w)\n",
        "\n",
        "**Step 1: Take Schrodinger Equation, remove $V$ and focus on differential equation of kinetic energy term**\n",
        "\n",
        "* now we want to compute the wavefunction of this particle with the (Time Independent) Schr√∂dinger Equation\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}+V \\Psi=E \\Psi$\n",
        "\n",
        "* the potential energy V is zero between 0 and a because nothing influences the particle, so it becomes this **differential equation** that we need to solve:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}=E \\Psi$\n",
        "\n",
        "* $\\frac{d^{2} \\Psi}{d x^{2}}$ is second derivative of $\\Psi$ with respect to x\n",
        "\n",
        "We want to solve this equation:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m}$ <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> $=E \\Psi$\n",
        "\n",
        "* Normally try to solve <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> which is second derivative of $\\Psi$ with respect to x, when you know what $\\Psi$ is,\n",
        "\n",
        "* but we don't. We want to go the other way around, which is trickier.\n",
        "\n",
        "**Step 2: Rearrange the constants**\n",
        "\n",
        "Luckily we have two constants in our equation (blue):\n",
        "\n",
        "> <font color=\"blue\">$\\frac{-\\hbar^{2}}{2 m}$</font> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$E$</font> $\\Psi$\n",
        "\n",
        "Which means we can rearrange the equation to this:\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$\n",
        "\n",
        "Now we can combine all constants in one constant $-k^2$ = $\\frac{-2m E}{\\hbar^{2}}$\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "**Step 3: Identify suitable function for this equation**\n",
        "\n",
        "* So which type of function obeys this relation $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$? - Would be a [sinusoid](https://de.wikipedia.org/wiki/Sinusoid)!\n",
        "\n",
        "* $\\frac{d^2 y}{d x^{2}}=-y$ - when you start with a sine and differentiate it twice you still end up with a sinusoidal term\n",
        "\n",
        "* so if we carefully account for the constants in our equation, our solution is going to look like a sinusoid:\n",
        "\n",
        "> $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ and replacing <font color=\"blue\">$\\frac{\\sqrt{2 m E}}{\\hbar}$</font> with $k$ $\\rightarrow$ $\\Psi$ = $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "*Compare this with before (above is no minus and root taken is first term):*\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$ =  <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_176.png)\n",
        "\n",
        "**Step 4: Work our the boundaries to solve the differential equation, which means to get wavefunction**\n",
        "\n",
        "\n",
        "**When x = 0 $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* so at the wall at point a $\\Psi$ = 0, so that we first derivative is not getting infinite!\n",
        "\n",
        "* this works quite nice with sinusoidal, since $\\Psi = sin(kx)$ $\\rightarrow$ 0 = sin(k(0)) = 0\n",
        "\n",
        "**When x = a $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* 0 = sin(k(a)) since $\\Psi$ = sin(kx) $\\rightarrow$ 0 = sin(k(a)) = 0\n",
        "\n",
        "* we essentially find a restriction on the kind of sine wave that we can have as a solution\n",
        "\n",
        "* for example half a sine wave is a possible solution\n",
        "\n",
        "  * it's y=0 at point x=0 and x=a (at the walls), so $sin(ka) = 0$\n",
        "\n",
        "  * so we went through half a sine wave which means that this part in brackets (ka) must be equal to 180 degrees (because that's half a sine wave) $y = \\frac{1}{2}sin(x)$\n",
        "\n",
        "  * and if we use radians instead of degrees, which is the other unit of measuring angles, and a much more natural unit of measuring angles, then 180 degrees is actually equal to <font color=\"blue\">$\\pi$ radians = (ka)</font>\n",
        "\n",
        "  * So: $y = \\frac{1}{2}sin(x) = \\pi$\n",
        "\n",
        "* this means that this equation holds true if our wavefunction is half a sine wave <font color=\"blue\">$(ka)$ = $\\pi$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$</font> , recall: k = $\\frac{\\sqrt{2m E}}{\\hbar}$\n",
        "\n",
        "* from earlier: $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ =  $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "**Step 5: Rearrange that equation to get the energy value**\n",
        "\n",
        "\n",
        "* and if we rearrange that <font color=\"blue\">$(ka)$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$ = $\\pi$ </font> we have something that tells us the value of the energy $E$:\n",
        "\n",
        "> $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* (with reduced Planck constant: $\\hbar=\\frac{h}{2 \\pi} =1.054571817 \\ldots \\times 10^{-34} \\mathrm{~J} \\cdot \\mathrm{s}$)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_178.png)\n",
        "\n",
        "* in other words: if our wavefunction looks like this (half a sine function), then the energy of our particle is this $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* another possibe solution is a full sine wave fitting into this region, just that the value at the end of the wall is 360 degrees, because we went through the whole sine wave = 2*$\\pi$ radians\n",
        "\n",
        "  * if the wavefunction looks like a whole sine wave <font color=\"blue\">$\\frac{\\sqrt{2m E}}{\\hbar}a$</font> = 2*$\\pi$\n",
        "\n",
        "  * then the  energy of the particle is $E=\\frac{4h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* We can continue doing this for lots of half sine waves, so we could have three or four half fine waves in our region - and in each case we can calculate the energy of a particle when its wavefunction looks like those sine waves.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_177.png)\n",
        "\n",
        "* This phenomenom is called \"Quantization\".\n",
        "\n",
        "  * because we can only have specific wave functions, and they correspond to specific energies, a particle can therefore only have specific energies\n",
        "\n",
        "  * so it cannot be anyhting in between and it cannot be less than the minimum of half a sine wave $E_{1}=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "  * this is also why for this particular setup cosine doesnt work (normally it does though) $\\Psi=\\cos \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "**Step 6: Normalization to get probabilities**\n",
        "\n",
        "*Normalization of the wavefunction*\n",
        "\n",
        "* there is one more thing to consider when finding a solution to the Schrodinger equation: Normalization\n",
        "\n",
        "> $\\Psi = \\sqrt{\\frac{2}{a}} \\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "* it adds a factor of $\\sqrt{\\frac{2}{a}}$ to our solution\n",
        "\n",
        "* physical meaning: if our particle is in the lowest energy level $E_1$. Then in our specific setup with the two walls the wavefunction looks like half a sine wave. And remember the wavefunction corresponds directly to the probability of us finding that particle at a particular point in space. And this relationshipn is if we square our wavefunction $|\\Psi|^2$ (we take the square modulus), then we get the probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnCYfCNzqmG9"
      },
      "source": [
        "<font color=\"blue\">**Schr√∂dinger Equation (Time Dependent)**\n",
        "\n",
        "**Consider: Difference of probability in the position basis (changes over time) and the energy basis (doesn't change):**\n",
        "\n",
        "> <font color=\"red\">**The electron is still in the same shell, represented by the principal quantum number for example, because if the electron changes the shell, energy needs to be added or removed from the overall system. however if energy stays the same, it means the electron is still in the same shell, but \"moving\" around = probability distribution of finding it somewhere in this shell changes over time which is represented by the rotation $e^{i \\frac{\\hat{H} * t}{\\hbar}}$**</font>\\\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right] \\psi(\\vec{r})=E \\psi(\\vec{r})\n",
        "$\n",
        "\n",
        "The object on the left that acts on $\\psi(x)$ is an example of an operator.\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right]\n",
        "$ = Operator\n",
        "\n",
        "In effect, what is says to do is \"take the second derivative of $\\psi(x)$, multiply the result by $-\\left(\\hbar^{2} / 2 m\\right)$ and then add $V(x) \\psi(x)$ to the result of that.\"\n",
        "\n",
        "Quantum mechanics involves many different types of operators. This one, however, plays a special role because it appears on the left side of the Schr√∂dinger equation. **It is called the Hamiltonian operator and is denoted as**\n",
        "\n",
        "> $\n",
        "\\hat{H}=-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\n",
        "$\n",
        "\n",
        "**Therefore the time-dependent Schr√∂dinger equation can be written as**:\n",
        "\n",
        "> $\n",
        "\\hat{H} \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "with $\\hat{H}$ = $(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r}))$ will be:\n",
        "\n",
        "> $\n",
        "(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})) \\; \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "bzw. rewritten:\n",
        "\n",
        "> $\\left[-\\frac{\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}+V(x, t)\\right] \\Psi(x, t) = i \\hbar \\frac{\\partial}{\\partial t} \\Psi(x, t)$\n",
        "\n",
        "bzw written in another way (single particle variant):\n",
        "\n",
        "> <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> + <font color=\"green\">$V(x, t)$</font> ]\n",
        " <font color=\"blue\">$|\\psi\\rangle$</font> = $i \\hbar \\frac{\\partial}{\\partial t}$ <font color=\"blue\">$|\\psi\\rangle$</font>\n",
        "\n",
        "* <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> Kinetic energy\n",
        "\n",
        "* <font color=\"green\">$V(x, t)$</font> Potential energy\n",
        "\n",
        "* <font color=\"blue\">$|\\psi\\rangle$</font>  the wave function\n",
        "\n",
        "<font color=\"blue\">*Schrodinger equation: Derivation and how to use it (in Time Evolution)*\n",
        "\n",
        "Important rules of physics:\n",
        "\n",
        "* Conservation of energy -> deeply integrated into Schrodinger equation\n",
        "* total energy doesn't change\n",
        "* you can't make of destroy energy\n",
        "\n",
        "**Since we can write a quantum state $|\\Psi \\rangle$ in whatever basis we want, we can choose the energy Eigenbasis**. <font color=\"red\">what is meant by that? is that the principle quantum number for example</font>\n",
        "\n",
        "* We can write a state as the superposition of different energies.\n",
        "\n",
        "* And if we measure the energy of the particle it will be one of these with their probability\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* with probability for example $|\\beta|^2$ for measuring second state\n",
        "\n",
        "**Say the state evolves in time, in other words we apply the time evolution $U$ (or $T$) to $|\\Psi \\rangle$, so $T |\\Psi \\rangle$**\n",
        "\n",
        "* what condition do we want to impose on the new energies of the state?\n",
        "\n",
        "* In other words: how we want conservation of energy to look in quantum mechanics?\n",
        "\n",
        "**Let's start where a particle just has one energy $E$ when we start**\n",
        "\n",
        "* means: it is an energy Eigenstart !!\n",
        "\n",
        "* we evolve it forward in time and look at the energy of the new state. That energy should be also $E$, otherwise energy wouldn't be conserved (Like in classical mechanics).\n",
        "\n",
        "> $|\\Psi \\rangle$ = $|1 \\rangle$ $\\rightarrow$ $T|\\Psi \\rangle$\n",
        "\n",
        "* Now also the average energy shouldn't change after some time, otherwise the energy wouldn't be conserved either.\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* if you measured athe particle's energy initially with a certain probability $|\\beta|^2$, and then after time evolution again, it should be the same probability to measure that energy!\n",
        "\n",
        "* this is so strong, it gives us the schroedinger equation\n",
        "\n",
        "\n",
        "**We need to how the coefficients have changed in the new equation after time evolution**:\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$ (before)\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $\\alpha' |\\Psi \\rangle + \\beta' |\\Psi \\rangle + \\gamma' |\\Psi \\rangle$ (after)\n",
        "\n",
        "* We want the probability to be the same, but that probability is just the lenght of this compex number squared $|\\gamma|^2 = |\\gamma'|^2 = 1$\n",
        "\n",
        "> <font color=\"red\">**So each coefficient can be represented as an arrow with equal length $|\\gamma|^2$ and $|\\gamma'|^2$ (hence the probability of measuring that energy this state is still the same!!), BUT $|\\gamma'|^2$ may be rotated by an angle $\\phi$**. This angle is new vector = rotation * old vector:</font>\n",
        "\n",
        "> <font color=\"red\">$\\gamma' = e^{i\\phi}\\gamma$</font>\n",
        "\n",
        "Let's plug that rotation $e^{i\\phi}$ in to our previous equation:\n",
        "\n",
        "> <font color=\"red\">$T |\\Psi \\rangle$ = $e^{i\\phi_1}\\alpha |\\Psi \\rangle + e^{i\\phi_2}\\beta |\\Psi \\rangle + e^{i\\phi_3}\\gamma |\\Psi \\rangle$</font>\n",
        "\n",
        "* where the angles / rotations $e^{i\\phi}$ are different for every energy = they are all rotated by a different amount !! Otherwise the rotation can be brought out and present and future state would be the essentially same:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}\\alpha |\\Psi \\rangle + e^{i\\phi}\\beta |\\Psi \\rangle + e^{i\\phi}\\gamma |\\Psi \\rangle$ = $e^{i\\phi} (\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle)$ (this is showing that it's wrong!)\n",
        "\n",
        "The overall rotation wouldn't affect any measurement outcomes. Means no matter in which crazy situation you brought the particle in, it does nothing, which can't be right.\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}|\\Psi \\rangle$ (this is showing that it's wrong!\n",
        "\n",
        "\n",
        "* also the amount of rotation depends on time (little going forwardf = little rotation). That suggests the right amount of angle to rotate is Energy x Time. Plus some constants to deal with units and scaling etc.\n",
        "\n",
        "> <font color=\"red\">$\\phi = \\frac{E * t}{\\hbar}$</font>\n",
        "\n",
        "* And that's what the Schroedinger equation will tell you will happen to the state:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i \\frac{E * t}{\\hbar}}\\alpha |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\beta |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\gamma |\\Psi \\rangle$\n",
        "\n",
        "* And that's the same: (with $\\hat{H}$ for energy measurement operator, Hamiltonian):\n",
        "\n",
        "> <font color=\"red\">$T(t) |\\Psi \\rangle = e^{i \\frac{\\hat{H} * t}{\\hbar}}|\\Psi \\rangle$</font>\n",
        "\n",
        "*Common result for 2 observations:*\n",
        "\n",
        "Time Evolution per each step, observer 1:\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_1$ = $e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$</font>\n",
        "\n",
        "Time Evolution at the end for observer 2 (not seeing time step 1):\n",
        "\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font>\n",
        "\n",
        "Where:\n",
        "\n",
        "> <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font> = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$\n",
        "\n",
        "Why? - because our angle of rotation depends on $t$ (and not $t^2$ or anything): $T\\left(t_{1}+t_{2}\\right)=T\\left(t_{2}\\right) T\\left(t_{1}\\right)$\n",
        "\n",
        "Taken from [Schrodinger equation comment response and homework answers video](https://www.youtube.com/watch?v=M_2h5uQ0SIc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Harmonic Oscillator (Hamiltonians / Solution of Schrodinger equation)**\n",
        "\n",
        "* The nuclear motion Schr√∂dinger equation can be solved in a space-fixed (laboratory) frame, **but then the translational and rotational (external) energies are not accounted for**. Only the (internal) atomic vibrations enter the problem.\n",
        "\n",
        "* Further, for molecules larger than triatomic ones, it is quite common to introduce the **harmonic approximation, which approximates the potential energy surface** as a [quadratic function](https://en.m.wikipedia.org/wiki/Quadratic_function) of the atomic displacements. **This gives the harmonic nuclear motion Hamiltonian**.\n",
        "\n",
        "* Making the harmonic approximation, we can **convert the Hamiltonian into a sum of uncoupled one-dimensional [harmonic oscillator](https://en.m.wikipedia.org/wiki/Harmonic_oscillator) Hamiltonians**.\n",
        "\n",
        "> **The one-dimensional harmonic oscillator is one of the few systems that allows an exact solution of the Schr√∂dinger equation.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Nullpunktsenergie#Harmonischer_Oszillator\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Harmonischer_Oszillator_(Quantenmechanik)"
      ],
      "metadata": {
        "id": "EKeKuaJkf4Pw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sf5tauFtttx"
      },
      "source": [
        "###### ***Matrix Mechanics*** *(Heisenberg, discrete basis, spin representation, Kronecker delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Mechanics (spin representation - discrete basis) - (Heisenberg)**\n",
        "\n",
        "* Matrix Mechanics (spin representation - discrete basis)\n",
        "\n",
        "* Video: [Matrix formulation of quantum mechanics](https://www.youtube.com/watch?v=wIwnb1ldYTI)\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin representation).**\n",
        "\n",
        "* **Matrix formulation of quantum mechanics reduces to the rules of simple matrix multiplication.**\n",
        "\n",
        "> $\\hat{A}=\\sum_{i j} A_{i j}\\left|u_{i}\\right\\rangle\\left\\langle u_{j}\\right| \\quad A_{i j}=\\left\\langle u_{i}|\\hat{A}| u_{j}\\right\\rangle$\n",
        "\n",
        "* An operator A can be written in the u basis as the sum over the outer products of the basis states\n",
        "* And the expansion coefficients Aij are given by the matrix elements of A with respect to the basis states\n",
        "* The expansion coefficients for an operator are labeled by 2 indices, so we will arrange them in a form of a square matrix, with the first index denoting the row of the matrix and the second index the column of the matrix\n",
        "* There operators are written as matrices\n",
        "\n",
        "> $\\left(\\begin{array}{ccccc}A_{11} & A_{12} & \\cdots & A_{1 j} & \\cdots \\\\ A_{21} & A_{22} & \\cdots & A_{2 j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\\\ A_{i 1} & A_{i 2} & \\cdots & A_{i j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\end{array}\\right)$\n",
        "\n",
        "Kets are written as column vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_218.png)\n",
        "\n",
        "Matrix formulation of Bra's: re-arrange complex, conjugate coefficient as a row vector:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_219.png)\n",
        "\n",
        "An operator A can be written in the u basis as the sum over the outer products of the basis states. An Aij are the expansion coefficient in this case. Operators are written as matrices:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_220.png)\n",
        "\n",
        "Summary of the matrix formulation of quantum mechanics for kets, bras and operators:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_221.png)\n",
        "\n",
        "We will see how simple matrix multiplication rules work for the following 4 operations:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_222.png)\n",
        "\n",
        "First, in the matrix formulation of quantum mechanics, a bracket is the matrix product of a row vector with a column vector, and gives a scalar:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_223.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_224.png)\n",
        "\n",
        "Adjoint operator: describe the action of an operator in the dual space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_225.png)\n",
        "\n",
        "Write an operator as an outer product of two states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_226.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_227.png)"
      ],
      "metadata": {
        "id": "FdyLdYgFeqFK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdJ5yNTZibYy"
      },
      "source": [
        "###### ***Wave Mechanics*** *(Schr√∂dinger, continuous basis, position representation, Dirac delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIaQDNv0C1c"
      },
      "source": [
        "**First: let's go from discrete basis $u_i$ to continuous basis $v_{\\alpha}$**\n",
        "\n",
        "* Much used **discrete basis**: Spin of quantum particles\n",
        "\n",
        "* Much used **continuous basis**: position of quantum particles (this one leads to the idea of wave function)\n",
        "\n",
        "> **The generalization is straightforward: it amounts to replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$ of two continuous variables and sum over these indices by integrals over continuous indices**.\n",
        "\n",
        "* first concept: we work with an orthonormal basis $\\left\\langle u_{i} \\mid u_{j}\\right\\rangle=\\delta_{i j}$. replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$\n",
        "\n",
        "* then we look at the expansion of Ket in a particular basis: replacing a sum over i with an integral over alpha\n",
        "\n",
        "* in yellow: just a proof why we would write a Dirac delta function only under an integral sign\n",
        "\n",
        "* last part: representation of an operator in a particular basis (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_217.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWBTMFaxI7Ie"
      },
      "source": [
        "**Wave Mechanics (Schr√∂dinger)**\n",
        "\n",
        "* Wave Mechanics: Wave Function (position representation - continuous basis)\n",
        "\n",
        "* Check also part under Operator: Translation Operators (**Wave Mechanics: Translation Operator**)\n",
        "\n",
        "* Video: [Wave functions in quantum mechanics](https://www.youtube.com/watch?v=2lr3aA4vaBs)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Wave_function\n",
        "\n",
        "* wave functions is one possible way looking at a quantum system\n",
        "\n",
        "* is the so called position representation of quantum mechanics\n",
        "\n",
        "* leads to wave mechanics (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_232.png)\n",
        "\n",
        "Computing scalar between two states and the normalization:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_233.png)\n",
        "\n",
        "How to get from the position representation to the momentum representation (via a first order differential equation):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_234.png)\n",
        "\n",
        "**Transformation matrix $\\langle x|p\\rangle$ to go from the position representation to the momentum representation**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_235.png)\n",
        "\n",
        "To change between both representations we use Fourier transform:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_236.png)\n",
        "\n",
        "If we go to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_237.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_238.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XCE4EGuJJij"
      },
      "source": [
        "**Wave Mechanics: Position and momentum operators acting on wave functions**\n",
        "\n",
        "* Video: [Position and momentum operators acting on wave functions](https://www.youtube.com/watch?v=Yw2YrTLSq5U)\n",
        "\n",
        "* Action of position and momentum operators on wave functions\n",
        "\n",
        "* The action of the position operator: it multiplies a wave function by x:\n",
        "\n",
        "> $x \\psi(x)$\n",
        "\n",
        "* the action of a momentum operator: it acts by calculating the derivative of a wave function\n",
        "\n",
        "> $-i \\hbar \\frac{d \\psi(x)}{d x}$\n",
        "\n",
        "* First: describe the act of a position operator in the position basis, and the act of a momentum operator in the momentum basis:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_239.png)\n",
        "\n",
        "Second: describe the act of the momentum operator on a state Psi when written in the position representation (=basis) is such that the momentum operator calculates the derivative of the wavefunction and then multiplies the result by minus i h-bar (we need the translation operator):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_240.png)\n",
        "\n",
        "Third: what happens when we act with the position operator in the momentum basis (proof: the momentum space wavefunction is related to the real space wavefcunction by a Fourier transform):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_241.png)\n",
        "\n",
        "Generalize this to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_242.png)\n",
        "\n",
        "Summary: as you can see there are different representations for the same thing, but the maths is differently difficult. So the task is to find a representation that is easy for a certain problem:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_243.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-w7Fv4O91T"
      },
      "source": [
        "###### *Wavefunction & Global Phase Difference (Why we factor it out and leave phase difference only)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIwU3Qnjifn-"
      },
      "source": [
        "**How does the Wavefunction (one type) look like?**\n",
        "\n",
        "> $\\psi=e^{\\frac{1}{\\hbar}(px - Et)}$\n",
        "\n",
        "* p = momentum in direction x, x = position along x direction, E = energy, t = time\n",
        "\n",
        "* e is the exponential function, normally it doesn't look like a wave, like $e^{-x}$ or $e^{x}$\n",
        "\n",
        "* but the imaginary number $i=\\sqrt{-1}$ turns an exponential function into a wave\n",
        "\n",
        "* sinoisdal functions (sine and cosine) can be written in terms of the exponential function with $i$ in the exponent\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_244.png)\n",
        "\n",
        "* we could take one complex wave function and break it down into simpler waves, then we apply same maths on the simpler waves:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_245.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVcEGLc6rS9p"
      },
      "source": [
        "Global phase factor $e^{i\\theta}$:\n",
        "\n",
        "* Eigenvalues and Eigenvectors exist also in other vector spaces than state spaces, but because state space is a complex vector space there is one important extra subtlety compared to real vector spaces which has to do with the global phase factor $e^{i\\theta}$\n",
        "\n",
        "* after choosing alpha to make the length of an Eigenstate equal to 1, we still have some extra freedom in the Eigenstate\n",
        "\n",
        "* multiplying $|\\Psi\\rangle$ with a Global phase factor $e^{i\\theta}$ makes the length of the resulting $|\\Psi'\\rangle$ still = 1\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_258.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens when we square the wave function:\n",
        "* the function is an oscillation in quantum possibilities moving through space and time\n",
        "* but it's a complex wave with one real and one imaginary component\n",
        "* **the components oscillate in sync with each other - but they are offset, shifted in phase by a constant amount**\n",
        "> phase is just the wave's current state in its up-down oscillation\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0964.png)"
      ],
      "metadata": {
        "id": "KiDe4HzbgNh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we apply the Born rule we are squaring these two waves and adding them together\n",
        "* but it turns out that this value doesn't depend on phase. The magnitude squared of the real and imaginary components stays the same, even as those components move up and down\n",
        "* It is that magnitude squared that we can observe, it determines the particles position\n",
        "* the phase itself is fundamentally unobservable. You can shift phase by any amount and you wouldnt change the resulting position of the particle, as long as you do the same shift to both the real and the imaginary components.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0965.png)\n"
      ],
      "metadata": {
        "id": "T6Kn28ZZgUsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact as long as you make the same shift across the entire wave function, all the observables are unchanged.\n",
        "* We call this form of transformation a global phase shift, and it's analogous to transforming our altitude zero point up or down by the same amount everywhere.\n",
        "* the equations of quantum mechanics have what we call **global phase invariance**\n",
        "* **Global phase is a Gauge symmetry of the system**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0966.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "70R04sWsgbFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminder: Trigonometrie:\n",
        "\n",
        "> $\\begin{aligned} \\mathrm{e}^{\\mathrm{i} x} &=\\sum_{k=0}^{\\infty} \\frac{(\\mathrm{i} x)^{k}}{k !}=\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l}}{(2 l) !}+\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l+1}}{(2 l+1) !} \\\\ &=\\underbrace{\\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l}}{(2 l) !}}_{\\cos x}+\\underbrace{\\mathrm{i} \\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l+1}}{(2 l+1) !}}_{\\sin x} \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos x+\\mathrm{i} \\sin x \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos \\varphi+\\mathrm{i} \\sin \\varphi \\\\ \\mathrm{e}^{\\mathrm{i} x}&=x+\\mathrm{i} y \\end{aligned}$ Das ist die sogenannte [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel)!\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/600px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_040.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/3/3b/Circle_cos_sin.gif)"
      ],
      "metadata": {
        "id": "QX7SeO65gi0O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBGbzlHLPGlp"
      },
      "source": [
        "* The only reason phase is important is because it brings about interference effects. And interference effects are only dependent on the difference in phase between the two waves (we will abstract basis states to waves for now).\n",
        "\n",
        "> **Therefore, we can say that it‚Äôs the difference that counts, and not the absolute value.**\n",
        "\n",
        "* For example, if the phase difference is œÄ radians then the waves would cancel each other out.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sd8WkT9PbZX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_179.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8WPjzqBPfSd"
      },
      "source": [
        "We can conclude that the absolute value of the phase shift for both waves is meaningless to the interference. For as long as they both retain the phase difference, then the interference effect will be constant, and that‚Äôs what matters!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOiHZhJKPkMX"
      },
      "source": [
        "**Why the global phase doesn‚Äôt matter**\n",
        "\n",
        "As observed above, what matters is the phase difference. So think about it, if you‚Äôre describing the phase of two waves, it‚Äôs redundant to state both phases. **The better approach is to just state the phase difference**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjLdM9G6PojT"
      },
      "source": [
        "* The global phase is the absolute value of the phase shift for both waves.\n",
        "\n",
        "* For example, wave one and two each have a phase of (œÄ/2) radians and (3œÄ/2) radians, respectively. In this case, the phase (œÄ/2) is a global phase since the phase difference (what actually counts) is œÄ radians ‚Üí (3œÄ/2 - œÄ/2 ).\n",
        "\n",
        "* Using the same example, we can define the relative phase (also known as the local phase) as the phase difference (œÄ rad)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUI_HZzP43M"
      },
      "source": [
        "We‚Äôve found a method to **bypass the need for a fourth dimension by factoring out the global phase** and replacing the second phase with the relative phase. We can represent this mathematically.\n",
        "\n",
        "A general qubit state can be written\n",
        "\n",
        "> $|\\psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$\n",
        "\n",
        "with complex numbers, Œ± and Œ≤, and the normalization constraint require that:\n",
        "\n",
        "\n",
        "> $|\\alpha|^{2}+|\\beta|^{2}=1$\n",
        "\n",
        "As previously stated, we can express the amplitudes in polar coordinates as (General equation for a qubit state):\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha} e^{i \\phi_{\\alpha}}|0\\rangle+r_{\\beta} e^{i \\phi_{\\beta}}|1\\rangle$\n",
        "\n",
        "with four real parameters:\n",
        "\n",
        "> $r_{\\alpha}, \\phi_{\\alpha}, r_{\\beta}$ and $\\phi_{\\beta}$\n",
        "\n",
        "**However, the only measurable quantities are the probabilities |Œ±|¬≤ and |Œ≤|¬≤, so multiplying the state by an arbitrary factor $e^{iŒ≥}$ (global phase) has no observable consequences, because**:\n",
        "\n",
        "> $\\left|e^{i \\gamma} \\alpha\\right|^{2}=\\left(e^{i \\gamma} \\alpha\\right)^{*}\\left(e^{i \\gamma} \\alpha\\right)=\\left(e^{-i \\gamma} \\alpha^{*}\\right)\\left(e^{i \\gamma} \\alpha\\right)=\\alpha^{*} \\alpha=|\\alpha|^{2}$\n",
        "\n",
        "Therefore, we can factor out $e^{iŒ¶}$ from the general equation:\n",
        "\n",
        "> $|\\psi\\rangle=e^{i \\phi_{\\alpha}}\\left(r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "Now, if you calculate the amplitude |œà|¬≤, the factor ($e^{iŒ¶_Œ±}$) in front will vanish by the argument above. This is why we called it the global phase. However, the relative phase is the phase difference noted as (Œ¶_Œ± - Œ¶_Œ≤). This is an observable-ish quantity which manifests through interference effects.\n",
        "\n",
        "Let‚Äôs consolidate the above equation into:\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i \\phi}|1\\rangle$\n",
        "\n",
        "> $r_{\\alpha} \\in \\mathbb{R}, r_{\\beta} \\in \\mathbb{R}, \\phi \\in \\mathbb{R} \\mid \\phi=\\phi_{\\beta}-\\phi_{\\alpha}$\n",
        "\n",
        "where r_Œ±, r_Œ≤ and Œ¶ all real parameters.\n",
        "\n",
        "**Notice that this equation can be represented in 3-D, as the global phase is gone.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odWaJ8hJPDMa"
      },
      "source": [
        "https://pavanjayasinha.medium.com/but-what-is-a-quantum-phase-factor-d05c15c321fe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8P-evW3U0Vx"
      },
      "source": [
        "**Physical Meaning of Phase**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Remember: $e^{2 \\pi i}$ = 1 (Identity)"
      ],
      "metadata": {
        "id": "lQpy1dj0iHji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNcqwo6R4lV"
      },
      "source": [
        "* In quantum mechanics, a phase factor is a complex coefficient $e^{i \\theta}$ that multiplies a ket $|\\psi\\rangle$ or bra $\\langle\\phi|$.\n",
        "\n",
        "* <font color=\"blue\">**It does not, in itself, have any physical meaning**, since the introduction of a phase factor does not change the expectation values of a Hermitian operator.\n",
        "\n",
        "> That is, the values of $\\langle\\phi|A| \\phi\\rangle$ and $\\left\\langle\\phi\\left|e^{-i \\theta} A e^{i \\theta}\\right| \\phi\\right\\rangle$ are the same.\n",
        "\n",
        "* <font color=\"red\">However, differences in phase factors between two interacting quantum states can sometimes be measurable (such as in the Berry phase) and this can have important consequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syr76IkvRoPD"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Phase_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T3nptBTPNo"
      },
      "source": [
        "* When people say that the phase doesn't matter, they mean the overall, \"global\" phase. In other words, the state $|0\\rangle$ is equivalent to $e^{i \\theta}|0\\rangle$, the state $|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime}}|1\\rangle$, and the state $|0\\rangle+|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime \\prime}}(|0\\rangle+|1\\rangle)$.\n",
        "\n",
        "> Siehe auch Eulersche Formel: $e^{i \\phi}$ https://mathepedia.de/Eulersche_Formel.html\n",
        "\n",
        "> Note that \"equivalence\" is not preserved under addition, since $e^{i \\theta}|0\\rangle+e^{i \\theta^{\\prime}}|1\\rangle$ is not equivalent to $|0\\rangle+|1\\rangle$, because there can be a relative phase $e^{i\\left(\\theta-\\theta^{\\prime}\\right)}$.\n",
        "\n",
        "* If we wanted to describe this very simple fact with unnecessarily big words, we could say something like \"the complex projective Hilbert space of rays, the set of equivalence classes of nonzero vectors in the Hilbert space under multiplication by complex phase, cannot be endowed with the structure of a vector space\".\n",
        "\n",
        "* Because the equivalence doesn't play nicely with addition, **it's best to just ignore the global phase ambiguity whenever you're doing real calculations**. Finally, when you're done with the entire calculation, and arrive at a state, you are free to multiply that final result by an overall phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBLDuK4vRpXR"
      },
      "source": [
        "https://physics.stackexchange.com/questions/552796/the-importance-of-the-phase-in-quantum-mechanics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hew4HucSHKJ"
      },
      "source": [
        "* Phase: Any one point or portion in a recurring series of changes, as in the changes of motion of one of the particles constituting a wave or vibration; one portion of a series of such changes, in distinction from a contrasted portion, as the portion on one side of a position of equilibrium, in contrast with that on the opposite side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxiSY8ddRqi1"
      },
      "source": [
        "https://courses.lumenlearning.com/boundless-chemistry/chapter/orbital-shapes/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CThWPZdFEXlO"
      },
      "source": [
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Applications*"
      ],
      "metadata": {
        "id": "XdelGAe0UcAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Effects in Everyday Life**\n",
        "\n",
        "*Casimir effect from quantum (zero point) energy* [Source](https://www.youtube.com/watch?v=Rh898Yr5YZ8)\n",
        "\n",
        "  * Geckos are climbing with use of Van der Vals forces - essentially same things as Casimir force: Gekko's feet with Setae (microscopic hairs)**\n",
        "\n",
        "  * These hairs split into millions of spatula shaped ends that are around point 2 micrometers in diameter\n",
        "\n",
        "  * when a gecko presses its cute little feet onto any surface a Fraction of these hairs are close enough to the surface so that Casimir forces come into effect\n",
        "\n",
        "  * Gecko is literally manipulate quantum vacuum energy to climb walls\n",
        "\n",
        "*Quantum Tunneling* [Source](https://youtu.be/1zDzbXbt7Hc?t=537)\n",
        "\n",
        "  * Microchips: Quantum tunneling of electrons through barrier of less than 1 nanometer. affects minimum size of electronic components in microchips\n",
        "\n",
        "  * *Electrons in the process of photosynthesis via quantum tunneling (Hypothesis)**\n",
        "\n",
        "  * Quantum tunneling maybe responsbile for aging and cancer (Hypothesis)\n",
        "\n",
        "* Laser: in fiber optic cables https://youtu.be/Gbfj_FlEjI4"
      ],
      "metadata": {
        "id": "oGKjlC9pVYKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chemistry**\n",
        "\n",
        "* https://www.spektrum.de/news/ki-findet-neue-struktur-von-wasserstoff/2132715"
      ],
      "metadata": {
        "id": "64Io24NvUews"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Astronomy**\n",
        "\n",
        "* Optical interferometry"
      ],
      "metadata": {
        "id": "D-tYe0KjUhPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finance**\n",
        "\n",
        "Cosa √® stato detto: I domini di impiego pi√π rilevanti del Quantum Computing nell‚Äôambito dei servizi finanziari riguardano\n",
        "* Ottimizzazione dei portafogli titoli, ad esempio, per massimizzarne la rendita e minimizzarne la volatilit√†.\n",
        "* Simulazioni¬†di scenario per il calcolo del pricing di un certo asset o per fare previsioni di mercato pi√π accurate.\n",
        "* Machine learning¬†per analizzare elevati flussi di dati pi√π velocemente e con una maggior precisione.\n",
        "* Cybersecurity¬†per mitigare la minaccia di un attacco quantistico ai protocolli crittografici impiegati per proteggere le transazioni bancarie."
      ],
      "metadata": {
        "id": "1VVCf5tiryXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Material**\n",
        "\n",
        "Online Course: [Quantum Computing](https://youtube.com/playlist?list=PLnK6MrIqGXsL1KShnocSdwNSiKnBodpie)\n",
        "\n",
        "https://www.youtube.com/watch?v=Kk2zMpDaPgs&list=WL&index=8\n",
        "\n",
        "https://youtu.be/mD6m-XkDgTY\n",
        "\n",
        "https://youtu.be/SIehCk3WQEg\n",
        "\n",
        "https://youtu.be/JpRO7yXbFYE\n",
        "\n",
        "https://youtu.be/gY5lisUprLg\n",
        "\n",
        "https://youtu.be/XzcTb11CTgE\n",
        "\n",
        "https://youtu.be/Pq2DVlru0UI\n",
        "\n",
        "https://youtu.be/i-sCFVt52bI\n",
        "\n",
        "https://youtu.be/40OFnZTPqyc\n",
        "\n",
        "[Efficient quantum algorithm for solving travelling salesman problem: An IBM quantum experience](https://arxiv.org/abs/1805.10928)"
      ],
      "metadata": {
        "id": "wRL3P2mDmgJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Sensing**"
      ],
      "metadata": {
        "id": "66-byTWA1_ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terrestrial Very-Long-Baseline Atom Interferometry\n",
        "\n",
        "https://indico.cern.ch/event/1208783/\n",
        "\n",
        "https://webcast.web.cern.ch/event/i1208783\n",
        "\n",
        "Two extensive works on quantum metrology. \n",
        "\n",
        "https://phys.au.dk/fileadmin/user_upload/Phd_thesis/AlexanderHolmKiilerich_thesis.pdf\n",
        "\n",
        "https://www.tdx.cat/bitstream/handle/10803/371132/bgc1de1.pdf?sequence=1"
      ],
      "metadata": {
        "id": "BjWlca6t3I0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " am excited to share that our work on sensitive detection of spins using superconducting circuits has been published in Science Advances! Spin resonance spectroscopy is an important technique in fields as diverse as materials science, physics, chemistry and biology. In this study we demonstrate a device capable of beating the spin detection sensitivity of conventional spectrometers by many orders of magnitude. The performance matches previous demonstrations using superconducting quantum technologies, but importantly integrates the core components into a single device and works at higher magnetic fields and temperatures, which is critical for the practical use of this technology. See the paper and a brief explainer below.\n",
        "\n",
        "Wyatt Vine, Mykhailo S., Arjen Vaartjes, Anders Kringh√∏j, James Slack-Smith, Thomas Schenkel, Brett Johnson, Andrea Morello \n",
        "\n",
        "https://lnkd.in/gf5-h8Bc\n",
        "\n",
        "https://lnkd.in/g-Rsp6Kb"
      ],
      "metadata": {
        "id": "gUJbVIN4Tg3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Quantum Sensing](https://en.m.wikipedia.org/wiki/Quantum_sensor) **Differenzierung**\n",
        "\n",
        "* [Interferometry](https://en.m.wikipedia.org/wiki/Interferometry): technique which uses the interference of superimposed waves to extract information\n",
        "\n",
        "  * [Atom Interferometer](https://en.m.wikipedia.org/wiki/Atom_interferometer): interferometer which uses the wave character of atoms.\n",
        "\n",
        "  * Electromagnetic Wave Interferometry\n",
        "\n",
        "    * [Astronomical_optical_interferometry](https://en.m.wikipedia.org/wiki/Astronomical_optical_interferometry)\n"
      ],
      "metadata": {
        "id": "hWVAIMtl4Ljq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Astronomical Optical Interferometry**\n",
        "\n",
        "* https://www.quantamagazine.org/famous-quantum-experiment-offers-hope-for-earth-size-telescope-20210505/\n",
        "  * radio interferometry: relatively easy in radio astronomy, both because radio-emitting objects tend to be extremely bright, and because radio waves are relatively large and thus easy to line up.\n",
        "  * Optical interferometry is much harder. Visible wavelengths measure hundreds of nanometers long, leaving far less room for error in aligning waves according to when they arrived at different telescopes. Moreover, optical telescopes build images photon-by-photon from very dim sources. It‚Äôs impossible to save these grainy signals onto normal hard drives without losing information that‚Äôs vital for doing interferometry.\n",
        "  * ‚ÄúIf there was a way of recording photon events at an optical telescope with some kind of quantum device, that would be a great boon to the science.‚Äù\n",
        "* https://en.m.wikipedia.org/wiki/Astronomical_optical_interferometry\n",
        "* https://en.wikipedia.org/wiki/Photometric_system\n",
        "* https://en.wikipedia.org/wiki/List_of_astronomical_interferometers_at_visible_and_infrared_wavelengths"
      ],
      "metadata": {
        "id": "s_8-fq9p4Pqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix**\n",
        "\n",
        "sensing changes in motion, and electric and magnetic fields. The analyzed data is collected at the atomic level.\n",
        "\n",
        "https://www.baesystems.com/en-us/definition/what-is-quantum-sensing\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_sensor\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_metrology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_imaging\n",
        "\n",
        "diamond-based quantum sensors that can detect magnetic and electric fields\n",
        "* https://www.iaf.fraunhofer.de/en/researchers/quantum-systems/quantum-sensors.html\n",
        "* https://www.iaf.fraunhofer.de/en/media-library/press-releases/world-s-first-measurement-of-magnetic-field-dependent-stimulated.html\n",
        "* https://www.iaf.fraunhofer.de/en/media-library/press-releases/more-precise-diagnoses-and-personalized-therapies-due-to-hyperpolarized-nuclear-magnetic-resonance.html"
      ],
      "metadata": {
        "id": "FtAuAa_t6yQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Sensing**\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neuartiger_Quantenzustand_durch_exotisches_Wechselspiel_der_Elektronen.html\n",
        "\n",
        "https://singularityhub.com/2022/10/31/new-3d-quantum-accelerometer-leaves-classical-sensors-in-the-dust/\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Beschleunigungssensor"
      ],
      "metadata": {
        "id": "xw801dHeA42F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum sensing technology is capable of creating new measurement principles or sitting on top of conventional measurement technology and upgrading them (either as an component or theoretical concept).\n",
        "This paper written by Mr. Reichert et. al. in pj Quantur Information is a prime example of such improvements!"
      ],
      "metadata": {
        "id": "NhWE7u-MucGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Error Correction*"
      ],
      "metadata": {
        "id": "OWqmJDTmXzvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Information Theory*"
      ],
      "metadata": {
        "id": "P0qED2XtCTnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Information Theory is a vast field with many interconnected topics. Here are some of the most important areas to study if you're getting into this field:\n",
        "\n",
        "1. **Quantum Mechanics Basics**: To begin, a solid understanding of quantum mechanics is crucial. This includes understanding the principles of superposition, entanglement, quantum measurement, and the mathematical language of quantum mechanics (e.g., Dirac notation, Hilbert spaces, operators).\n",
        "\n",
        "2. **Qubits and Quantum Gates**: The fundamental unit of quantum information is the qubit, so understanding this concept is crucial. Along with this, it's important to understand quantum gates, which are the basic operations performed on qubits.\n",
        "\n",
        "3. **Quantum Circuits**: Quantum algorithms are typically depicted as quantum circuits. Understanding how these circuits work, how to construct them, and how to use them to implement quantum algorithms is key.\n",
        "\n",
        "4. **Quantum Algorithms**: There are several quantum algorithms that provide a speedup over classical algorithms (for certain types of problems). Some of the most famous include Shor's algorithm for factoring and Grover's algorithm for search.\n",
        "\n",
        "5. **Quantum Error Correction and Fault Tolerance**: Quantum information is very fragile, so learning how to protect it from errors is critical. This is where stabilizer codes and other quantum error correction codes come into play.\n",
        "\n",
        "6. **Quantum Entanglement and Bell's Theorem**: These are fundamental concepts in quantum information theory that have deep implications for the nature of quantum systems.\n",
        "\n",
        "7. **Quantum Cryptography and Quantum Communication**: Quantum mechanics offers new ways to secure communications, which is the focus of quantum cryptography.\n",
        "\n",
        "8. **Quantum Computing Hardware**: Understanding the physical systems used to implement quantum computers (like superconducting circuits, trapped ions, topological qubits, etc.) is also important.\n",
        "\n",
        "9. **Quantum Complexity Theory**: This is a field of study that classifies quantum algorithms according to their computational complexity, analogous to classical complexity theory.\n",
        "\n",
        "10. **Quantum Machine Learning**: This is a new and rapidly-growing field that looks at how quantum computing can be used to improve machine learning algorithms.\n",
        "\n",
        "Remember, quantum information theory is a deeply interdisciplinary field that combines elements of computer science, physics, mathematics, and engineering. It can take time to build up the necessary background in these areas, so don't be discouraged if you find some of these topics challenging at first. It's a field where lifelong learning is the norm."
      ],
      "metadata": {
        "id": "cC2WH84vCShx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Introduction*"
      ],
      "metadata": {
        "id": "rMqIXS4-cxRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does quantum error correction work?**\n",
        "\n",
        "Quantum error correction is a set of techniques for protecting quantum information from errors due to decoherence and other quantum noise. Here's a high-level summary:\n",
        "\n",
        "1. **Encoding**: The first step in quantum error correction is to encode the quantum information. Rather than storing a quantum bit of information (a \"qubit\") in a single physical qubit, we store it in multiple physical qubits. This is done in such a way that, even if some of the physical qubits are corrupted by noise, the original quantum information can still be recovered. This encoding is done using a quantum error-correcting code. There are many different types of quantum error-correcting codes, each with its strengths and weaknesses.\n",
        "\n",
        "2. **Syndrome measurement**: Once the quantum information has been encoded, the next step is to periodically check for errors. This is done by performing a syndrome measurement, which is a special kind of quantum measurement that can detect whether an error has occurred, and if so, what kind of error it was. Importantly, this measurement does not disturb the encoded quantum information.\n",
        "\n",
        "3. **Error correction**: If the syndrome measurement indicates that an error has occurred, the next step is to perform an error correction. This involves applying a series of quantum gates to the physical qubits to correct the error, based on the result of the syndrome measurement.\n",
        "\n",
        "4. **Repeat**: Because quantum systems are always subject to noise, this process of syndrome measurement and error correction needs to be repeated periodically to keep the errors in check.\n",
        "\n",
        "Challenges: \n",
        "\n",
        "* It's worth noting that the whole process of quantum error correction requires a significant overhead in terms of additional physical qubits and quantum operations. This is a major challenge in the development of large-scale, fault-tolerant quantum computers.\n",
        "\n",
        "* It's also worth noting that, although quantum error correction can protect against many types of errors, it cannot protect against all possible errors. In particular, **it's assumed that the errors are relatively rare and do not all occur at once, and that they are independent and identically distributed across the physical qubits**. **If these assumptions are violated, then quantum error correction may not be able to correct the errors**. This is another major challenge in the field of quantum error correction."
      ],
      "metadata": {
        "id": "JjYa1KwBkNRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) very good\n",
        "Video: [The superconducting transmon qubit](https://youtu.be/dKTNBN99xLw)\n",
        "Video: [The transmon qubit](https://youtu.be/cb_f9KpYipk)\n",
        "Video: [Making quantum error correction practical](https://youtu.be/YPFpll1NFQc)\n",
        "Video: [Steven Girvin](https://youtu.be/nhUKHf-GN_Y)\n",
        "Video: [Quantum Industry Talks](https://youtu.be/eyICn3KCUPI)\n",
        "Video: [Qiskit QEC](https://youtu.be/ZY8PddknCos), [Qiskit](https://youtu.be/SHr3uSv9Bts), [qiskit](https://youtu.be/96a0G4G5ZH8)"
      ],
      "metadata": {
        "id": "68dkfSxfVj8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Concepts:*** *Ancilla qubits, surface code, distance code, X (bit flip) and Z (phase flip) error, threshold theory, required fault-tolerance, logical vs physical qubit*\n",
        "\n",
        "*Tasks: Error detection, error mitigation, error correction, error suppression*\n",
        "\n",
        "Video [Progress Towards Quantum Error Correction with the Surface Code | Qiskit Seminar Series](https://www.youtube.com/watch?v=si5a9RJP01A)"
      ],
      "metadata": {
        "id": "uRkoUVMsuEg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use more than one qubit to represent a state, use neighboring qubit check (so you don't measure the exact state which would collapse the quantum state). You can decode with it and see that the error was on the last qubit. Then you can correct the physical qubit to get back the correct logical qubit state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1278.png)"
      ],
      "metadata": {
        "id": "l3Ja_nTmmvIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[An Introduction to Quantum Error Correction and Fault-Tolerant Quantum Computation](https://arxiv.org/pdf/0904.2557.pdf)"
      ],
      "metadata": {
        "id": "2OdDTy_mW_Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For what do you need abelian groups in quantum computing?\n",
        "\n",
        "In quantum computing, Abelian groups are used to describe the symmetry of a quantum system. Symmetry is a fundamental concept in quantum mechanics, and it refers to the idea that a physical system will remain unchanged under certain transformations. For example, the symmetry of a quantum system may be described by a group of rotations, translations, or reflections. Abelian groups are used to describe the symmetry of a quantum system because they have the useful property of being commutative, meaning that the order in which the transformations are applied does not affect the outcome. This property makes it possible to use Abelian groups to describe the symmetries of a quantum system in a way that is mathematically tractable and easy to work with."
      ],
      "metadata": {
        "id": "4KQRd-aYkmU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error detection and correction**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Error_detection_and_correction\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Repetition_code"
      ],
      "metadata": {
        "id": "mGJ-sWNFvF47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faut-Tolerant QC (Quantum Error Correction)**\n",
        "\n",
        "* Bit flip (from 0 to 1) or dephasing (from superposition to exact state)\n",
        "\n",
        "* Challenge: we need to keep that states correct without looking at them (because then WE dephase them)\n",
        "\n",
        "* We need a method to **build relatively noiseless qubits (logical qubits)out of many noisy ones (physical qubits)**. This is quantum error correction.\n",
        "\n",
        "* Solution: one way (repetition encoding), sit our qubits on a line. We then go along and ask every pair of next-door-neighbours whether they agree or disagree with each other. This tells us nothing about whether they are 0 or 1. But repetition encoding, which protects against bit flip errors so well, actually makes dephasing more likely!\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_error_correction\n",
        "\n",
        "\n",
        "* Examples of QEC: **repetition code** (simplest QEC) and **surface code** (and color codes?)\n",
        "\n",
        "* techniques: syndrome measurements, decoding, logical operations\n",
        "\n",
        "* https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/\n",
        "\n",
        "\n",
        "* [An introduction to Fault-tolerant Quantum Computing](https://arxiv.org/abs/1508.03695)\n",
        "\n",
        "http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html\n",
        "\n",
        "* [INTRODUCTION TO\n",
        "QUANTUM ERROR\n",
        "CORRECTION](https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2327/files/2019/11/QECIntro.pdf)"
      ],
      "metadata": {
        "id": "B-hRX5qBqHdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Error Mitigation**\n",
        "\n",
        "* quantum error correction is long term goal, meanwhile we try to mitigate it\n",
        "\n",
        "* Error mitigation techniques: statistical corrections (on histogram for example)\n",
        "\n",
        "\t* https://qiskit.org/textbook/ch-quantum-hardware/measurement-error-mitigation.html\n",
        "\n",
        "\t* https://arxiv.org/abs/2005.10189"
      ],
      "metadata": {
        "id": "jX7JYnTLqNo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *DiVincenzo's criteria*"
      ],
      "metadata": {
        "id": "psPsoE2eVNiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) (very good!)\n",
        "\n",
        "Video: [Gottesman 1](https://youtu.be/ltJ1jXQeDl8) and [Gottesman 2](https://youtu.be/cUqys29d0YA)"
      ],
      "metadata": {
        "id": "udDc1Y1NqwMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DiVincenzo's criteria**\n",
        "\n",
        "[DiVincenzo's criteria](https://en.m.wikipedia.org/wiki/DiVincenzo%27s_criteria) are conditions necessary for constructing a quantum computer, conditions proposed in 2000 by the theoretical physicist David P. DiVincenzo.\n",
        "\n",
        "**1. A scalable physical system with well-characterized qubits.**\n",
        "\n",
        "**2. The ability to initialize the state of the qubits to a simple fiducial state, such as 000...).**\n",
        "\n",
        "**3. Long relevant decoherence times, much longer than the gate operation time.**\n",
        "\n",
        "**4. A \"universal\" set of quantum gates** (that approximate any unitary operation - a unitary transformation preserves the inner product, which is a property of the Hilbert space)\n",
        "\n",
        "**5. A qubit-specific measurement capability** (ability to measure individual qubits)\n",
        "\n",
        "* Trapped Ion and superconducting qubits do really well on all five criteria\n",
        "\n",
        "6. *The ability to interconvert stationary and flying qubits.*\n",
        "\n",
        "7. *The ability to faithfully transmit flying qubits between specified locations.*\n",
        "\n",
        "* The DiVincenzo criteria consist of seven conditions an experimental setup must satisfy to successfully implement quantum algorithms such as Grover's search algorithm or Shor factorization. \n",
        "\n",
        "* The first five conditions regard quantum computation itself. Two additional conditions regard implementing quantum communication, such as that used in quantum key distribution. One can demonstrate that DiVincenzo's criteria are satisfied by a classical computer.\n",
        "\n",
        "* Comparing the ability of classical and quantum regimes to satisfy the criteria highlights both the complications that arise in dealing with quantum systems and the source of the quantum speed up.\n",
        "\n",
        "*Universal quantum computing and quantum annealer: not all criteria match the quantum annealers*\n",
        "\n",
        "**Definition of Quantum Computing**\n",
        "\n",
        "[Quantum computing](https://en.m.wikipedia.org/wiki/Quantum_computing) is a type of computation whose operations can harness the phenomena of quantum mechanics, such as superposition, interference, and entanglement to perform computation. Devices that perform quantum computations are known as quantum computers.\n",
        "\n",
        "*Harnessing effects of quantum mechanics: by this definition also quantum annealers are quyantum computers.*"
      ],
      "metadata": {
        "id": "9b6pMJ8JVL_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fidelity (Error Probability)*"
      ],
      "metadata": {
        "id": "f0O6pdoUYhec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity**: For Shor's algorithm with estimated 10^9 physical gates required, the error should be less than 10^-9, ideally 10^-10. We are still several orders of magnitude away from that accuracy / faut-tolerance.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1279.png)"
      ],
      "metadata": {
        "id": "EM0u6HMroENg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many applications call for error rates in the 10‚àí15 regime [2‚Äì9], but state-of-the-art quantum platforms typically have physical error rates near 10‚àí3\n",
        "https://arxiv.org/pdf/2102.06132.pdf"
      ],
      "metadata": {
        "id": "FcbSD88fkhOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run quantum algorithms perfectly we need error probability of 1 in a billion or 1 in a trillion - but we are at 1 in a thousand\n",
        "\n",
        "Video: [Suppressing quantum errors by scaling a surface code logical qubit](https://www.youtube.com/watch?v=dVkLNwSTBU0)"
      ],
      "metadata": {
        "id": "pQbRXxY779rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Fidelity of quantum states**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Fidelity_of_quantum_states\n",
        "\n",
        "* Noise transforms pure states into mixed states.\n",
        "\n",
        "  * There are also simpler ones: Fidelity between two pure states\n",
        "\n",
        "  * And there are also more complex ones: Fidelity between two mixed states\n",
        "\n",
        "* fidelity is generally defined as the quantity:\n",
        "\n",
        "> ${\\displaystyle F(\\rho ,\\sigma )=\\left(\\operatorname {tr} {\\sqrt {{\\sqrt {\\rho }}\\sigma {\\sqrt {\\rho }}}}\\right)^{2}}$\n",
        "\n",
        "* most useless state is fidelity 0,5. because fidelity = 0 means orthogonal, and =1 means exactly the same.\n",
        "\n",
        "* Video: [Fidelity](https://www.youtube.com/watch?v=GWi_HIVz2B4)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1275.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1276.png)"
      ],
      "metadata": {
        "id": "GV4K-Cl0qIIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Physical and Logical Qubits*"
      ],
      "metadata": {
        "id": "MkJxJltjAV7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Physical_and_logical_qubits"
      ],
      "metadata": {
        "id": "ioWe4Z-6sBxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Threshold Theory & Code Distance*"
      ],
      "metadata": {
        "id": "dpys_ybfYkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum **Threshold theory** ensure that there is a limit that helps to get error under control even with larger numbers of physical qubits. The **code distance** is then a result of the max error rate and represents the number of physical qubits for one state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1280.png)"
      ],
      "metadata": {
        "id": "RuL0PiU_okd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Quantum Threshold Theorem**\n",
        "\n",
        "* Challenge: quantum computer will not be able to perform gate operations perfectly, some small constant error is inevitable\n",
        "\n",
        "* [quantum threshold theorem](https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem) (or quantum fault-tolerance theorem) states that a quantum computer\n",
        "  * **with a physical error rate below a certain threshold** can,\n",
        "  * **through application of quantum error correction schemes**,\n",
        "  * suppress the logical error rate to arbitrarily low levels.\n",
        "\n",
        "* This shows that quantum computers can be made fault-tolerant, as an analogue to von Neumann's threshold theorem for classical computation\n",
        "\n",
        "* The formal statement of the threshold theorem depends on the types of error correction codes and error model being considered.\n",
        "\n",
        "* for any particular error model (such as having each gate fail with independent probability p), use **error correcting codes** to build better gates out of existing gates.\n",
        "\n",
        "  * Though these \"better gates\" are larger, and so are more prone to errors within them, their error-correction properties mean that they have a lower chance of failing than the original gate (provided p is a small-enough constant).\n",
        "\n",
        "  * Then, one can use these better gates to recursively create even better gates, until one has gates with the desired failure probability, which can be used for the desired quantum circuit.\n",
        "\n",
        "* Current estimates put the threshold for the [surface code](https://en.m.wikipedia.org/wiki/Toric_code) (here: Toric code) on the order of 1%, though estimates range widely and are difficult to calculate due to the exponential difficulty of simulating large quantum systems.\n",
        "\n",
        "* At a 0.1% probability of a [depolarizing](https://en.m.wikipedia.org/wiki/Depolarization) error, the surface code would require approximately 1,000-10,000 physical qubits per logical data qubit, though more pathological error types could change this figure drastically.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1xEbTDdNwar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Distance Code / Code Distance**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory\n",
        "\n",
        "\n",
        "* Over all 25 cycles of error correction, the distance-5 code realises lower logi- cal error probabilities pL than the average of the subset distance-3 codes - [Paper](https://arxiv.org/pdf/2207.06431.pdf)\n",
        "\n",
        "* the distance is the shortest path in a certain \"space of errors\" which maps between two orthogonal quantum states that are in the code.\n",
        "\n",
        "* The natural space of errors is that of single qubit errors of the form ùúéùëã, ùúéùëå or ùúéùëß, in the case where the Hilbert space is that of ùëõ qubits.\n",
        "\n",
        "* So you can think of distance as the shortest path to get from one state to another by operations on single qubits, applied one at a time sequentially.\n",
        "\n",
        "* [Source](https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory)"
      ],
      "metadata": {
        "id": "ieotWtPN7sz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Types of Errors (Bit flip, phase flip, total loss)*"
      ],
      "metadata": {
        "id": "GYcJxiyZYuYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of errors that you want to detect: **Bit-flip** (represented with Pauli X gate) and **Phase-flip** (represented with Pauli-Z gate). But 1D string physical qubits cannot protect from bit and phase flip at the same time. You need a 2D string.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1281.png)"
      ],
      "metadata": {
        "id": "In3mQfhOp0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene: Qubits k√∂nnen auch ganz verloren gehen**\n",
        "\n",
        "* Inzwischen k√∂nnen Quantencomputer mit einer gewissen Anzahl von Rechenfehlern, wie zum Beispiel Bitflip- oder Phasenflip-Fehlern, umgehen. Zus√§tzlich zu diesen Fehlern k√∂nnen jedoch auch Qubits ganz aus dem Quantenregister verloren gehen.\n",
        "\n",
        "* Je nach Art des Quantencomputers kann dies auf den tats√§chlichen Verlust von Teilchen wie Atomen oder Ionen zur√ºckzuf√ºhren sein, oder darauf, dass Quantenteilchen beispielsweise in unerw√ºnschte Energiezust√§nde √ºbergehen, welche nicht mehr als Qubit erkannt werden. Wenn ein Qubit verloren geht, wird die Information in den verbleibenden Qubits unlesbar und ungesch√ºtzt. F√ºr das Ergebnis der Berechnung kann dieser Prozess zu einem potentiell verheerenden Fehler werden.\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neue_Methode_sch√ºtzt_Quantencomputer_vor_Ausf√§llen.html\n",
        "\n",
        "Resolving catastrophic error bursts from cosmic rays in large arrays of superconducting qubits.\n",
        "\n",
        "https://arxiv.org/abs/2104.05219\n",
        "\n",
        "https://physicsworld.com/a/cosmic-ray-threat-to-quantum-computing-greater-than-previously-thought/"
      ],
      "metadata": {
        "id": "0N8AlBcAz4Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Error Syndrome Measurement*"
      ],
      "metadata": {
        "id": "hZTQLaKGel6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syndrome: what qubit the error is on\n",
        "\n",
        "Error syndrome measurement:\n",
        "\n",
        "https://www.quora.com/Error-Correcting-Codes-What-is-a-syndrome:\n",
        "\n",
        "* The syndrome measurement provides information about the error that has happened, but not about the information that is stored in the logical qubit‚Äîas otherwise the measurement would destroy any quantum superposition of this logical qubit with other qubits in the quantum computer, which would prevent it from being used to convey quantum information. (https://en.m.wikipedia.org/wiki/Quantum_error_correction)\n",
        "\n",
        "* It is the result of multiplying a parity check matrix times a vector. By convention, codewords of a code have syndrome zero, so that by linearity of the code, the syndrome of a word is the syndrome of the \"error\" vector. Typically from the syndrome you would either try to determine whether there was an error (is the syndrome nonzero?) and recover the error from it, so that in turn you can recover the data from the received word.\n",
        "\n",
        "syndrome. = error?\n",
        "\n",
        "here slide 4: https://people.engr.tamu.edu/andreas-klappenecker/689/stabilizer.pdf"
      ],
      "metadata": {
        "id": "fYAPsGZXAPsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Circuit Depth*"
      ],
      "metadata": {
        "id": "3Ifz4t5vA6Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/new-algorithm-closes-quantum-supremacy-window-20230109/\n",
        "\n",
        "If you imagine continually increasing the number of qubits as complexity theorists do, and you also want to account for errors, you need to decide whether you‚Äôre also going to keep adding more layers of gates ‚Äî increasing the circuit depth, as researchers say. Suppose you keep the circuit depth constant at, say, a relatively shallow three layers, as you increase the number of qubits. You won‚Äôt get much entanglement, and the output will still be amenable to classical simulation. On the other hand, if you increase the circuit depth to keep up with the growing number of qubits, the cumulative effects of gate errors will wash out the entanglement, and the output will again become easy to simulate classically."
      ],
      "metadata": {
        "id": "kgZP7ruQA8s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *2D Surface Code*"
      ],
      "metadata": {
        "id": "KSaDCK89Y3_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D Surface code** protects from X error and Z error (X and Z - that's why 2 D). Errors typically arise only locally. The gate structure needs to fit the physical geometry of the quantum processor. Error per gate should be 0,5%, but overall threshold depends on case.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1282.png)"
      ],
      "metadata": {
        "id": "jQZePon1slGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code $\\rightarrow$ Surface Code $\\rightarrow$ Toric Code**\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/2106/what-is-the-surface-code-in-the-context-of-quantum-error-correction\n",
        "\n",
        "* **Surface codes**: family of quantum error correcting codes defined on a 2D lattice of qubits.\n",
        "\n",
        "* Each code has [stabilizers](https://en.m.wikipedia.org/wiki/Stabilizer_code) that are defined equivalently in the bulk, but differ from one another in their boundary conditions.\n",
        "\n",
        "* The members of the surface code family are sometimes also described by more specific names:\n",
        "\n",
        "  * The [toric code](https://en.m.wikipedia.org/wiki/Toric_code) is a surface code with periodic boundary conditions,\n",
        "\n",
        "  * the planar code is one defined on a plane, etc.\n",
        "\n",
        "* How many qubits are in a surface code? - While the surface code requires four-qubit measurements to encode a single logical qubit, we introduce families of quantum error correcting codes that use only three-qubit measurements. [Paper](https://www.ucl.ac.uk/quantum/news/2021/aug/subsystem-codes-outperform-surface-code)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [Topological quantum memory (paper)](https://arxiv.org/abs/quant-ph/0110143)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation (paper)](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [My blog series introducing surface codes](http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html)\n",
        "\n",
        "* The surface codes can also be generalized to qudits. For more on that, [see here (Fault-tolerant quantum computation by anyons)](https://arxiv.org/abs/quant-ph/9707021)"
      ],
      "metadata": {
        "id": "Ur4bkT3KooVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Toric Code**\n",
        "\n",
        "* For the toric code we don‚Äôt put our qubits in a line, we put them in a grid pattern.\n",
        "\n",
        "* Video: [INTRODUCTION TO TOPOLOGICAL ORDER, DEMONSTRATION VIA THE TORIC CODE](https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s)\n",
        "\n",
        "* https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code-part-2.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/8-toric-code-part-3.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/9-toric-code-part-4.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/10-toric-code-part-5.html"
      ],
      "metadata": {
        "id": "zpXlgK2jyNPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Parity Check Measurement, Stabilizer Code, Repition Code & Ancilla Qubit*"
      ],
      "metadata": {
        "id": "v41iaGUgY8hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stabilizer code with parity check circuits (for Z and X errors)"
      ],
      "metadata": {
        "id": "4c1Ooe-Odl3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parity measurement**: is it even (correct) or odd (error)? You will have a square with one dimension for X error and another dimension for Z error measurement. You have an **ancilla qubit** to make the measurements with C-Z-gate and C-X gate (but you first put it into an equal superposition).\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1283.png)"
      ],
      "metadata": {
        "id": "iCtG8eoVtaGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You put both X and Z together and get a 2D lattice of surface code with a determined code distance. We have now data qubits, X ancilla qubits and Z ancilla qubits.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1284.png)"
      ],
      "metadata": {
        "id": "ujX9f7Wevt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code & Ancilla qubits**\n",
        "\n",
        "* A [stabilizer](https://en.m.wikipedia.org/wiki/Stabilizer_code) quantum error-correcting code appends [ancilla qubits](https://en.m.wikipedia.org/wiki/Ancilla_bit) to qubits that we want to protect."
      ],
      "metadata": {
        "id": "ldWaXAIIRrHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code**\n",
        "\n",
        "**A stabilizer quantum error-correcting code appends ancilla qubits to qubits that we want to protect**. A unitary encoding circuit rotates the global state into a subspace of a larger Hilbert space. This highly entangled, encoded state corrects for local noisy errors.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Stabilizer_code\n",
        "\n",
        "In quantum computing, **a stabilizer code is a type of error-correcting cod**e that is used to protect quantum information from the effects of noise and decoherence. These codes are based on the concept of stabilizer operators, which are a special type of operator that can be used to detect and correct errors in a quantum system. The basic idea behind stabilizer codes is to encode the quantum information using a set of stabilizer operators, such that any errors that occur in the system can be detected and corrected by measuring the values of these operators. GPT\n",
        "\n",
        "\n",
        "Many quantum error correction schemes can be classified as stabilizer codes, where a single bit of **quantum information is encoded in the joint state of many physical qubits**, which we refer to as data qubits. Interspersed among the data qubits are **measure qubits**, which periodically measure the parity of chosen combinations of data qubits. https://arxiv.org/pdf/2102.06132.pdf\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s\n",
        "\n",
        "https://leftasexercise.com/2019/01/28/basics-of-quantum-error-correction/\n",
        "\n",
        "https://leftasexercise.com/2019/02/04/q-fault-tolerant-quantum-computing/\n",
        "\n",
        "https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "https://leftasexercise.com/2019/04/08/quantum-error-correction-the-surface-code/\n",
        "\n",
        "https://leftasexercise.com/2019/02/11/quantum-error-correction-with-stabilizer-codes/\n",
        "\n",
        "https://leftasexercise.com/2018/09/10/quantum-computing-an-overview/"
      ],
      "metadata": {
        "id": "2ns0241plUDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Repetition Codes**\n",
        "\n",
        "* look at majority of bits\n",
        "\n",
        "* https://qiskit.org/textbook/ch-quantum-hardware/error-correction-repetition-code.html\n",
        "\n",
        "* repetition code: redundancy (repetition) is a way to make sure the message gets delivered (i.e. with majority voting, for d repetition: $P=\\sum_{n=0}^{[ a / 2]}\\left(\\begin{array}{l}d \\\\ n\\end{array}\\right) p^{n}(1-p)^{d-n} \\sim\\left(\\frac{p}{(1-p)}\\right)^{[ d / 2]}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_100.png)"
      ],
      "metadata": {
        "id": "zEwG8U1SwWhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Magic States*"
      ],
      "metadata": {
        "id": "kSmK9R6Hbm2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic States**\n",
        "\n",
        "* [Magic states](https://en.wikipedia.org/wiki/Magic_state_distillation#Magic_states) are certain states that have very nice properties with respect to fault-tolerant quantum computation.\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/13629/what-are-magic-states"
      ],
      "metadata": {
        "id": "2oUvm0fxqESw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distance 2 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "cNo9Qsp7ZTQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go down from a seven qubit surface code to a simpler two qubit surface code. Smallest meaningful is a 2x2 lattice. But it's just an error detection code, because it's too small to do error correction.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1286.png)"
      ],
      "metadata": {
        "id": "d503fihiwLGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green line is a flux line**. We put a magnetic flux through the script loop of each qubit to put a a specific frequency where we want it to be. **Pink line is a charge line** that is used for single qubit gates. All the rest (red, blue, purple box) are part of the readout. It's very important to have a good readout - we need to measure the ancilla qubits during the operation to see if there wasn't an error. You see that sort of resonator over each Qubit (die dinger die aussehen wir alte Heizungskoerper) - this is very standard, there is a harmonic oscillator.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1287.png)"
      ],
      "metadata": {
        "id": "ugRZQcwN0eoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1288.png)"
      ],
      "metadata": {
        "id": "Mbmct0-22-2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1289.png)"
      ],
      "metadata": {
        "id": "sddTSG2S7J3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *7 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "QHclLheAYhtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a 7 qubit gate circuit. You can't correct the error, but you can detect it. At the end we verify by measuring the actual state of each qubit.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1290.png)\n"
      ],
      "metadata": {
        "id": "eqXaY73s5YSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run multiple measurements in (20) microseconds for Z and X operator:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1291.png)"
      ],
      "metadata": {
        "id": "ywM2-Zqb6dQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D = 3 Surface Code Stabilizer Gate Sequence**\n",
        "\n",
        "* Red: data qubits\n",
        "* Blue: X-type ancilla qubit\n",
        "* Green Z-type ancilla qubit\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1300.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1301.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1302.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1303.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1304.png)\n"
      ],
      "metadata": {
        "id": "5sejATAN9-FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1305.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1306.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1307.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1308.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1L2hW2GU_SsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *ZZ-Crosstalks*"
      ],
      "metadata": {
        "id": "Hz-PMZ5rZvs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZZ: Residual ZZ coupling, circular ZZ coupling, ZZ crosstalk**\n",
        "\n",
        "*  Noise is a significant obstacle to quantum computing, and ùëç ùëç cross- talk is one of the most destructive types of noise affecting supercon- ducting qubits. Previous approaches to suppressing ùëçùëç crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to sup- press ùëçùëç crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved. https://arxiv.org/pdf/2202.07628.pdf\n",
        "\n",
        "* In superconductors a destructive type of noise known as ùëçùëç crosstalk. This refers to an always-on ùúéùëß ‚äó ùúéùëß inter- action between qubits connected by couplings, which originates from the interaction between the computational and non-computational energy levels of qubits.\n",
        "\n",
        "* different types of crosstalks:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1295.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1296.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1297.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1298.png)\n",
        "\n",
        "\n",
        "*Source: https://www.youtube.com/watch?v=si5a9RJP01A&t=3645s*\n",
        "\n",
        "* second graph: top red is perfect readout, and black light is for max 10% readout\n",
        "* We need very good readout and very small ZZ error\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1294.png)"
      ],
      "metadata": {
        "id": "y9wvhg9DyWQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Error Correction Zoo*"
      ],
      "metadata": {
        "id": "WV3e_E50rXit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://errorcorrectionzoo.org/c/honeycomb"
      ],
      "metadata": {
        "id": "gayt45_DreBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://errorcorrectionzoo.org/list/ag"
      ],
      "metadata": {
        "id": "KaRMehyhr761"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://errorcorrectionzoo.org/list/homological\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0110143"
      ],
      "metadata": {
        "id": "N8zOwuitr9nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Qubit Braiding and Topological Quantum Computing*"
      ],
      "metadata": {
        "id": "M9vZNTRvVN_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/#loop-excitations-and-error-correction\n",
        "\n",
        "https://www.quantamagazine.org/physicists-create-elusive-particles-that-remember-their-pasts-20230509/\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1568.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFsxn-qNA9zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arthurpesah.me/blog/2023-05-13-surface-code/\n",
        "\n",
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/\n",
        "\n",
        "https://arthurpesah.me/blog/2023-01-31-stabilizer-formalism-1/\n",
        "\n",
        "https://arthurpesah.me/blog/2022-01-25-intro-qec-1/"
      ],
      "metadata": {
        "id": "Um2QgMzhxUcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.google/technology/research/an-important-step-towards-improved-quantum-computers/\n",
        "\n",
        "https://phys.org/news/2023-05-google-quantum-ai-braids-non-abelian.html\n",
        "\n",
        "https://www.spektrum.de/news/nichtabelsche-anyonen-auf-quantenprozessor-simuliert/2138241\n",
        "\n"
      ],
      "metadata": {
        "id": "N0VGhVGOVTeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I am also frustrated by us and the media representing these anyon simulations as a \"step towards fault-tolerance\". Certainly topological codes are inspired by the physics of anyons and there are direct analogies between how those codes work and are realized and these simulations. I also have no doubt that certain aspects of these simulations have some degree of error robustness. But my understanding is that once you get serious about turning such simulations into a form of fault-tolerance appropriate for our devices and then try to optimize its realization you end up with the surface code. So while thinking about the physics of error-correction from this perspective might be useful and there are myriad mathematical similarities between what is going on in these simulations and topological codes, I think it is misleading/wrong to claim that performing these simulations is taking us any closer to fault-tolerance (whether the surface code or a different form of it). People keep telling me that I am misunderstanding this (and maybe I am!) but I have not been convinced of that yet.\n",
        "\n",
        "my objection, which might be different from Cody's, is that the title of this blog post suggests that this experiment is getting us closer to realizing universal fault-tolerant quantum computers. In particular, when we say it is a step closer to improved quantum computers, it makes it sound like this approach is somehow a step on our roadmap for realizing fault-tolerant quantum computers, or that it is going to make our ultimate goal easier or something like that. That is reading between the lines a little bit, but it is how it sounds to me. Is that actually true? Or is this in the category of \"nice demonstration, but not something we're going to follow up on, and thus not a step towards us improving quantum computers other than in the very broad sense that it helps test hardware capabilities like most physics team experiments do\"? My understanding is that nothing about this experiment is likely to change anything about how we are planning to realize fault-tolerance. Maybe that is incorrect, or it is correct and I am just reading too much into the implications of statements like the title. It would be nice to understand this better.\n",
        "\n",
        "The main sense in which the non-abelian anyons are useful for fault tolerance is that they are isomorphic to twists in the surface code, and you can use twists to store logical qubits. But we already knew about twists independent of this work e.g. from https://arxiv.org/abs/1609.04673 so I don't know what it adds on the fault tolerance side.\n",
        "\n",
        "I did give a presentation to the authors of our abelian paper on how to do it fault tolerantly (they were technically \"using\" a surface code, but they were preparing it entirely unitarily which is not fault tolerant; you have to use measurements for everything). They weren't so interested I think, because the measurement version is harder and maybe also because they didn't consider it \"really doing it\" in the same way. I actually did run some quick shots of my versions of the circuits on the device back when pink was in M2 shape, and it worked, in that I got non-zero signal (the error rate was close to max but not max). I wasn't doing adept or etc or etc; if an experimentalist did it it would have been better. Also I now have much better versions of the circuit, which are described in https://arxiv.org/abs/2302.07395 .\n",
        "\n",
        "My point was that if you interpret TQC as broadly (\"you have things that act like anyons\") then this is demonstrating something interesting (multiparticle entangled EC state, FT gates on that state).  But I also agree with Ryan in that I think the blog post does not convey this well.\n",
        "\n",
        "Fair enough, I'm just trying to put forth a perspective on why it is fine to say this is an interesting experiment in topological quantum computation.  And if you want some idea of how it might lead to noise protection, yes  https://arxiv.org/abs/quant-ph/9912040 which was expanded upon in https://arxiv.org/abs/0907.3988\n",
        "\n",
        "And indeed our own team's work investigating how simulations of many body systems are related does or does not give protection of topologically protected information is in this direction.\n",
        "\n",
        "I think I agree with everything you've said Dave. But I don't think any of that really justifies the framing of the blog post title, which to me suggests that this is moving us closer to building a fault-tolerant quantum computer as opposed to exploring the error robustness of anyon simulation."
      ],
      "metadata": {
        "id": "yvGciNEzWO-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Future of Quantum Error Correction*"
      ],
      "metadata": {
        "id": "0PltPkkfZ1Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future of quantum error correction**\n",
        "* we still have problems like leakage etc\n",
        "  * When they ask about thresholds, they are also derived from ideal models. Things like circular ZZ coupling can make it much harder.\n",
        "  * If you add leakage to your CZ gates, you could take a surface code that would sort of there was zero leakage below the physical error rate. But if you add leakage to that 0.1 percent degrees (the red line), which is a second. When you do the decoding (the green line), suddenly you are not below the threshold anymore\n",
        "* Also, if we want distance n=17 (mentioned in the beginning) you have an insane amount of data coming out of the device:\n",
        "  * we need ($n^2 -1$) physical qubits for error correction. So if one readout is 1 bit, we need 288 bits per microsecond ($\\mu$s) per qubit).\n",
        "  * This amounts to 288 Gbits per s for 1000 logical qubits\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1292.png)"
      ],
      "metadata": {
        "id": "yps3FbFbuHOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color Code**\n",
        "\n",
        "* [Fault-tolerant quantum computing with color codes](https://arxiv.org/abs/1108.5738)\n",
        "\n",
        "* https://physics.stackexchange.com/questions/169176/quantum-error-correction-surface-code-vs-color-code\n",
        "\n",
        "The color code and surface code are very similar. They are stabilizer codes composed of qubits arranged in two dimensions, requiring only geometrically local stabilizer measurements.\n",
        "\n",
        "From the theory point of view, the codes are very similar. In fact, with collaborators we have proven that the color code is equivalent to a surface code (paper) up to a geometrically local unitary (one which only makes nearby qubits interact). One can think by analogy of the surface code* as a napkin with two rough and two smooth sides and the color code as folding this napkin along its diagonal. Because in the folded napkin, there are new things that are now close, it is possible to do more logical gates \"transversally\". This is good because it keeps errors from propagating and is relatively easy. However, the color code needs more qubits to interact in each stabilizer so ends up leading to a lower noise threshold. So one can say that although very similar, each code has its advantages and disadvantages.\n",
        "\n",
        "At this point, only very small versions of either of these codes are being demonstrated. The Rainer Blatt group demonstrated the smallest possible color-code which also uses 7 qubits (this instance is also referred to as the Steane code). However, the underlying geometry in which the qubits are laid out in the Blatt setup is a linear chain of ions, so I would say that this is not the natural setting to extend to larger and larger system sizes.\n",
        "\n",
        "**The superconducting qubit people (Martinis, IBM, DiCarlo, ...) on the other hand, are concentrating more on surface codes**. While in principle, their architecture should allow them to go full fledge 2D, for now, they are having the classical logic come in from the sides, which is something that needs to change.\n",
        "\n",
        "*There is actually an ambiguity as to what to call surface codes, but I will refer to the quantum double of Z2 with rough and smooth boundaries defined by Bravyi and Kitaev (paper)."
      ],
      "metadata": {
        "id": "9tWyVLKvxLE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Hardware*"
      ],
      "metadata": {
        "id": "oQZJ3rU97I-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Superconductivity*"
      ],
      "metadata": {
        "id": "-BAUgIGmirEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video [Map of Superconductivity](https://youtu.be/bD2M7P6dTVA)\n"
      ],
      "metadata": {
        "id": "YYP9SNpu4Uck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyon (Quasi Particles)**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Anyon\n",
        "\n",
        "* PBS Video on Quasiparticles: https://youtu.be/le_ORQZzkmE"
      ],
      "metadata": {
        "id": "gy8QFKOMICxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Jellium\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Cooper-Paar\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Bose‚ÄìEinstein_condensate\n",
        "\n",
        "https://opg.optica.org/oe/fulltext.cfm?uri=oe-2-8-299&id=63264\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Phonon\n",
        "\n",
        "\n",
        "Part of this correlation is the formation of pairs of electrons called Cooper pairs. According to Josephson, under certain circumstances these Cooper pairs move from one superconductor to the other across the thin insulating layer. Such motion of pairs of electrons constitutes the Josephson current, and the process by which the pairs cross the insulating layer is called Josephson tunneling.\n",
        "\n",
        "https://www.britannica.com/science/Josephson-effect\n",
        "\n",
        "\n",
        "Meissner effect\n",
        "\n",
        "Meissner effect, the expulsion of a magnetic field from the interior of a material that is in the process of becoming a superconductor, that is, losing its resistance to the flow of electrical currents when cooled below a certain temperature, called the transition temperature, usually close to absolute zero. The Meissner effect, a property of all superconductors, was discovered by the German physicists W. Meissner and R. Ochsenfeld in 1933.\n",
        "\n",
        "https://slideplayer.com/slide/5010186/\n",
        "\n",
        "Squid"
      ],
      "metadata": {
        "id": "I2GgHQvDATOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Josephson Junction*"
      ],
      "metadata": {
        "id": "siEtU4-r3-Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Superconducting integrated circuits**\n",
        "\n",
        "- Conductance is not constant but varies with how much current is flowing, making it an unharmonic oscillator\n",
        "- Potential of the conductor is a cosine\n",
        "- Low energy excitations are pairs of electrons slashing back and forth between the two antenna pads\n",
        "- From ground state to first excited state: 5 gigahertz\n",
        "- From first excited state to second excited state transition: 4.9 Ghz (due to flattened curve of cosine)\n",
        "\n",
        "More details: https://www.youtube.com/watch?v=uD69GCYF9Zg&t=2023s\n"
      ],
      "metadata": {
        "id": "yx8LP-FXATe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "√úbergangsdipolmoment (Transition dipole moment)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/√úbergangsdipolmoment"
      ],
      "metadata": {
        "id": "MTADPYzI5eaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transmon and Fluxonium Qubit**\n",
        "\n",
        "* Better hardware to protect against noise orders of magnitude better (to get down to 10^-5 instead of 10^-9\n",
        "* they can show cherence times above 1 millisecond, they had single qubit errors of 0.9999 (only 4x) with Fluxonium\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1293.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1299.png)\n",
        "\n",
        "*Source: https://theorie.physik.uni-konstanz.de/burkard/sites/default/files/images/Seminar_3_TrFl.pdf*\n",
        "\n",
        "\n",
        "Video: [Google Keynote: Superconducting qubits for quantum computation: transmon vs fluxonium](https://www.youtube.com/watch?v=qsizrKrUZDg)"
      ],
      "metadata": {
        "id": "We68Qy_9vgEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bsiegelwax.medium.com/i-love-neutral-atoms-47dd41b7a8d5"
      ],
      "metadata": {
        "id": "WoGx5IijM-q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Hardware**\n",
        "\n",
        "* [superconducting qubits](https://en.m.wikipedia.org/wiki/Superconducting_quantum_computing)\n",
        "\n",
        "  * [Building a quantum computer with superconducting qubits](https://www.youtube.com/watch?v=uPw9nkJAwDY)\n",
        "\n",
        "* [trapped ions](https://en.m.wikipedia.org/wiki/Trapped_ion_quantum_computer)\n",
        "\n",
        "* [liquid and solid state nuclear magnetic resonance](https://en.m.wikipedia.org/wiki/Nuclear_magnetic_resonance_quantum_computer)\n",
        "\n",
        "* [optical cluster states](https://en.m.wikipedia.org/wiki/One-way_quantum_computer)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1272.JPG)"
      ],
      "metadata": {
        "id": "puysuv7M8d-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hardware**\n",
        "\n",
        "\n",
        "[Simulations Using a Quantum Computer Show the Technology‚Äôs Current Limits](https://physics.aps.org/articles/v15/175)\n",
        "\n",
        "https://www.techexplorist.com/introducing-unimon-new-superconducting-qubit-quantum-computers/54880/?amp\n",
        "\n",
        "Rydbergs atoms and measuring time: https://www.sciencealert.com/scientists-just-discovered-an-entirely-new-way-of-measuring-time\n",
        "\n",
        "https://www.scinexx.de/news/physik/ein-moebiusband-aus-licht/\n",
        "\n",
        "https://phys.org/news/2022-11-erbium-atoms-silicon-prime-candidate.html\n",
        "\n",
        "https://medium.com/pasqal-io/why-analog-neutral-atoms-quantum-computing-is-the-most-promising-direction-for-early-quantum-77b462cefee0\n",
        "\n",
        "\n",
        "https://phys.org/news/2022-11-quantum-component-graphene.amp\n",
        "\n",
        "https://phys.org/news/2022-10-universal-parity-quantum-architecture-limitations.amp\n",
        "\n",
        "https://www.golem.de/news/quantencomputer-silizium-chip-liest-quantenpunkte-in-rekordzeit-2211-169385.amp.html\n",
        "\n",
        "https://aws.amazon.com/de/blogs/quantum-computing/an-illustrated-introduction-to-quantum-networks-and-quantum-repeaters/\n",
        "\n",
        "https://scitechdaily.com/100-times-longer-than-previous-benchmarks-a-quantum-breakthrough/amp/\n",
        "\n",
        "https://scitechdaily.com/physicists-create-first-quasiparticle-bose-einstein-condensate-the-mysterious-fifth-state-of-matter/amp/\n",
        "\n",
        "https://www.eetimes.eu/quantum-computers-a-technology-assessment/\n",
        "\n",
        "https://medium.com/qiskit/using-quantum-computers-to-tackle-complex-chemistry-simulations-with-quantum-embedding-7b7e4306b676"
      ],
      "metadata": {
        "id": "HMxnncRIeKR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Gates*"
      ],
      "metadata": {
        "id": "zpgWYl3LkxVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you only implement unitary operations on quantum circuits?**\n",
        "\n",
        "Quantum circuits are primarily built using unitary operations (also known as quantum gates), but that's not the entire story. There are two other crucial components that are involved: measurements and initialization operations, which are not unitary.\n",
        "\n",
        "1. **Unitary Operations**: These are the most common operations performed in quantum circuits and they play a crucial role because they preserve the quantum mechanical property of preserving the sum of probabilities. They are reversible and their inverses are their adjoints. Examples include the Pauli X, Y, and Z gates, Hadamard gate, phase gate, CNOT gate, etc.\n",
        "\n",
        "2. **Measurement Operations**: Measurements are another essential part of quantum circuits and they are not unitary. When a measurement is made, the quantum state collapses from a superposition of states to one particular state according to a certain probability distribution. This is a fundamentally irreversible process, and thus not unitary.\n",
        "\n",
        "3. **Initialization Operations**: When a quantum circuit starts, qubits are typically initialized to a certain state, most commonly the |0> state. This initialization is a non-unitary process as it doesn't conserve the previous information of the qubit.\n",
        "\n",
        "Moreover, in the real world, quantum systems experience noise and decoherence, which are also non-unitary processes. Error-correction techniques are implemented to deal with these issues, but they also cannot be modeled with unitary operations.\n",
        "\n",
        "So, while unitary operations are indeed a fundamental part of quantum circuits, they're not the only operations that can be implemented in a quantum circuit. Non-unitary operations like measurements and initializations also play an integral role in quantum computing."
      ],
      "metadata": {
        "id": "BkqIqXdpdTjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Qubits & Quantum Gates*"
      ],
      "metadata": {
        "id": "vSshMr2W7qVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gate Depth**\n",
        "\n",
        "Increasing qubit numbers on Quantum Hardware are good. However, an important question is how much gate depth will be allowed within the single qubit decoherence time window? I would like to know the state-of-the-art on this subject, in terms of noise models, analytical bounds or experimental findings. It will help assess the practical limitations on the Quantum circuits we design or Quantum Algorithms we formulate. From my end I found these two papers, it will be great if fellow LinkedIn friend can advise more apers on the state-of-the-art,\n",
        "\n",
        "1.https://https://Inkd.in/dTM3NJsU\n",
        "\n",
        "2.https://https://Inkd.in/dMCTcCzR"
      ],
      "metadata": {
        "id": "lNiIqfr1plUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/qiskit/the-atoms-of-computation-ae2b27799eaa"
      ],
      "metadata": {
        "id": "TUZ_lRaeftG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qutech.nl/wp-content/uploads/2018/02/t7-Luke-Schaeffer-tuesday_schaeffer.pdf"
      ],
      "metadata": {
        "id": "udmrmumE8x4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qubits and Quantum Gates**\n",
        "\n",
        "* a [quantum logic gate](https://en.m.wikipedia.org/wiki/Quantum_logic_gate) (or simply quantum gate) is a basic quantum circuit operating on a small number of qubits. They are the building blocks of quantum circuits, like classical logic gates are for conventional digital circuits.\n",
        "\n",
        "* Types of Qubits:\n",
        "\n",
        "  * **Unary**: one qubit (X, Y, Z, Hadamard and rotation gates)\n",
        "\n",
        "  * **Binary**: two qubits (CNOT can entangle two qubits, CZ gate, Swap gate)\n",
        "\n",
        "  * **Terniary**: three qubits (Fredkin = CSWAP gate, and Toffoli = CCNOT)\n",
        "\n",
        "* **Charge qubit & Transmon**\n",
        "\n",
        "  * In quantum computing, a [charge qubit](https://en.m.wikipedia.org/wiki/Charge_qubit) (also known as Cooper-pair box) is a qubit whose basis states are charge states (i.e. states which represent the presence or absence of excess Cooper pairs in the island)\n",
        "\n",
        "  * [transmon](https://en.m.wikipedia.org/wiki/Transmon) is a type of superconducting charge qubit that was designed to have reduced sensitivity to charge noise\n",
        "\n",
        "  * Google video superconductors: https://youtu.be/uPw9nkJAwDY\n",
        "\n",
        "  * Google Video: transmon vs fluxion: https://youtu.be/qsizrKrUZDg\n",
        "\n",
        "* **Fun Facts & important to know**\n",
        "\n",
        "  * all gates are reversable, because irreversable gates will destroy the entanglement and superposition\n",
        "\n",
        "  * a classical computer can theoretically simulate a quantum computer in any size, assuming they have enough bits, we can't solve something like the Halting problem with a quantum computer. It can solve a certain set of problems faster, but not everything faster.\n",
        "\n",
        "  * If we use four qubits, for example, then there are 2^4=16 different states. Starting from |0000‚ü©, to |0001‚ü©, ending at |1111‚ü©. Each qubit can have a specific meaning. We could interpret it as a letter. A letter that does not have 26 different options but only two, |0‚ü© and |1‚ü©. **With a sufficient number of qubits, we could represent all living humans. With 33 qubits, we can represent around 8.5 billion different states**. A phonebook of mankind. And we haven‚Äôt sorted it. https://javafxpert.github.io/grok-bloch/\n",
        "\n",
        "* **Essential Gates**\n",
        "\n",
        "  * the **minimal set** of gates is {T, H, CNOT} = Toffoli Gate\n",
        "\n",
        "  * Hadamard + Toffoli gates constitute a **universal quantum gate**.\n",
        "\n",
        "  * Toffoli cannot be constructed directly, builds up on others.\n",
        "\n",
        "  * T-gate is the **most expensive gate**"
      ],
      "metadata": {
        "id": "W12vgL1G6tG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Circuits (and Quantum Turing machines)**\n",
        "\n",
        "* A [quantum Turing machine (QTM)](https://en.m.wikipedia.org/wiki/Quantum_Turing_machine) or universal quantum computer is an abstract machine used to model the effects of a quantum computer.\n",
        "\n",
        "* It provides a simple model that captures all of the power of quantum computation‚Äîthat is, any quantum algorithm can be expressed formally as a particular quantum Turing machine.\n",
        "\n",
        "* However, the computationally equivalent [quantum circuit](https://en.m.wikipedia.org/wiki/Quantum_circuit) is a more common model.\n"
      ],
      "metadata": {
        "id": "TonYa1ZQxmVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Clifford Gates: Introduction (Hadamard, Pauli X Y Z, CNOT and Swap Gate)*"
      ],
      "metadata": {
        "id": "R-pP8oZC7zxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/quantum-untangled/visualizing-quantum-logic-gates-part-1-515bb7b58916"
      ],
      "metadata": {
        "id": "PGcrDhdstYmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "  * A common universal gate set is the Clifford + T gate set, which is composed of the CNOT, H, S and T gates\n",
        "\n",
        "  * The [rotation operators](https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates) Rx(Œ∏), Ry(Œ∏), Rz(Œ∏), the [phase shift gate](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#Phase_shift_gates) P(œÜ) and [CNOT](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#CNOT) form a universal set of quantum gates [Source](https://en.m.wikipedia.org/wiki/Quantum_logic_gate) (due to phase shift it is universal)\n",
        "\n",
        "* **Pro**: relatively easy to implement fault-tolerantly (i.e., with resistance to errors)\n",
        "\n",
        "* **Contra**: alone they do not permit universal quantum computation (not all quantum operations can be implemented using these gates)\n",
        "\n",
        "  * Clifford gates \"preserve\" the Pauli group: hen you apply a Clifford gate to a Pauli operator, the result is another Pauli operator. \n",
        "\n",
        "  * However, applying T-Gate (non-Clifford gate) to a Pauli operator does not preserve the Pauli group. \n",
        "\n",
        "  * Because there are quantum operations that don't preserve the Pauli group, and because the Clifford gates can only produce operations that do preserve the Pauli group, it follows that there are quantum operations that can't be produced by applying Clifford gates. That's why the Clifford gates by themselves are not a universal set of quantum gates.\n",
        "\n",
        "  * if you add just one non-Clifford gate (like the T gate) to the Clifford gates, you can form a universal gate set, which means any quantum operation can be approximated to an arbitrary degree of precision by a sequence of those gates. This is why non-Clifford gates, and especially the T gate, are so important in quantum computing.\n",
        "\n",
        "* The Clifford group is generated by three gates, Hadamard, S and CNOT gates (are all clifford gates themselves), the [Clifford Gates](https://en.m.wikipedia.org/wiki/Clifford_gates). See full [List of Clifford gates](https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates).\n",
        "\n",
        "* The Clifford set can be efficiently simulated classically by the Gottesman‚ÄìKnill theorem. [Source](https://en.m.wikipedia.org/wiki/Quantum_logic_gate)\n",
        "\n",
        "* Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)"
      ],
      "metadata": {
        "id": "liK-Ms2f3Xig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why is the T-gate being used to create a universal quantum gate?**\n",
        "\n",
        "In quantum computing, a universal gate set is a set of quantum gates from which any possible quantum operation can be constructed to any desired level of precision. A commonly used set of universal quantum gates consists of the set of all single-qubit gates and the CNOT (Controlled-NOT) gate.\n",
        "\n",
        "The CNOT gate along with single-qubit rotations (which can be formed from combinations of Hadamard gates and phase gates) allow us to generate a very large number of quantum states and operations. However, this set of gates isn't sufficient to create all possible quantum operations, which is a requirement for universality.\n",
        "\n",
        "The T-gate, which represents a œÄ/8 rotation around the Z-axis on the Bloch sphere (or a phase shift of e^(iœÄ/4)), is commonly added to the gate set to make it universal. \n",
        "\n",
        "This is because the T-gate is a non-Clifford gate, and adding any non-Clifford gate to the set of Clifford gates (the Hadamard gate, phase gate, and CNOT gate are all Clifford gates) results in a universal gate set. The non-Clifford nature of the T-gate allows us to generate quantum operations that cannot be created from Clifford gates alone.\n",
        "\n",
        "Therefore, the T-gate is often added to the gate set in order to achieve universality, enabling the performance of any arbitrary quantum computation.\n",
        "\n",
        "It's worth mentioning that the T-gate isn't the only choice for completing the universal gate set. Other choices could be used as well, as long as they are non-Clifford. The T-gate, however, has properties that make it a convenient and commonly-used choice.\n",
        "\n",
        "**what are these properties?**\n",
        "\n",
        "The T-gate has a few properties that make it particularly convenient as a choice for completing a universal gate set:\n",
        "\n",
        "1. **It's a single-qubit gate**: This makes it relatively simple to implement compared to multi-qubit gates.\n",
        "\n",
        "2. **It's a simple rotation**: The T-gate represents a œÄ/8 (or 45 degrees) phase shift, which is a relatively simple operation compared to many other non-Clifford gates.\n",
        "\n",
        "3. **Its matrix representation is simple**: The T-gate has a simple matrix representation, making it convenient for theoretical calculations and analyses.\n",
        "\n",
        "4. **It has a known fault-tolerant implementation**: There exists a known protocol for implementing the T-gate in a fault-tolerant manner using a procedure called magic state distillation. This is a crucial requirement for large-scale, error-corrected quantum computing. Although the process is resource-intensive, it's a well-studied method that allows the implementation of T-gates with an arbitrarily low error rate.\n",
        "\n",
        "These properties together make the T-gate a popular choice for use in universal gate sets for quantum computing. It's worth noting that other choices of non-Clifford gates can also be used to achieve universality, but they might not have the same combination of desirable properties."
      ],
      "metadata": {
        "id": "jAEVz58R5MqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.st-andrews.ac.uk/physics/quvis/simulations_html5/sims/blochsphere/blochsphere.html\n",
        "\n",
        "https://bits-and-electrons.github.io/bloch-sphere-simulator/\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1591.png)\n",
        "\n",
        "https://deeplearninguniversity.com/qiskit/qiskit-phase-gate/"
      ],
      "metadata": {
        "id": "9mG7aZketkyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1590.png)\n",
        "\n",
        "https://pme.uchicago.edu/awschalom-group/all-optical-holonomic-single-qubit-gates\n",
        "\n",
        "https://www.nature.com/articles/nphoton.2017.40"
      ],
      "metadata": {
        "id": "bNJPN8ccs9lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli Algebra, Pauli Matrices (Pauli Operators)**\n",
        "\n",
        "The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse), meaning that the square of a Pauli matrix is the identity matrix.\n",
        "\n",
        ">$\n",
        "I^{2}=X^{2}=Y^{2}=Z^{2}=-i X Y Z=I\n",
        "$\n",
        "\n",
        "The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example $Z X=i Y=-X Z$.\n",
        "\n",
        "*Anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. **Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments**. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation, such as addition.*\n",
        "\n",
        "Single spin one half particle, focus on spin degrees of freedom:"
      ],
      "metadata": {
        "id": "FDizpChrrBz3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGvwll0SoLoA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_122.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Quantum_Logic_Gates.png/500px-Quantum_Logic_Gates.png)"
      ],
      "metadata": {
        "id": "eMSx6ntm6p8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-X Gate (Flip Computational States)**\n",
        "\n",
        "> $X=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\sigma_{x}=\\mathrm{NOT}$\n",
        "\n",
        "* The Pauli- $X$ gate is the quantum equivalent of the [NOT gate](https://en.m.wikipedia.org/wiki/Inverter_(logic_gate)) for classical computers (its main function is to invert the input signal applied) with respect to the standard basis $|0\\rangle,|1\\rangle$, which distinguishes the $z$ axis on the Bloch sphere. It is sometimes called a bit-flip as it maps $|0\\rangle$ to $|1\\rangle$ and $|1\\rangle$ to $|0\\rangle .$\n",
        "\n",
        "* The Pauli gates $(X, Y, Z)$ are the three Pauli matrices $\\left(\\sigma_{x}, \\sigma_{y}, \\sigma_{z}\\right)$ and act on a single qubit. The Pauli $X_{1} Y$ and $Z$ equate, respectively, to a rotation around the $x, y$ and $z$ axes of the Bloch sphere by $\\pi$ radians.\n",
        "\n",
        "* Nice visualisations: https://medium.com/analytics-vidhya/quantum-gates-7fe83817b684\n",
        "\n"
      ],
      "metadata": {
        "id": "zDXxSuLakwyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root of Pauli-X Gate**\n",
        "\n",
        "The square root of NOT gate (or square root of Pauli- $X, \\sqrt{X}$ ) acts on a single qubit. It maps the basis state $|0\\rangle$ to $\\frac{(1+i)|0\\rangle+(1-i)|1\\rangle}{2}$ and $|1\\rangle$ to $\\frac{(1-i)|0\\rangle+(1+i)|1\\rangle}{2} .$\n",
        "\n",
        "In matrix form it is given by\n",
        "\n",
        ">$\n",
        "\\sqrt{X}=\\sqrt{\\mathrm{NOT}}=\\frac{1}{2}\\left[\\begin{array}{cc}\n",
        "1+i & 1-i \\\\\n",
        "1-i & 1+i\n",
        "\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}\n",
        "e^{i \\pi / 4} & e^{-i \\pi / 4} \\\\\n",
        "e^{-i \\pi / 4} & e^{i \\pi / 4}\n",
        "\\end{array}\\right]$\n",
        "\n",
        "such that\n",
        "\n",
        ">$\n",
        "(\\sqrt{X})^{2}=(\\sqrt{\\mathrm{NOT}})^{2}=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right]=X\n",
        "$\n",
        "\n",
        "> *This operation represents a rotation of $\\pi / 2$ about $x$ -axis at the Bloch sphere*."
      ],
      "metadata": {
        "id": "PeMSkgSzlzsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Y Gate (Phase Flip between i and -i)**\n",
        "\n",
        "> $Y=\\sigma_{y}=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right]$\n",
        "\n",
        "* Similarly, the Pauli- $Y$ maps $|0\\rangle$ to $i|1\\rangle$ and $|1\\rangle$ to $-i|0\\rangle$ (NOT gate with i-multiple):\n"
      ],
      "metadata": {
        "id": "htDSfNdzJulw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Z Gate ($\\pi$ Flip Phase between +1 and -1)**\n",
        "\n",
        "> $Z=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]=\\sigma_{z}\\quad Z=|0\\rangle\\langle 0|-| 1\\rangle\\langle 1|$\n",
        "\n",
        "* Pauli Z gate is a phase flip gate that causes rotation around the z-axis by œÄ radians. Z flippt zwischen den Polen der X Achse (+ und -).\n",
        "\n",
        "* *Z Gate flippt zwischen den Hadamard gegen√ºberliegenden Richtungen auf der X-Achse. We change the phase (or sign) on a Qubit*:\n",
        "\n",
        "> $\\mathbf{Z}|0\\rangle=|0\\rangle$\n",
        "\n",
        "> $\\mathbf{Z}|1\\rangle=-|1\\rangle$\n",
        "\n",
        "* Since |0‚ü© and |1‚ü© lie on the z-axis, the Z-gate will not affect these states. To put it in other terms *|0‚ü© and |1‚ü© are the two eigenstates of the Z-gate*. On the other hand, it flips |+‚ü© to |-‚ü© and |-‚ü© to |+‚ü©.\n",
        "\n",
        "* Pauli $Z$ leaves the basis state $|0\\rangle$ unchanged and maps $|1\\rangle$ to $-|1\\rangle$. Due to this nature, it is sometimes called *phase-flip* (flips sign of second entangled state)\n",
        "\n",
        "* *The Z gate is like the X gate but in the hadamard basis, flipping states*"
      ],
      "metadata": {
        "id": "qj2bMNDwLoDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-S Gate: $\\frac{\\pi}{2}$ Flip Phase between Z and Y (with $\\mu$=i and $\\nu$=-i)**\n",
        "\n",
        "> ${S}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & \\sqrt{-1}\\end{array}\\right]=\\sqrt{\\mathbf{Z}}$\n",
        "\n",
        "* The phase gate $\\mathbf{Z}$ transforms $|+\\rangle$ to $|-\\rangle$, and we can find the gate that does \"half of\" this transformation by finding the square root of the matrix $\\mathbf{Z}$\n",
        "\n",
        "* fintuning Z gate in der Hadamard basis: S-gate is die H√§lfte vom Z-Gate, und R-Gate ein viertel vom Z-Gate\n",
        "\n",
        "* Use the matrix $\\mathbf{S}$ to find the state $|\\mu\\rangle$ which is \"halfway\" between $|+\\rangle$ and $|-\\rangle$ :\n",
        "\n",
        "> $\\mathbf{S}|+\\rangle=|\\mu\\rangle =\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "If you begin with $|+\\rangle$ and apply the $\\mathbf{S}$ gate three times in a row, you find a new state, $|\\nu\\rangle$ which appears to mirror the complex state $|\\mu\\rangle$ :\n",
        "\n",
        "> $|\\mu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "> $|\\nu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-i|1\\rangle) .$"
      ],
      "metadata": {
        "id": "Frcfbnlgr6d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already uncovered the gates we need to perform any rotations around the $x$ - and $z$-axes, which can bring us from an initialized qubit to any other quantum state.\n",
        "\n",
        "Unfortunately, this result comes with a pretty serious caveat: all of the gates we have used so far are discrete. They perform rotations around the Bloch sphere, but since they rotate in discrete hops (of $\\pi / 2$ or $\\pi$ ), they are unable to reach most of the intermediate states on the surface of the sphere.\n",
        "\n",
        "One solution to this problem is to define smaller and smaller rotations around these axes. The gate $\\mathbf{S}$ is equal to $\\sqrt{\\mathbf{Z}}$ and halves its rotation angle from $\\pi$ to $\\pi / 2$. Even greater division of these gates is possible, and some of them have\n",
        "common names:\n",
        "\n",
        "$\\mathbf{Z}=\\mathbf{Z}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]$ Rotation: $\\pi$\n",
        "\n",
        "$\\mathbf{S}=\\sqrt[2]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]$ Rotation: $\\pi / 2$\n",
        "\n",
        "\n",
        "$\\mathbf{T}=\\sqrt[4]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$ Rotation: $\\pi / 4$ not clifford\n",
        "\n",
        "$\\mathbf{R 8}=\\sqrt[8]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 8}\\end{array}\\right] \\quad$ Rotation: $\\pi / 8 .$ not clifford"
      ],
      "metadata": {
        "id": "6jspQ2kndBZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNOT Gate**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Controlled_NOT_gate"
      ],
      "metadata": {
        "id": "cBI__-2S2o7Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjqz8Hkol68T"
      },
      "source": [
        "**Swap Gate**\n",
        "\n",
        "*2 Qubits - Swap Gate (& Swap Square root): Swap two Qubits (like in Quantum Fourier Transform)*\n",
        "\n",
        "The swap gate **swaps two qubits**.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\text { SWAP }=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzlOABhWmMms"
      },
      "source": [
        "**Square root of swap gate**\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate performs half-way of a two-qubit swap.\n",
        "\n",
        "It is universal such that any many-qubit gate can be constructed from only $\\sqrt{\\text { SWAP }}$ and single qubit gates.\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate is not, however maximally entangling; more than one application of it is required to produce a Bell state from product states.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\sqrt{\\mathrm{SWAP}}=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & \\frac{1}{2}(1+i) & \\frac{1}{2}(1-i) & 0 \\\\\n",
        "0 & \\frac{1}{2}(1-i) & \\frac{1}{2}(1+i) & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "This gate arises naturally in systems that exploit exchange interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hadamard Gate (Superposition)*\n",
        "\n",
        "> $H=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**The Hadamard states ‚à£+‚ü© and ‚à£‚àí‚ü© are considered superposition states**\n",
        "\n",
        "because they are a combination of the two computational states:\n",
        "\n",
        "> $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "**Apply Hadamard gate on a qubit that is in the |0> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "**Now apply Hadamard gate on a qubit that is in the |1> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{-1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "Hence, in both cases (qubit |0> or qubit |1>) applying a Hadamard Gate gives an equal chance for the qubit to be 0 or 1' when measured.\n",
        "\n",
        "Source: https://freecontent.manning.com/all-about-hadamard-gates/\n"
      ],
      "metadata": {
        "id": "_T8yeOt7mKxA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjeGeBvlLhN"
      },
      "source": [
        "**Herleitung Hadamard (Wichtig!)**\n",
        "\n",
        "For an equal (or uniform) superposition of the two computational states, we can set the two coefficients equal to each other:\n",
        "\n",
        "$a_{1}=a_{2}=a$\n",
        "\n",
        "The normalization condition for a well-behaved quantum state requires that the sum of the squared magnitudes of the coefficients be equal to one; this is sufficient to find\n",
        "a\n",
        "a for a uniform superposition:\n",
        "\n",
        "$|a|^{2}+|a|^{2}=1$\n",
        "\n",
        "$2|a|^{2}=1$\n",
        "\n",
        "$|a|^{2}=\\frac{1}{2}$\n",
        "\n",
        "$a=\\frac{1}{\\sqrt{2}}$\n",
        "\n",
        "In vector form, this state can represented as\n",
        "\n",
        "> $\\left[\\begin{array}{l}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ = $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right]$ + $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right]$\n",
        "\n",
        "This is what we can use now to understand Hadamard, where you want \"halfway\" a 50/50 chance of basis states 0 and 1 (Bloch sphere representation of superposition state):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_025.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlCP-muCpOWK"
      },
      "source": [
        "*Hadamard gate operations:*\n",
        "\n",
        "> $\\begin{aligned} H(|0\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle=:|+\\rangle \\\\ H(|1\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle=:|-\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)+\\frac{1}{2}(|0\\rangle-|1\\rangle)=|0\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)-\\frac{1}{2}(|0\\rangle-|1\\rangle)=|1\\rangle \\end{aligned}$\n",
        "\n",
        "One application of the Hadamard gate to either a 0 or 1 qubit will produce a quantum state that, if observed, **will be a 0 or 1 with equal probability** (as seen in the first two operations). This is exactly like flipping a fair coin in the standard probabilistic model of computation. However, if the Hadamard gate is applied twice in succession (as is effectively being done in the last two operations), then the final state is always the same as the initial state (because quantum operations are reversable, unlike operations on classical computers)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Clifford Gates: Magic States and Gottesman‚ÄìKnill theorem*"
      ],
      "metadata": {
        "id": "EHcRBgP8rE9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizer formalism**\n",
        "\n",
        "Stabilizer formalism bzw [Stabilizer code](https://en.m.wikipedia.org/wiki/Stabilizer_code)\n",
        "\n",
        "The Clifford group consists of a set of n-qubit operations generated by the gates {H, S, CNOT} (where H is Hadamard and S is ${\\displaystyle {\\begin{bmatrix}1&0\\\\0&i\\end{bmatrix}}}$) called Clifford gates. \n",
        "\n",
        "The Clifford group generates stabilizer states which can be efficiently simulated classically, as shown by the Gottesman‚ÄìKnill theorem. This set of gates with a non-Clifford operation is universal for quantum computation."
      ],
      "metadata": {
        "id": "kUizH3eMz2sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gottesman‚ÄìKnill theorem**\n",
        "\n",
        "* the [Gottesman‚ÄìKnill theorem](https://en.m.wikipedia.org/wiki/Gottesman‚ÄìKnill_theorem) is a theoretical result by Daniel Gottesman and Emanuel Knill that states that stabilizer circuits, **circuits that only consist of gates from the normalizer of the qubit Pauli group**, also called Clifford group, can be perfectly simulated in polynomial time on a probabilistic classical computer. The Clifford group can be generated solely by using CNOT, Hadamard, and phase gate S;[1] and therefore stabilizer circuits can be constructed using only these gates.\n",
        "\n",
        "* Thanks to the Gottesman‚ÄìKnill theorem, it is known that some quantum operations (operations in the Clifford algebra) can be perfectly simulated in polynomial time on a probabilistic classical computer. In order to achieve universal quantum computation, a quantum computer must be able to perform operations outside this set. [Magic States](https://en.m.wikipedia.org/wiki/Magic_state_distillation) can do this.\n",
        "\n",
        "Theorem: A quantum circuit using only the following elements can be simulated efficiently on a classical computer:\n",
        "\n",
        "1. Preparation of qubits in computational basis states,\n",
        "2. Clifford gates (Hadamard gates, controlled NOT gates, phase gate S ), and\n",
        "3. Measurements in the computational basis."
      ],
      "metadata": {
        "id": "8HXV4TT9l54i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic States**\n",
        "\n",
        "* if we have access to certain specific quantum states, known as magic states, then even with only Clifford gates, we can perform universal quantum computation. This is done by using these magic states as resources in a specific type of quantum protocol called magic state distillation.\n",
        "\n",
        "* These magic states are especially important in quantum error correction because they are resistant to certain types of quantum noise and can be \"distilled\" to produce high-fidelity magic states even when we start with imperfect copies. This makes them very useful in designing fault-tolerant quantum computers, as they can help us overcome the errors that inevitably occur in any real-world quantum system.\n",
        "\n",
        "Magic states are purified from n copies of a mixed state $\\rho$ . These states are typically provided via an ancilla to the circuit. A magic state for the $T$ gate is ${\\displaystyle |M\\rangle =\\cos(\\beta /2)|0\\rangle +e^{i{\\frac {\\pi }{4}}}\\sin(\\beta /2)|1\\rangle }$ where ${\\displaystyle \\beta =\\arccos \\left({\\frac {1}{\\sqrt {3}}}\\right)}$. By combining (copies of) magic states with Clifford gates, can be used to make a non-Clifford gate\n",
        "\n",
        "*The first magic state distillation algorithm, invented by Sergey Bravyi and Alexei Kitaev, is a follows.*\n",
        "\n",
        "* Input: Prepare 5 imperfect states.\n",
        "* Output: An almost pure state having a small error probability.\n",
        "* repeat\n",
        "  * Apply the decoding operation of the five-qubit error correcting code and measure the syndrome.\n",
        "  * If the measured syndrome is ${\\displaystyle |00000\\rangle }$, the distillation attempt is successful.\n",
        "  * else Get rid of the resulting state and restart the algorithm.\n",
        "* until The states have been distilled to the desired purity."
      ],
      "metadata": {
        "id": "9_FS5V9hlPyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbf{T}=\\sqrt[4]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$ Rotation: $\\pi / 4$ not clifford\n"
      ],
      "metadata": {
        "id": "Tye0_ZN24jlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Magic state distillation**](https://en.m.wikipedia.org/wiki/Magic_state_distillation) is a procedure in quantum computation that is used to create high-fidelity quantum states, starting from several copies of a \"noisy\" state. The name \"distillation\" is used because it's similar to the idea of distilling a liquid to increase its purity. \n",
        "\n",
        "The technique was first proposed by Emanuel Knill in 2004, and further analyzed by Sergey Bravyi and Alexei Kitaev the same year.\n",
        "\n",
        "Here's a high-level overview of how it works:\n",
        "\n",
        "1. **Preparation**: Prepare several copies of a quantum state. These states are typically \"noisy\" copies of a desired magic state, meaning they have some errors or deviations from the perfect magic state. \n",
        "\n",
        "2. **Error Detection**: Apply a quantum error-detecting code to these copies. This code is a series of quantum operations that can identify (but not correct) certain errors in the quantum states.\n",
        "\n",
        "3. **Measurement**: Measure the error-detecting code. This does not directly measure the quantum states themselves but measures whether an error has been detected.\n",
        "\n",
        "4. **Post-selection**: If the measurement indicates that no errors were detected, then (due to the properties of quantum mechanics) the quantum states are projected into a state that is closer to the perfect magic state than the original states were. If an error was detected, discard the states and start over.\n",
        "\n",
        "5. **Repeat**: Repeat this process several times. Each time, the states get closer to the perfect magic state.\n",
        "\n",
        "By repeating this process, one can \"distill\" a set of noisy magic states into a smaller number of higher-fidelity magic states. This is a crucial part of many fault-tolerant quantum computing schemes, as it allows one to perform universal quantum computation even in the presence of noise."
      ],
      "metadata": {
        "id": "iFeGkcT-lTDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Non-Clifford Gates (Phase Shift)*"
      ],
      "metadata": {
        "id": "Rjqwr4miC9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relative Phase Gates**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates\n",
        "\n",
        "includes t gate"
      ],
      "metadata": {
        "id": "ksB-lSPRDB_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rotation Operator Gates**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates\n",
        "\n",
        "non clifford"
      ],
      "metadata": {
        "id": "Q971EvdWC1KC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DomGPGBSmzGZ"
      },
      "source": [
        "**Non-Clifford swap gates**\n",
        "\n",
        "*3 Qubits - Fredkin Gate (CSWAP or CS): Swap identity gate for qubit 2+3, if qubit 1 is in state 1*\n",
        "\n",
        "The Fredkin gate is a universal reversible 3-bit gate that swaps the last two bits if the first bit is 1; a controlled-swap operation.\n",
        "\n",
        "The [Fredkin gate](https://en.m.wikipedia.org/wiki/Fredkin_gate) (also CSWAP or CS gate), named after Edward Fredkin, is a 3-bit gate that performs a controlled swap. It is universal for classical computation.\n",
        "\n",
        "It has the useful property that the numbers of 0s and 1s are conserved throughout, which in the billiard ball model means the same number of balls are output as input.\n",
        "\n",
        "The basic Fredkin gate[1] is a controlled swap gate that maps three inputs (C, I1, I2) onto three outputs (C, O1, O2). The C input is mapped directly to the C output.\n",
        "\n",
        "If C = 0, no swap is performed; I1 maps to O1, and I2 maps to O2.\n",
        "\n",
        "Otherwise, the two outputs are swapped so that I1 maps to O2, and I2 maps to O1.\n",
        "\n",
        "https://www.science.org/doi/10.1126/sciadv.1501531\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: T-Gate (Non-Clifford)*"
      ],
      "metadata": {
        "id": "jb3vfKj27tso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T-Gate**\n",
        "\n",
        "* T-gate is the most expensive gate\n",
        "\n",
        "* is not a Clifford gate\n",
        "\n",
        "\n",
        "Clifford gates (such as the Pauli gates and the Hadamard gate) can be implemented fault-tolerantly relatively easily, while non-Clifford gates are generally much harder to implement fault-tolerantly\n",
        "\n",
        "The **standard method for implementing a T-gate (non Clifford) fault-tolerantly involves a procedure known as magic state distillation**, which requires a significant overhead in terms of additional quantum gates and qubits, making it relatively \"expensive\" compared to other gates.\n",
        "\n",
        "Furthermore, the T-gate is a crucial gate for universal quantum computation. In **many quantum algorithms, the majority of the non-Clifford gates that are needed are T-gates**. This makes the T-gate a major contributor to the overall resource requirements of many quantum computations, further contributing to its reputation as the \"most expensive\" gate.\n",
        "\n"
      ],
      "metadata": {
        "id": "pShsyvvyo5a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*T Gate $\\frac{\\pi}{4}$ and R8 Gate $\\frac{\\pi}{8}$: Change Phase (Parametrizing it as continuous)*\n",
        "\n",
        "*Let's take as an example the T-gate, and use Quantum Phase Estimation to estimate its phase.*\n",
        "\n",
        "You will remember that the $T$-gate adds a phase of $e^{\\frac{i \\pi}{4}}$ to the state $|1\\rangle$ :\n",
        "\n",
        "$\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$\n",
        "\n",
        "Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$\n",
        "\n",
        "<font color=\"red\">We expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "Calculate the algreba of T-gate applied:\n",
        "\n",
        "$\\mathbf{T}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$\n",
        "\n",
        "First we have a qubit in state 0 and apply Hadamard:\n",
        "\n",
        "$\\left[\\begin{array}{l}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ = $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right]$ + $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right]$\n",
        "\n",
        "$\n",
        "H=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}\\langle 0|+\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}\\langle 1|\n",
        "$\n",
        "\n",
        "in Dirac notation. This corresponds to the transformation matrix\n",
        "\n",
        "> $\n",
        "H_{T}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "1 & -1\n",
        "\\end{array}\\right)\n",
        "$\n"
      ],
      "metadata": {
        "id": "h5msiZLrucIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Toffoli-Gate*"
      ],
      "metadata": {
        "id": "LAH8nB-d7vrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Toffoli_gate#"
      ],
      "metadata": {
        "id": "byRVo1WiV1u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The Toffoli gate can be constructed from single qubit T- and Hadamard-gates, and a minimum of six CNOTs* \n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Qcircuit_ToffolifromCNOT.svg/799px-Qcircuit_ToffolifromCNOT.svg.png)"
      ],
      "metadata": {
        "id": "XPdY_aRRVpZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Toffoli gate used in Shor's algorithm*\n",
        "\n",
        "https://quantum-computing.ibm.com/composer/docs/iqx/guide/shors-algorithm"
      ],
      "metadata": {
        "id": "WOVbVEBFtdqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toffoli Gate**\n",
        "\n",
        "* Google search: toffoli gates in quantum algorithms\n",
        "\n",
        "* **The Toffoli can be constructed from single qubit gates and a few CNOTs**\n",
        "\n",
        "> **The minimal circuit to implement the Toffoli involves 6 CNOTs and a number of Hadamard and T and T‚Ä† gates**\n",
        "\n",
        "* **The Toffoli gate is a key component for many important quantum algorithms**, notably:\n",
        "\n",
        "  * the Shor algorithm, quantum error correction, fault-tolerant computation40 and quantum arithmetic operations, and, together with the Hadamard gate, is universal for quantum computation (https://www.nature.com/articles/npjqi201619)\n",
        "\n",
        "* the Toffoli gate is reversible. This means it's its own inverse: if you apply a Toffoli gate to the result of a Toffoli operation, you get back the original input.\n",
        "\n",
        "* **The Toffoli gate is universal when combined with the single qubit Hadamard gate.**\n",
        "\n",
        "* Hence in principle the Toffoli gate itself is not needed, i.e. it is not a ‚Äúfundamental‚Äù gate; however, it is very useful, and ‚Äúexpensive‚Äù to implement with more fun- damental gates, so a hardware system that supports the Toffoli can afford some computational efficiency.\n",
        "\n",
        "* Source: An introduction to the surface code, Andrew N. Cleland, University of Chicago, Chicago IL 60637, USA \n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/11915/are-toffoli-gates-actually-used-in-designing-quantum-circuits That said, recently there has been work assuming a Clifford+T gate set where the T gate is the most expensive, based on the surface code"
      ],
      "metadata": {
        "id": "oSFoh65_qNui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Toffoli gate, also known as the CCNOT (controlled-controlled-NOT) gate, is used in a number of quantum algorithms due to its unique characteristics. Here are a few examples:\n",
        "\n",
        "1. **Quantum Error Correction Codes**: Quantum error correction codes are essential for stabilizing quantum information against decoherence and other noise. The Toffoli gate is often used in the implementation of these codes because of its ability to apply controlled operations.\n",
        "\n",
        "2. **Quantum Arithmetic**: The Toffoli gate is used extensively in quantum arithmetic circuits, such as adders or multipliers. These are building blocks for more complex quantum algorithms. One prominent example is Shor's algorithm, which uses quantum arithmetic for efficient integer factorization.\n",
        "\n",
        "3. **Quantum Fourier Transform (QFT)**: The QFT is a crucial component of several quantum algorithms, including Shor's algorithm and the quantum phase estimation algorithm. The implementation of QFT often involves controlled rotation gates, which can be constructed using Toffoli gates and single-qubit gates.\n",
        "\n",
        "4. **Grover's Algorithm**: This quantum search algorithm can also be implemented using Toffoli gates, although they are not fundamentally necessary for the algorithm. The Toffoli gate can be used to construct the required oracle and inversion-about-the-mean components of the algorithm.\n",
        "\n",
        "5. **Deutsch‚ÄìJozsa Algorithm**: While not strictly required, Toffoli gates can be used in the construction of the oracle for this algorithm.\n",
        "\n",
        "These examples demonstrate the versatility and importance of the Toffoli gate in quantum computation. It's worth noting that because Toffoli gates are non-trivial to implement in real quantum systems, much research is dedicated to devising efficient ways to construct them from simpler gates."
      ],
      "metadata": {
        "id": "CFS0G7s4Sn8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3 Qubits - Toffoli gate (CCNOT): Negate a target bit if both control qubits are 1*\n",
        "\n",
        "TOFFOLI gate can be used to simulate standard boolean operations. It negates a target bit if both control qubits are 1.\n",
        "\n",
        "> **The Toffoli gate is essentially the atom of mathematics. It is the simplest element, from which every other problem-solving technique can be compiled**.\n",
        "\n",
        "* The [Toffoli gate](https://en.m.wikipedia.org/wiki/Toffoli_gate), named after Tommaso Toffoli; also called CCNOT gate or Deutsch gate $D(\\pi / 2)$; is a 3-bit gate, which is universal for classical computation but not for quantum computation.\n",
        "\n",
        "* The quantum Toffoli gate is the same gate, defined for 3 qubits. If we limit ourselves to only accepting input qubits that are $|0\\rangle$ and $|1\\rangle$, then if the first two bits are in the state $|1\\rangle$ it applies a Pauli- $X$ (or NOT) on the third bit, else it does nothing.\n",
        "\n",
        "* It is an example of a controlled gate. Since it is the quantum analog of a classical gate, it is completely specified by its truth table.\n",
        "\n",
        "* **The Toffoli gate is universal when combined with the single qubit Hadamard gate. $^{[13]}$**"
      ],
      "metadata": {
        "id": "xiRdxCDp8Xfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Block Encoding*"
      ],
      "metadata": {
        "id": "bOsPQjh-8FP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Encoding**\n",
        "\n",
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "\n",
        "block encodings (unitary, signal proicesing, transformation)\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "h7nV48-uvauA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-1lpMAgXzD9"
      },
      "source": [
        "### <font color=\"blue\">**Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Linear Algebra*"
      ],
      "metadata": {
        "id": "04vVZZU1Q4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Linear algebra for Quantum Mechanics](https://www.youtube.com/watch?v=FF05fXg03A0)"
      ],
      "metadata": {
        "id": "ihAL0jVFhV5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *SVD vs PCA*"
      ],
      "metadata": {
        "id": "Ak7yEH2CwIV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a low rank matrix?**\n",
        "\n",
        "In linear algebra, the rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. This concept is important because it provides insight into the structure of the matrix, its null space, column space, etc.\n",
        "\n",
        "A \"low-rank\" matrix is a matrix where the rank is significantly less than the number of rows or columns. For example, if you have a 1000x1000 matrix (which could potentially have a rank up to 1000), but its rank is only 10, that matrix would be considered low-rank.\n",
        "\n",
        "Low-rank matrices are useful in many applications, including machine learning and data analysis. They are often used in techniques like principal component analysis (PCA) or singular value decomposition (SVD), where a high-dimensional dataset is approximated by a lower-dimensional one. This is possible because the data can often be accurately represented in a lower-dimensional space (i.e., with a low-rank matrix), which simplifies analysis and reduces computational complexity.\n",
        "\n",
        "In the context of quantum computing, low-rank matrices are often easier to work with because they require fewer quantum resources to implement in quantum algorithms. For example, certain operations might require a number of qubits proportional to the rank of a matrix, meaning low-rank matrices can be more efficiently processed on a quantum computer."
      ],
      "metadata": {
        "id": "HCWS6hviwLHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference between pca and svd?**\n",
        "\n",
        "Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are both techniques used in linear algebra and data analysis, but they are used for somewhat different purposes and their computations are based on different principles. Here's a comparison:\n",
        "\n",
        "1. **PCA (Principal Component Analysis)** is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. The transformation is done in such a way that the first principal component has the largest possible variance (i.e., accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. PCA is often used in exploratory data analysis and for making predictive models.\n",
        "\n",
        "2. **SVD (Singular Value Decomposition)** is a method of decomposing a matrix into three other matrices. Given a matrix A, you can decompose it into three matrices U, Œ£, and V^T such that A = UŒ£V^T. The U and V matrices contain orthogonal vectors (the left and right singular vectors), and Œ£ contains the singular values, which can be thought of as the \"strength\" or \"importance\" of the corresponding singular vectors. The SVD is a fundamental operation in linear algebra, and it has a wide range of applications. It's used in solving systems of linear equations, inverting matrices, and is the backbone of many data reduction techniques.\n",
        "\n",
        "The relationship between PCA and SVD is as follows: PCA can be solved by applying the SVD on the data matrix (more precisely, on the covariance or correlation matrix of the data). The principal components resulting from PCA are equivalent to the right singular vectors resulting from the SVD (after some scaling). In other words, PCA can be considered a special case of SVD.\n",
        "\n",
        "Despite their relationship, the methods are typically used in different contexts: **PCA is mainly used in the context of data analysis, for tasks like dimensionality reduction and extracting useful features from data, while SVD is a more general tool in linear algebra with a wider range of applications**."
      ],
      "metadata": {
        "id": "fi7ujY932UC1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8I3t063LsOf"
      },
      "source": [
        "###### *Eigenwerte*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQKdBUmBB1w"
      },
      "source": [
        "**Kriterien fur die Existenz von Eigenwerten**\n",
        "\n",
        "(*Wenn eine dieser Aussagen wahr ist, dann alle. Und wenn eine falsch, dann sind alle falsch*)\n",
        "\n",
        "1. $\\operatorname{rg}(B) < n$\n",
        "\n",
        "2. $\\operatorname{det}(B) = 0$ -> dieses Kriterium zu prufen ist am einfachsten und daher am haufigsten!\n",
        "\n",
        "3. $B^{-1}$ existiert nicht (nicht invertierbar)\n",
        "\n",
        "4. $B \\vec{X}$ = 0 hat mehr als nur die Losung $\\vec{x}$ = 0\n",
        "\n",
        "5. $\\lambda$ = 0 ist ein Eigenwert von $B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RIOxr9LuLh"
      },
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1192.png)\n",
        "\n",
        "*Calculating Eigenvalues via determinant: The tweaked transformation squishes space into a lower dimension (Daher muss rang < n sein)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rayleigh-Quotient & Satz von Courant-Fischer*"
      ],
      "metadata": {
        "id": "30ncah90297J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), auch Rayleigh-Koeffizient genannt, ist ein Objekt aus der linearen Algebra.\n",
        "\n",
        "Der Rayleigh-Quotient wird insbesondere zur numerischen Berechnung von Eigenwerten einer quadratischen Matrix $A$ verwendet.\n",
        "\n",
        "Sei $A \\in {\\mathbb K}^{n \\times n}$ eine reelle symmetrische oder komplexe hermitesche Matrix und $x\\in {\\mathbb {K} }^{n}$ mit $x\\neq 0$ ein Vektor, dann ist der Rayleigh-Quotient von $A$ zum Vektor $x$ definiert durch:\n",
        "\n",
        "> $R_{A}(x)={\\frac  {x^{*}Ax}{x^{*}x}}$\n",
        "\n",
        "Der Rayleigh-Quotient hat eine enge Beziehung zu den Eigenwerten von $A$. Ist $v$ ein Eigenvektor der Matrix $A$ und $\\lambda$  der zugeh√∂rige Eigenwert, dann gilt:\n",
        "\n",
        "> $R_{A}(v)={\\frac  {v^{*}Av}{v^{*}v}}={\\frac  {v^{*}\\lambda v}{v^{*}v}}=\\lambda$\n",
        "\n",
        "Durch den Rayleigh-Quotienten wird also jeder Eigenvektor von $A$ auf den dazugeh√∂rigen Eigenwert $\\lambda$ abgebildet. Diese Eigenschaft wird unter anderem in der numerischen Berechnung von Eigenwerten benutzt.\n",
        "\n",
        "Insbesondere gilt f√ºr eine symmetrische oder hermitesche Matrix $A$ mit dem kleinsten Eigenwert $\\lambda _{{{\\rm {min}}}}$ und dem gr√∂√üten Eigenwert $\\lambda _{{{\\rm {max}}}}$ nach dem [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) (Der Satz von Courant-Fischer stellt die Eigenwerte einer symmetrischen oder hermiteschen Matrix als minimale beziehungsweise maximale Rayleigh-Quotienten):\n",
        "\n",
        "> $\\lambda _{{{\\rm {min}}}}\\leq R_{A}(x)\\leq \\lambda _{{{\\rm {max}}}}$\n",
        "\n",
        "Die Berechnung des kleinsten bzw. gr√∂√üten Eigenwerts ist damit √§quivalent zum Auffinden des Minimums bzw. Maximums des Rayleigh-Quotienten. Das l√§sst sich unter geeigneten Voraussetzungen auch noch auf den unendlichdimensionalen Fall verallgemeinern und ist als Rayleigh-Ritz-Prinzip bekannt.\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem Ellipsoid (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "q7aw0uQL3AnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kernel*"
      ],
      "metadata": {
        "id": "YfOrrAOFvF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel = Nullspace**\n",
        "\n",
        "The [kernel of this linear map (linear algebra)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) is the set of solutions to the equation Ax = 0, where 0 is understood as the zero vector."
      ],
      "metadata": {
        "id": "nHdImLnovH1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In algebra, the [kernel (algebra)](https://en.m.wikipedia.org/wiki/Kernel_(algebra)) of a homomorphism (function that preserves the structure) is generally the inverse image of 0.\n",
        "\n",
        "* The kernel of a homomorphism is reduced to 0 (or 1) if and only if the homomorphism is injective, that is if the inverse image of every element consists of a single element (jedes element im ziel hat nur ein element im ursprung, es kann aber mehr elemente im ziel ohne ein element im ursprung geben).\n",
        "\n",
        "* This means that the kernel can be viewed as a measure of the degree to which the homomorphism fails to be injective.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/320px-KerIm_2015Joz_L2.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "hLbt81t-PVPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "der Kern einer linearen Abbildung f genau dann nichttrivial ist, wenn eine linear unabh√§ngige Menge (bzw. genauer Familie) S existiert sodass f(S) linear abh√§ngig ist. Durch √úbergang zu den Negationen erhalten wir dann die √§quivalente Aussage:\n",
        "\n",
        "- kerf = {0}\n",
        "- ...genau dann, wenn gilt...\n",
        "- f√ºr alle linear unabh√§ngigen S ist auch f(S) linear unabh√§ngig.\n",
        "\n",
        "https://www.youtube.com/watch?v=GNf3StvaiFA&list=WL&index=9"
      ],
      "metadata": {
        "id": "UbcK-n4ZJ5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Diagonalisierbarkeit*"
      ],
      "metadata": {
        "id": "-wUJznaApPv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-5FB4sQ_XI0"
      },
      "source": [
        "**[Diagonalisierbarkeit](https://en.m.wikipedia.org/wiki/Diagonalizable_matrix) von Matrizen**:\n",
        "\n",
        "* **Eigenwerte liegen auf der Diagonalen**. Dabei: Spur = Summer aller Eigenwerte. Determinante = Produkt aller Eigenwerte = 0.\n",
        "\n",
        "* Square matrix $A$ into **invertible matrix** $P$ and **diagonal matrix** $D$ such that  ${\\displaystyle P^{-1}AP=D}$. Approach: [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix).\n",
        "\n",
        "> $P^{-1} A P=\\left[\\begin{array}{cccc}\n",
        "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right]$\n",
        "\n",
        "* Ein rotierender K√∂rper ohne √§u√üere Kr√§fte verbleibt in seiner Bewegung, wenn er um seine Symmetrieachse rotiert. Wenn eine Basis aus Eigenvektoren existiert, so ist die Darstellungsmatrix bez√ºglich dieser Basis eine Diagonalmatrix\n",
        "\n",
        "* *The diagonalization of a symmetric matrix can be interpreted as a rotation of the axes to align them with the eigenvectors:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/4/4e/Diagonalization_as_rotation.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Determinant (Permanent, Immanant)*"
      ],
      "metadata": {
        "id": "yIoPrdtMpRd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Determinante](https://de.m.wikipedia.org/wiki/Determinante)**:\n",
        "\n",
        "* [Determinante](https://de.m.wikipedia.org/wiki/Determinante) (nur fur quadratischen Matrix - bei $n \\cdot m$ Matrizen nutzt man zB SVD)\n",
        "\n",
        "  * gibt an, wie sich Fl√§che bzw. Volumen durch linearen Abbildung √§ndert. det(A) = 4 $\\rightarrow$ Matrix vervierfacht Fl√§cheninhalt.\n",
        "\n",
        "  * gibt an, ob lineares Gleichungssystem l√∂sbar ist (L√∂sung dann mit der Cramerschen Regel)\n",
        "\n",
        "  * Determinante ist Produkt aller Eigenwerte: $\\prod_{i=1}^{n} \\lambda_{i}=\\operatorname{det}(A) = 0$\n",
        "\n",
        "* Berechnung: $\\operatorname{det}(A)$: $\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right)\n",
        "$ $\\rightarrow$ $\n",
        "\\operatorname{det} A=\\left|\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$. *Die 2x2-Determinante **ist gleich dem orientierten Fl√§cheninhalt** des von ihren Spaltenvektoren aufgespannten Parallelogramms:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Area_parallellogram_as_determinant.svg/220px-Area_parallellogram_as_determinant.svg.png)\n",
        "\n",
        "* [Entwicklungssatz](https://de.m.wikipedia.org/wiki/Determinante#Laplacescher_Entwicklungssatz) oder  [Leibniz-Formel](https://de.m.wikipedia.org/wiki/Determinante#Leibniz-Formel) (geschlossene Form, theoretisch). [Gau√ü-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren) in Dreiecksform - Determinante ist Produkt der [Hauptdiagonale](https://de.m.wikipedia.org/wiki/Hauptdiagonale). Computeralgorithmus: [LU-Zerlegung](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren#LR-Zerlegung).\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) (=has the eigenvalues as roots, and it has the determinant and the trace / sum o fEigenvalues of the matrix among its coefficients):\n",
        "\n",
        "> <font color=\"red\">$f(\\lambda) = \\operatorname{det}(A) = 0$</font> $\\quad (= \\prod_{i=1}^{n} \\lambda_{i})$\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, <font color=\"red\">**the solutions of $f(\\lambda) = 0$** (determinant is equal to zero, Eigenvalues are \"coefficients in characteristic polynomial\" (it's trace to be precise))</font> are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "J1Ir4QyrBark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Permanent_(mathematics)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Immanant"
      ],
      "metadata": {
        "id": "U3D3fAGop0ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Characteristics polynomial (Fundamental theorem of algebra)*"
      ],
      "metadata": {
        "id": "xemut5KHpjBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Characteristics polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) = has the Eigenvalues as [roots](https://en.m.wikipedia.org/wiki/Zero_of_a_function) (zero of a function)\n",
        "\n",
        "Root (Zero of a function): The function f attains the value of 0 at x, or equivalently, x is the solution to the equation f(x) = 0. A root of a polynomial is a zero of the corresponding polynomial function. The [fundamental theorem of algebra](https://en.m.wikipedia.org/wiki/Fundamental_theorem_of_algebra) shows that any non-zero polynomial has a number of roots at most equal to its degree.\n",
        "\n",
        "For example, the polynomial f of degree two, defined by $f(x)=x^{2}-5x+6$ has the two roots (or zeros) that are $\\lambda_1=2$ and $\\lambda_2=3$:\n",
        "\n",
        "> ${\\displaystyle f(2)=2^{2}-5\\times 2+6=0{\\text{ and }}f(3)=3^{2}-5\\times 3+6=0.}$\n",
        "\n",
        "<font color=\"red\">Zero of a function is important because every equation in the unknown x may be rewritten as $f(x)=0$ by regrouping all the terms in the left-hand side. Hence the study of zeros of functions is exactly the same as the study of solutions of equations.</font>\n",
        "\n",
        "Siehe [Nullstellen](https://de.m.wikipedia.org/wiki/Nullstelle) sind bei einer Funktion diejenigen Werte der Ausgangsmenge (des Definitionsbereichs D), bei denen das im Rahmen der Abbildung zugeordnete Element der Zielmenge (des Wertebereichs W) die Null ist (${\\displaystyle 0\\in W}$). Nullstellen von Polynomfunktionen werden auch als Wurzeln bezeichnet.\n",
        "\n",
        "[Kern](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) is L√∂sungsmenge der [homogenen linearen Gleichung](https://de.m.wikipedia.org/wiki/Lineare_Gleichung) f(x)=0 und wird hier auch Nullraum genannt (denjenigen Vektoren in V, die auf den Nullvektor in W abgebildet werden)\n",
        "\n",
        "Example: A graph of the function $\\cos (x)$ for $x$ in $[-2 \\pi, 2 \\pi]$, with zeros at $-\\frac{3 \\pi}{2},-\\frac{\\pi}{2}, \\frac{\\pi}{2}$, and $\\frac{3 \\pi}{2}$, marked in red.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/X-intercepts.svg/480px-X-intercepts.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "0jYqsd3QptJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Invertible Matrix*"
      ],
      "metadata": {
        "id": "cLeG1PswpSaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Invertible Matrix**\n",
        "\n",
        "* ($B^{-1}$ existiert nicht (Matrix nicht invertierbar) $\\rightarrow$ dann existieren Eigenwerte)\n",
        "\n",
        "* [Invertible matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix) is used in methods to solve systems of linear equations and **help to get Eigenvalues**\n",
        "\n",
        "> Apply Eigendecomposition to matrix (or other method) $\\rightarrow$ Get matrix inverse (with Eigenvectors and a diagonal with Eigenvalues) $\\rightarrow$ Solve systems of linear equations\n",
        "\n",
        "* You can also use pseudo-inverse [Moore-Penrose](https://en.m.wikipedia.org/wiki/Moore‚ÄìPenrose_inverse) with [matrix solution](https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution) to solve systems of linear equations or in [curve fitting](https://de.wikipedia.org/wiki/Ausgleichungsrechnung) (like regression or ML). Methods: QR, Cholesky, Rank decomposition, SVD. 'The pseudoinverse provides a [Linear Least Squares](https://en.m.wikipedia.org/wiki/Linear_least_squares) solution to a system of linear equations.'\n",
        "\n",
        "  * Exkurs: [Linear least squares (LLS)](https://en.wikipedia.org/wiki/Linear_least_squares) for solving systems of linear equations: is the least squares approximation of linear functions to data (linear regression). **Numerical methods include inverting the matrix of the normal equations and orthogonal decomposition methods**.\n",
        "\n",
        "  * **Overdetermined case**: A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution. See under [Applications](https://en.m.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Applications)\n",
        "\n",
        "  * **Underdetermined case**: Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. [Underdetermined system](https://en.m.wikipedia.org/wiki/Underdetermined_system): there are **fewer equations than unknowns**. Has an infinite number of solutions, if any. In optimization problems that are subject to linear equality constraints, only one of the solutions is relevant, namely the one giving the **highest or lowest value of an objective function**. The use of the **(Moore Pensore) pseudoinverse is to find the minimum (Euclidean) norm solution** to a system of linear equations with multiple solutions. Can be computed using the singular value decomposition. *Example: The solution set for two equations in three variables is, in general, a line ([Source](https://en.m.wikipedia.org/wiki/System_of_linear_equations#General_behavior)):*\n",
        "\n",
        "  ![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Intersecting_Planes_2.svg/240px-Intersecting_Planes_2.svg.png)\n",
        "\n",
        "* [Methods to compute inverse of matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion): Gaussian elimination, Newton's method, Cayley‚ÄìHamilton method, Eigendecomposition, Cholesky decomposition, Analytic solution (Cramer's rule), Blockwise inversion.\n",
        "\n",
        "* Example: If matrix A can be [eigendecomposed](https://en.m.wikipedia.org/wiki/Invertible_matrix), and if none of its eigenvalues are zero, then A is invertible and its inverse is given by\n",
        "\n",
        ">${\\displaystyle \\mathbf {A} ^{-1}=\\mathbf {Q} \\mathbf {\\Lambda } ^{-1}\\mathbf {Q} ^{-1}}$\n",
        "\n",
        "* where $\\mathbf {Q}$  is the square ($N√óN$) matrix whose i-th column is the **eigenvector** $q_{i}$ of $\\mathbf {A}$\n",
        "* ${\\displaystyle \\mathbf {\\Lambda } }$ is the diagonal matrix whose diagonal elements are the corresponding **eigenvalues** ${\\displaystyle \\Lambda _{ii}=\\lambda _{i}}$."
      ],
      "metadata": {
        "id": "49Uc9N-uV6_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rank*"
      ],
      "metadata": {
        "id": "919bw_THpTYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a matrix**\n",
        "\n",
        "\n",
        "The rank of a matrix is the number of linearly independent vectors (columns).\n",
        "\n",
        "So **low rank is a low number** of linearly independent vectors. A low rank matrix can be this:\n",
        "\n",
        "1\n",
        "\n",
        "2\n",
        "\n",
        "3\n",
        "\n",
        "\n",
        "* a low rank matrix (whether approximation or not) is simply a matrix for which the number of linearly independent row or columns is much smaller than the actual number of rows or columns.\n",
        "\n",
        "* Viewed as a linear transformation, the span of its range is small or the span of its null space is large.\n",
        "\n",
        "* Quantum computers give exponential speedups for high rank matrices"
      ],
      "metadata": {
        "id": "t9uYK8aS8akn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Sparse Matrix vs Low Rank Matrix**\n",
        "\n",
        "Sparse matrices and low-rank matrices are two very different types of objects. The matrix\n",
        "\n",
        "$\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4\\end{array}\\right]$\n",
        "\n",
        "\n",
        "is sparse (meaning it has a lot of zero entries) but not low-rank (as a matter of fact it‚Äôs full rank). On the other hand, the matrix\n",
        "\n",
        "$\\left[\\begin{array}{cccc}1 & 2 & 3 & 4 \\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 9 & 12 \\\\ 4 & 8 & 12 & 16\\end{array}\\right]$\n",
        "\n",
        "is low-rank but not sparse!\n",
        "\n",
        "There's however a connection to be made between the two concepts: **a low-rank matrix has a sparse set of singular values**.\n",
        "\n",
        "Take the following singular value decomposition for a general matrix $\\mathrm{X}$\n",
        "\n",
        "> $\n",
        "\\mathrm{X}=\\mathrm{USV}^T\n",
        "$\n",
        "\n",
        "Then the number of non-zero entries in (the diagonal of) $S$ is precisely equal to the rank of $\\mathbf{X}$. Thus, a sparse $\\mathbf{S}$ leads to a low-rank $\\mathbf{X}$ and vice-versa.\n"
      ],
      "metadata": {
        "id": "4eqnSMcl83mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Algebraic and geometric multiplicity of eigenvalues*"
      ],
      "metadata": {
        "id": "h0hcBLukpUcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic and geometric multiplicity of eigenvalues**\n",
        "\n",
        "> Two or more distinct eigenvalues = algebraic multiplicity\n",
        "\n",
        "> By how many linearly independent vectors is the Eigenspace of $\\lambda_1$ (which is an Eigenvalue with multiplicity 1 or more) formed? = geometric multiplicity\n",
        "\n",
        "> Geometric multiplicity is max equal or less than its algebraic multiplicity\n",
        "\n",
        "> When the geometric multiplicity of a repeated eigenvalue is **strictly less** than its algebraic multiplicity, then that eigenvalue is said to be **defective**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multiplicity_(mathematics)#Multiplicity_of_a_root_of_a_polynomial\n",
        "\n",
        "*algebraic multiplicity*\n",
        "\n",
        "The **algebraic multiplicity** of an eigenvalue is the number of times it appears as a root of the characteristic polynomial (i.e., the polynomial whose roots are the eigenvalues of a matrix).\n",
        "\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its characteristic polynomial is:\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, the solutions of $f(\\lambda) = 0$  are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Thus, $A$ has one repeated eigenvalue whose algebraic multiplicity is\n",
        "\n",
        ">$\n",
        "\\mu\\left(\\lambda_1\\right)=\\mu\\left(\\lambda_2\\right)=2\n",
        "$\n",
        "\n",
        "*geometric multiplicity*\n",
        "\n",
        "The **geometric multiplicity** of an eigenvalue is the dimension of the linear space of its associated eigenvectors (i.e., its eigenspace).\n",
        "\n",
        "If the Eigenspace of $\\lambda_1$ is generated only by a single vector, it has dimension 1. As a consequence, the geometric multiplicity of $\\lambda_1$ is 1, less than its algebraic multiplicity, which is equal to 2.\n",
        "\n",
        "See complete example in [this pdf document](https://raw.githubusercontent.com/deltorobarba/repo/master/multiplicity.pdf)."
      ],
      "metadata": {
        "id": "VR06jupDflnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Condition Numbers*"
      ],
      "metadata": {
        "id": "0p0KFlAypVZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Condition Numbers**\n",
        "\n",
        "https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/"
      ],
      "metadata": {
        "id": "0JY7J0768m4Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwJ8QbNvEbX"
      },
      "source": [
        "###### *Get Eigenvalues: **Algorithms** (Power & Inverse Iteration, Charakteristisches Polynom) & **Matrix Decomposition** (Eigendecomposition/Spectrum, Schur, SVD)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenvalue algorithm](https://en.m.wikipedia.org/wiki/Eigenvalue_algorithm) - matrices are diagonalized numerically using computer software. See [List of Eigenvalue algorithms](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Eigenvalue_algorithms)."
      ],
      "metadata": {
        "id": "cjzaJcIsQrtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbDi3hDN8p4"
      },
      "source": [
        "**Small Matrices: Ermittlung der Eigenwerte der Matrix $A$ mit Determinante und charakteristischem Polynom**\n",
        "\n",
        "In practice, eigenvalues of large matrices are not computed using the characteristic polynomial [Source](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Numerical_computations)\n",
        "\n",
        "> $A=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)$\n",
        "\n",
        "**Step 1: Bilde mit der Einheitsmatrix $E_{n}$ die Matrix $\\left(A-\\lambda E_{n}\\right)$**\n",
        "\n",
        "> $\\left(A-\\lambda E_{n}\\right)=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)-\\left(\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{array}\\right)=\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "**Step 2: Berechne die Determinante von $\\operatorname{det}\\left(A-\\lambda E_{n}\\right) = \\chi_{A}(\\lambda)$ $\\rightarrow$ [charakteristisches Polynom](https://de.m.wikipedia.org/wiki/Charakteristisches_Polynom)**\n",
        "\n",
        "> $\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=\\operatorname{det}\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "$=(2-\\lambda)^{2} \\cdot(3-\\lambda)+2+2-2 \\cdot(2-\\lambda)-2 \\cdot(2-\\lambda)-(3-\\lambda)$\n",
        "$=-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5$ $\\quad$ (= Polynom)\n",
        "\n",
        "**Step 3: Bestimme die Nullstellen des charakteristischen Polynoms, weil das sind die Eigenwerte der Matrix $A$ (und Determinante wird Null)**\n",
        "\n",
        "> $(A-\\lambda E_{n}) \\cdot v=0$\n",
        "\n",
        "* Durch Ausprobieren: erste Nullstelle $\\lambda_{1}=1$.\n",
        "\n",
        "* Klammern wir dann den Faktor $(\\lambda-1)$ aus, erhalten wir:\n",
        "$-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5=(\\lambda-1) \\cdot\\left(-\\lambda^{2}+6 \\lambda-5\\right)\n",
        "$.\n",
        "\n",
        "* Anwendung der [Mitternachtsformel](https://de.m.wikipedia.org/wiki/Quadratische_Gleichung#L√∂sungsformel_f√ºr_die_allgemeine_quadratische_Gleichung_(a-b-c-Formel)): $\n",
        "\\lambda_{2,3}=\\frac{-6 \\pm \\sqrt{36-20}}{-2}=3 \\mp 2$\n",
        "\n",
        "* Somit lauten die drei Eigenwerte der Matrix $\\lambda_{1}=\\lambda_{2}=1$ sowie $\\lambda_{3}=5$\n",
        "\n",
        "*Gibt es eine Zahl $\\lambda$ und einen Vektor $v$, sodass dieser durch Multiplikation mit der Matrix $\\left(A-\\lambda E_{n}\\right)$ auf den Nullvektor abgebildet wird, so ist diese Matrix nicht von vollem Rang. Das bedeutet, dass ihre Determinante Null ist. Vektoren wie $v$ und $w$ sind linear abh√§ngig.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooWX-rgS12w"
      },
      "source": [
        "**Eigendecomposition (spectral decomposition)**\n",
        "\n",
        "* [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or sometimes spectral decomposition, see [spectral theorem](https://en.m.wikipedia.org/wiki/Spectral_theorem), is the factorization of a matrix into a canonical form (Normalform) - the matrix is represented in terms of its eigenvalues and eigenvectors. Matrix must be diagonalizable. See also [Summary of Eigendecomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition#Decompositions_based_on_eigenvalues_and_related_concepts).\n",
        "\n",
        "> ${\\displaystyle A=VDV^{-1}}A=VDV^{{-1}}$\n",
        "\n",
        "* $D$ = eigenvalues of $A$ (diagonal)\n",
        "* $V$ = eigenvectors of $A$\n",
        "\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur decomposition**\n",
        "\n",
        "* [Schur decomposition](https://en.m.wikipedia.org/wiki/Schur_decomposition) is a [matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition).\n",
        "\n",
        "* Take complex square matrix and **get an upper triangular matrix whose diagonal elements are the eigenvalues** of the original matrix."
      ],
      "metadata": {
        "id": "wqlosIz9egUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singular Value Decomposition**\n",
        "\n",
        "* [Singular Value Decomposition](https://de.m.wikipedia.org/wiki/Singul√§rwertzerlegung) is used in calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.\n",
        "\n",
        "* SVD can also be used in least squares linear regression, image compression, and denoising data. Siehe auch [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm)\n"
      ],
      "metadata": {
        "id": "lCrTpBOyevh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Decomposition** (Gaussian Elimination, Cholesky, QR, LU)*"
      ],
      "metadata": {
        "id": "0ChC-FyHZ41O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical: [Gaussian Elimination](https://en.m.wikipedia.org/wiki/Gaussian_elimination) (row reduction). See [List of numerical algorithms to solve systems of linear equation](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Solving_systems_of_linear_equations)"
      ],
      "metadata": {
        "id": "KzcuDJOLhCGd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8EITyoKDXA"
      },
      "source": [
        "**Cholesky Decomposition** (Hermitian / squared)\n",
        "\n",
        "* **alternative to Eigendecomposition** to get matrix inverse for solving linear equations\n",
        "\n",
        "* [Cholesky decomposition](https://en.m.wikipedia.org/wiki/Cholesky_decomposition) of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,\n",
        "\n",
        "* useful for efficient numerical solutions, e.g., Monte Carlo simulations (solving linear least squares for linear regression, as well as simulation and optimization methods)\n",
        "\n",
        "* When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.\n",
        "\n",
        "> $A = LL^T$\n",
        "\n",
        "* $L$ is the lower triangular matrix and $L^T$ is the transpose of L. Decompose as the product of upper triangular matrix $U$ is also possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NVXqSfFlBt"
      },
      "source": [
        "**QR Decomposition**\n",
        "\n",
        "* We can use QR decomposition to [find the determinant of a square matrix](https://en.m.wikipedia.org/wiki/QR_decomposition#Connection_to_a_determinant_or_a_product_of_eigenvalues)\n",
        "\n",
        "* The [QR decomposition](https://en.m.wikipedia.org/wiki/QR_decomposition) is for m x n matrices (not limited to square matrices) and decomposes a matrix into $Q$ (orthogonal $Q^T Q = I$ or unitary $Q \\cdot Q = I$, size m x m) and $R$ (upper triangle matrix with the size m x n) components.\n",
        "\n",
        "> $A = Q R$\n",
        "\n",
        "* Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, **although is <u>not</u> limited to square matrices**.\n",
        "\n",
        "* There are several methods for actually computing the QR decomposition, such as by [Householder transformations](https://de.m.wikipedia.org/wiki/Householdertransformation), [Givens rotations](https://de.m.wikipedia.org/wiki/Givens-Rotation) and  [Gram-Schmidtsch's orthogonalization method](https://de.m.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDrmirVPSXe"
      },
      "source": [
        "**LU Decomposition**\n",
        "\n",
        "* The [LU decomposition](https://en.m.wikipedia.org/wiki/LU_decomposition) is often used to simplify the **solving of systems of linear equations**, such as **finding the coefficients in a linear regression**, as well as in **calculating the determinant and inverse** of a matrix.\n",
        "\n",
        "* Lower‚Äìupper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix.\n",
        "\n",
        "* The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian elimination.\n",
        "\n",
        "* Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix.\n",
        "\n",
        "The **LU decomposition is for square matrices** and decomposes a matrix into L and U components. Let A be a square matrix. An LU factorization refers to the factorization of A, with proper row and/or column orderings or permutations, into two factors ‚Äì a **lower triangular matrix L** and an **upper triangular matrix U**:\n",
        "\n",
        "> A = L U\n",
        "\n",
        "* The LU decomposition is found using an <u>iterative numerical process</u> and **can fail for those matrices that cannot be decomposed or decomposed easily**.\n",
        "\n",
        "* In the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero. For example, for a 3 √ó 3 matrix A, its LU decomposition looks like this:\n",
        "\n",
        "> $\\left[\\begin{array}{lll}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
        "u_{11} & u_{12} & u_{13} \\\\\n",
        "0 & u_{22} & u_{23} \\\\\n",
        "0 & 0 & u_{33}\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Type** (Squared & Non-Squared)*"
      ],
      "metadata": {
        "id": "PREcYcRRPEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Solving Systems of Linear Equations: <u>Squared Matrix</u> (for HHL) - Eigendecomposition to get Pseudo-inverse**\n",
        "\n",
        "> https://towardsdatascience.com/from-eigendecomposition-to-determinant-fundamental-mathematics-for-machine-learning-with-1b6b449a82c6\n",
        "\n",
        "\n",
        "<font color=\"red\">**If A is squared is a matrix (and has [full rank](https://de.m.wikipedia.org/wiki/Rang_(Mathematik))) in a linear system of equations: Getting the matrix inverse via Eigendecomposition (pseudoinverse Moore-Penrose provides a least squares solution to a system of linear equations.')**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "then you can take the [inverse](https://de.m.wikipedia.org/wiki/Inverse_Matrix) if A to solve for x:\n",
        "\n",
        "> $x = A^{-1} b$\n",
        "\n",
        "<font color=\"red\">*This part is important for HHL:*\n",
        "\n",
        "* $\\hat{A} = \\hat{A}^{\\dagger}$ $\\quad$ - Hermitian operators are [Self-adjoint operators](https://en.m.wikipedia.org/wiki/Self-adjoint_operator)\n",
        "\n",
        "* Adjungierte Matrix = transponiert + complex konjugiert (Vorzeichen umgekehrt). Hermetian: selbstadjunktiert = symmetrisch\n",
        "\n",
        "* Following from this, in bra-ket notation: $\n",
        "\\left\\langle\\phi_{i}|\\hat{A}| \\phi_{j}\\right\\rangle=\\left\\langle\\phi_{j}|\\hat{A}| \\phi_{i}\\right\\rangle^{*}\n",
        "$\n",
        "\n",
        "\n",
        "<font color=\"red\">*Since $A$ is Hermitian, it has a spectral decomposition : $\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$*\n",
        "\n",
        "<font color=\"red\">*You need the Eigendecomposition (spectral decomposition) to get the inverse of a matrix (=here unitary and hence normal)*\n",
        "\n",
        "Getting the [Matrix inverse via eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition): If a matrix $\\mathbf{A}$ can be eigendecomposed and if none of its eigenvalues are zero, then $\\mathbf{A}$ is invertible and its inverse is given by\n",
        "\n",
        ">$\n",
        "\\mathbf{A}^{-1}=\\mathbf{Q}^{-1} \\mathbf{\\Lambda}^{-1} \\mathbf{Q}\n",
        "$\n",
        "\n",
        "If $\\mathbf{A}$ is a symmetric matrix, since $\\mathbf{Q}$ is formed from the eigenvectors of $\\mathbf{A}, \\mathbf{Q}$ is guaranteed to be an orthogonal matrix, therefore $\\mathbf{Q}^{-1}=\\mathbf{Q}^{\\mathrm{T}}$. Furthermore, because $\\mathbf{\\Lambda}$ is a diagonal matrix, its inverse is easy to calculate:\n",
        "\n",
        ">$\n",
        "\\left[\\Lambda^{-1}\\right]_{i i}=\\frac{1}{\\lambda_{i}}\n",
        "$\n",
        "\n",
        "**Example - the matrix:**\n",
        "\n",
        "$A=\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]$\n",
        "\n",
        "has the eigenvectors:\n",
        "\n",
        "$\\mathbf{u}_{1}=\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{1}=4$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\mathbf{u}_{2}=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{2}=-1$\n",
        "\n",
        "We can verify (as illustrated in Figure 1) that only the length of $\\mathbf{u}_{1}$ and $\\mathbf{u}_{2}$ is changed when one of these two vectors is multiplied by the matrix $\\mathbf{A}$ :\n",
        "\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=4\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 8\\end{array}\\right]$\n",
        "\n",
        "and\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=-1\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{r}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "For most applications we normalize the eigenvectors (i.e. transform them such that their length is equal to one):\n",
        "\n",
        "$\n",
        "\\mathbf{u}^{\\top} \\mathbf{u}=1 \\text {. }\n",
        "$\n",
        "\n",
        "For the previous example we obtain:\n",
        "\n",
        "$\n",
        "\\mathbf{u}_{1}=\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Exkurs: wie man einen Vektor normiert:\n",
        "\n",
        "1) Betrag von $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ist gleich $\\sqrt{3^{2}+2^{2}}=3.6055$ Vektor normieren, also mit 1/Betrag malnehmen:\n",
        "\n",
        "2) $\n",
        "\\frac{1}{3.6055} \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right)= 0.27735 \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right) =\\left(\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\n",
        "We can check that:\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "3.3284 \\\\\n",
        "2.2188\n",
        "\\end{array}\\right]=4\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "and\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
        ".7071 \\\\\n",
        "-.7071\n",
        "\\end{array}\\right]=-1\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5z-Ez-u4RWhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Solving Systems of Linear Equations: <u>Non-Squared Matrix</u> - Singular value decomposition (SVD) to get Pseudo-inverse**\n",
        "\n",
        "<font color=\"red\">**If a matrix A is not squared: singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.**\n",
        "\n",
        "> $A x = b$ $\\quad$ ($A$ is not regular)\n",
        "\n",
        "You multiply both sides by the $A^{T}$, which is the transpose of A:\n",
        "\n",
        "> $A^{T} A x = A^{T} b$\n",
        "\n",
        "Then move $A^{T} A$ on right side (by taking their inverse). This is not the original x anymore, but the [least squares](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate#Lineare_Modellfunktion) $\\hat{x}$\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "And that term <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> is know as (Moore‚ÄìPenrose) pseudo-inverse <font color=\"blue\">$A^{+}$</font>:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$A^{+}$</font> $b$\n",
        "\n",
        "And a pseudo-inverse is nothing else than our least-squares solutions. See more details under [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)\n",
        "\n",
        "*In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.*\n",
        "\n",
        "* non squared matrix\n",
        "\n",
        "* to solve you need the inverse, but normal eigendecomposition doesn work\n",
        "\n",
        "> you need to compute mooore penrose pseudo-inverse, and here you need to apply singular value decomposition to get eigendecomposition\n",
        "\n",
        "* SVD is a type of [Matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition), specicially one based on eigenvalue concepts. and SVD is a full rank decomposition (besides broader [Rank factorization methods](https://en.m.wikipedia.org/wiki/Rank_factorization))\n",
        "\n",
        "* matrix decompositions in general are used to take a large matrix apart (i.e. factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U) so the new matrices require fewer additions and multiplications to solve, compared with the original system $A\\mathbf {x} =\\mathbf {b}$\n",
        "\n",
        "<font color=\"red\">**Example:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right] x=\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "* We can immediately see that matrix A is of rank 2 because the first two rows are multiples of each other (2 and -2 and -2 and 2), just the last row numbers are not multiples of each other (5 and 3).\n",
        "\n",
        "* Now we can apply the pseudo-inverse to find the least-squares solution:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "> $\\hat{x}=$ <font color=\"blue\">$\\left(\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]$</font>$\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "> $\\hat{x}=$$\\left[\\begin{array}{l}-4 \\\\ -2\\end{array}\\right]$\n",
        "\n",
        "\n",
        "*Inserting the least-squares $\\hat{x}$ into the original equation:*\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\left[\\begin{array}{l}-4 \\\\ -3\\end{array}\\right]$ = $\\left[\\begin{array}{c}2 \\cdot(-4)+(-2) \\cdot(-3) \\\\ (-2)\\cdot (-4)+2 \\cdot (-3) \\\\ 5\\cdot (-4)+3 \\cdot (-3)\\end{array}\\right]$ = $\\left[\\begin{array}{c}-2 \\\\ 2 \\\\ -29\\end{array}\\right]$ $\\approx$ $\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$"
      ],
      "metadata": {
        "id": "g9VGZpsMZff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Group Theory*"
      ],
      "metadata": {
        "id": "_OexAhQsM_ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kongruenzklassen, √Ñquivalenzklassen & Restklassen*"
      ],
      "metadata": {
        "id": "gS-QUpcFmXh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Group_theory"
      ],
      "metadata": {
        "id": "nCPfqdkENfK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kongruenzklassen, √Ñquivalenzklassen & Restklassen (Modulorechnung)**\n",
        "\n",
        "* **Kongruenz**: [Kongruenz (Zahlentheorie)](https://de.m.wikipedia.org/wiki/Kongruenz_(Zahlentheorie))) Die Kongruenz zwischen zwei ganzen Zahlen ist in Bezug auf einen Teiler definiert. **Der Teiler hei√üt in diesem Zusammenhang Modul.**\n",
        "  * Gegeben sei ein Modul $m \\in \\mathbb{N}$. Zwei ganze Zahlen $a$ und $b$ hei√üen kongruent modulo $m$, **wenn die Division von $a$ und $b$ durch $m$ den gleichen Rest $r$ l√§sst**.\n",
        "  * Kongruenz ist eine Art Erweiterung der Modulorechnung: **Modulorechnung**: 17 mod 3 = 2.\n",
        "  * **Kongruenz: 11 $\\equiv$ 17 mod 3 (weil bei beiden der Rest 2 ist - beide sind in der der gleichen Restklasse)**\n",
        "  * Man kann alternativ zur [**Restklassenermittlung**](https://de.m.wikipedia.org/wiki/Restklasse) auch sagen, dass zwei Zahlen kongruent sind (modulo der nat√ºrlichen Zahl n), wenn ihre Differenz durch n teilbar ist. Hier: 17 - 11 = 6, ist teilbar durch 3.\n",
        "\n",
        "* [**√Ñquivalenz und √Ñquivalenzklassen** (Congruence Classes)](https://de.m.wikipedia.org/wiki/√Ñquivalenzrelation). √Ñquivalenz: Objekte, die sich in einem bestimmten Zusammenhang gleichen, als gleichwertig bzw. √§quivalent angesehen. The result of the modulo operation is an equivalence class (√Ñquivalenzklassen). Any member of the class may be chosen as representative.\n",
        "\n",
        "  * Von besonderem Interesse sind jedoch solche √Ñquivalenzrelationen $\\equiv$ , deren Quotientenabbildung $\\mathrm{q}_{\\mathrm{z}}: A \\rightarrow A / \\equiv, a \\mapsto[a]_{\\equiv}$ **mit der Struktur auf $A$ vertr√§glich bzw. ein Homomorphismus ist**, weil dann die von $\\mathrm{q}_{=}$ erzeugte Struktur auf der [Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) $A / \\equiv$ von der gleichen Art ist wie die von $A$. **Eine solche √Ñquivalenzrelation $\\equiv$ nennt man eine Kongruenzrelation auf der strukturierten Menge $A$.** <font color=\"blue\">Die Quotientengruppe G/N ist homomorph zur Gruppe G.</font>\n",
        "\n",
        "* **[Restklassen](https://de.wikipedia.org/wiki/Restklasse)** sind die √Ñquivalenzklassen in der Kongruenzrelation. Eine Zahl a modulo einer Zahl m die Menge aller Zahlen, die bei Division durch m denselben Rest lassen wie a. See also [Modulo Operation](https://en.m.wikipedia.org/wiki/Modulo_operation) (Restklassenrechnung).\n",
        "\n",
        "* In der Gruppentheorie werden √Ñquivalenzklassen als [Nebenklassen (Cosets)](https://de.m.wikipedia.org/wiki/Gruppentheorie#Nebenklassen) bezeichnet."
      ],
      "metadata": {
        "id": "0a8DDz9Vbn1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Subgroups, Normal Subgroups & Cosets*"
      ],
      "metadata": {
        "id": "DxxVSdYniUxt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfs10URbe-RG"
      },
      "source": [
        "**Subgroups, Normal Subgroups & Cosets**\n",
        "\n",
        "* **Subgroups (Unterguppen):**\n",
        "\n",
        "  * Gegeben sei $\\mathbb{Z}$ die Gruppe der ganzen Zahlen mit der Addition als Gruppenoperation. Man kann fur diese Gruppe verschiedene (unendlich viele) **[Untergruppen](https://de.m.wikipedia.org/wiki/Untergruppe) (Subgroups)** bilden: 2$\\mathbb{Z}$, 3$\\mathbb{Z}$, 4$\\mathbb{Z}$, 5$\\mathbb{Z}$, 6$\\mathbb{Z}$, 7$\\mathbb{Z}$.\n",
        "\n",
        "  * Beispiel: Die ganzen Zahlen $\\mathbb {Z} $ sind bez√ºglich der Addition eine Untergruppe der rationalen Zahlen $\\mathbb {Q} $.\n",
        "\n",
        "  * Two standard subgroups / every group has at least 2 subgroups: the identity element {e} and the entire group G. These are technically **normal subgroups (Normalteiler)**.\n",
        "\n",
        "  * If a group has no other normal subgroups then these two, than it's called a **simple group**. A simple group does not have any factor (quotient) groups, but they are the building blocks of other groups.\n",
        "\n",
        "* **Normal Subgroups (Normalteiler)**:\n",
        "\n",
        "  * <font color=\"blue\">**Die √Ñquivalenzklasse mit dem Rest Null ist der Normalteiler**. Normalteiler sind spezielle Untergruppen, und ihre Bedeutung liegt vor allem darin, **dass sie genau die Kerne von Gruppenhomomorphismen sind** (=L√∂sungsmenge ist Null, also Null als Rest).</font> Normal Subgroups heissen auch \"invariant or self-conjugate subgroups\".\n",
        "\n",
        "  * Normal subgroups determine what kinds of homomorphisms are possible from a group $G$ to other groups $f : G -> H$.\n",
        "\n",
        "  * Trivial examples: Standard subgroups identity element {e} and the entire group G.\n",
        "\n",
        "  * Die Gruppe $\\mathbb{Z}$ ist **abelsch** (kommutativ) und somit ist jede Untergruppe auch ein **[Normalteiler](https://de.wikipedia.org/wiki/Normalteiler) bzw. [Normal Subgroup](https://en.wikipedia.org/wiki/Normal_subgroup)**.\n",
        "\n",
        "* **Nebenklassen (Cosets)**:\n",
        "\n",
        "  * Nebenklassen werden benutzt, um den [Satz von Lagrange](https://de.m.wikipedia.org/wiki/Satz_von_Lagrange) zu beweisen, um die Begriffe [Normal Subgroup (Normalteiler)](https://de.m.wikipedia.org/wiki/Normalteiler) und [Quotient Group (Faktorgruppe)](https://de.m.wikipedia.org/wiki/Faktorgruppe) zu erkl√§ren und um Gruppenoperationen zu studieren.\n",
        "\n",
        "  * In contrast to Subgroups, the Coset (Nebenklasse) are not closed under addition, have no inverse and don't contain the identity element\n",
        "\n",
        "  * Cosets are there to define how many (finite) possible subgroups exist in a group. There are **two types of cosets**: left cosets and right cosets.\n",
        "\n",
        "  * A subgroup $H$ of a group $G$ may be used to decompose the underlying set of $G$ into disjoint, equal-size subsets called [cosets](https://en.m.wikipedia.org/wiki/Coset)\n",
        "\n",
        "* <font color=\"blue\">**Example: Use Normal Subgroup 5$\\mathbb{Z}$ von $\\mathbb{Z}$ (integers mod 5) to divide a Group into Cosets. We get 5 sets of remainders (the congruence classes) which are the [Quotient Group (Faktorgruppe)](https://en.m.wikipedia.org/wiki/Quotient_group), a group with 5 elements: $\\mathbb{Z}$ mod 5 = {$\\overline{0}$, $\\overline{1}$, $\\overline{2}$, $\\overline{3}$, $\\overline{4}$}.**</font>\n",
        "\n",
        "    * 5$\\mathbb{Z}$ + Rest 0 = $\\overline{0} : \\{\\ldots-10,-5,0,5,10 \\ldots\\}$ **(Normal) Subgroup**, which is technically also a Coset 0+ 5 $\\mathbb{Z}$\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 1 = $\\overline{1} : \\{\\ldots-9,-4,1,6,11 \\ldots\\}$ **Coset 1+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 2 = $\\overline{2} : \\{\\ldots-8,-3,2,7,12 \\ldots\\}$ **Coset 2+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 3 = $\\overline{3} : \\{\\ldots-7,-2,3,8,13 \\ldots\\}$ **Coset 3+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 4 = $\\overline{4} : \\{\\ldots-6,-1,4,9,14 \\ldots\\}$ **Coset 4+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "* **Quotient Group (Faktorgruppe)**:\n",
        "\n",
        "  * Die Faktorgruppe oder Quotientengruppe wird mit $G / N$ bezeichnet und ist die Menge der Nebenklassen (Cosets). Aus einer Gruppe $G$ und jedem ihrer Normalteiler $N$ l√§sst sich eine Faktorgruppe $G/N$ bilden.\n",
        "\n",
        "    * Die Quotientengruppe unterteilt eine Menge in √Ñquivalenzklassen bzw. eine Gruppe in Restklassen / Nebenklassen. Diese Menge der Restklassen (√Ñquivalenzklassen) heisst **[Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) bzw. Faktormenge**.\n",
        "\n",
        "  * Diese [Quotientengruppen](https://de.wikipedia.org/wiki/Faktorgruppe) sind homomorphe Bilder von G, **und jedes homomorphe Bild von G ist zu einer solchen Quotientengruppe G/N isomorph**. See fundamental theorem on homomorphisms of groups as \"**Every homomorphic image of a group is isomorphic to a quotient group**\". [Source](https://en.m.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms)\n",
        "\n",
        "  * For finite groups you can find a chain of normal subgroups called a \"composition series\" which acts as a kind of 'prime factorization' of the group (1 ‚óÉ N1, ‚óÉ N2 ‚óÉ ... ‚óÉ Nr ‚óÉ G ). Normal subgroups can also be used to study fields (K√∂rper), i.e. in Galois theory (Field extension K / F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Centralizer and Normalizer*"
      ],
      "metadata": {
        "id": "iEPXBs9Yx37x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Centralizer_and_normalizer\n",
        "\n",
        "In group theory, a branch of mathematics, the normalizer of a subset of a group is a certain associated subgroup. \n",
        "\n",
        "Let's suppose we have a group G and a subset S of G. The normalizer of S in G is the set of all elements in G that commute with S in a certain sense. More specifically, the normalizer is the largest subgroup of G in which S is a subset that commutes with each element of the subgroup under the group operation of conjugation.\n",
        "\n",
        "Formally, the normalizer of S in G, denoted by N_G(S), is defined as:\n",
        "\n",
        "$N_G(S)$ = $g$ in $G$ : $gSg^{(-1)} = S$\n",
        "\n",
        "Here, $gSg^{(-1)})$ = S means that for every element s in S, there exists some s' in S such that gsg^(-1) = s'. In words, the normalizer of S in G consists of all elements g of G such that when any element of S is conjugated by g (i.e., you multiply it by g on the left and by the inverse of g on the right), the result is still an element of S.\n",
        "\n",
        "In simple terms, the normalizer of a subset S of a group G is the largest subgroup of G in which S \"behaves nicely\", in the sense that the action of conjugation by elements of the subgroup leaves S unchanged. The concept of the normalizer is particularly important in the study of the structure and symmetries of groups.\n"
      ],
      "metadata": {
        "id": "GvmH89qHx5_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizer: conjugating with elements of the group results still in same group**\n",
        "\n",
        "Sure, let's consider a simple example with a finite group. Let's take the symmetric group S_3 (the group of all permutations of three elements), which has six elements: {e, (12), (13), (23), (123), (132)} where e is the identity permutation, (12) swaps elements 1 and 2, etc.\n",
        "\n",
        "Now let's consider the subset S = {(12), e}. \n",
        "\n",
        "We want to find the normalizer of this set. **By the definition of the normalizer, it consists of all elements g in S_3 such that when any element of S is conjugated by g, the result is still an element of S**. So we need to find all elements g in S_3 such that g(12)g^(-1) is in S and g(e)g^(-1) is in S. \n",
        "\n",
        "Well, conjugating the identity e by any element gives e back, so that part is trivial. \n",
        "\n",
        "So let's look at the conjugation of (12) by each element of S_3:\n",
        "\n",
        "- Conjugating by e gives (12), which is in S.\n",
        "- Conjugating by (12) gives (12), which is in S.\n",
        "- Conjugating by (13) gives (23), which is not in S.\n",
        "- Conjugating by (23) gives (13), which is not in S.\n",
        "- Conjugating by (123) gives (13), which is not in S.\n",
        "- Conjugating by (132) gives (23), which is not in S.\n",
        "\n",
        "So, the normalizer of S in S_3 is {e, (12)}. This is the largest subgroup of S_3 in which S is a subset that commutes with each element of the subgroup under the operation of conjugation."
      ],
      "metadata": {
        "id": "3OEGqwka1zGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Group Generators*"
      ],
      "metadata": {
        "id": "bTmTARGQiRbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generators of a group*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0942.jpg)\n",
        "\n",
        "> **Each group SU(3) x SU(2) x U(1) leads to a symmetry resulting in a conservation law**"
      ],
      "metadata": {
        "id": "Pp3Z0FI7ipEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Group Operations*"
      ],
      "metadata": {
        "id": "-6UGcA85iMH0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVfPVtLVGzQ"
      },
      "source": [
        "**Zusammenfassung algebraischer Strukturen mit Operationen**\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ is a Group\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ is a Ring\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ $√∑$ $\\quad$ is a Field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvvUdIJw3cu"
      },
      "source": [
        "A group is a set $G$ with an operation $*$ such that\n",
        "\n",
        "1. **Closure** [Abgeschlossenheit](https://de.m.wikipedia.org/wiki/Abgeschlossenheit_(algebraische_Struktur)): If $x$ and $y$ are in $G$ then $x * y$ is in $\\mathrm{G}$\n",
        "  * until here it is a [Magma (Groupoid)](https://de.m.wikipedia.org/wiki/Magma_(Mathematik))\n",
        "  * Beispiele (Magmen, die keine Halbgruppen sind):\n",
        "    * $(\\mathbb{Z},-):$ die ganzen Zahlen mit der Subtraktion\n",
        "    * (R $\\backslash\\{0\\}, /)$ : die reellen Zahlen ungleich 0 mit der Division\n",
        "    * Die nat√ºrlichen Zahlen mit der Exponentiation, also mit der Verkn√ºpfung $a * b=a^{b}$\n",
        "    * Die reellen Zahlen mit der Bildung des arithmetischen Mittels als Verkn√ºpfung\n",
        "\n",
        "2. **Associativity** [Assoziativit√§t](https://de.m.wikipedia.org/wiki/Assoziativgesetz): For all $x, y, z$ in $G$ $\\text { we have }(x * y) * z=x *(y * z)$\n",
        "  * until here it is a [Halbgruppe](https://de.m.wikipedia.org/wiki/Halbgruppe)\n",
        "  * Beispiel: Die Menge $\\mathbb  N$ $_0$ = {0, 1, 2 ..} der nat√ºrlichen Zahlen bildet mit der gew√∂hnlichen Addition eine kommutative und k√ºrzbare Halbgruppe ($\\mathbb  N$ $_0$,+), die keine Gruppe ist. Da hier die negativen Zahlen fehlen, also die ‚ÄûH√§lfte‚Äú der abelschen Gruppe ($\\mathbb Z,+$) der ganzen Zahlen, lag der Name Halbgruppe f√ºr diese mathematische Struktur nahe.\n",
        "\n",
        "3. **Identity Element** [Neutrales Element](https://en.m.wikipedia.org/wiki/Identity_element): There is an element $e$ in $G$ such that $e * x=x * e=x$ for all $x$ in $G$\n",
        "  * until here it is a [Monoid](https://de.m.wikipedia.org/wiki/Monoid)\n",
        "  * Beispiel: die nat√ºrlichen Zahlen mit der Addition und der Zahl 0 als neutralem Element.\n",
        "\n",
        "4. **Inverse Elements** [Inverse Elemente](https://de.m.wikipedia.org/wiki/Inverses_Element): For each element $x$ in $G,$ there is an element $x^{-1}$ such that $x * x^{-1}=x^{-1} * x=e$\n",
        "  * until here it is a [Group](https://de.m.wikipedia.org/wiki/Gruppe_(Mathematik))\n",
        "  * Beispiel: die Menge der ganzen Zahlen zusammen mit der Addition\n",
        "  * Ringe, K√∂rper (Field), Moduln und Vektorr√§ume sind Gruppen mit zus√§tzlichen Strukturen und Eigenschaften\n",
        "\n",
        "5. **Kommutative** [Kommutativit√§t](https://de.m.wikipedia.org/wiki/Kommutativgesetz): F√ºr alle $a, b \\in G$ gilt: $a * b=b * a$\n",
        "  * until here it is an [Abelian Group](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe)\n",
        "  * Beispiel:\n",
        "    * $(\\mathbb {Z} ,+)$ ist die wichtigste abelsche Gruppe. Dabei ist Z die Menge der ganzen Zahlen und + die gew√∂hnliche Addition.\n",
        "    * $(\\mathbb {Q}^{\\cdot} , \\cdot)$ ist eine abelsche Gruppe. Dabei ist $\\mathbb {Q}^{\\cdot}$ die Menge der rationalen Zahlen ohne die 0 und ‚ãÖ ist die gew√∂hnliche Multiplikation. Die Null muss hierbei ausgeschlossen werden, da sie kein inverses Element besitzt: ‚Äû1/0‚Äú ist nicht definiert\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "    * Die Menge der Drehungen in einer Ebene um einen Punkt bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Drehungen.\n",
        "    * Die Menge der Drehstreckungen in einer Ebene bilden eine abelsche Gruppe.\n",
        "    * Die Menge der endlichen Dezimalzahlen sind bez√ºglich der Multiplikation keine abelsche Gruppe. Zum Beispiel hat die Zahl 3 kein Inverses bez√ºglich der Multiplikation. $\\displaystyle {\\frac {1}{3}}$ l√§sst sich nicht als endlicher Dezimalbruch schreiben. Bez√ºglich der normalen Addition bilden die endlichen Dezimalbr√ºche eine abelsche Gruppe.\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "  * [Free Abelian Group](https://de.m.wikipedia.org/wiki/Freie_abelsche_Gruppe): eine abelsche Gruppe, die als $\\mathbb {Z}$-Modul eine Basis hat. Ist G eine freie abelsche Gruppe ist, so wird man eine Basis w√§hlen und alle Elemente als Linearkombinationen von Elementen dieser Basis ausdr√ºcken. Allerdings sollte man betonen, dass es meist keine ausgezeichnete Basis geben wird.\n",
        "\n",
        "6. **Distributive law** [Distributivgesetz](https://de.m.wikipedia.org/wiki/Distributivgesetz): a(b + c) = ab + ac.\n",
        "  * Gilt zus√§tzlich Distributivgesetz gilt f√ºr Ring (da man hier zwei Operationen ben√∂tigt). Gruppen haben immer nur eine Operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/groups.png)"
      ],
      "metadata": {
        "id": "y12kc1i6O7GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Group Theory**\n",
        "\n",
        "* Solutions to polynomial equations, like find 2 roots of a quadratic equation. there is also q cubic formula and a quartic for degree 4 polynomial. Find formula to solve degree 5 polynomial - group theory showed it doesnt exist. Has to do with permutation group S5.\n",
        "\n",
        "* Connection to Physics - Noether's theorem: Conservation law - symmetry. Momentum - translation in space. Energy - translation in time.\n",
        "\n",
        "> [Researchers Use Group Theory to Speed Up Algorithms ‚Äî Introduction to Groups](https://www.youtube.com/watch?v=KufsL2VgELo&list=WL&index=3&t=1579s)"
      ],
      "metadata": {
        "id": "wwGvCcMgGFMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Trivial Group*"
      ],
      "metadata": {
        "id": "0pnekZtsh-ON"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCaLVo_nP2aQ"
      },
      "source": [
        "**Trivial Group (Zero Group)**\n",
        "\n",
        "Die [triviale Gruppe](https://de.m.wikipedia.org/wiki/Triviale_Gruppe) ist in der Gruppentheorie eine Gruppe, deren Tr√§germenge genau ein Element enth√§lt. Die triviale Gruppe ist bis auf Isomorphie eindeutig bestimmt. **Jede Gruppe enth√§lt die triviale Gruppe als Untergruppe**.\n",
        "\n",
        "Die triviale Gruppe $(\\{e\\}, *)$ ist eine Gruppe, die aus der einelementigen Menge $\\{e\\}$ besteht und versehen ist mit der einzig m√∂glichen Gruppenoperation\n",
        "\n",
        "$\n",
        "e * e=e\n",
        "$\n",
        "\n",
        "Das Element $e$ ist damit das **neutrale Element** der Gruppe.\n",
        "\n",
        "Alle trivialen Gruppen sind zueinander isomorph. Beispiele f√ºr triviale Gruppen sind:\n",
        "\n",
        "* die zyklische Gruppe $C_{1}$ vom Grad 1\n",
        "\n",
        "* die alternierende Gruppe $A_{2}$ vom Grad 2\n",
        "\n",
        "* die symmetrische Gruppe $S_{1}$ einer einelementigen Menge\n",
        "\n",
        "*Eigenschaften trivialer Gruppen*:\n",
        "\n",
        "* Da die Gruppenoperation $\\ast$ kommutativ ist, ist die triviale Gruppe eine abelsche Gruppe.\n",
        "\n",
        "* Die einzige Untergruppe der trivialen Gruppe ist die triviale Gruppe selbst.\n",
        "\n",
        "* Die triviale Gruppe wird von der leeren Menge erzeugt:\n",
        "$\\{e\\}=\\langle \\emptyset \\rangle$ . Hierbei ergibt das leere Produkt nach √ºblicher Konvention das neutrale Element.\n",
        "\n",
        "* Jede Gruppe enth√§lt die triviale Gruppe und sich selbst als (triviale) Normalteiler. **Die triviale Gruppe wird daher meistens nicht als einfache Gruppe angesehen**, die aus genau 2 Normalteilern besteht).\n",
        "\n",
        "* In der Kategorie der Gruppen Grp fungiert die triviale Gruppe als Nullobjekt."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Simple Groups*"
      ],
      "metadata": {
        "id": "YW49URUwh8mn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVFg6eAKyF0"
      },
      "source": [
        "**Simple Group**\n",
        "\n",
        "* [Simple Groups](https://en.wikipedia.org/wiki/Simple_group) bzw. [einfache Gruppen](https://de.wikipedia.org/wiki/Einfache_Gruppe_(Mathematik)) **are the fundamental building blocks of finite groups** (just like Prime numbers are fundamental building blocks in number theory)\n",
        "\n",
        "* Just as you can factor integers into prime numbers, you can break apart some groups into a direct product of simpler groups.\n",
        "\n",
        "*  **a simple group is a <u>nontrivial</u> group whose only normal subgroups are the trivial group and the group itself.**\n",
        "\n",
        "* **Jede Gruppe hat sich selbst und die nur das neutrale Element enthaltende Menge als Normalteiler.**\n",
        "\n",
        "Damit stellt sich die Frage, welche Gruppen keine weitere Normalteiler besitzen. Bei diesen handelt es sich per definitionem gerade um die einfachen Gruppen.\n",
        "\n",
        "  * Eine Gruppe $G$ heisst einfach, falls sie als Normalteiler nur $G$ und $\\{e\\}$ mit dem neutralen Element $e$ hat.\n",
        "  * Au√üerdem wird zusƒÉtzlich $G \\neq\\{e\\}$ gefordert, wonach man knapper sagen kann:\n",
        "  * **Eine Gruppe hei√üt einfach, wenn sie genau zwei Normalteiler besitzt.**\n",
        "\n",
        "* A group that is not simple can be broken into two smaller groups, namely a nontrivial [normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup) and the corresponding quotient group. This process can be repeated, and for finite groups one eventually arrives at uniquely determined simple groups, by the [Jordan‚ÄìH√∂lder theorem (Composition series)](https://en.wikipedia.org/wiki/Composition_series).\n",
        "\n",
        "* The complete classification of finite simple groups, completed in 2004, is a major milestone in the history of mathematics.\n",
        "\n",
        "**Seit 1982 sind die endlichen einfachen Gruppen vollst√§ndig klassifiziert, die Liste besteht aus**\n",
        "\n",
        "* den zyklischen Gruppen von Primzahlordnung,\n",
        "\n",
        "* den alternierenden Gruppen $A_{n}$ mit $n\\geq 5$,\n",
        "\n",
        "* den Gruppen vom Lie-Typ (16 jeweils unendliche Serien)\n",
        "\n",
        "* 26 sporadischen Gruppen (Es handelt sich um die endlichen einfachen Gruppen, die sich nicht in eine der (18) systematischen Familien mit unendlich vielen Mitgliedern (von endlichen einfachen Gruppen) einordnen lassen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Finite Groups*"
      ],
      "metadata": {
        "id": "Qwm0Keqzh64L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSRFOXm2QgM"
      },
      "source": [
        "**Finite Groups**\n",
        "\n",
        "* Eine Gruppe ($G$,*) hei√üt [endliche Gruppe](https://de.m.wikipedia.org/wiki/Endliche_Gruppe), wenn $G$ eine endliche Menge ist, also eine endliche Anzahl von Elementen hat.\n",
        "\n",
        "* Die Annahme der Endlichkeit erm√∂glicht ein vereinfachtes Axiomensystem\n",
        "\n",
        "Ein Paar $(G, *)$ mit einer endlichen Menge $G$ und einer inneren zweistelligen Verkn√ºpfung $*: G \\times G \\rightarrow G$ hei√üt Gruppe, wenn folgende Axiome erf√ºllt sind:\n",
        "\n",
        "* Assoziativit√§t: F√ºr alle Gruppenelemente $a, b, c$ gilt $(a * b) * c=a *(b * c)$,\n",
        "\n",
        "* [K√ºrzungsregel](https://de.m.wikipedia.org/wiki/K√ºrzbarkeit): Aus $a * x=a * x^{\\prime}$ oder $x * a=x^{\\prime} * a$ folgt $x=x^{\\prime}$\n",
        "\n",
        "Aus der K√ºrzungsregel folgt, dass die Links- und Rechtsmultiplikationen $x \\mapsto a * x$ und $x \\mapsto x * a$\n",
        "injektiv sind, woraus wegen der Endlichkeit auch die Surjektivit√§t folgt. Daher gibt es ein $x$ mit\n",
        "$a * x=a,$ was zur Existenz des neutralen Elementes $e$ f√ºhrt, und dann ein $x$ mit $a * x=e$, was\n",
        "die Existenz der inversen Elemente zeigt.\n",
        "\n",
        "* The [List of small groups](https://en.wikipedia.org/wiki/List_of_small_groups) contains finite groups of small [order](https://en.wikipedia.org/wiki/Order_(group_theory)) [up to](https://en.wikipedia.org/wiki/Up_to) [group isomorphism](https://en.wikipedia.org/wiki/Group_isomorphism).\n",
        "\n",
        "* Die folgende Liste enth√§lt eine Auswahl [endlicher Gruppen kleiner Ordnung](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen).\n",
        "\n",
        "  * Diese Liste kann benutzt werden, um herauszufinden, zu welchen bekannten endlichen Gruppen eine Gruppe G isomorph ist.\n",
        "\n",
        "  * Als erstes bestimmt man die Ordnung von G und vergleicht sie mit den unten aufgelisteten Gruppen gleicher Ordnung.\n",
        "\n",
        "  * Ist bekannt, ob G abelsch (kommutativ) ist, so kann man einige Gruppen ausschlie√üen. Anschlie√üend vergleicht man die Ordnung einzelner Elemente von G mit den Elementen der aufgelisteten Gruppen, wodurch man G bis auf Isomorphie eindeutig bestimmen kann.\n",
        "\n",
        "In der nachfolgenden Liste werden folgende Bezeichnungen verwendet:\n",
        "\n",
        "- $\\mathbb{Z}_{n}$ ist die zyklische Gruppe der Ordnung $n$ (die auch als $C_{n}$ oder $\\mathbb{Z} / n \\mathbb{Z}$ geschrieben wird).\n",
        "\n",
        "- $D_{n}$ ist die Diedergruppe der Ordnung $2 n$.\n",
        "\n",
        "- $S_{n}$ ist die symmetrische Gruppe vom Grad $n$, mit $n !$ Permutationen von $n$ Elementen.\n",
        "\n",
        "- $A_{n}$ ist die alternierende Gruppe vom Grad $n$, mit $n ! / 2$ Permutationen von $n$ Elementen f√ºr $n \\geq 2$.\n",
        "\n",
        "- Dic $_{n}$ ist die dizyklische Gruppe der Ordnung $4 n$.\n",
        "\n",
        "- $V_{4}$ ist die [Klein'sche Vierergruppe](https://de.m.wikipedia.org/wiki/Kleinsche_Vierergruppe) der Ordnung $4 .$\n",
        "\n",
        "- $Q_{4 n}$ ist die Quaternionengruppe der Ordnung $4 n$ fur $n \\geq 2$.\n",
        "\n",
        "[Liste aller Gruppen bis Ordnung 20](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen#Liste_aller_Gruppen_bis_Ordnung_20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Finite Simple Groups*"
      ],
      "metadata": {
        "id": "Fpc3Bxykh5KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.spektrum.de/kolumne/endliche-einfache-gruppen-das-monster-und-der-laengste-beweis/2137146"
      ],
      "metadata": {
        "id": "5Y80lw5Zh3XE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN_XWVc0s5p"
      },
      "source": [
        "**Finite Simple Groups**\n",
        "\n",
        "* [Endliche einfache Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe) gelten in der Gruppentheorie als die Bausteine der [endlichen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_Gruppe).\n",
        "\n",
        "* Die endlichen einfachen Gruppen spielen f√ºr die endlichen Gruppen eine √§hnliche Rolle wie die Primzahlen f√ºr die nat√ºrlichen Zahlen: Jede endliche Gruppe l√§sst sich in ihre einfachen Gruppen ‚Äûzerteilen‚Äú (f√ºr die Art der Eindeutigkeit siehe den Satz von Jordan-H√∂lder).\n",
        "\n",
        "* Die Rekonstruktion einer endlichen Gruppe aus diesen ihren ‚ÄûFaktoren‚Äú ist aber nicht eindeutig.\n",
        "\n",
        "* Es gibt jedoch keine ‚Äûnoch einfacheren Gruppen‚Äú, aus denen sich die endlichen einfachen Gruppen konstruieren lassen.\n",
        "\n",
        "Obwohl die endlichen einfachen Gruppen seit 1982 als vollst√§ndig klassifiziert galten, schlossen Mathematiker um Aschbacher die Klassifikation erst im Jahre 2002 mit einem 1200 Seiten langen Beweis ab:\n",
        "\n",
        "* Fast alle dieser Gruppen lassen sich einer von 18 Familien endlicher einfacher Gruppen zuordnen.\n",
        "\n",
        "* Es existieren 26 Ausnahmen. Diese Gruppen werden als **sporadische Gruppen** bezeichnet (Zu den sporadischen Gruppen z√§hlen die Conway-Gruppe, das Babymonster und die [**Monstergruppe**](\n",
        "https://de.m.wikipedia.org/wiki/Monstergruppe) (mit fast 1054 Elementen die gr√∂√üte sporadische Gruppe).\n",
        "\n",
        "* Die [sporadischen Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe) sind 26 spezielle Gruppen in der Gruppentheorie. Es handelt sich um die [endlichen einfachen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe), die sich nicht in eine der [(18) systematischen Familien mit unendlich vielen Mitgliedern](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Familien_endlicher_einfacher_Gruppen) (von endlichen einfachen Gruppen) einordnen lassen.\n",
        "\n",
        "> https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/\n",
        "\n",
        "\n",
        "*Klassifikation der endlichen einfachen Gruppe*\n",
        "\n",
        "Die endlichen einfachen Gruppen [lassen sich einteilen in](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Klassifikation) bzw [Classification of finite simple groups](https://en.m.wikipedia.org/wiki/Classification_of_finite_simple_groups), Every finite simple group is isomorphic to one of the following groups:\n",
        "\n",
        "* a member of one of three infinite classes of such, namely:\n",
        "\n",
        "  * (1) [zyklische Gruppen](https://de.m.wikipedia.org/wiki/Zyklische_Gruppe) von Primzahlordnung,\n",
        "\n",
        "  * (1) [alternierende Gruppen](https://de.m.wikipedia.org/wiki/Alternierende_Gruppe) $A_{n}$ mit $n>4$,\n",
        "\n",
        "  * (16) [Gruppen vom Lie-Typ](https://de.m.wikipedia.org/wiki/Gruppe_vom_Lie-Typ) √ºber einem [endlichen K√∂rper](https://de.m.wikipedia.org/wiki/Endlicher_K√∂rper) (16 jeweils unendliche Familien),\n",
        "\n",
        "* (26) one of 26 groups called the \"sporadic groups\" / [26 sporadische Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe).\n",
        "\n",
        "* (1) the [Tits group](https://en.m.wikipedia.org/wiki/Tits_group) (which is sometimes considered a 27th sporadic group)\n",
        "\n",
        "\n",
        "[2004: Classification of Quasithin group](https://en.m.wikipedia.org/wiki/Quasithin_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Other Algebraic Structures*"
      ],
      "metadata": {
        "id": "rJzNsbqaiC3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Structures (Moduln, Ring, Field)*"
      ],
      "metadata": {
        "id": "KHUTVWQOmCm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/%C3%9Cbersicht_K%C3%B6rper.svg/775px-%C3%9Cbersicht_K%C3%B6rper.svg.png)"
      ],
      "metadata": {
        "id": "MdAk53GrsPJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Modul](\n",
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)) ist ein n-dimensionaler Ring.**\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativit√§t)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where ‚Äúscalar multiplication‚Äù refers to multiplication by integer scalars.\n",
        "\n",
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$\n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$\n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ],
      "metadata": {
        "id": "xPGuj4OPrOPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Ring](https://en.m.wikipedia.org/wiki/Ring_theory) ist eine Menge R mit <u>zwei inneren bin√§ren Verkn√ºpfungen</u> ‚Äû+‚Äú und ‚Äû‚àô‚Äú, sodass gilt:**\n",
        "\n",
        "1. **Addition: (R, +) ist eine abelsche Gruppe**\n",
        "\n",
        "* Addition is associative and commutative;\n",
        "\n",
        "* There is an additive identity, zero;\n",
        "\n",
        "* Every element has an additive inverse;\n",
        "\n",
        "2. **Multiplikation: (R, ‚àô) ist eine Halbgruppe**, das bedeutet:\n",
        "\n",
        "* Halbgruppe in der Multiplikation im Ring: **nur nur die Assoziativit√§t, aber keine Inverse, neutrales element oder kommutativit√§t**)\n",
        "\n",
        "> **Das bedeutet: -> Sowohl Ringe als auch K√∂rper verlangen, dass bzgl. der Addition eine kommutative Gruppe vorliegt (abelsch!). Bei der Multiplikation erfolgt der √úbergang vom Ring zum K√∂rper durch die Versch√§rfung der Forderungen**\n",
        "\n",
        "* Unlike a field, a ring is not required to have multiplicative inverses, and the multiplication is not required to be commutative.\n",
        "\n",
        "> **A good example of a ring is the set of all n√ón matrices under the operations of matrix addition and matrix multiplication.** [Matrix-Multiplication is non-commutative!](https://en.m.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity)\n",
        "\n",
        "$\\mathbf{A B} \\neq \\mathbf{B A}$\n",
        "For example\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "but\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "> **The integers Z also form a ring under the operations of addition and multiplication.**\n",
        "\n",
        "3. **Die Distributivgesetze a*(b+c)=a*b+a*c und (a+b)*c = a*c+b*c sind f√ºr alle a,b,c Œµ $R$ erf√ºllt.**\n",
        "\n",
        "4. **Das neutrale Element 0 von (R, +) hei√üt Nullelement von R.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Ring_(Algebra)\n",
        "\n",
        "**Ein Ring hei√üt kommutativ**, falls er bez√ºglich der Multiplikation kommutativ ist (Ein Ring hei√üt kommutativ, falls er bez√ºglich der Multiplikation kommutativ ist, ansonsten spricht man von einem nicht-kommutativen Ring.)\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* 2√ó2 Real matrices.\n",
        "\n",
        "* Das wichtigste Beispiel eines Ringes sind die Integers / ist die Menge (Ùè∞Å$\\mathbb Z$,+,‚àô) der ganzen Zahlen mit der √ºblichen Addition und Multiplikation. Es handelt sich dabei um einen nullteilerfreien kommutativen Ring mit Einselement, also einen Integrit√§tsring.\n",
        "\n",
        "* the Integers modulo some Natural number greater than one;\n",
        "\n",
        "* Ebenso bildet ($\\mathbb Q$,+,‚àô) der rationalen Zahlen mit der √ºblichen Addition und Multiplikation einen Ring. Da in diesem Fall nicht nur ($\\mathbb Q$,+), sondern auch ($\\mathbb Q$ \\ {0},‚àô) eine abelsche Gruppe bildet, liegt sogar ein K√∂rper vor; es handelt sich dabei um den Quotientenk√∂rper des Integrit√§tsringes (Ùè∞Å$\\mathbb Z$,+,‚àô).\n",
        "\n",
        "* Kein Ring ist die Menge ($\\mathbb N$Ùè∞Ä,+,‚àô) der nat√ºrlichen Zahlen mit der √ºblichen Addition und Multiplikation, da die Addition √ºber den nat√ºrlichen Zahlen nicht invertierbar ist.\n",
        "\n",
        "https://www.quora.com/What-are-the-differences-between-rings-and-fields"
      ],
      "metadata": {
        "id": "usbC2QwNr7Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [K√∂rper (Field)](https://de.m.wikipedia.org/wiki/K%C3%B6rper_(Algebra)) ist eine spezielle Form von Ring**:\n",
        "\n",
        "> Man nennt die Elemente im K√∂rper nicht Vektoren, sondern Skalare. √úber dem Skalark√∂rper betrachtet man einen Vektorraum und dessen Elemente hei√üen Vektoren.\n",
        "\n",
        "* **A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)**\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, hei√üt ein K√∂rper, wenn in ihm jedes von Null verschiedene Element multiplikativ invertierbar ist.\n",
        "Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring K, in dem die Einheitengruppe K* gleich K \\ {0}, also maximal gro√ü, ist.\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, ist ein K√∂rper, wenn in ihm jedes von Null verschiedene Element ein Inverses bez√ºglich der Multiplikation besitzt. Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring $K$, in dem die Einheitengruppe $K^{*}$ gleich $K \\backslash\\{0\\}$ ist.\n",
        "\n",
        "\n",
        "Ein Tripel (K,+,‚Ä¢), bestehend aus einer Menge K und zwei bin√§ren Verkn√ºpfungen ‚Äû+‚Äú und ‚Äû‚Ä¢‚Äú (die √ºblicherweise Addition und Multiplikation genannt werden), ist genau dann ein K√∂rper, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "* $(K,+)$ ist eine abelsche Gruppe (mit Neutralelement 0)\n",
        "\n",
        "* $(K \\backslash\\{0\\}, ‚Ä¢)$ ist eine abelsche Gruppe (mit Neutralelement 1)\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ und $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ (Distributivgesetz)\n",
        "\n",
        "Additive Eigenschaften:\n",
        "\n",
        "* $a+(b+c)=(a+b)+c$ (Assoziativgesetz)\n",
        "\n",
        "* $a+b=b+a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $0 \\in K$ mit $0+a=a$ (neutrales Element)\n",
        "\n",
        "* Zu jedem $a \\in K$ existiert das additive Inverse $(-a)$ mit $(-a)+a=0$\n",
        "\n",
        "Multiplikative Eigenschaften:\n",
        "\n",
        "* $\\cdot a \\cdot(b \\cdot c)=(a \\cdot b) \\cdot c$ (Assoziativgesetz)\n",
        "\n",
        "* $a \\cdot b=b \\cdot a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $1 \\in K$ mit $1 \\cdot a=a$ (neutrales Element), und es ist $1 \\neq 0$.\n",
        "\n",
        "* Zu jedem $a \\in K \\backslash\\{0\\}$ existiert das multiplikative Inverse $a^{-1}$ mit $a^{-1} \\cdot a=1$\n",
        "\n",
        "Zusammenspiel von additiver und multiplikativer Struktur:\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ (Links-Distributivgesetz)\n",
        "\n",
        "* Das Rechts-Distributivgesetz $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ folgt dann aus den √ºbrigen Eigenschaften:\n",
        "$(a+b) \\cdot c=c \\cdot(a+b)=c \\cdot a+c \\cdot b=a \\cdot c+b \\cdot c$\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* The most familiar form of algebra is the elementary algebra that you learned in high school, namely the algebra of the real numbers. From an abstract point of view, this is the algebra of fields.\n",
        "\n",
        "* Note that the axioms for a field are precisely the axioms for algebra on the real numbers. As a result, the real numbers R form a field under the usual operations of addition and multiplication. However, the real numbers are not the only possible field. Indeed, you are already familiar with a few other examples:\n",
        "\n",
        "* set of rational numbers under addition and multiplication. The rational numbers Q form a field under the usual operations of addition and multiplication. In particular, we can add or multiply two elements of Q to obtain another element of Q, and these operations obey all of the axioms listed above.\n",
        "\n",
        "* The complex numbers C form a field under the commonly defined operations of addition and multiplication. Complex numbers do obey all of the listed axioms for a field, which is why elementary algebra works as usual for complex numbers.\n",
        "\n",
        "* The Integers modulo a Prime number.\n",
        "\n",
        "*An example of a set of numbers that is **not a field** is the set of integers. It is an \"integral domain.\" It is not a field because it lacks multiplicative inverses. Without multiplicative inverses, division may be impossible.*\n",
        "\n",
        "* Both are algebraic objects with a notion of addition and multiplication, **but the multiplication in a field is more specialized**: it is necessarily commutative and every nonzero element has a multiplicative inverse.\n"
      ],
      "metadata": {
        "id": "j07gGLGUtEU6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxw9DSyjiah"
      },
      "source": [
        "**Siehe auch**\n",
        "\n",
        "* [Algebraische Struktur](https://de.m.wikipedia.org/wiki/Algebraische_Struktur)\n",
        "\n",
        "* [Outline_of_algebraic_structures](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures)\n",
        "\n",
        "* [Mathematische Strukturen](https://de.m.wikipedia.org/wiki/Mathematische_Struktur) (neben topologischen Strukturen, geometrischen Strukturen und Zahlbereichen)\n",
        "\n",
        "* [Geometrische Gruppentheorie](https://de.wikipedia.org/wiki/Geometrische_Gruppentheorie) (Gruppenoperationen auf Graphen und metrischen R√§umen, letztlich werden die Gruppen selbst zu solchen geometrischen Objekten)\n",
        "\n",
        "* [Verkn√ºpfungen](https://de.m.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik))\n",
        "\n",
        "* [Gruppenoperation](https://de.wikipedia.org/wiki/Gruppenoperation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Icosahedral Symmetry](https://youtu.be/B-WI8JZR140)"
      ],
      "metadata": {
        "id": "ibeXZKTToqNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Homomorphism*"
      ],
      "metadata": {
        "id": "iqHduf9WDZMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Homomorphiesatz (Isomorphie on quotient group)\n",
        "\n",
        "Video: [First Isomorphism Theorem for Groups](https://www.youtube.com/watch?v=JiS43Twomsk&list=WL&index=10)"
      ],
      "metadata": {
        "id": "M8aSZ3c541kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Persistent Homology](https://youtu.be/ktKCzMmDXDk)\n",
        "\n",
        "Video: [Chapter 6: Homomorphism and (first) isomorphism theorem | Essence of Group Theory](https://youtu.be/2kmIHyD8zTk)\n",
        "\n",
        "Kern is a normal subgroup (normalteiler) von H, weil Rest is 0. alles andere sind cosets (mit jeweils rest 1,2,3, etc nachdem was der normalteiler ist)"
      ],
      "metadata": {
        "id": "Z8eBrD-9qS_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Monad_(linear_algebra)"
      ],
      "metadata": {
        "id": "8QS1Q_s8JBFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cantorsparadise.com/an-intro-to-topology-9e0478313b63"
      ],
      "metadata": {
        "id": "6hBdWGAqr4lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence (simplicial homology). Ist das die normal subgroup (normal subgroup)??\n",
        "\n",
        "Der Kern von f ist stets ein Normalteiler von G und das Bild von f ist eine Untergruppe von H. Nach dem Homomorphiesatz ist die Faktorgruppe G/Kern(f) isomorph zu Bild (f)."
      ],
      "metadata": {
        "id": "R4CVKRmNV-E5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35B6lF4hR7zq"
      },
      "source": [
        "**Homomorphiesatz (Fundamental Theorem on Homomorphism)**\n",
        "\n",
        "* [Homomorphiesatz](https://de.m.wikipedia.org/wiki/Homomorphiesatz)\n",
        "\n",
        "* https://youtu.be/QA9rrDMlaHc (Homomorphiesatz mit Hasen und Jaegern)\n",
        "\n",
        "* https://youtu.be/390eRzVSC2k (Homomorphie mit Modulo und kommutativen Diagramm)\n",
        "\n",
        "**Homomorphiesatz (allgemein)**:\n",
        "\n",
        "* Aus einer Abbildung $f$ zwischen zwei Gruppen $G$ und $H$, die weder injektiv noch surjektiv ist, wollen wir eine bijektive Abbildung herleiten.\n",
        "\n",
        "* Schritt 1: Das macht man, indem man zuerst auf der rechten Seite alle Elemente ausschliesst, die nicht Teil der Zielmenge sind, und sich nur auf die 'getroffenen' Elemente fokussiert (The image of $f$ is hierbei a subgroup of $H$.)\n",
        "\n",
        "* Schritt 2: Jetzt hat man nur noch das Problem auf der linken Seite, dass mehrere Startelemente in $G$ auf ein und dasselbe Zielelement in $H$ verweisen. Man betrachtet die Startelemente dann einfach als identisch. Das macht man in dem man $G$ umwandelt in $G$ / Kern ($f$) (man bildet die Faktorgruppe, und spricht aus: modulo Kern von f. Der Quotientenvektorraum von $G$ nach kern von $f$.\n",
        "  * Das geht, weil $f$ ein Homomorphismus ist\n",
        "  * Das bedeutet, wenn zwei Elemente a und b im Start auf ein Element im Ziel verweisen, dann unterscheiden sie sich um ein Kernelement. Heisst, a minus b ist ein Element, das auf 0 geschickt wird, und damit ein Kernelement.\n",
        "  * Und wenn ich jetzt modulo des kerns rechne, dann tue ich so, als ob es die Differenz nicht gibt. Weil es bedeutet a =b. Der Quotientenraum ist ein kunstlich geschaffener Raum, wo a und b identisch gemacht wurden (kongruent).\n",
        "  * Remember aus Modulorechnung: Zwei Zahlen sind kongruent (modulo des Moduls m), wenn ihre Differenz durch m teilbar ist. Hier ist das Modulo der Kern. Also Rest muss 0 sein.\n",
        "  * Modulo n (Reste berechnen, hierbei 0): die Differenz zweier Elemente ist teilbar durch n. Die beiden Elemente sind dann kongruent (identisch).\n",
        "  * $G$ modulo Kern $f$ ist isomorph (identisch =bijektiv und homomorph)) zu Bild $f$, das in $H$ liegt.\n",
        "\n",
        "* **Der Kern von $f$ ist stets ein Normalteiler von $G$ und das Bild von $f$ ist eine Untergruppe von $H$. Nach dem Homomorphiesatz ist die Faktorgruppe $G / \\operatorname{Kern}(f)$ [isomorph (bijektiv)](https://de.m.wikipedia.org/wiki/Isomorphismus) zu Bild $(f)$.**\n",
        "\n",
        "**Bedingungen:**\n",
        "\n",
        "* Let $G$ and $H$ be two groups.\n",
        "* and let $f$ : $G \\rightarrow H$ be a [group homomorphism](https://de.wikipedia.org/wiki/Gruppenhomomorphismus).\n",
        "* and let $K$ be a normal subgroup (Normalteiler) in $G$ and $\\varphi$ the natural surjective homomorphism $G \\rightarrow G / K$ (where $G / K$ is a quotient group). Diese Faktorgruppen sind homomorphe Bilder von G und **jedes homomorphe Bild von G ist zu einer solchen Faktorgruppe G/K isomorph**.\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/homomorphy.jpg)\n",
        "\n",
        "**Then:**\n",
        "\n",
        "1. **Dann ist der Kern von $f$ ein Normalteiler von $G$.**\n",
        "  * Normalteiler sind die [Kerne](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) von Gruppenhomomorphismen, weshalb dann klar ist, dass umgekehrt der Kern von $f$ ein Normalteiler von $G$ ist.\n",
        "\n",
        "  * If $K$ is a **subset** of ker $(f)$ then there exists a unique homomorphism $h: G / K \\rightarrow H$ such that $f=h$ $\\varphi$. In other words, the natural projection $\\varphi$ is universal among homomorphisms on $G$ that map $K$ to the identity element.\n",
        "\n",
        "2. **und daher kann die Faktorgruppe $G /$ ker $f$ gebildet werden.**\n",
        "\n",
        "3. **Nach dem [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) ist diese Faktorgruppe $G /$ ker $f$ isomorph zum Bild von $f$, das eine Untergruppe von $H$ ist.**\n",
        "  * The image of $f$ is isomorphic to the quotient group $G /$ ker ($f$). And in particular, if $f$ is surjective then $H$ is isomorphic to $G$ / ker $(f)$. [Source](https://en.m.wikipedia.org/wiki/Isomorphism_theorems#First_Isomorphism_Theorem_4)\n",
        "  * The image of $f$ is hierbei a subgroup of $H$.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Group_homomorphism_ver.2.svg/500px-Group_homomorphism_ver.2.svg.png)\n",
        "\n",
        "*Image of a group homomorphism (h) from G (left) to H (right).*\n",
        "\n",
        "*The smaller oval inside H is the image of h. N is the kernel of h and aN is a coset of N.* [Source](https://en.m.wikipedia.org/wiki/Group_homomorphism)\n",
        "\n",
        "See also: https://mathepedia.de/Kern_und_Bild_Homomorphismus.html\n",
        "\n",
        "*Diagram of the fundamental theorem on homomorphisms where f is a homomorphism, N is a normal subgroup of G and e is the identity element of G.*\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Diagram_of_the_fundamental_theorem_on_homomorphisms.svg/440px-Diagram_of_the_fundamental_theorem_on_homomorphisms.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Category Theory*"
      ],
      "metadata": {
        "id": "jNQs-MXfBmVV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eLPSGAeDW0j"
      },
      "source": [
        "[Category](https://en.m.wikipedia.org/wiki/Category_(mathematics)) and [Category theory](https://en.m.wikipedia.org/wiki/Category_theory)\n",
        "\n",
        "* Eine Kategorie besteht aus Objekten und Morphismen. Man m√∂chte jedem Objekt in einer Kategorie ein Objekt in der anderen Kategorie zuordnen, und das gleiche mit den Morphismen zwischen den Objekten.\n",
        "\n",
        "* [Objects](https://ncatlab.org/nlab/show/object)\n",
        "\n",
        "* [Product und Coproduct](https://de.m.wikipedia.org/wiki/Produkt_und_Koprodukt)\n",
        "\n",
        "* [Anfangsobjekt (initiales), Endobjekt (terminales, finales) und Nullobjekt](https://de.m.wikipedia.org/wiki/Anfangsobjekt,_Endobjekt_und_Nullobjekt)\n",
        "\n",
        "* [Functor](\n",
        "https://ncatlab.org/nlab/show/functor): A homomorphism between categories is a functor. Zuordnung zwischen zwei Kategorien\n",
        "\n",
        "  * Funktoren werden auch Diagramme genannt (mitunter nur in bestimmten Kontexten), da sie eine formale Abstraktion [kommutativer Diagramme](https://de.m.wikipedia.org/wiki/Kommutatives_Diagramm) darstellen.\n",
        "\n",
        "  * functors must preserve [identity morphisms](https://en.m.wikipedia.org/wiki/Morphism#Definition) and [composition of morphisms](https://en.m.wikipedia.org/wiki/Function_composition)\n",
        "\n",
        "* [Natural Transformations](https://en.m.wikipedia.org/wiki/Natural_transformation) are Maps between functors (and functors are maps between categories)\n",
        "\n",
        "* [Duality](https://en.m.wikipedia.org/wiki/Dual_(category_theory)) is a correspondence between the properties of a category C and the dual properties of the opposite category C<sup>op</sup>.\n",
        "\n",
        "* Topos: Category theorists have proposed [topos theory](https://en.m.wikipedia.org/wiki/Topos) as an alternative to traditional [axiomatic set theory](https://de.m.wikipedia.org/wiki/Axiomatische_Mengenlehre). Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory.\n",
        "\n",
        "> [Outline of category theory](https://en.m.wikipedia.org/wiki/Outline_of_category_theory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlwPUfSMCiu"
      },
      "source": [
        "**Category Theory and Higher Category Theory**\n",
        "\n",
        "0) A category is a collection of objects and morphisms between those objects that satisfy some rules.\n",
        "\n",
        "1) A functor is a morphism in the category of categories.\n",
        "\n",
        "2) A natural transformation is a morphism in the category of functors.\n",
        "\n",
        "But they all stop right there. What about:\n",
        "\n",
        "3) the morphisms in the category of natural transformations?\n",
        "\n",
        "4) Or the \"morphisms in the category of the morphisms in the category of natural transformations\"\n",
        "\n",
        "*Definition*\n",
        "\n",
        "* [higher category theory](https://en.m.wikipedia.org/wiki/Higher_category_theory) bzw. [higher category theory](https://ncatlab.org/nlab/show/higher+category+theory) is the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities.\n",
        "\n",
        "* Higher category theory is often applied in algebraic topology (especially in homotopy theory), where one studies algebraic invariants of spaces, such as their fundamental weak ‚àû-groupoid.\n",
        "\n",
        "* *From 2-category to Higher Order Category*: The concept of 2-category generalizes further in higher category theory to n-categories, which have k-morphisms for all\n",
        "k\n",
        "‚â§\n",
        "n\n",
        ". The morphisms can be composed along the objects, while the 2-morphisms can be composed in two different directions: along objects ‚Äì called horizontal composition ‚Äì and along morphisms ‚Äì called vertical composition. The composition of morphisms is allowed to be associative only up to coherent associator 2-morphisms.\n",
        "\n",
        "* See also: [infinity-category](https://ncatlab.org/nlab/show/infinity-category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsmc7LDx_K-"
      },
      "source": [
        "*Morphismen (Linear Maps)*\n",
        "\n",
        "Ein [Morphismus](https://de.m.wikipedia.org/wiki/Morphismus) ist eine Funktion in Kategorientheorie. Man schreibt: $f\\colon X\\to Y$. Image Source: [Morphismen](https://youtu.be/0wKsFNLR15g)\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "* [Automorphismus](https://de.m.wikipedia.org/wiki/Automorphismus): bijektiv, Definitionsmenge = Zielmenge\n",
        "\n",
        "* [Isomorphismus](https://de.m.wikipedia.org/wiki/Isomorphismus): bijektiv, Definitionsmenge ‚â† Zielmenge\n",
        "\n",
        "* [Endomorphismus](https://de.m.wikipedia.org/wiki/Endomorphismus): Definitionsmenge = Zielmenge. aber Bildmenge umfasst nicht den ganzen Vektorraum (Ziele < Uspr√ºnge)\n",
        "\n",
        "* [Monomorphismus](https://de.m.wikipedia.org/wiki/Monomorphismus): injektiv (jedes Element des Usprungs hat ein exklusives Element)\n",
        "\n",
        "* [Epimorphismus](https://de.m.wikipedia.org/wiki/Epimorphismus): surjektiv (jedes Element im Ziel ist mind 1 mal getroffen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNkozbcHgYo"
      },
      "source": [
        "**Deep Dive: Homomorphismus**\n",
        "\n",
        "> Eine lineare Abbildung ist ein (Homo-)Morphismus zwischen Vektorr√§umen.\n",
        "\n",
        "*Unterschied zwischen Isomorphismus und Homomorphismus:*\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/isomorphismus.JPG)\n",
        "\n",
        "Ein [Vektorraum-Homomorphismus](https://de.m.wikipedia.org/wiki/Homomorphismus) (\"Lineare Abbildung\") ist eine Abbildung $\\varphi: V \\rightarrow W$ zwischen $K$ -Vektorr√§umen $V$ und $W$ (gemeinsamer Grundk√∂rper $K$) mit den folgenden Eigenschaften:\n",
        "\n",
        "\n",
        "1. **Additivit√§t**: $\\varphi(v+u)=\\varphi(v)+\\varphi(u)$ f√ºr alle $u$ und $v \\in V$\n",
        "\n",
        "2. **Homogenit√§t**: $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v) \\quad$ f√ºr alle $\\lambda \\in K$ und $v \\in V$ (Skalarmultiplikation)\n",
        "\n",
        "* Beispiel: Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt f√ºr die Multiplikation mit einem Skalar aus dem Grundk√∂rper.\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung unendlichdimensionaler Vektorr√§ume, die eine Topologie tragen, spricht man meist von linearen Operatoren statt von [linearen Abbildungen](https://de.m.wikipedia.org/wiki/Lineare_Abbildung).\n",
        "\n",
        "\n",
        "![xyz](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Reflection_of_a_triangle_about_the_y_axis.svg/320px-Reflection_of_a_triangle_about_the_y_axis.svg.png)\n",
        "\n",
        "*Achsenspiegelung als Beispiel einer linearen Abbildung*\n",
        "\n",
        "**Beispiele f√ºr Vektorraumhomomorphismus**\n",
        "\n",
        "* F√ºr $V=W=\\mathbb{R}$ hat jede lineare Abbildung die Gestalt $f(x)=m x$ mit $m \\in \\mathbb{R}$\n",
        "\n",
        "* Es sei $V=\\mathbb{R}^{n}$ und $W=\\mathbb{R}^{m}$. Dann wird f√ºr jede $m \\times n$ -Matrix $A$ mit Hilfe der Matrizenmultiplikation eine lineare Abbildung $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ durch\n",
        "\n",
        "$f(x)=A x=\\left(\\begin{array}{ccc}a_{11} & \\cdots & a_{1 n} \\\\ \\vdots & & \\vdots \\\\ a_{m 1} & \\cdots & a_{m n}\\end{array}\\right)\\left(\\begin{array}{c}x_{1} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right)$\n",
        "definiert.\n",
        "\n",
        "Jede lineare Abbildung von $\\mathbb{R}^{n}$ nach $\\mathbb{R}^{m}$ kann so dargestellt werden.\n",
        "\n",
        "Siehe auch: [Gruppenhomomorphismus](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Morphismen:**\n",
        "\n",
        "F√ºr manche Kategorien gibt es besondere Bezeichnungen f√ºr Morphismen.\n",
        "\n",
        "* Ein [Hom√∂omorphismus](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist ein Isomorphismus zwischen topologischen R√§umen. Sind (beispielsweise) die [Fundamentalgruppen](https://de.m.wikipedia.org/wiki/Fundamentalgruppe) zweier R√§ume isomorph, so sind die R√§ume hom√∂omorph.\n",
        "\n",
        "* Ein [Diffeomorphismus](https://de.m.wikipedia.org/wiki/Diffeomorphismus) ist ein Isomorphismus zwischen differenzierbaren Mannigfaltigkeiten.\n",
        "\n",
        "* Eine [Isometrie](https://de.m.wikipedia.org/wiki/Isometrie) ist ein Isomorphismus in der Kategorie der metrischen R√§umen mit den nichtexpansiven stetigen Abbildungen."
      ],
      "metadata": {
        "id": "p_vf2S1FOc2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Number Theory*"
      ],
      "metadata": {
        "id": "Qa3-DIbUt0Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mathworld.wolfram.com/topics/NumberTheory.html"
      ],
      "metadata": {
        "id": "oTRwcbs24qtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Number theory Full Course [A to Z]](https://www.youtube.com/watch?v=19SW3P_PRHQ)"
      ],
      "metadata": {
        "id": "QQ94xvSShDVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [The Continuity of Splines](https://www.youtube.com/watch?v=jvPPXbo87ds)"
      ],
      "metadata": {
        "id": "1nsbz4yAiP6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zahlentheorie**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebraische_Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Diophantische_Gleichung: Polynomfunktion mit ganzzahligen Koeffizienten ist und nur ganzzahlige L√∂sungen gesucht werden"
      ],
      "metadata": {
        "id": "NGT5msJ1sIRa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g878nE6q7lX"
      },
      "source": [
        "**Zahlenarten**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Zahldarstellung\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/number_001.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carmichael-Zahl**\n",
        "\n",
        "Eine [Carmichael-Zahl](https://de.m.wikipedia.org/wiki/Carmichael-Zahl) ist eine nat√ºrliche Zahl mit besonderer Primfaktorzerlegung\n",
        "\n",
        "Eine Carmichael-Zahl ist stets ungerade und enth√§lt mindestens 3 verschiedene Primfaktoren. Die kleinsten Carmichael-Zahlen sind 561, 1105, 1729.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/carmichael-zahl/1414\n",
        "\n",
        "https://www.faz.net/aktuell/wissen/daniel-larsen-findet-einen-mathebeweis-zu-carmichael-zahlen-18583248.html"
      ],
      "metadata": {
        "id": "dImJp0SNWxsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Narcissistic Number**\n",
        "\n",
        "number theory, a narcissistic number is a number that can be expressed as the sum of its own digits raised to the power of the number of digits.\n",
        "\n",
        "> $153 = 1^3 + 5^3 + 3^3$\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Narcissistic_number"
      ],
      "metadata": {
        "id": "04WEUT_eCbW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex Numbers**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/155762/complex-numbers-in-quantum-mechanics-and-in-special-relativity\n",
        "\n",
        "* you can use split-complex numbers in relativity, but ironically complex numbers have proved more popular for this)."
      ],
      "metadata": {
        "id": "klTQgi0O_Vnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quaternions are useful. In particular for representation of rotations of 3-space. ‚Äì\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Hyperkomplexe_Zahlen.svg/519px-Hyperkomplexe_Zahlen.svg.png)"
      ],
      "metadata": {
        "id": "ueOfwEOht3VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dual Numbers**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Dual_number\n",
        "\n",
        "* Video: https://youtu.be/ceaNqdHdqtg"
      ],
      "metadata": {
        "id": "RG0Fv5AulqaE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y_nhc_-u7P"
      },
      "source": [
        "**Hyperreelle Zahlen & Nichtstandardanalysis (Infinitesimalrechnung)**\n",
        "\n",
        "* There are also applications of nonstandard analysis to the theory of stochastic processes, particularly constructions of Brownian motion as random walks.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Nichtstandardanalysis\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Hyperreelle_Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Infinitesimalrechnung\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Surreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Hyperreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Infinitesimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2-azta81gIJ"
      },
      "source": [
        "**Hyperkomplexe Zahlen**\n",
        "\n",
        "* [Hyperkomplexe Zahlen](https://de.m.wikipedia.org/wiki/Hyperkomplexe_Zahl) sind Verallgemeinerungen der komplexen Zahlen.\n",
        "\n",
        "https://www.quantamagazine.org/the-octonion-math-that-could-underpin-physics-20180720/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNHMgPsrj4B"
      },
      "source": [
        "**Transcendental numbers:**\n",
        "\n",
        "* Zahlen, die L√∂sung einer algebraischen Gleichung: (Wurzel aus 2) - 2 = 0\n",
        "\n",
        "* Transzendente Zahlen (in der Menge der reellen Zahlen): Zahlen, die nicht L√∂sung einer algebraischen Gleichung sind: e oder pi\n",
        "\n",
        "> from: https://www.youtube.com/watch?v=P24tmohytXs\n",
        "\n",
        "* pie œÄ or Euler number\n",
        "\n",
        "* Never end after comma: 3.14159265358979323846....\n",
        "\n",
        "* Cannot be displayed as fraction\n",
        "\n",
        "* [Transzedente Zahl](https://de.m.wikipedia.org/wiki/Transzendente_Zahl) heisst eine reelle Zahl (oder allgemeiner eine komplexe Zahl), wenn sie nicht Nullstelle eines (vom Nullpolynom verschiedenen) Polynoms mit ganzzahligen Koeffizienten ist. Andernfalls handelt es sich um eine algebraische Zahl. Jede reelle transzendente Zahl ist √ºberdies irrational.\n",
        "\n",
        "* omnem rationem transcendunt, lat.: Sie sind jenseits aller Vernunft\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplex Numbers**\n",
        "\n",
        "* --> real tessarines\n",
        "\n",
        "* also: [split complex number](https://en.m.wikipedia.org/wiki/Split-complex_number), hyperbolic number, double number)\n",
        "\n",
        "* In algebra, a split complex number (or hyperbolic number, also perplex number, double number) has two real number components x and y, and is written $z = x + y‚Äâj$, where $j^2$ = 1. The conjugate of z is $z^{‚àó}$ = x ‚àí y j. Since j2 = 1, the product of a number z with its conjugate is $zz^{‚àó}$ = $x^2 ‚àí y^2$, an [isotropic quadratic form](https://en.m.wikipedia.org/wiki/Isotropic_quadratic_form), =N(z) = $x^2 ‚àí y^2$.\n",
        "\n",
        "* Very perplexing, right? They are defined as of form a+hb, where a and b are real numbers, h¬≤=1.\n",
        "\n",
        "  * But wait, that sounds too easy! Until you realize that h isn't +1 or -1. It is \"something else\".\n",
        "\n",
        "* Perplex numbers have many applications outside of algebra and geometry, such as in Quantum Mechanics and the Theory of Relativity.\n",
        "\n",
        "* Special Relativity:https://aapt.scitation.org/doi/10.1119/1.14605\n",
        "\n",
        "* Quantum Mechanics: https://www.intlpress.com/site/pub/files/_fulltext/journals/cis/2014/0014/0003/CIS-2014-0014-0003-a001.pdf"
      ],
      "metadata": {
        "id": "VKqM5OuK6jYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bicomplex number**\n",
        "\n",
        "* --> \"Tessarine\" redirects here. For real tessarines, see Split-complex number.\n",
        "\n",
        "* In abstract algebra, a bicomplex number is a pair (w, z) of complex numbers constructed by the Cayley‚ÄìDickson process that defines the bicomplex conjugat ${\\displaystyle (w,z)^{*}=(w,-z)}$, and the product of two bicomplex numbers as\n",
        "\n",
        "> {\\displaystyle (u,v)(w,z)=(uw-vz,uz+vw).}\n",
        "\n",
        "* Then the bicomplex norm is given by\n",
        "\n",
        "> ${\\displaystyle (w,z)^{*}(w,z)=(w,-z)(w,z)=(w^{2}+z^{2},0),}$\n",
        "\n",
        "* a quadratic form in the first component.\n",
        "\n",
        "* https://hsm.stackexchange.com/questions/12866/why-are-quaternions-more-popular-than-tessarines-despite-being-non-commutative\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1200.png)\n",
        "\n",
        "Images source:https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.527.356&rep=rep1&type=pdf\n"
      ],
      "metadata": {
        "id": "bZqRMeca9IqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVu9D8nrvsY"
      },
      "source": [
        "**p-adic numbers**\n",
        "\n",
        "* Size is different in p-adic numbers\n",
        "\n",
        "* they have nothing to do with the real number line: here two numbers are close, when their first several digits are the same\n",
        "\n",
        "* in p-adic absolute value, two numbers are close when their last digits are the same. so two numbers are close in this field, when they agree on\n",
        "\n",
        "https://youtu.be/3gyHKCDq1YA\n",
        "\n",
        "* infinite before comma: ....985356295141.3\n",
        "\n",
        "* [p-adische Zahl](https://de.m.wikipedia.org/wiki/P-adische_Zahl) ist eine Zahl, die sich in einer Potenzreihe zu einer Primzahl darstellen l√§sst\n",
        "\n",
        "* p-adic number systems emerge from modular arithmetic\n",
        "\n",
        "* https://www.quantamagazine.org/how-the-towering-p-adic-numbers-work-20201019/\n",
        "\n",
        "* https://www.quantamagazine.org/peter-scholze-and-the-future-of-arithmetic-geometry-20160628/\n",
        "\n",
        "* https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/\n",
        "\n",
        "* \"Das Dualsystem ist das Stellenwertsystem mit der Basis 2, liefert also die dyadische (2-adische) Darstellung von Zahlen (Dyadik) (gr. Œ¥œçŒø = zwei).\" [Source](https://de.m.wikipedia.org/wiki/Dualsystem#Grundrechenarten_im_Dualsystem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPr6YYL5n9sy"
      },
      "source": [
        "**Modulform**\n",
        "\n",
        "* used in stringtheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Modulform\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Ramanujan-Thetafunktion\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Thetafunktion\n",
        "\n",
        "* https://www.quantamagazine.org/moonshine-link-discovered-for-pariah-symmetries-20170922/\n",
        "\n",
        "* https://www.quantamagazine.org/universal-math-solutions-in-dimensions-8-and-24-20190513/\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Kaluza-Klein-Theorie"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infinity $\\infty$**\n",
        "\n",
        "* Kontinuumshypothese (Cantor) kann man nicht beweisen oder widerlegen (G√∂del). Die Mathematik kann funktionieren, wenn man sowohl davon ausgeht, dass die Kontiuumshypothese gilt, als auch, dass sie nicht gilt.\n",
        "\n",
        "* term of size (how many?) is: **Cardinality**\n",
        "\n",
        "* Rules for comparing cardinalities: injection, subjection, bijection\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_319.png)\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite\n",
        "\n",
        "* [How Big are All Infinities Combined? (Cantor's Paradox) | Infinite Series](https://www.youtube.com/watch?v=TbeA1rhV0D0&list=WL&index=12&t=86s)\n",
        "\n",
        "**Smallest Sizes of Infinity**\n",
        "\n",
        "* intuition is often misleading in mathematics!\n",
        "\n",
        "* There are infinitely many sizes of infinity\n",
        "\n",
        "* Smallest infinity: natural numbers (counting numbers). Here: even numbers and natural numbers are the same size!\n",
        "\n",
        "  * Natural numbers: $\\aleph$ \"Aleph-naught\" (the least infinity cardinality)\n",
        "\n",
        "* We need to find a way to tell which infinity is bigger without counting them.\n",
        "\n",
        "* You use something called Bijection: if you can pair up two sets, they are the same size\n",
        "\n",
        "\t* even numbers and natural numbers are the same size, because there is a bijection between the two sets! Each natural number is [aired with 2 times itself: 1 with 2, 2 with 4, 4 with 6 etc. Damit sind alles odd numbers out, but the set didn't get smaller\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_318.png)\n",
        "\n",
        "\t* the natural numbers are also the same size as the integers: Integers include all natural numbers + all the negative whole numbers. We can pair them up exactly, so they must be the same size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_317.png)\n",
        "\n",
        "* Above the natural numbers are the real numbers in terms of infinity size. After that the real numbers.\n",
        "\n",
        "\t* an interval on the real number line is also an infinity. And an interval on the real line between 0 and 5 has the same size as an interval between 0 and 10. They are ll as big as the real numbers!!\n",
        "\n",
        "\t* You can show that any interval is the same size as the entire real number line !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_316.png)\n",
        "\n",
        "* Cantor wondered if there is an infinity between the natural and the real numbers?\n",
        "\n",
        "\t* CONTINUUM HYPOTHESIS: there is no size of infinity between the natural numbers and the real numbers\n",
        "\n",
        "\t* Decades later, mathematicians found out that the Continuum hypothesis is independent of the Zermela-Fraenkel set theory with choice (ZFC) - the Continuum hypothesis can not be proved or disproved using the standard rules of mathematics\n",
        "\n",
        "* So in one model of the tower of infinities the real numbers sit directly above the natural numbers\n",
        "\n",
        "* But in other models there are many infinities in between\n",
        "\n",
        "* The rules of maths don't say that one tower is correct and the other wrong.\n",
        "\n",
        "> So it seems that mathematics seems to be surprisingly agnostics with regards to which hierarchy of infinities is correct\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite"
      ],
      "metadata": {
        "id": "rniZL8bpenwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFaA3R5k-Wi"
      },
      "source": [
        "**Primzahlen**\n",
        "\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Skewes-Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahlsatz\n",
        "\n",
        "*Riemannsche Vermutung (Primzahlverteilung & Zeta-Funktion)*\n",
        "\n",
        "* Die Verteilung der Primzahlen ist sehr merkw√ºrdig und damit interessant. So zeigt die Verteilung von Primzahlen in (relativ) kurzen Intervallen eine gewisse ‚ÄûZuf√§lligkeit‚Äú, w√§hrend andererseits beliebig lange Intervalle existieren, die keine Primzahl enthalten.\n",
        "\n",
        "* Bernhard Riemann setzte sich in seiner Arbeit ‚ÄûUeber die Anzahl der Primzahlen unter einer gegebenen Gr√∂sse‚Äú (1859) zum Ziel, die Verteilung der Primzahlen mit analytischen Methoden zu bestimmen, stie√ü dabei auf Riemannsche Œ∂-Funktion und formulierte die Riemannsche Vermutung. Basierend auf den Riemannschen Ideen gelang 1896 der Beweis des Primzahlsatzes, mit dem man f√ºr gro√üe Zahlen x mit immer gr√∂√üerer relativer Genauigkeit sagen kann, wieviele Primzahlen ‚â§ x es gibt.\n",
        "\n",
        "* Will man diese Anzahlen noch genauer wissen, so kommt man schnell in einen Bereich mathematischer Fragestellungen mit zahlreichen offenen Problemen, z. B. den Goldbach-Problemen oder Fragen √ºber Primzahlzwillinge.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/primzahlverteilung/8085\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Vermutung\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Zetafunktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDNzWru2eKpK"
      },
      "source": [
        "##### <font color=\"blue\">*Topology*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Topology*"
      ],
      "metadata": {
        "id": "uOwaDiWZDTxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Homological_algebra"
      ],
      "metadata": {
        "id": "4-rErV2smYu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantum-journal.org/views/qv-2023-01-26-70/"
      ],
      "metadata": {
        "id": "pRRqjqbIK3yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology (TDA)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1416.png)\n"
      ],
      "metadata": {
        "id": "0l5UtGJTKpJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1417.png)\n"
      ],
      "metadata": {
        "id": "U13kdlWrKrds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1418.png)\n"
      ],
      "metadata": {
        "id": "wVu6RaMlKsPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1419.png)\n"
      ],
      "metadata": {
        "id": "ud88megeKtzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1420.png)\n"
      ],
      "metadata": {
        "id": "_hlWriNLKu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1421.png)\n"
      ],
      "metadata": {
        "id": "2b-HFVA1KwWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1422.png)\n"
      ],
      "metadata": {
        "id": "9m7jXEzgKxIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1423.png)\n"
      ],
      "metadata": {
        "id": "_mR6uRUtKx_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Free abelian groups: https://youtu.be/xPeeFp_Hd3A\n",
        "\n",
        "simplicial complexes: https://youtu.be/Uq4dTjHfLpI\n",
        "\n",
        "computing homology groups: https://youtu.be/YNBi4Ix3cY0\n",
        "\n",
        "computing more homology groups: https://youtu.be/l7QWg0UzBRA\n",
        "\n",
        "betti numbers: https://youtu.be/NgrIPPqYKjQ\n",
        "\n",
        "persistent homology: https://youtu.be/ktKCzMmDXDk\n",
        "\n",
        "first isomorphism theorem: https://youtu.be/2kmIHyD8zTk\n",
        "\n",
        "isomorphic graphs: https://youtu.be/EwV4Puk2coU\n",
        "\n",
        "cohomoly and forms: https://youtu.be/2ptFnIj71SM\n",
        "\n",
        "differential forms: https://youtu.be/CYz_s82JnY8\n",
        "\n",
        "covectors and one-forms: https://youtu.be/ziD8ewQjaf4"
      ],
      "metadata": {
        "id": "gNk02DYZ_HXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Homology explained via Graph and Group Theory**\n",
        "\n",
        "* Find the nullspace of the matrix, then we can find all the cycles (min 29)\n",
        "\n",
        "* The number of vectors in the spanning set of the null space of the reduced matrix representation would be the number of generating cycles\n",
        "\n",
        "* A [free abelian group](https://en.m.wikipedia.org/wiki/Free_abelian_group) is an abelian group (commutative) with a basis.\n",
        "\n",
        "Source: [An introduction to homology | Algebraic Topology | NJ Wildberger](https://www.youtube.com/watch?v=ShWdSNJeuOg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1391.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1392.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1394.png)\n",
        "\n",
        "Now adding a higher dimensional disc:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1395.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1396.png)\n",
        "\n",
        "Continue: https://www.youtube.com/watch?v=2wn10l9qbJI&list=PL6763F57A61FE6FE8&index=36&t=1362s\n",
        "\n",
        "Playlist: https://www.youtube.com/playlist?list=PL6763F57A61FE6FE8"
      ],
      "metadata": {
        "id": "QdlLIADIS-QI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeX5czO1VjRi"
      },
      "source": [
        "**Simplicial Homology (Application of Quotient Groups)**\n",
        "\n",
        "> <font color=\"blue\">**Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence**\n",
        "\n",
        "[Simplicial Homology: On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://m0nads.wordpress.com/tag/persistent-homology/)\n",
        "\n",
        "*Since Bn is a subgroup of Zn, we may form the quotient group Hn = Zn/Bn -> so Modulo (=Restwerte) ist dann die Dimension der L√∂cher (=invarianten)*\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_07.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_10.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_08.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_09.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_06.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.intmath.com/differential-equations/3-integrable-combinations.php"
      ],
      "metadata": {
        "id": "xWw52dxv_93x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple examples of homology:\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Graph_homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology"
      ],
      "metadata": {
        "id": "iP7S89gV1zb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In biology, [homology](https://en.m.wikipedia.org/wiki/Homology_(biology)) is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles **are all derived from the same ancestral tetrapod structure**.\n",
        "\n",
        "https://de.wiktionary.org/wiki/homolog\n",
        "\n",
        "Biologie: **von einer gemeinsamen Urform ableitbar**.\n",
        "\n",
        "Worttrennung: ho¬∑mo¬∑log, keine Steigerung\n",
        "\n",
        "Beispiel: Der Fl√ºgelbug des Vogels ist unserem Handgelenk homolog.\n",
        "\n",
        "https://de.wiktionary.org/wiki/Homologie\n",
        "\n",
        "Biologie: √úbereinstimmung von biologischen Strukturen (Organe, Makromolek√ºle et cetera) oder Verhaltensanteilen hinsichtlich ihrer Entwicklungsgeschichte und nicht in Hinsicht auf ihre Funktion"
      ],
      "metadata": {
        "id": "XvGbLiHsv5na"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FhQOJufFMx0"
      },
      "source": [
        "**Homology**\n",
        "\n",
        "* [Homology](https://en.m.wikipedia.org/wiki/Homology_(mathematics)) is a general way of **associating a sequence of algebraic objects, such as abelian groups (or modules), to other mathematical objects such as topological spaces.**\n",
        "\n",
        "* **Homology Groups**: Homology groups were originally defined in algebraic topology. Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry. See [Construction of homology groups](https://en.m.wikipedia.org/wiki/Homology_(mathematics)#Construction_of_homology_groups)\n",
        "\n",
        "* In homology we want to Linearize the equivalence relation! Homology and cohomology are linear theories (easier to compute, and methods of linear algebra applicable) (though in the process you loose a bit information\n",
        "\n",
        "\n",
        "* Homology counts components, holds, voids etc.\n",
        "\n",
        "* In topology we define something called homology for simplicial complexes.\n",
        "\n",
        "* **Homology of a simplicial complex is computable via linear algebra**.\n",
        "\n",
        "* Die Homologie ist ein mathematischer Ansatz, die Existenz von L√∂chern zu formalisieren.\n",
        "\n",
        "* Gewisse ‚Äûsehr feine‚Äú L√∂cher sind f√ºr die Homologie unsichtbar; hier kann u. U. auf die schwerer zu bestimmenden Homotopiegruppen zur√ºckgegriffen werden.\n",
        "\n",
        "1. define set of vertices V0 (=vector space V0 with basis given by the set of vertices) and set of edges V1\n",
        "\n",
        "2. linearize equivalence relation: instead of looking at ‚Äûtail of edge is equivalent to head of edge‚Äú, we consider the difference between these two\n",
        "\n",
        "  * **e $\\mapsto$ h(e) - t(e)** is the linear combination of two vertices, and hence an element of e zero! e wird abgebildet auf dem Element (h(e) - t(e))\n",
        "  * **h(e) $\\equiv$ t(e) mod (im d)** das heisst: h(e) ist identisch zu t(e) modulo dem Bild von d\n",
        "\n",
        "* The linear map d : V1 -> V0 sends an edge e to (h(e) - t(e))\n",
        "* e is an edge which is a basis element inside V1\n",
        "* So the image of d (difference) is just the supspace of V0 generated by these elements\n",
        "* Generate [equivalence relation](https://en.m.wikipedia.org/wiki/Equivalence_relation) (Homomorphiesatz?): impose reflexivity, symmetry and transivity.\n",
        "* So: to get the image of d you look at the subspace generated by e |‚Äî> h(e) - t(e)\n",
        "    * Closure under addition more or less corresponds to transitivity,\n",
        "    * closure under negation corresponds to symmetry, and\n",
        "    * closure under zero corresponds to reflexivity.\n",
        "    * So this precisely linearizes the equivalence relation from before.\n",
        "* Now we have one linear map. But we need two linear maps which composes 0 to get homology or cohomology. There are two ways you can get a composite to get 0 very easily.\n",
        "    * Either start from zero and map to V1 and then go to V0, or\n",
        "    * start at V1 and use d to get to V0 and then go to 0 with the zero map there.\n",
        "* Modular the image of this map which is zero\n",
        "\n",
        "* https://ncatlab.org/nlab/show/homology\n",
        "\n",
        "**Homological Algebra**\n",
        "\n",
        "* [Homological algebra](https://en.m.wikipedia.org/wiki/Homological_algebra) is the study of homological functors and the intricate algebraic structures that they entail; its development was closely intertwined with the emergence of category theory. A central concept is that of chain complexes, which can be studied through both their homology and cohomology.\n",
        "\n",
        "*A diagram used in the snake lemma, a basic result in homological algebra:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Snake_lemma_origin.svg/375px-Snake_lemma_origin.svg.png)\n",
        "\n",
        "\n",
        "* Why using Homological Algebra: **translate a problem of interest into a sequence of ‚Äûhigher‚Äú (=homological algebra) linear algebra problems**\n",
        "\n",
        "* homological algebra is the **study of homological functors** and the intricate algebraic structures that they entail.\n",
        "\n",
        "* One quite useful and ubiquitous concept in mathematics is that of **chain complexes**, which can be studied through both their homology and cohomology.\n",
        "\n",
        "* Homological algebra affords the means to **extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces**, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.\n",
        "\n",
        "**Simplicial Homology**\n",
        "\n",
        "* Die [Simpliziale Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie) ist in der Algebraischen Topologie, einem Teilgebiet der Mathematik, eine Methode, die einem beliebigen Simplizialkomplex **eine Folge [abelscher Gruppen](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe) zuordnet**.\n",
        "\n",
        "* Anschaulich gesprochen z√§hlt sie die L√∂cher unterschiedlicher Dimension des zugrunde liegenden Raumes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph homology**\n",
        "\n",
        "In algebraic topology and graph theory, [graph homology](https://en.m.wikipedia.org/wiki/Graph_homology) describes the homology groups of a graph, where the graph is considered as a topological space. It formalizes the idea of the number of \"holes\" in the graph. It is a special case of a simplicial homology, as a graph is a special case of a simplicial complex. Since a finite graph is a 1-complex (i.e., its 'faces' are the vertices - which are 0-dimensional, and the edges - which are 1-dimensional), the only non-trivial homology groups are the 0-th group and the 1-th group.\n",
        "\n",
        "The 1st homology group\n",
        "\n",
        "Similarly, $\\partial _{1}$ maps any cycle in C1 to the zero element of C0. In other words, the set of cycles in C1 **generates the null space (the kernel)** of $\\partial _{1}$.\n",
        "\n",
        "https://www.youtube.com/watch?v=ShWdSNJeuOg&t=240s"
      ],
      "metadata": {
        "id": "nKnPM0x-6BG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Graph Theory**\n",
        "\n",
        "In mathematics, [topological graph theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory) is a branch of graph theory. It studies the embedding of graphs in surfaces, spatial embeddings of graphs, and graphs as topological spaces.[1] It also studies immersions of graphs.\n",
        "\n",
        "To an undirected graph we may associate an abstract simplicial complex C with a single-element set per vertex and a two-element set per edge.[2] The geometric realization |C| of the complex consists of a copy of the unit interval [0,1] per edge, with the endpoints of these intervals glued together at vertices. In this view, embeddings of graphs into a surface or as subdivisions of other graphs are both instances of topological embedding, homeomorphism of graphs is just the specialization of topological homeomorphism, the notion of a connected graph coincides with topological connectedness, and a connected graph is a tree if and only if its fundamental group is trivial.\n",
        "\n",
        "Other simplicial complexes associated with graphs include the **Whitney complex or clique complex**, with a set per clique of the graph, and the matching complex, with a set per matching of the graph (equivalently, the clique complex of the complement of the line graph). The matching complex of a complete bipartite graph is called a chessboard complex, as it can be also described as the complex of sets of nonattacking rooks on a chessboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvWc8p5p4Qlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph (topology)**\n",
        "\n",
        "In topology, a branch of mathematics, a [graph is a topological space](https://en.m.wikipedia.org/wiki/Graph_(topology)) which arises from a usual graph\n",
        "G\n",
        "=\n",
        "(\n",
        "E\n",
        ",\n",
        "V\n",
        ")\n",
        "{\\displaystyle G=(E,V)} by replacing vertices by points and each edge\n",
        "e\n",
        "=\n",
        "x\n",
        "y\n",
        "‚àà\n",
        "E\n",
        "{\\displaystyle e=xy\\in E} by a copy of the unit interval\n",
        "I\n",
        "=\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "{\\displaystyle I=[0,1]}, where\n",
        "0\n",
        "{\\displaystyle 0} is identified with the point associated to\n",
        "x\n",
        "x and\n",
        "1\n",
        "1 with the point associated to\n",
        "y\n",
        "y. That is, as topological spaces, graphs are exactly the simplicial 1-complexes and also exactly the one-dimensional **CW complexes**."
      ],
      "metadata": {
        "id": "40IB_N2Q6Mk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **path** is a walk with no repeated vertices\n",
        "\n",
        "A closed path is a **cycle**."
      ],
      "metadata": {
        "id": "kSH33jQVs3bM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLoLSdtySVs"
      },
      "source": [
        "**Cycles (ker), boundaries (img) and chains**\n",
        "\n",
        "* Cycles (and its Kern): A cycle is a closed submanifold,\n",
        "\n",
        "* boundary is a cycle which is also the boundary of a submanifold.\n",
        "\n",
        "* Boundary operator on [chains](https://en.m.wikipedia.org/wiki/Chain_(algebraic_topology)) (and its image): The boundary of a chain is the linear combination of boundaries of the simplices in the chain. The boundary of a k-chain is a (k‚àí1)-chain. Note that the boundary of a simplex is not a simplex, but a chain with coefficients 1 or ‚àí1 ‚Äì thus chains are the closure of simplices under the boundary operator.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Chainline.svg/320px-Chainline.svg.png)\n",
        "\n",
        "* *The boundary of a polygonal curve is a linear combination of its nodes; in this case, some linear combination of A1 through A6. Assuming the segments all are oriented left-to-right (in increasing order from Ak to Ak+1), the boundary is A6 ‚àí A1.*\n",
        "\n",
        "* If the 1 -chain $c=t_{1}+t_{2}+t_{3}$ is a path from point $v_{1}$ to point $v_{4},$ where $t_{1}=\\left[v_{1}, v_{2}\\right], t_{2}=\\left[v_{2}, v_{3}\\right]$ and $t_{3}=\\left[v_{3}, v_{4}\\right]$ are its\n",
        "constituent 1 -simplices, then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\partial_{1} c &=\\partial_{1}\\left(t_{1}+t_{2}+t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(t_{1}\\right)+\\partial_{1}\\left(t_{2}\\right)+\\partial_{1}\\left(t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(\\left[v_{1}, v_{2}\\right]\\right)+\\partial_{1}\\left(\\left[v_{2}, v_{3}\\right]\\right)+\\partial_{1}\\left(\\left[v_{3}, v_{4}\\right]\\right) \\\\\n",
        "&=\\left(\\left[v_{2}\\right]-\\left[v_{1}\\right]\\right)+\\left(\\left[v_{3}\\right]-\\left[v_{2}\\right]\\right)+\\left(\\left[v_{4}\\right]-\\left[v_{3}\\right]\\right) \\\\\n",
        "&=\\left[v_{4}\\right]-\\left[v_{1}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Closed_polygonal_line.svg/320px-Closed_polygonal_line.svg.png)\n",
        "\n",
        "* *A closed polygonal curve, assuming consistent orientation, has null boundary. (deswegen f√ºhrt der boundary operator Œ¥1 alle Werte immer in Null, zumindest bei geschlossenen objekten)*\n",
        "\n",
        "* Example 2: The boundary of the triangle is a formal sum of its edges with signs arranged to make the traversal of the boundary counterclockwise.\n",
        "\n",
        "   * Cycle: **A chain is called a cycle when its boundary is zero**.\n",
        "\n",
        "   * Boundary: A chain that is the boundary of another chain is called a boundary.\n",
        "\n",
        "   * **Boundaries are cycles**, so chains form a chain complex, whose homology groups (cycles modulo boundaries) are called simplicial homology groups.\n",
        "\n",
        "* Example 3: A 0-cycle is a linear combination of points such that the sum of all the coefficients is 0. Thus, the 0-homology group measures the number of path connected components of the space.\n",
        "\n",
        "* Example 4: The plane punctured at the origin has nontrivial 1-homology group **since the unit circle is a cycle, but not a boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazhAwJAFmL-"
      },
      "source": [
        "**Chain Complex**\n",
        "\n",
        "A chain complex\n",
        "(\n",
        "A\n",
        "‚àô\n",
        ",\n",
        "d\n",
        "‚àô\n",
        ")\n",
        "(A_{\\bullet },d_{\\bullet }) is a sequence of abelian groups or modules ..., A0, A1, A2, A3, A4, ... connected by homomorphisms (called boundary operators or differentials)\n",
        "\n",
        "> **[Chain Complex](https://en.m.wikipedia.org/wiki/Chain_complex) is a sequence of homomorphism of abelian groups**\n",
        "\n",
        "A chain complex $V$. is a sequence $\\left\\{V_{n}\\right\\}_{n \\in \\mathbb{Z}}$ of abelian groups or modules (for instance yector spaces) or similar equipped with linear maps $\\left\\{d_{n}: V_{n+1} \\rightarrow V_{n}\\right\\}$ such that $d^{2}=0,$ i.e. the composite of two consecutive such maps is the zero morphism $d_{n} \\circ d_{n+1}=0$\n",
        "\n",
        "* https://ncatlab.org/nlab/show/chain+complex\n",
        "\n",
        "* https://ncatlab.org/nlab/show/zero+morphism\n",
        "\n",
        "* https://m0nads.wordpress.com/tag/persistent-homology/\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Simplicial_homology_-_exactness_of_boundary_maps.svg/384px-Simplicial_homology_-_exactness_of_boundary_maps.svg.png)\n",
        "\n",
        "* The boundary of a boundary of a 2-simplex (left) and the boundary of a 1-chain (right) are taken.\n",
        "\n",
        "* Both are 0, being sums in which both the positive and negative of a 0-simplex occur once. The boundary of a boundary is always 0.\n",
        "\n",
        "* A nontrivial cycle is something that closes up like the boundary of a simplex, in that its boundary sums to 0, but which isn't actually the boundary of a simplex or chain.\n",
        "\n",
        "* Because trivial 1-cycles are equivalent to 0 in H1, the 1-cycle at right-middle is homologous to its sum with the boundary of the 2-simplex at left.\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_04.png)\n",
        "\n",
        "\n",
        "[Vergleich mit paper von Krishna 2017]\n",
        "\n",
        "5 kongruent 11 und 17 etc. modulo 3, weil (3 * 1) + 2 = 5, und (3 * 3) + 2 = 11. Reste m√ºssen identisch sein.\n",
        "* **Zp (Kern)**: 5, 11, 17 etc. (alle Objekte aus einer Filtration zB in Cp, die kongruent zueinander sind, **weil deren Differenz ein ganzzahliges Vielfaches von Bp (modulo) ist)**.\n",
        "* **Hp (Hom)**: 2 (= das Loch)\n",
        "* **Bp (Img)**: 3 (modulo)\n",
        "- drei Basiselemente: 0, 1 und 2 (?)\n",
        "\n",
        "Before:\n",
        "\n",
        "5 kongruent 11 modulo 3, weil 3 * 1 + 2 = 5, und 3 * 3 + 2 + 11, Reste m√ºssen identisch sein, sowie 17 etc.\n",
        "\n",
        "- drei Basiselemente: 0, 1 und 2\n",
        "- 3 ist wie Bp, also Image (element des vektorraums, und ist linear, weil zB multiplizier mit Skala bleibt im vektorraum)\n",
        "- Rest ware Hp, also 2 (das Loch)\n",
        "- 5 und 11 und 17 sind Zp (Kern)\n",
        "- Berate alle Objekte aus der Filtration\n",
        "- Ein element aus Zp was plus ein Element aus Bp\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDP2ORM8EGQ"
      },
      "source": [
        "**Exact Sequences**\n",
        "\n",
        "An [exact sequence](https://en.m.wikipedia.org/wiki/Exact_sequence) is a sequence of morphisms between objects (for example, groups, rings, modules, and, more generally, objects of an abelian category) such that the image of one morphism equals the kernel of the next.\n",
        "\n",
        "In the context of group theory, a sequence\n",
        "\n",
        "> $G_{0} \\stackrel{f_{1}}{\\longrightarrow} G_{1} \\stackrel{f_{2}}{\\longrightarrow} G_{2} \\stackrel{f_{3}}{\\longrightarrow} \\cdots \\stackrel{f_{n}}{\\longrightarrow} G_{n}$\n",
        "\n",
        "of groups and group homomorphisms is called exact if the image of each homomorphism is equal to the kernel of the next:\n",
        "\n",
        "> $\\operatorname{im}\\left(f_{k}\\right)=\\operatorname{ker}\\left(f_{k+1}\\right)$\n",
        "\n",
        "The sequence of groups and homomorphisms may be either finite or infinite.\n",
        "\n",
        "> **Every exact sequence is a [chain complex](https://en.m.wikipedia.org/wiki/Chain_complex)**\n",
        "\n",
        "**Vergleich mit exact sequence: im‚Å°( f k ) = ker( f k + 1 ) bzw. Zp = Bp mit Hp = 0**\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Illustration_of_an_Exact_Sequence_of_Groups.svg/640px-Illustration_of_an_Exact_Sequence_of_Groups.svg.png)\n",
        "\n",
        "![ccc](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/640px-KerIm_2015Joz_L2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti Numbers (Betti Sequence)**\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Topologische_Invariante\n",
        "\n",
        "* The formal definition of homology uses the language of group theory. (The equivalence class of loops surrounding a hole have a group structure.) Persistent homology examines these homological features from a multiscale perspective.\n",
        "\n",
        "* Persistent homology is a powerful tool to compute, study and encode efficiently multiscale topological features of nested families of simplicial complexes and topological spaces.\n",
        "\n",
        "* It does not only provide efficient algorithms to compute the Betti numbers of each complex in the considered families, as required for homology inference in the previous section, but also encodes the evolution of the homology groups of the nested complexes across the scales.\n",
        "Im Bereich der algebraischen Topologie sind die Homologien beziehungsweise die **Homologiegruppen Invarianten eines topologischen Raums**.\n",
        "\n",
        "* **Simplicial homology groups and Betti numbers are topological invariants**: if K,K‚Ä≤ are two simplicial complexes whose geometric realizations are homotopy equivalent, then their homology groups are isomorphic and their Betti numbers are the same.\n",
        "\n",
        "* Persistent Homology, a recent breakthrough idea, extends Homology theory to work across a range of parameterized Simplicial Complexes, like the one arising from a point cloud, instead of just a single, isolated complex.\n",
        "It looks for topological invariants across various scales of a topological manifold.\n",
        "\n",
        "* ‚ÄûBut there is a problem. Betti numbers are computationally demanding to calculate, ‚Äúquickly overwhelming even the most powerful classical computers, even for not-so-large data sets,‚Äù\n",
        "\n",
        "* https://www.technologyreview.com/s/610138/a-small-scale-demonstration-shows-how-quantum-computing-could-revolutionize-data-analysis/\n",
        "\n",
        "* In algebraic topology, the Betti numbers are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* **The nth Betti number represents the rank of the nth homology group, denoted Hn, which tells us the maximum amount of cuts that must be made before separating a surface into two pieces or 0-cycles, 1-cycles, etc.[1] These numbers are used today in fields such as simplicial homology, computer science, digital images, etc**.\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Betti_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Homology_(mathematics)\n",
        "\n",
        "* Betti numbers is a compact method to present this Homology Groups by investigating the properties of the topological spaces.\n",
        "\n",
        "* It distinguishes topological spaces according to the connectivity of n-dimensional simplicial complexes. The nth Betti number represents the rank of the nth homology group, denoted as Hn.\n",
        "\n",
        "* Informally, the nth Betti number refers to the number of n-dimensional holes on a topological surface.\n",
        "\n",
        "* Figure 9 shows that the first three Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2- dimensional simplicial complexes: Œ≤0 is the number of connected components Œ≤1 is the number of holes(one-dimensional) and Œ≤2 is the number of two-dimensional \"voids\".\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_437.png)\n",
        "\n",
        "Die Bettizahlen geben an, wie viele k-dimensionale nicht zusammenh√§ngende Fl√§chen der entsprechende topologische Raum hat. Die ersten drei Bettizahlen besagen anschaulich also:\n",
        "\n",
        "* b0 ist die Anzahl der Wegzusammenhangskomponenten* (connected components)\n",
        "\n",
        "* b1 ist die Anzahl der ‚Äûzweidimensionalen L√∂cher‚Äú.\n",
        "\n",
        "* b2 ist die Anzahl der dreidimensionalen Hohlr√§ume.\n",
        "\n",
        "Der unten abgebildete Torus (gemeint ist Oberfl√§che) besteht aus einer Zusammenhangskomponente, hat zwei ‚Äûzweidimensionale L√∂cher‚Äú, zum einen das in der Mitte, zum andern das im Inneren des Torus, und hat einen dreidimensionalen Hohlraum. Die Bettizahlen des Torus sind daher 1, 2, 1, die weiteren Bettizahlen sind 0.\n",
        "\n",
        "Ist der zu betrachtende topologische Raum jedoch keine orientierbare kompakte Mannigfaltigkeit, so versagt diese Anschauung allerdings schon.\n"
      ],
      "metadata": {
        "id": "8v0mv3YqBQGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti number & Rank of a group**\n",
        "\n",
        "* In algebraic topology, the [Betti numbers](https://en.m.wikipedia.org/wiki/Betti_number) are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* The nth Betti number represents the [rank (of a group)](https://en.m.wikipedia.org/wiki/Rank_of_a_group) of the nth [homology group](https://en.m.wikipedia.org/wiki/Homology_(mathematics)), denoted Hn, **which tells us the maximum number of cuts that can be made before separating a surface into two pieces or 0-cycles, 1-cycles**, etc.\n",
        "\n",
        "* The first few Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2-dimensional simplicial complexes:\n",
        "\n",
        "  * b0 is the number of (connected) components;\n",
        "  * b1 is the number of one-dimensional or \"circular\" holes;\n",
        "  * b2 is the number of two-dimensional \"voids\" or \"cavities\".\n",
        "\n",
        "* for example, a torus has one connected surface component so b0 = 1, two \"circular\" holes (one equatorial and one meridional) so b1 = 2, and a single cavity enclosed within the surface so b2 = 1.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/5/54/Torus_cycles.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvR1-z919rUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oP2QkFIRXLX"
      },
      "source": [
        "**Homology Group & Betti Number**\n",
        "\n",
        "Let $\\sigma=\\left(v_{0}, \\ldots, v_{k}\\right)$ be an oriented $k$ -simplex, viewed as a basis element of $C_{k}$. The boundary operator\n",
        "\n",
        "$\n",
        "\\partial_{k}: C_{k} \\rightarrow C_{k-1}\n",
        "$\n",
        "\n",
        "is the homomorphism defined by:\n",
        "\n",
        "$\n",
        "\\partial_{k}(\\sigma)=\\sum_{i=0}^{k}(-1)^{i}\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "where the oriented simplex\n",
        "\n",
        "$\n",
        "\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "is the $I^{\\text {th }}$ face of $\\sigma,$ obtained by deleting its $i^{\\text {th }}$ vertex.\n",
        "In $C_{k},$ elements of the subgroup\n",
        "\n",
        "$\n",
        "Z_{k}:=\\operatorname{ker} \\partial_{k}\n",
        "$\n",
        "\n",
        "are referred to as cycles, and the subgroup\n",
        "\n",
        "$\n",
        "B_{k}:=\\operatorname{im} \\partial_{k+1}\n",
        "$\n",
        "\n",
        "is said to consist of boundaries.\n",
        "\n",
        "The $k^{\\text {th }}$ homology group $H_{k}$ of $S$ is defined to be the [quotient abelian group](https://en.m.wikipedia.org/wiki/Quotient_group)\n",
        "\n",
        "$\n",
        "H_{k}(S)=Z_{k} / B_{k}\n",
        "$\n",
        "\n",
        "* It follows that the **homology group $H_{k}(S)$ is nonzero exactly when there are $k-$\n",
        "cycles on $S$ which are not boundaries**. In a sense, this means that there are $k$ -\n",
        "dimensional holes in the complex.\n",
        "\n",
        "* For example, consider the complex $S$ obtained by gluing two triangles (with no interior) along one edge, shown in the image. The edges of each triangle can be oriented so as to form a cycle. These two cycles are by construction not boundaries (since every 2 -chain is zero).\n",
        "\n",
        "* One can compute that the homology group $\\mathrm{H}_{1}(\\mathrm{S})$ is isomorphic to $\\mathrm{Z}^{2}$, with a basis given by the two cycles mentioned. This makes precise the informal idea that $S$ has two \"1-\n",
        "dimensional holes\".\n",
        "\n",
        "* Holes can be of different dimensions. The rank of the $k$ th homology group, the\n",
        "number\n",
        "\n",
        "$\n",
        "\\beta_{k}=\\operatorname{rank}\\left(H_{k}(S)\\right)\n",
        "$\n",
        "\n",
        "**is called the $k$ th Betti number of $S$. It gives a measure of the number of $k$ - dimensional holes in $S$.**\n",
        "\n",
        "**A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is \"not there\".**\n",
        "\n",
        "* An homology class Hn (which represents a hole) is an [equivalence class](https://en.m.wikipedia.org/wiki/Equivalence_class) of\n",
        "    * [cycles](https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles) Ker(Œ¥) modulo boundaries Im(Œ¥) bzw.\n",
        "    * h(e) $\\equiv$ t(e) mod (im d)\n",
        "\n",
        "* Ker(Œ¥) kann beschrieben werden: h(e) $\\equiv$ t(e) bzw. e $\\mapsto$ h(e) - t(e) or: ‚àÇn‚àí1 ‚ó¶ ‚àÇn = 0\n",
        "\n",
        "* This is the linear combination of two vertices, and hence an element of e zero\n",
        "\n",
        "* Get the image of d bzw. Im(Œ¥) when you look at the subspace generated by e $\\mapsto$ h(e) - t(e)\n",
        "\n",
        "*  Im(Œ¥): boundaries?\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles\n",
        "\n",
        "https://ncatlab.org/nlab/show/boundary+of+a+simplex\n",
        "\n",
        "https://ncatlab.org/nlab/show/chain+map\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental group**\n",
        "\n",
        "In the mathematical field of algebraic topology, the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) of a topological space is the group of the equivalence classes under homotopy of the loops contained in the space. It records information about the basic shape, or holes, of the topological space.\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))\n",
        "\n",
        "If $X$ is a graph and ${\\displaystyle T\\subseteq X}$ a maximal tree, then the fundamental group $\\pi _{1}(X)$ equals the free group generated by elements ${\\displaystyle (f_{\\alpha })_{\\alpha \\in A}}$, where the ${\\displaystyle \\{f_{\\alpha }\\}}$ correspond bijectively to the edges of ${\\displaystyle X\\setminus T}$; in fact, $X$ is homotopy equivalent to a wedge sum of circles."
      ],
      "metadata": {
        "id": "7WtsJdDE9oVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Topological Data Analysis*\n",
        "\n",
        "https://www.spektrum.de/news/topologische-datenanalyse-big-data-fuer-quantencomputer/2128650\n",
        "\n",
        "* Der Physiker hat auch eine Vermutung, weshalb Quantencomputer diese Art von Aufgaben besser meistern. Es k√∂nnte eine bisher unerwartete Verbindung zwischen der Quantenmechanik und der TDA geben: die Supersymmetrie.\n",
        "\n",
        "* Damit der Quantenalgorithmus exponentiell schneller l√§uft als klassische Verfahren (was der √ºblichen Messlatte f√ºr einen Quantenvorteil entspricht), **muss die Anzahl hochdimensionaler L√∂cher in den Datens√§tzen unvorstellbar gro√ü sein ‚Äì in der Gr√∂√üenordnung von Billionen**.\n",
        "\n",
        "* ¬ªSolche Bedingungen sind in der realen Welt nur schwer zu finden¬´, sagt Cade. **Es sei unklar, ob derartige Daten √ºberhaupt existieren**, so Ryan Babbush, einer der Hauptautoren der Google-Studie. \n",
        "\n",
        "* Ewin Tang, die jetzt an der University of Washington promoviert, h√§lt die TDA nicht f√ºr jene praktische Quantenanwendung, nach der die Informatiker suchen. Sie geht davon aus, dass **Quantencomputer am ehesten n√ºtzlich sein werden, um etwas √ºber Quantensysteme zu lernen ‚Äì und nicht, um klassische Daten zu analysieren**. \n",
        "\n",
        "* Ein neuer kreativer Ansatz k√∂nnte jederzeit das schaffen, was Tang und ihren Kollegen bisher nicht gelang: ein effizientes TDA-Verfahren zu entwerfen, das auf gew√∂hnlichen Rechnern l√§uft. ¬ªIch w√ºrde weder mein Haus noch mein Auto oder meine Katze darauf verwetten, dass das nicht passieren wird¬´, sagt Dunjko.\n",
        "\n",
        "https://www.nature.com/articles/ncomms10138"
      ],
      "metadata": {
        "id": "EBdBGIsgZodW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAh4XJMHLDp9"
      },
      "source": [
        "**Persistent Barcode, Diagram & Landscape**\n",
        "\n",
        "* What is the ideal size of d? - Consider all distances d between d1 min (to connect two balls) and d2 max size (all are connected). Each hole appears at a particular value of d and disappears at anotjer value of d. We can represent the persistence of this hole as a pair (d1, d2).\n",
        "* Out of this distance we get a bar. Several holes result in a barcode. Short bars represent noise. Long bars are features.\n",
        "* Persistent barcodes are stable with respect to pertubations if data (Edelsbrunner 2007).\n",
        "* Barcode is computable via linear algebra. Runtime is O (n3), it‚Äòs cubic, where n is the number of simplices (Carlsson 2005).\n",
        "* A barcode is a visualization of an algebraic structure.\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_06.jpg)\n",
        "\n",
        "> **Homology of a simplicial complex is computable via linear algebra**\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_07.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pjdD_O2saB"
      },
      "source": [
        "**Topological Data Analysis + Machine Learning**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/tda_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tropical Geometry*"
      ],
      "metadata": {
        "id": "lh3x3_1K_e1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Sturmfels - Tropical Geometry](https://youtu.be/TNwCzl02uck)"
      ],
      "metadata": {
        "id": "1gdFgCSJfTg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Geometry**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_geometry\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_variety\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/System_of_polynomial_equations\n",
        "\n",
        "Particle physics and cosmology are linked to combinatorics and algebraic geometry in an unexpected way. Novel geometric objects hint at new mathematical structures that challenge our current understanding of the laws of nature. The workshop ‚ÄûPositive Geometry\" on January 23 at the Campus of the Technische Universit√§t M√ºnchen will discuss the latest developments. The event is organized by Bernd Sturmfels, Director at the Max-Planck-Institut f√ºr Mathematik in den Naturwissenschaften, J√ºrgen Richter-Gebert, Chair for Geometry and Visualization at TUM, Johannes Henn, Director at MPP in collaboration with #SFB Transregio 109, Discretization in Geometry and Dynamics.\n",
        "More information: https://Inkd.in/eQVnYtyD\n",
        "particlephysics geometry cosmology"
      ],
      "metadata": {
        "id": "mFfIs1nSCsxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tropical Geometry**\n",
        "\n",
        "http://www.mittag-leffler.se/langa-program/tropical-geometry-amoebas-and-polytopes-0\n",
        "\n",
        "Video: [What is tropical mathematics?](https://youtu.be/DV-8kEn8udY)"
      ],
      "metadata": {
        "id": "hjvAO915_hkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Geometry in Astrophysics*"
      ],
      "metadata": {
        "id": "jZhLaeTPPc7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tropical geometry: https://scitalks.ca/PIRSA/22110028\n",
        "\n",
        "https://arxiv.org/abs/2008.12310\n",
        "\n",
        "https://arxiv.org/abs/2204.06414\n",
        "\n",
        "https://www.pnas.org/doi/10.1073/pnas.0406010101"
      ],
      "metadata": {
        "id": "Gp8_JfPRQ8_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.ias.edu/ideas/2007/algebraic-geometry-interface"
      ],
      "metadata": {
        "id": "f7v-yPRLQW_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quora.com/Which-branch-of-astrophysics-deals-with-geometry\n",
        "\n",
        "Some physicists tend to claim that differential geometry is not important anywhere except for general relativity where it is inevitable. Partially it‚Äôs true because you can do a lot of physics, even with underlying geometrical structure, without mentioning the geometry explicitly. On the other modern theoretical physics is using geometrical language in more and more areas. Sometimes it gives you really a new insight which is not seen without geometry, more often it is just an elegant way how to describe the things.\n",
        "\n",
        "Leaving the string theory aside (I am not expert on that), these are some usual applications of differential geometry.\n",
        "\n",
        "Lagrangian mechanics\n",
        "Lagrange‚Äôs formalism is essentially a geometrical thing, although it is not usually discussed on the abstract level in introductory courses. The picture is that physical particles live in Euclidean space and each is described by three Cartesian coordinates. This already gives you larger configuration space which is 3N-dimensional, where N is the number of particles. However, if your particles are subject to constraints (e.g., particle can move only along a circle), you configuration space is reduced (e.g. to a circle, instead of Euclidean plane) and it is actually a submanifold of 3N-dimensional space. What you do in Lagrange‚Äôs formalism is that you write the equations of motion solely in terms of the coordinates on this submanifold.\n",
        "\n",
        "On this submanifold some interesting structures arise. The velocity of particle is tangent to the submanifold so it is useful to introduce the tangent bundle (i.e. set of all tangent spaces at each points of the submanifold) and Lagrangian mechanics can be geometrically formulated on this bundle. It even makes sense to introduce a metric: it is simply a metric induced by embedding of submanifold into Euclidean space. Physically, the norm of any vector of the tangent bundle is related to the kinetic energy.\n",
        "\n",
        "2. Hamilton‚Äôs mechanics\n",
        "\n",
        "This is even more interesting. It turns out to be useful to interpret the momenta (canonically conjugate to coordinates) as the co-vectors, i.e. as the elements of cotangent bundle. The structure of Hamilton‚Äôs equations can be reformulated in terms of the so-called symplectic form. Transformations preserving the form of Hamilton‚Äôs equations are called symplectomorphisms. The symplectic form plays the role of the metric, although it is antisymetric rather than symmetric and symplectic geometry (as opposed to Riemannian geometry) is a very interesting area. It gives you deeper insight into the Legendre transformation (which relates Lagrangian and Hamiltonian picture), for example.\n",
        "\n",
        "4. Lie groups\n",
        "\n",
        "Lie groups are continuous transformations under which something is symmetric. In quantum mechanics and quantum field theory, it is important to formulate the field equations which are compatible with equations of special relativity. In order to understand this in detail, you need a differential geometry and group theory. In this language, physical fields form a representation space for the Lorentz (Poincare) group. Representations of Lorentz group are labelled by spin (roughly), so for example spin 0 corresponds to particular representation of Lorentz group which gives you Klein-Gordon equation, similarly for spin 1 (Maxwell or Proca equations), and for spin 1/2, 3/2 (Dirac, Rarita-Schwinger). This is related to the next point.\n",
        "\n",
        "3. Spinors\n",
        "\n",
        "In quantum mechanics we need to describe particles with spin. As I said, field for spin 1/2 particle is a particular representation of the Lorentz group. This representation has a funny property that rotation by  360‚àò\n",
        "360\n",
        "‚àò\n",
        "  does not act like an identity, by like minus identity. Objects which have this property are called spinors. Dirac discovered them by intuition and guessing, but the real understanding of spinors comes from differential geometry. You start with the definition of the so-called Clifford algebra which is kind of generalization of exterior algebra of differential forms. Clifford algebra is (in certain sense) adapted to the metric and hence is suitable for study of orthogonal transformations. What is known in physics as Dirac‚Äôs  Œ≥\n",
        "Œ≥\n",
        " -matrices arises again from some representations of Clifford algebra (or Clifford groups).\n",
        "\n",
        "4. Gauge theories\n",
        "\n",
        "It turns out that physical fields have (beside the Lorentz invariance) also another symmetry known as the gauge symmetry. When you have a free field described by a Lagrangian and you want to let it interact with another field, requiring the gauge invariance you can derive more-less uniquely correct Lagrangian for interacting fields. This was discovered by physicists but later they realized that these gauge symmetries can be conveniently described in geometrical language. We understand the gauge symmetry as some kind of internal symmetry. To each point of the spacetime we attach a copy of the space (called fibre) representing the internal states at that point. Hence, we obtain a structure called principal G-bundle where G is the group acting on the fibres. The interaction between the two fields then arises as the covariant derivative on principal G-bundle. So, differential geometry is very efficient tool for studying these things.\n",
        "\n",
        "There are many more applications, for example in thermodynamics (in TD you discuss all the time which infinitesimals are exact differentials and which are not; this can be conveniently expressed as the integrability of distributions which can be decided by Frobeniu‚Äôs theorem), in condensed matter physics (for example, graphene is quite popular today; there is a connection between the properties of graphene crystals and Riemannian geometry), etc. I have enumerated just 4 examples which came to my mind at the moment and which belong to fairly standard physics."
      ],
      "metadata": {
        "id": "RzgJ1aLePgqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hopf Fibration*"
      ],
      "metadata": {
        "id": "GssEWmSFaZdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map of 4D sphere into 3D sphere\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Hopf-Faserung\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Principal_bundle\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Group_action#Orbit-stabilizer_theorem\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Villarceau-Kreise\n",
        "\n",
        "https://youtu.be/-lZTnEgbVvI\n",
        "\n",
        "https://youtu.be/nsHcKO7HvFY"
      ],
      "metadata": {
        "id": "I5Di4JSmaeeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Graph Theory*"
      ],
      "metadata": {
        "id": "f4lHqJyMa24i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=M9F7zT9Gg-k\n",
        "\n",
        "https://www.youtube.com/watch?v=skaXrTtQyJA\n",
        "\n",
        "https://www.youtube.com/watch?v=2h12m-3zQ0M\n",
        "\n",
        "https://www.youtube.com/watch?v=-Afa1WI3iug"
      ],
      "metadata": {
        "id": "X13DUDyLt9db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Graph Theory**\n",
        "\n",
        "* In [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory), the [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) of a [graph](https://en.m.wikipedia.org/wiki/Graph_(discrete_mathematics)) is the characteristic polynomial of its [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix).\n",
        "\n",
        "* Spectral graph theory is the study of the properties of a graph in relationship to the **characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph**, such as its adjacency matrix or [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix) = Edge Matrix - Adjacency Matrix\n",
        "\n",
        "* One of the reasons that the eigenvalues of matrices have meaning is that they arise as the solution to natural optimization problems\n",
        "\n",
        "> Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen. (The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**.)\n",
        "\n",
        "\n",
        "\n",
        "http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf\n",
        "\n",
        "https://www.cs.cmu.edu/~venkatg/teaching/15252-sp20/notes/Spectral-graph-theory.pdf"
      ],
      "metadata": {
        "id": "Zxze3ZZQpmmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian matrix:**\n",
        "\n",
        "> $L(G) = D - A = B^T B \\quad$ for some matrix $B$\n",
        "\n",
        "* $L$ = [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix)\n",
        "* $D$ = [degree matrix](https://en.m.wikipedia.org/wiki/Degree_matrix), entries contain degrees of each vertex\n",
        "* $A$ = [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix): 1 if edge, 0 if no edge\n",
        "* $B$ = the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "**Description of Laplacian matrix**\n",
        "* Laplacian matrix is an operator mapping from graph to the real numbers\n",
        "* Diagonals have degrees of vertices\n",
        "* Off-diagonal contain edges marked as -1\n",
        "* Contains all information about graph (you can recreate the graph just from laplacian).\n",
        "* And you can nicely apply linear algebra to analyse the graph\n",
        "\n",
        "**Properties of Laplacian matrix:**\n",
        "1. L(G) is symmetric\n",
        "2. L(G) has:\n",
        "  * real-valued, non-negative eigenvalues and\n",
        "  * real-valued, orthogonal eigenvectors\n",
        "* Is always positive-semidefinite matrix\n",
        "* $\\lambda$ = 0 is always an Eigenvalue with Eigenvector 1 (because all rows add to zero and multiplied by vector with 1‚Äòs leads to zero)\n"
      ],
      "metadata": {
        "id": "yk02m201n6n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of calculating Eigenvalues and Eigenvectors of Laplacian matrix**\n",
        "* second smallest Eigenvalue (**spectral gap**) is a solution (i.e. minimal energy) for many problems, if computable.\n",
        "* You can use the spectrum of the Laplacian matrix to solve\n",
        "  * **optimization problems** (i.e. shortest path, Hamiltonain circuit), like traveling salesman problem\n",
        "  * **graph partitioning problem** (optimal cut). reveal sparse connections and where there are more clusters, find bottlenecks (in social media it relates to communities, in networks it's a critical point\n",
        "  * both problems are np-hard\n",
        "* In physics: **Fundamental modes (harmonics)** are given by the Eigenvectors of the graph laplacian [Spectral Partitioning Part 3 Algebraic Connectivity](https://www.youtube.com/watch?v=Vng9lkibGEE)"
      ],
      "metadata": {
        "id": "9MumAIvqp8cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 1: Graph laplacian spectrum, meaning it's eigenvalues, tells us something about the underlying connectivity of the graph**\n",
        "* = Graph has k connected components if and only if the k-smallest Eigenvalues are identically zero: $\\lambda_0$ = $\\lambda_1$ = .. $\\lambda_{k-1}$ = 0 (Fiedler 1973).\n",
        "* Spectrum of L(G) $\\rightleftharpoons$ Connectivity of G\n",
        "* = The number of (connected) components in the graph is the dimension of the nullspace (=kernel / set of solutions) of the Laplacian and the algebraic multiplicity (=how many times the same Eigenvalue) of the 0 eigenvalue.\n",
        "* Laplacian: its Eigenvectors, which do not rotate, you can see an ellipse like section of space they draw.\n",
        "* **Spectrograph theory asked questions about this ellipse to determine graph behavior, what the fastest way from dot-to-dot is**, how many pathways there are dot to dot [Source](https://www.youtube.com/watch?v=njatNunHC_o)\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem **Ellipsoid** (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)"
      ],
      "metadata": {
        "id": "cZ4ROEDbn9zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 2: $L(G)$ contains $B^T B$ for some matrix $B$**\n",
        "* the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "> $\\min _{y \\in \\Re^{\\prime \\prime}} f(y)=\\sum_{(i, j) \\in E^{\\prime}}\\left(y_i-y_j\\right)^2=y^T L y$\n",
        "\n",
        "* [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient) with Rayleigh theorem:\n",
        "  * $\\lambda_2=\\min f(y)$ : The minimum value of $f(y)$ is given by the $2^{\\text {nd }}$ smallest eigenvalue $\\lambda_2$ of the Laplacian matrix $\\boldsymbol{L}$\n",
        "  * $\\mathrm{x}=\\arg \\min _{\\mathrm{y}} \\boldsymbol{f}(\\boldsymbol{y})$ : The optimal solution for $y$ is given by the corresponding eigenvector $\\boldsymbol{x}$, referred as the [Fiedler vector (Algebraic_connectivity)](https://en.m.wikipedia.org/wiki/Algebraic_connectivity)\n",
        "\n",
        "* see how to create Laplacian matrix from incidence matrix: [Spectral Partitioning, Part 1 The Graph Laplacian](https://www.youtube.com/watch?v=rVnOANM0oJE)\n",
        "* B captures relationship between different nodes and edges, rows are nodes and columns are edges of G (pair start-sink)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1390.png)\n",
        "\n",
        "Source: [(Lecture 14) Graph Laplacians](https://www.youtube.com/watch?v=M9F7zT9Gg-kI)\n",
        "\n",
        "**How can you use this property? - The number of cut edges equals $\\frac{1}{4} x^T L(G) x$ under specific constraints**.\n",
        "  * Means: if you want to minimize edge cuts, you should try minimizing this product (it‚Äôs a combinatorial optimization problem).\n",
        "  * Cut edges: find minimal number of connections, bottleneck, borders between two clusters.\n",
        "  * This problem is however np complete. You need to relax sum contraints.\n",
        "\n",
        "*If you relax it you can combine it with the [Courant-Fisher minimax theorem (Satz von Courant-Fischer)](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer):*\n",
        "* if we allowed to use any vector y, where y is normalized in a certain way, and it‚Äôs elements sum to 0, then the vector y that minimizes this quantity is actually q1.\n",
        "* And q1 is the Eigenvector corresponding to the second smallest Eigenvalue.\n",
        "* And in fact the minimum value simplifies to something that is proportional to that eigenvalue.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1388.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1387.png)"
      ],
      "metadata": {
        "id": "Nw6nUk4bb4ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application example: Problem statement for using the Laplacian matrix of a graph:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1384.png)\n",
        "\n",
        "Source: [Lecture 30 ‚Äî The Graph Laplacian Matrix (Advanced) | Stanford University](https://www.youtube.com/watch?v=FRZvgNvALJ4)\n",
        "\n",
        "\n",
        "Sum over all neighbouring edges:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1385.png)\n",
        "\n",
        "\n",
        "**Solution siehe** [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), um den zweitkleinsten Eigenwert der Laplacian matrix zu finden (spectral gap):\n",
        "\n",
        "> $(R_{A}(x)=) \\quad \\lambda_2 = {min \\frac {x^{T} M x}{x^{T}x}}$ with matrix $M$ being the Laplacian matrix $L$\n",
        "\n",
        "Meaning of (min $x^T L x$): I take the label of one endpoint edge, subtract the value of the other endpoint of an edge, then square it up, and sum this over all the edges. (see also in green on the bottom why quadratic forms are relevant here!)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1381.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1382.png)\n",
        "\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1383.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "> **Note that the largest eigenvalue of the adjacency matrix corresponds to the smallest eigenvalue of the Laplacian.** But: Where the smallest eigenvector of the Laplacian is a constant vector, the largest eigenvector of an adjacency matrix, called the Perron vector, need not be. The Perron-Frobenius theory tells us that the largest eigenvector of an adjacency matrix is non-negative, and that its value is an upper bound on the absolute value of the smallest eigenvalue. These are equal precisely when the graph is bipartite.\n"
      ],
      "metadata": {
        "id": "CYNJpwY7BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Quantum Application with Graphs: https://medium.com/quandela/exploring-graph-problems-with-single-photons-and-linear-optics-4f3d5848add8"
      ],
      "metadata": {
        "id": "Yye9sR8Zq-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph & Graph Theory**\n",
        "\n",
        "* In [discrete mathematics](https://en.m.wikipedia.org/wiki/Discrete_mathematics), and more specifically in [graph theory](https://en.m.wikipedia.org/wiki/Graph_theory), a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\".\n",
        "\n",
        "* The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called link or line).\n"
      ],
      "metadata": {
        "id": "b-7FPIb42kSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Graph Theory**\n",
        "\n",
        "* [Algebraic graph theory](https://en.m.wikipedia.org/wiki/Algebraic_graph_theory) is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches.\n",
        "\n",
        "* There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.\n",
        "\n",
        "* **Using linear algebra**:\n",
        "\n",
        "  * The first branch of algebraic graph theory involves the study of graphs in connection with linear algebra. Especially, it studies the [spectrum (Eigendecomposition of a matrix)](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the adjacency matrix, or the Laplacian matrix of a graph - this part of algebraic graph theory is also called [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory).\n",
        "\n",
        "  * **Several theorems relate properties of the spectrum to other [graph properties (invariant)](https://en.m.wikipedia.org/wiki/Graph_property).** As a simple example, a connected graph with diameter D will have at least D+1 distinct values in its spectrum. Aspects of graph spectra have been used in analysing the synchronizability of networks.\n",
        "\n",
        "* Video: [Daniel Spielman ‚ÄúMiracles of Algebraic Graph Theory‚Äù](https://www.youtube.com/watch?v=CDMQR422LGM)\n",
        "\n",
        "* Video: [The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry and Physics](https://www.youtube.com/watch?v=8XJes6XFjxM)"
      ],
      "metadata": {
        "id": "sg75zioGELCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Types of Graphs*"
      ],
      "metadata": {
        "id": "9ELeT7od--k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete graph**\n",
        "\n",
        "* In graph theory, a [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge.\n",
        "\n",
        "* A complete digraph is a directed graph in which every pair of distinct vertices is connected by a pair of unique edges (one in each direction).\n",
        "\n",
        "*K7, a complete graph with 7 vertices with $n$ vertices and ${\\displaystyle \\textstyle {\\frac {n(n-1)}{2}}}$ edges*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Complete_graph_K7.svg/245px-Complete_graph_K7.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "suoc9GoR3FCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete bipartite graph**\n",
        "\n",
        "* [Complete bipartite graph](https://en.m.wikipedia.org/wiki/Complete_bipartite_graph) (or biclique), a special [bipartite graph](https://en.m.wikipedia.org/wiki/Bipartite_graph) where every vertex on one side of the bipartition is connected to every vertex on the other side\n",
        "\n",
        "* A complete bipartite graph with m = 5 and n = 3:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Biclique_K_3_5.svg/320px-Biclique_K_3_5.svg.png)\n",
        "\n",
        "Bipartite Graph https://www.youtube.com/watch?v=HqlUbSA9cEY\n",
        "\n"
      ],
      "metadata": {
        "id": "p0YwqBrL3qCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directed vs Undirected Graphs**\n",
        "\n",
        "The applications for directed graphs are many and varied. They can be used to analyze electrical circuits, develop project schedules, find shortest routes, analyze social relationships, and construct models for the analysis and solution of many other problems.\n",
        "\n",
        "https://faculty.cs.niu.edu/~freedman/241/241notes/241graph.htm\n",
        "\n",
        "\n",
        "Undirected graphs are more restrictive kinds of graphs. They represent only whether or not a relationship exists between two vertices. They don't however represent a distinction between subject and object in that relationship. One type of graph can sometimes be used to approximate the other.\n",
        "\n",
        "https://www.baeldung.com/cs/graphs-directed-vs-undirected-graph"
      ],
      "metadata": {
        "id": "V0ExwcX52H9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hamiltonian path**\n",
        "\n",
        "A [Hamiltonian path](https://en.m.wikipedia.org/wiki/Hamiltonian_path) (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle around a network of six vertices:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/b/be/Hamiltonian.png)"
      ],
      "metadata": {
        "id": "E26jLergoS4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Graph Properties*"
      ],
      "metadata": {
        "id": "oAiyy65E-8Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Length** is the number of edges in a graph.\n",
        "\n",
        "A **closed walk** occurs when x = y.\n",
        "\n",
        "A **trail** is a walk with no repeated edges.\n",
        "\n",
        "A closed trail is a **circuit**.\n",
        "\n",
        "A **path** is a walk with no repeated vertices (A path is a sequence of distinct edges you can follow through the graph)\n",
        "\n",
        "A closed path is a **cycle** (and circuit)\n",
        "\n",
        "A **simple graph** is loop-free, undirected and has no multiple edges.\n",
        "\n",
        "[INTRODUCTION to GRAPH THEORY - DISCRETE MATHEMATICS](https://www.youtube.com/watch?v=HkNdNpKUByM)\n",
        "\n",
        "[Introduction to Graph Theory: A Computer Science Perspective](https://www.youtube.com/watch?v=LFKZLXVO-Dg)\n"
      ],
      "metadata": {
        "id": "yOjUf8ACsQ8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wichtige Zyklen und Pfade in der Graphentheorie**\n",
        "\n",
        "* [**Eulerian path** (Eulerkreisproblem)](https://de.m.wikipedia.org/wiki/Eulerkreisproblem) = every edge only once\n",
        "  * **Euler path** (every edge once only, vertices can be multiple: exists if either 0 or 2 odd degree vertices exist with all else even degree\n",
        "  * **Euler circuit** (more constraint): same as Euler path, but return to starting point (das Haus vom weihachtsmann). Use also Fleurys algorithm: works if all vertices have even degrees\n",
        "\n",
        "* [**Hamiltonian circuit / cycle** (Hamiltonian path problem)](https://de.m.wikipedia.org/wiki/Hamiltonkreisproblem): every vertex only once\n",
        "  * every vertex only once, but edges don‚Äòt matter how often. And return to starting point.\n",
        "  * **Minimum cost hamiltonian circuit = [traveling salesman problem](https://de.m.wikipedia.org/wiki/Problem_des_Handlungsreisenden).**. Is np-complete. Look for [shortest path](https://de.m.wikipedia.org/wiki/K√ºrzester_Pfad), siehe auch [Pathfinding](https://de.m.wikipedia.org/wiki/Pathfinding)\n",
        "  * https://www.quora.com/Why-are-quantum-computers-unable-to-solve-the-travelling-salesman-problem-in-polynomial-time: *Quantum computation offers speed improvements for some very specialized problems like integer factorization, but it isn‚Äôt known or expected to offer polynomial-time algorithms for generic NP-complete problems like the Traveling Salesman Problem. If it does then NP ‚äÜ BQP, and most experts don‚Äôt regard this as likely at all $^{*}$*.\n",
        "  * Heuristic algorithms as alternative to brute force:\n",
        "    * [Dijkstra algorithmus](https://de.m.wikipedia.org/wiki/Dijkstra-Algorithmus)\n",
        "    * Nearest neighbor algorithm is a heuristic, non optimal, but feasible - [greedy algorithm](https://de.m.wikipedia.org/wiki/Greedy-Algorithmus), cheapest route\n",
        "    * Repeated Nearest neighbor algorithm\n",
        "    * Sorted edges algorithm\n",
        "    * Kruskal algorithm (spanning tree) for minimum cost spanning tree (optimal and efficient). For example energy lines\n",
        "\n",
        "If you have a fully connected [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) and and want to compute unique Hamiltonain circuits:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1389.png)\n",
        "\n",
        "Source: [Graph theory full course for Beginners](https://www.youtube.com/watch?v=sWsXBY19o8I)\n",
        "\n",
        "$^{*}$ *Why can‚Äôt a quantum computer solve NP complete problems in a polynomial running time (relative to their input)?\n",
        "Quantum computers, like classical computers, are not currently believed to be able to solve NP-complete problems in polynomial time.\n",
        "\n",
        "Contrary to Kurt Behnke‚Äôs answer, this is not true by definition. It‚Äôs a profound open question in mathematics.\n",
        "\n",
        "As Vadim Yakovlevich correctly points out, it‚Äôs partly a matter of no one having found a polynomial-time quantum algorithm for NP-complete problems after decades of trying‚Äîjust like no one has found a polynomial-time classical algorithm.\n",
        "\n",
        "But it‚Äôs possible to say more than that. From reading popular articles, many people have the vague impression that a quantum computer could just try every possible solution in parallel. And if that‚Äôs all you know about them, then it‚Äôs indeed a mystery why they couldn‚Äôt solve NP-complete problems in polynomial time!\n",
        "\n",
        "The central difficulty is that, for a computer to be useful, at some point you need to measure it to observe an answer. And if you just measured an equal superposition over all possible answers, not having done anything else, the rules of QM dictate that you‚Äôll just see a random answer. And of course, if you‚Äôd just wanted a random answer, you could‚Äôve picked one yourself with a lot less trouble!\n",
        "\n",
        "So the name of the game, with every quantum algorithm, is somehow orchestrating a pattern of constructive and destructive quantum interference that boosts the probability of the correct answer (even though you yourself don‚Äôt know in advance which answer is the correct one!), and that does so efficiently. Famously, in 1994 Peter Shor showed how to do that for the problem of factoring integers, and a few related problems in number theory‚Äîbut he was able to do so only by exploiting extremely special structure in those problems.\n",
        "\n",
        "For more ‚Äúgeneric‚Äù search problems, like NP-complete problems, the best we generally know is Grover‚Äôs algorithm: a quantum algorithm that‚Äôs able to find the right answer (i.e., concentrate a large fraction of the amplitude on that answer) in roughly the square root of the number of steps that would be needed classically. So you get some speedup, but not an exponential one.\n",
        "\n",
        "But we also know that, if your search problem is a ‚Äúblack box‚Äù‚Äîi.e., you can just pick candidate solutions and evaluate if they‚Äôre correct, and you don‚Äôt know anything more about the problem‚Äôs structure‚Äîthen Grover‚Äôs algorithm is the best that even a quantum computer can do. This is the content of the so-called BBBV (Bennett, Bernstein, Brassard, Vazirani) Theorem, which has several proofs, all of them crucially relying on the linearity of unitary evolution. The BBBV Theorem gives a fundamental explanation for why any fast quantum algorithm for NP-complete problems would have to look very different from anything that we know today‚Äîmuch like a fast classical algorithm would have to*."
      ],
      "metadata": {
        "id": "6M6WUYqnfgAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph Properties (Invariants)**\n",
        "\n",
        "* A [graph property (invariant)](https://en.m.wikipedia.org/wiki/Graph_property) is a property of graphs that depends only on the abstract structure, not on graph representations such as particular labellings or drawings of the graph.\n",
        "\n",
        "* An example graph, with the properties of\n",
        "\n",
        "  * being [planar](https://en.m.wikipedia.org/wiki/Planar_graph) (it can be drawn on the plane in such a way that its edges intersect only at their endpoints / no edges cross each other)\n",
        "\n",
        "  * and being [connected](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)),\n",
        "\n",
        "  * and with order 6,\n",
        "\n",
        "  * size 7,\n",
        "\n",
        "  * [diameter (distance](https://en.m.wikipedia.org/wiki/Distance_(graph_theory)) 3 (number of edges in a shortest path, also called a graph geodesic, connecting them)\n",
        "\n",
        "  * [girth](https://en.m.wikipedia.org/wiki/Girth_(graph_theory)) 3 (girth of an undirected graph is the length of a shortest cycle contained in the graph)\n",
        "\n",
        "  * vertex [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) 1, and\n",
        "\n",
        "  * [degree sequence](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (3, 3, 3, 2, 2, 1):\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/333px-6n-graf.svg.png)"
      ],
      "metadata": {
        "id": "03Ad29FMyOx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph enumeration**\n",
        "\n",
        "* In combinatorics, an area of mathematics, [graph enumeration](https://en.m.wikipedia.org/wiki/Graph_enumeration) describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph.\n",
        "\n",
        "* These problems may be solved either exactly (as an [algebraic enumeration](https://en.m.wikipedia.org/wiki/Algebraic_enumeration) problem) or asymptotically.\n",
        "\n",
        "* The number of labeled n-vertex simple undirected graphs is $2^{\\frac{n(n‚Ää‚àí1)}{2}}$\n",
        "\n",
        "* The number of labeled n-vertex simple directed graphs is $2^{n(n‚Ää‚àí1)}$\n",
        "\n",
        "* **Number of connected labeled graphs with n nodes**: The number of distinct connected labeled graphs with n nodes is tabulated in the [On-Line Encyclopedia of Integer Sequences](https://en.m.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences) as sequence [A001187](https://oeis.org/A001187). The first few non-trivial terms are\n",
        "\n",
        "$\\begin{array}{|l|l|}\n",
        "\\hline {\\text { n }} & {\\text { graphs }} \\\\\n",
        "\\hline 1 & 1 \\\\\n",
        "\\hline 2 & 1 \\\\\n",
        "\\hline 3 & 4 \\\\\n",
        "\\hline 4 & 38 \\\\\n",
        "\\hline 5 & 728 \\\\\n",
        "\\hline6 & 26704 \\\\\n",
        "\\hline 7 & 1866256 \\\\\n",
        "\\hline 8 & 251548592 \\\\\n",
        "\\hline\n",
        "\\end{array}$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9u6fmPYO1EZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genus of a Graph**\n",
        "\n",
        "[genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (plural genera) has a few different, but closely related, meanings. Intuitively, the genus is the number of \"holes\" of a surface.[1] A sphere has genus 0, while a torus has genus 1.\n",
        "\n",
        "A genus-2 surface:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Double_torus_illustration.png/219px-Double_torus_illustration.png)\n",
        "\n",
        "**The genus of a graph is the minimal integer n such that the graph can be drawn without crossing itself on a sphere with n handles** (i.e. an oriented surface of the genus n). Thus, a planar graph has genus 0, because it can be drawn on a sphere without self-crossing.\n",
        "\n"
      ],
      "metadata": {
        "id": "4eb_PjW92TEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph embedding**\n",
        "\n",
        "In topological graph theory, an [embedding](https://en.m.wikipedia.org/wiki/Graph_embedding) (also spelled imbedding) of a graph G on a surface $\\Sigma$ is a representation of G on $\\Sigma$ in which points of $\\Sigma$ are associated with vertices and simple arcs (homeomorphic images of $[0,1])$ are associated with edges in such a way that:\n",
        "\n",
        "the endpoints of the arc associated with an edge $e$ are the points associated with the end vertices of e, no arcs include points associated with other vertices, two arcs never intersect at a point which is interior to either of the arcs.\n",
        "\n",
        "> Often, an embedding is regarded as an equivalence class (under homeomorphisms of $\\Sigma$) of representations of the kind just described."
      ],
      "metadata": {
        "id": "UiscJVPa4BY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spanning tree**\n",
        "\n",
        "A [spanning tree](https://en.m.wikipedia.org/wiki/Spanning_tree) T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/4x4_grid_spanning_tree.svg/252px-4x4_grid_spanning_tree.svg.png)\n",
        "\n",
        "A special kind of spanning tree, the Xuong tree, is used in topological graph theory to find graph embeddings with maximum [genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (is the number of \"holes\" of a surface)"
      ],
      "metadata": {
        "id": "Z72Y0JpX169N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of random walks on graphs**\n",
        "\n",
        "Cover times: A cover time is the number of steps needed for a random walk to visit all vertices in a given graph.\n",
        "\n",
        "http://uu.diva-portal.org/smash/get/diva2:319078/FULLTEXT01.pdf"
      ],
      "metadata": {
        "id": "ChVvi_4nAOHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of graphs and the Gaussian free field**\n",
        "\n",
        "Max point of a gaussian free field is the max number of time steps to visit each vertex in a graph\n",
        "\n",
        "https://tcsmath.wordpress.com/2010/12/09/open-question-cover-times-and-the-gaussian-free-field/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Gaussian_free_field\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/9/93/Discrete_Gaussian_free_field_on_60_x_60_square_grid.png)"
      ],
      "metadata": {
        "id": "6gIbaAJjACax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit rank**\n",
        "\n",
        "* [Circuit rank](https://en.m.wikipedia.org/wiki/Circuit_rank), cyclomatic number, cycle rank, or nullity of an undirected graph is the **minimum number of edges that must be removed from the graph to break all its cycles, making it into a tree or forest**.\n",
        "\n",
        "* It is equal to the number of independent cycles in the graph (the size of a cycle basis).\n",
        "\n",
        "* This graph has circuit rank r = 2 because it can be made into a tree by removing two edges, for instance the edges 1‚Äì2 and 2‚Äì3, but removing any one edge leaves a cycle in the graph:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/320px-6n-graf.svg.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wJ0FciA8LKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Degree**\n",
        "\n",
        "* Degree of a matrix = how many neighbours in each vertex?\n",
        "\n",
        "* [Degree](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (or valency) of a vertex of a graph is the number of edges that are incident to the vertex; in a multigraph, a loop contributes 2 to a vertex's degree, for the two ends of the edge.\n",
        "\n",
        "* The maximum degree of a graph G, denoted by $\\Delta (G)$, and the minimum degree of a graph, denoted by ${\\displaystyle \\delta (G)}$, are the maximum and minimum of its vertices' degrees. In the multigraph shown on the right, the maximum degree is 5 and the minimum degree is 0.\n",
        "\n",
        "* In a [regular graph](https://en.m.wikipedia.org/wiki/Regular_graph) (=each vertex has the same number of neighbors), every vertex has the same degree, and so we can speak of the degree of the graph.\n",
        "\n",
        "* A [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) (denoted $K_{n}$, where n is the number of vertices in the graph) is a special kind of regular graph where all vertices have the maximum possible degree, $n-1$.\n",
        "\n",
        "* Degree sum formula states that, given a graph $G=(V,E)$: ${\\displaystyle \\sum _{v\\in V}\\deg(v)=2|E|\\,}$ (Handshaking lemma)\n",
        "\n",
        "*Two non-isomorphic graphs with the same degree sequence (3, 2, 2, 2, 2, 1, 1, 1):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Conjugate-dessins.svg/240px-Conjugate-dessins.svg.png)"
      ],
      "metadata": {
        "id": "iMnZwQUq6F8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Connected) Component (isolated subgraph) = Betti number $B_0$**\n",
        "\n",
        "* The number of (connected) [components](https://en.m.wikipedia.org/wiki/Component_(graph_theory)) of a topological space is an important topological invariant, the zeroth Betti number $B_0$, and the number of components of a graph is an important graph invariant, and **in topological graph theory it can be interpreted as the zeroth Betti number of the graph**.\n",
        "\n",
        "* The number of components arises in other ways in graph theory as well: **In algebraic graph theory number of components equals the multiplicity of 0 as an eigenvalue of the Laplacian matrix of a finite graph**.\n",
        "\n",
        "* bzw: The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**. (Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen.) Taken from: https://de.m.wikipedia.org/wiki/Spektrum_(Graphentheorie)\n",
        "\n",
        "* Let G be a d-regular graph. The algebraic multiplicity of eigenvalue 0 for the Laplacian matrix is exactly 1 iff G is connected. https://math.uchicago.edu/~may/REU2013/REUPapers/Marsden.pdf\n",
        "\n",
        "* https://www.sciencedirect.com/science/article/pii/S0024379518300156"
      ],
      "metadata": {
        "id": "0tvO21EovW33"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgO45hovFfpL"
      },
      "source": [
        "**Connected Components**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Component_(graph_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Connected_space\n",
        "* How the graph can be used to capture connected component\n",
        "* Any graph gives us a topological space (like joining a square as a connected component)\n",
        "* Graph can capture connected components via putting an equivalence relation on the set of vertices.\n",
        "* **equivalence relation is generated by tail of an edge (vertex 1) is equivalent to a head of an edge (vertex 2) / between the vertices of an edge (which together forms a connected component)**.\n",
        "* To generate equivalence relation we have to make sure that we enforce reflexivity, symmetry and transivity.\n",
        "* You get equivalence classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a graph**\n",
        "\n",
        "* the [rank of an undirected graph](https://en.m.wikipedia.org/wiki/Rank_(graph_theory)) has two unrelated definitions. Let n equal the number of vertices of the graph.\n",
        "\n",
        "* In the [matrix theory](https://en.m.wikipedia.org/wiki/Matrix_(mathematics)) of graphs the rank r of an undirected graph is defined as the rank of its adjacency matrix. Analogously, the [nullity](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) of the graph is the nullity of its adjacency matrix, which equals n ‚àí r.\n",
        "\n",
        "* In the [matroid theory](https://en.m.wikipedia.org/wiki/Matroid) of graphs the rank of an undirected graph is defined as the number n ‚àí c, where c is the number of connected components of the graph.\n",
        "\n",
        "  * Equivalently, the rank of a graph is the rank of the oriented [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) associated with the graph.\n",
        "\n",
        "  * Analogously, the [nullity of the graph](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) is the [nullity (Kernel)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) of its oriented incidence matrix, given by the formula m ‚àí n + c, where n and c are as above and m is the number of edges in the graph.\n",
        "\n",
        "  * The nullity is equal to the first [Betti number](https://en.m.wikipedia.org/wiki/Betti_number) of the graph. The sum of the rank and the nullity is the number of edges.\n"
      ],
      "metadata": {
        "id": "Hju_JaTe8iLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity** (Menger's theorem, max-flow min-cut)\n",
        "\n",
        "* In mathematics and computer science, [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) is one of the basic concepts of graph theory: **it asks for the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more [isolated subgraphs (= Connected component)](https://en.m.wikipedia.org/wiki/Component_(graph_theory)).**\n",
        "\n",
        "* It is closely related to the theory of network flow problems. The connectivity of a graph is an important measure of its resilience as a network.\n",
        "\n",
        "* One of the most important facts about connectivity in graphs is [Menger's theorem](https://en.m.wikipedia.org/wiki/Menger%27s_theorem): for any two vertices u and v in a connected graph G, the numbers Œ∫(u, v) and Œª(u, v) can be determined efficiently using the [max-flow min-cut theorem](https://en.m.wikipedia.org/wiki/Max-flow_min-cut_theorem) algorithm.\n",
        "\n",
        "* The vertex- and edge-connectivities of a disconnected graph are both 0.\n",
        "\n",
        "* 1-connectedness is equivalent to connectedness for graphs of at least 2 vertices.\n",
        "\n",
        "* The [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) on n vertices has edge-connectivity equal to n ‚àí 1. Every other simple graph on n vertices has strictly smaller edge-connectivity.\n",
        "\n",
        "\n",
        "*This graph becomes disconnected when the right-most node in the gray area on the left is removed:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/389px-Network_Community_Structure.svg.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8L5-Pd_FzCPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique**\n",
        "\n",
        "* Eine [Clique](https://de.m.wikipedia.org/wiki/Clique_(Graphentheorie)) bezeichnet in der Graphentheorie **eine Teilmenge von Knoten in einem ungerichteten Graphen, bei der jedes Knotenpaar durch eine Kante verbunden ist**.\n",
        "\n",
        "* Zu entscheiden, ob ein Graph eine Clique einer bestimmten Mindestgr√∂√üe enth√§lt, wird [Cliquenproblem](https://de.m.wikipedia.org/wiki/Cliquenproblem) genannt und gilt, wie das Finden von gr√∂√üten Cliquen, als algorithmisch schwierig (NP-vollst√§ndig).\n",
        "\n",
        "* Das Finden einer Clique einer bestimmten Gr√∂√üe in einem Graphen ist ein [NP-vollst√§ndiges Problem](https://de.m.wikipedia.org/wiki/NP-Vollst√§ndigkeit) (= schwierigsten Problemen in der Klasse NP geh√∂rt, also sowohl in NP liegt als auch NP-schwer ist) und somit auch in der Informationstechnik ein relevantes Forschungs- und Anwendungsgebiet.\n",
        "\n",
        "*Ein Graph mit einer Clique der Gr√∂√üe 3:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/6n-graf-clique.svg/350px-6n-graf-clique.svg.png)\n"
      ],
      "metadata": {
        "id": "3CZpsY2coXHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique complex (Whitney complexes)**\n",
        "\n",
        "[Clique complexes](https://en.m.wikipedia.org/wiki/Clique_complex), independence complexes, flag complexes, Whitney complexes and conformal hypergraphs are closely related mathematical objects in graph theory and geometric topology that each describe the cliques (complete subgraphs) of an undirected graph.\n",
        "\n",
        "Siehe auch: [Topological Graph Theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory)"
      ],
      "metadata": {
        "id": "NENSroXr7Oh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CW Complex**\n",
        "\n",
        "A [CW complex](https://en.m.wikipedia.org/wiki/CW_complex) (also called cellular complex or cell complex) is a kind of a topological space that is particularly important in algebraic topology.[1] It was introduced by J. H. C. Whitehead[2] to meet the needs of homotopy theory. This class of spaces is broader and has some better categorical properties than simplicial complexes, but still retains a combinatorial nature that allows for computation (often with a much smaller complex). The C stands for \"closure-finite\", and the W for \"weak\" topology\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))"
      ],
      "metadata": {
        "id": "XdMET4Yk8PJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract simplicial complex**\n",
        "\n",
        "In combinatorics, an [abstract simplicial complex](https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex) (ASC), often called an abstract complex or just a complex, is a family of sets that is closed under taking subsets, i.e., every subset of a set in the family is also in the family. It is a purely combinatorial description of the geometric notion of a simplicial complex.[1]\n",
        "\n",
        "For example, in a 2-dimensional simplicial complex, the sets in the family are the triangles (sets of size 3), their edges (sets of size 2), and their vertices (sets of size 1).\n",
        "\n",
        "Geometric realization of a 3-dimensional abstract simplicial complex:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Simplicial_complex_example.svg/247px-Simplicial_complex_example.svg.png)\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Let G be an undirected graph. The [clique complex](https://en.m.wikipedia.org/wiki/Clique_complex) (flag complexes, Whitney complexes) of G is an ASC whose faces are all cliques (complete subgraphs) of G. The independence complex of G is an ASC whose faces are all independent sets of G (it is the clique complex of the complement graph of G). Clique complexes are the prototypical example of flag complexes. A flag complex is a complex K with the property that every set of elements that pairwise belong to faces of K is itself a face of K.\n",
        "\n",
        "* Let M be a metric space and Œ¥ a real number. The [Vietoris‚ÄìRips complex](https://en.m.wikipedia.org/wiki/Vietoris‚ÄìRips_complex) is an ASC whose faces are the finite subsets of M with diameter at most Œ¥. It has applications in homology theory, hyperbolic groups, image processing, and mobile ad hoc networking. It is another example of a flag complex (clique complex).\n",
        "\n"
      ],
      "metadata": {
        "id": "wAytmxq5-zzX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbqQuo7L2du"
      },
      "source": [
        "**Simplices**\n",
        "\n",
        "A [simplex](https://de.m.wikipedia.org/wiki/Simplex_(Mathematik)) consists of 3 components: vertices, edges and faces\n",
        "\n",
        "* 0-simplex: point\n",
        "* 1-simplex: edge (line)\n",
        "* 2-simplex: 3 connected points\n",
        "* 3-simplex: solid (3 dimensions)\n",
        "\n",
        "k-simplex is k-dimensional is formed using (k+1) vertices ('convex hull')\n",
        "\n",
        "* Complete graphs (every vertice is connected to all others) can be interpreted as simplices\n",
        "\n",
        "* you can interpret any graph as simplicial complex\n",
        "\n",
        "**Simplicial Complex**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex\n",
        "\n",
        "* Mit einem Simplizialkomplex k√∂nnen die entscheidenden Eigenschaften von triangulierbar topologischen R√§umen algebraisch charakterisiert werden k√∂nnen\n",
        "\n",
        "* Warum? Definition von Invarianten im topologischen Raum. Simplicial complexes can be seen as higher dimensional generalizations of neighboring graphs.\n",
        "\n",
        "* Wie? Untersuchung eines topologischen Raums durch Zusammenf√ºgen von Simplizes womit eine Menge im d-dimensionalen euklidischen Raum konstruiert wird, die [hom√∂omorph](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist zum gegebenen topologischen Raum.\n",
        "\n",
        "* Die ‚ÄûAnleitung zum Zusammenbau‚Äú der Simplizes, das hei√üt die Angaben dar√ºber, wie die Simplizes zusammengef√ºgt sind, wird dann in Form einer Sequenz von [Gruppenhomomorphismen](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus) rein algebraisch charakterisiert.\n",
        "\n",
        "* Cells can have various dimensions: vertices, edges, triangles, tetrahedra and their higher dimensional analogues. complexes reflect the correct topology of the data\n",
        "\n",
        "* If we glue many simplices together in such a way that the intersection is also a simplex (along an edge for example), we obtain a simplicial complex. If we see three points connected by edges that form a triangle, we fill in the triangle with a 2-dimensional face. Any four points that are all pairwise connected get filled in with a 3-simplex etc. The resulting simplicial complex is called (Vietoris) Rips complex.\n",
        "\n",
        "* The persistence diagram is not computed directly from LÙè∞ëŒµ. Instead, one forms an object called a Cech complex. Simplicial and cubical complexes are examples of cell complexes. Cech complex is an example of a simplicial complex. - in practice, often used Vietoris-Rips complex VŒµ - the persistent homology defined by VŒµ approximates the persistent homology defined by CŒµ.\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_03.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hSqsZ5LLdvz"
      },
      "source": [
        "**Triangulation**\n",
        "\n",
        "* In der Topologie ist eine [Triangulierung](https://en.m.wikipedia.org/wiki/Triangulation_(topology)) oder Triangulation eine **Zerlegung eines Raumes in Simplizes** (Dreiecke, Tetraeder oder deren h√∂her-dimensionale Verallgemeinerungen).\n",
        "\n",
        "* Mannigfaltigkeiten bis zur dritten Dimension sind stets triangulierbar.\n",
        "\n",
        "* Urspr√ºngliche Motivation f√ºr die Hauptvermutung war der Beweis der topologischen Invarianz kombinatorisch definierter Invarianten wie der [simplizialen Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie). Trotz des Scheiterns der Hauptvermutung lassen sich Fragen dieser Art oftmals mit dem [simplizialen Approximationssatz](https://de.m.wikipedia.org/wiki/Simpliziale_Approximation) beantworten.\n",
        "\n",
        "* Triangulation ist eine Zerlegung eines Raumes in Simplizes (Dreiecke, Tetraeder oder h√∂her-dimensionale Verallgemeinerungen) to tease out properties of manifolds\n",
        "\n",
        "* **A triangulation of a topological space X is a simplicial complex K, homeomorphic to X, together with a homeomorphism h: K ‚Üí X**\n",
        "\n",
        "* triangulation offers a concrete way of visualizing spaces that are difficult to see, and helps computing an invariant.\n",
        "\n",
        "* Ist gegeben durch einen (abstrakten) Simplizialkomplex K und Hom√∂omorphismus h : | K | ‚Üí X der geometrischen Realisierung | K | auf X.\n",
        "https://en.wikipedia.org/wiki/Triangulation_(topology)\n",
        "\n",
        "* a two-dimensional sphere (surface of a solid ball) can be approximated by gluing together two-dimensional triangles, and a three-dimensional sphere can be approximated by gluing together three-dimensional tetrahedra.\n",
        "\n",
        "* Triangles and tetrahedra are examples of more general shapes called simplices, which can be defined in any dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRA0yDZJGOjK"
      },
      "source": [
        "**[Filtration](https://de.m.wikipedia.org/wiki/Filtrierung_(Mathematik)) & Inclusion Maps**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtered_algebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtration_(mathematics)\n",
        "\n",
        "* **With increasing size of d, we are dealing with a sequence of simplicial complexes, each a sub-complex of the next. That is, a simplicial complex constructed from data for some small distance is a subset of the simplicial complex constructed for a larger distance**.\n",
        "\n",
        "* Equally there is an **[inclusion map](https://en.m.wikipedia.org/wiki/Inclusion_map) from each simplicial complex to the next**.\n",
        "\n",
        "* **This sequence of simplicial complexes, with inclusion maps, is called a filtration**.\n",
        "\n",
        "* When we apply homology to a filtration, we obtain an algebraic structure called persistent modul.\n",
        "\n",
        "* So if we want to compute **ith homology with coefficients from a field k**.\n",
        "\n",
        "* The **homology of any complex Cj is a vector space**, and the **inclusion maps between complexes induce linear maps between homology vector spaces**.\n",
        "\n",
        "* The direct sum of the homology vector spaces is an algebraic module - in fact a **graded module over the polynomial ring** k[x]. The variable x acts as a **shift map**, taking each homology generator to its image in the next vector space.\n",
        "\n",
        "* Furthermore, a structure theorem tells us that **a persistent module decomposes nicely into a direct sum of simple modules**, each corresponding to a bar in the barcode. This means: a barcode reall is an algebraic structure.\n",
        "\n",
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS9TpoyGEmjj"
      },
      "source": [
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)\n",
        "\n",
        "![cscsvs](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_02.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Geometry**"
      ],
      "metadata": {
        "id": "u6Cz4-k-vvpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Lie Algebra*"
      ],
      "metadata": {
        "id": "Q-zfzmbO4ZOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lie Groups Based Machine Learning\n",
        "- Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems\n",
        "https://lnkd.in/e-r-ChbX\n",
        "- Structure preserving deep learning\n",
        "https://lnkd.in/gPghU6x\n",
        "- Lie Group Cohomology and (Multi)Symplectic Integrators: New Geometric Tools for Lie Group Machine Learning Based on Souriau Geometric Statistical Mechanics\n",
        "https://lnkd.in/dCPaWBU\n",
        "\n",
        "more information at SEE GSI'23:\n",
        "https://gsi2023.org/\n"
      ],
      "metadata": {
        "id": "WQFsTGz04fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From: [QHack 2022: Marco Cerezo ‚ÄîBarren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)"
      ],
      "metadata": {
        "id": "imfDlpfHFb7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Theory**\n",
        "\n",
        "> ‚ÄúThe essential phenomenon of Lie theory is that one may associate in a natural way to a Lie group $\\mathcal{G}$ its Lie algebra $\\mathfrak{g}$. The Lie algebra $\\mathfrak{g}$ is first of all a vector space and secondly is endowed with a bilinear nonassociative product called the Lie bracket [...]. **Amazingly, the group $\\mathcal{G}$ is almost completely determined by $\\mathfrak{g}$ and its Lie bracket**. Thus for many purposes <font color=\"blue\">**one can replace $\\mathcal{G}$ with $\\mathfrak{g}$. Since $\\mathcal{G}$ is a complicated nonlinear object and $\\mathfrak{g}$ is just a vector space, it is usually vastly simpler to work with $\\mathfrak{g}$**</font>. [...] This is one source of the power of Lie theory.\" Stillwell: ‚Äúthe miracle of Lie theory‚Äù. (*https://arxiv.org/pdf/1812.01537.pdf*)\n",
        "\n",
        "* Article about [Lie-Gruppe](https://de.m.wikipedia.org/wiki/Lie-Gruppe)\n",
        "\n",
        "* See also [Lie algebra representation](https://en.m.wikipedia.org/wiki/Lie_algebra_representation) and [representation of a Lie group](https://en.m.wikipedia.org/wiki/Representation_of_a_Lie_group) (a linear action of a Lie group on a vector space). Representations play an important role in the study of continuous symmetry.\n",
        "\n",
        "* See this very good paper: [A micro Lie theory for state estimation in robotics](https://arxiv.org/abs/1812.01537)\n",
        "\n",
        "* Lie theory for the roboticist: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "* https://github.com/artivis/manif/blob/devel/paper/Lie_theory_cheat_sheet.pdf"
      ],
      "metadata": {
        "id": "NoSEZYMD4ijj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Group**\n",
        "\n",
        "* A [Lie group](https://en.wikipedia.org/wiki/Lie_group) is a group that is also a **differentiable manifold**. A manifold is a space that locally resembles Euclidean space, whereas groups define the abstract, generic concept of **multiplication and the taking of inverses (division)**. Combining these two ideas, one obtains a continuous group where points can be multiplied together, and their inverse can be taken.\n",
        "\n",
        "* Lie groups provide a natural model for the concept of **continuous symmetry**, a celebrated example of which is the rotational symmetry in three dimensions (given by the special orthogonal group ${\\text{SO}}(3)$).\n",
        "\n",
        "* Lie groups were first found by studying matrix subgroups\n",
        "$G$ contained in ${\\text{GL}}_{n}(\\mathbb {R} )$ or ${\\text{GL}}_{n}(\\mathbb {C} )$, the groups of $n\\times n$ invertible matrices over $\\mathbb {R}$  or $\\mathbb {C}$ (now called the classical groups).\n",
        "\n",
        "* Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations."
      ],
      "metadata": {
        "id": "1a43uSa04kxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of Lie groups**\n",
        "\n",
        "> [Table of some common Lie groups and their associated Lie algebras](https://en.m.wikipedia.org/wiki/Table_of_Lie_groups)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_01.png)\n",
        "\n",
        "* **Der Einheitskreis in der komplexen Zahlenebene**, d. h. die Menge $S^{1}=\\{z \\in \\mathbb{C}:|z|=1\\}$ der komplexen Zahlen vom Betrag 1, ist eine Untergruppe von $\\left(\\mathbb{C}^{*}, \\cdot\\right)$, die sogenannte **Kreisgruppe**: Das Produkt zweier Zahlen vom Betrag 1 hat wieder Betrag 1, ebenso das Inverse. Auch hier hat man eine $_{n}$ mit der Differentialrechnung vertr√§gliche Gruppenstruktur\", d. h. eine Lie-Gruppe.\n",
        "\n",
        "* **Die Menge $\\mathbb{C}^{*}=\\mathbb{C} \\backslash\\{0\\}$ der komplexen Zahlen ungleich 0 bildet mit der gew√∂hnlichen Multiplikation eine Gruppe $\\left(\\mathbb{C}^{*}, \\cdot\\right)$**. Die Multiplikation ist eine differenzierbare Abbildung $m: \\mathbb{C}^{*} \\times \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ definiert durch $m(x, y)=x y_{i}$ auch die durch $i(z)=z^{-1}=\\frac{1}{z}$ definierte Inversion $i: \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ ist differenzierbar. Die Gruppenstruktur der komplexen Ebene (bzgl. Multiplikation) ist also mit der Differentialrechnung vertr√§glich.\n",
        "\n",
        "* [Beispiele fur Lie-Gruppen](https://de.wikipedia.org/wiki/Lie-Gruppe#Beispiele) sind:  allgemeine lineare Gruppe,  Orthogonale Gruppe,  Unit√§re Gruppe & Spezielle Unit√§re Gruppe,  Affine Gruppe, [Poincar√©-Gruppe](https://en.m.wikipedia.org/wiki/Poincar√©_group), Galilei-Gruppe\n",
        "\n",
        "* **Simple Lie Groups**: a simple Lie group is a connected non-abelian Lie group G which does not have nontrivial connected normal subgroups. The list of simple Lie groups can be used to read off the list of simple Lie algebras and [Riemannian symmetric spaces](https://en.wikipedia.org/wiki/Symmetric_space). See also [Classification of semisimple Lie algebras](https://en.wikipedia.org/wiki/Dynkin_diagram#Classification_of_semisimple_Lie_algebras)\n"
      ],
      "metadata": {
        "id": "V9P7SfTo4m6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Algebra**\n",
        "\n",
        "* a [Lie algebra](https://en.m.wikipedia.org/wiki/Lie_algebra) **is a vector space $g$ together with an operation called the Lie bracket**, an alternating bilinear map $\\mathfrak{g} \\times \\mathfrak{g} \\rightarrow \\mathfrak{g},(x, y) \\mapsto[x, y]$, that satisfies the Jacobi identity.\n",
        "\n",
        "* The vector space $\\mathfrak{g}$ together with this operation is a non-associative algebra, meaning that the Lie bracket is not necessarily associative.\n",
        "\n",
        "> <font color=\"blue\">**Any Lie group gives rise to a Lie algebra, which is its tangent space at the identity.**\n",
        "\n",
        "* In physics, Lie groups appear as symmetry groups of physical systems, and their Lie algebras (tangent vectors near the identity) may be thought of as infinitesimal symmetry motions. Thus Lie algebras and their representations are used extensively in physics, notably in quantum mechanics and particle physics.\n",
        "\n",
        "* [Lie Algebra](https://de.m.wikipedia.org/wiki/Lie-Algebra) ist eine algebraische Struktur, die mit einer Lie-Klammer versehen ist, d. h. es existiert eine antisymmetrische Verkn√ºpfung, die die Jacobi-Identit√§t erf√ºllt.\n",
        "\n",
        "* Lie-Algebren werden haupts√§chlich zum Studium geometrischer Objekte wie Lie-Gruppen und differenzierbarer Mannigfaltigkeiten eingesetzt.\n",
        "\n",
        "* **Simple Lie Algebra**: a [simple Lie algebra](https://en.wikipedia.org/wiki/Simple_Lie_algebra) is a Lie algebra that is nonabelian and contains no nonzero proper ideals. The classification of real simple Lie algebras is one of major achievements of Wilhelm Killing and √âlie Cartan. A direct sum of simple Lie algebras is called a semisimple Lie algebra. A simple Lie group is a connected Lie group whose Lie algebra is simple.\n",
        "\n",
        "* **Semisimple Lie algebra**: a [Lie algebra is semisimple](https://en.wikipedia.org/wiki/Semisimple_Lie_algebra) if it is a direct sum of simple Lie algebras (non-abelian Lie algebras without any non-zero proper ideals)."
      ],
      "metadata": {
        "id": "xcVYeFvX4o4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie-Algebra der Lie-Gruppe**\n",
        "\n",
        "* Die Vektorfelder auf einer glatten Mannigfaltigkeit $M$ bilden mit der Lie-Klammer eine unendlich-dimensionale Lie-Algebra. Die zu einer Lie-Gruppe $G$ geh√∂rende Lie-Algebra $\\mathfrak{g}$ besteht aus dem Unterraum der [links-invarianten](https://de.m.wikipedia.org/wiki/Translationsinvarianz) Vektorfelder auf $G$.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum $T_{e} G$ am neutralen Element $e$ von $G$. Insbesondere gilt also $\\operatorname{dim} G=\\operatorname{dim} \\mathfrak{g}$. Bez√ºglich der LieKlammer $[\\cdot, \\cdot]$ ist der Vektorraum $\\mathfrak{g}$ abgeschlossen.\n",
        "\n",
        "* **Somit ist der Tangentialraum einer Lie-Gruppe $G$ am neutralen Element eine Lie-Algebra. Diese Lie-Algebra nennt man die Lie-Algebra der Lie-Gruppe $G$.**\n",
        "\n",
        "* Zu jeder Lie-Gruppe $G$ mit Lie-Algebra $\\mathfrak{g}$ gibt es eine **Exponentialabbildung exp (exponential map)**: $\\mathfrak{g} \\rightarrow G$. Diese Exponentialabbildung kann man definieren durch $\\exp (A)=\\Phi_{1}(e)$, wobei $\\Phi_{t}$ der Fluss des links-invarianten Vektorfelds $A$ und $e \\in G$ das neutrale Element ist. Falls $G$ eine abgeschlossene Untergruppe der $\\mathrm{GL}(n, \\mathbb{R})$ oder $\\mathrm{GL}(n, \\mathbb{C})$ ist, so ist die so definierte Exponentialabbildung identisch mit der Matrixexponentialfunktion.\n",
        "\n",
        "\n",
        "* Jedes Skalarprodukt auf $T_{e} G=\\mathfrak{g}$ definiert eine $G$ -links-invariante Riemannsche Metrik auf $G$. Im Spezialfall, dass diese Metrik zus√§tzlich auch\n",
        "rechtsinvariant ist, stimmt die Exponentialabbildung der Riemannschen\n",
        "Mannigfaltigkeit $G$ am Punkt $e$ mit der Lie-Gruppen-Exponentialabbildung\n",
        "√ºberein.\n",
        "\n",
        "* Mit der Lie-Gruppe $\\mathrm{SO}(n)$ ist eine Lie-Algebra $\\mathfrak{s o}(n)$ verkn√ºpft, ein Vektorraum mit einem bilinearen alternierenden Produkt (Lie-Klammer), wobei der Vektorraum bez√ºglich der Lie- Klammer abgeschlossen ist.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum am neutralen Element der $\\mathrm{SO}(n)$ (neutrales Element ist die Einheitsmatrix), sodass insbesondere $\\operatorname{dim}_\\mathfrak{s o}(n)=\\operatorname{dim} \\mathrm{SO}(n)$ gilt.\n",
        "\n",
        "* Die Lie-Algebra besteht aus allen schiefsymmetrischen $n \\times n$\n",
        "-Matrizen und ihre Basis sind die sog. Erzeugenden Generators).\n",
        "\n",
        "* Die Exponentialabbildung verkn√ºpft die Lie-Algebra mit der Lie-Gruppe:\n",
        "\n",
        "> $\\exp : \\mathfrak{s o}(n) \\rightarrow \\mathrm{SO}(n), J \\mapsto \\sum_{k=0}^{\\infty} \\frac{1}{k !} J^{k}$\n",
        "\n",
        "Exponentiation in quantum: $\\mathbb{G} = e^\\mathfrak{g}$"
      ],
      "metadata": {
        "id": "YiJEo1Y54q9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why using Lie Algebra?**\n",
        "\n",
        "* For example $\\mathcal{G}$ is all the operations on the surface of a ball (nonlinear): Lie Groups are Continuous Transformation Groups, like a 3D rotation vector on a curved surface. Very complicated to work with. (Lie Algebra SO(3)): $\n",
        "\\boldsymbol{W}^{\\wedge}=[\\boldsymbol{\\omega}]_{\\times}=\\left[\\begin{array}{ccc}\n",
        "0 & -\\omega_{z} & \\omega_{y} \\\\\n",
        "\\omega_{z} & 0 & -\\omega_{x} \\\\\n",
        "-\\omega_{y} & \\omega_{x} & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "* Meanwhile Lie Algebra $\\mathfrak{g}$ is at the origin of the tangent plane which is a linear vector space: Cartesian R3: $\n",
        "\\omega=\\left(\\omega_{x}, \\omega_{y}, \\omega_{z}\\right)\n",
        "$\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algebra\"** $\\rightarrow$ Exponential map translates between both\n",
        "\n",
        "**Lie Groups** were know as \"Continuous Transformation Groups\". **= a group that is also a smooth (differential) manifold**\n",
        "\n",
        "  * **a smooth manifold whose elements satisfy the group axioms**\n",
        "\n",
        "  * (so no singularities or breaks where differentiation or integration wouldn't work anymore)\n",
        "\n",
        "  * each point on a manifold represents one element of the Lie Group (i.e. 3d rotation matrix on the manifold)\n",
        "\n",
        "  * corresponding to it in the cartesian tangent space you can find a 3d rotation vector\n",
        "\n",
        "**Lie Algebra** is the in origin point on manifold (https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s)\n",
        "\n",
        "**Exponential Map** from (cartesian) tangent space to manifold (i.e 3 D surface):\n",
        "\n",
        "  * from tangent space a to manifold ('**exponential of a**'), like the exponential of a rotation vector (on tangent space) is the 3d rotation matrix (on manifold)\n",
        "\n",
        "  * and back: point on manifold x and its '**logarithm of x**' back on the tangent space\n",
        "\n",
        "  * it's an exact operation, and no approximation\n",
        "\n",
        "  * explanation and proof here: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_07.png)"
      ],
      "metadata": {
        "id": "9tPBGJIU4s4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie algebra is the origin point (= identity) on the tangent space!**\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algreba\"** **each time you take a derivative on the manifold, you are out of the manifold**, but you can stay on the tangent space for doing this operation (because there they are well defined)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_08.png)\n",
        "\n",
        "*https://arxiv.org/pdf/1812.01537.pdf*"
      ],
      "metadata": {
        "id": "YmuDaXN94zEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **antipodal point** to the origin: from origin point go half a turn around a 3d ball, you and up on the antipodal point, and there are many ways to go (any vector with length pie œÄ)\n",
        "\n",
        "* The tangent space will cover the manifold multiple times !!\n",
        "\n",
        "  * \"**First cover of the manifold by the tangent space**\": all points on the tangent space will end up in the antipodal point when applying any vector with length pie œÄ)\n",
        "\n",
        "  * there you can see relationship between Lie group and tangent space on manifold\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_02.png)"
      ],
      "metadata": {
        "id": "wiDgFHe2449f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following image: $R^{m}$ and $T_{E} M$ are the same vector space but with different representations\n",
        "\n",
        "> **Lie Algebra $T_{E} M \\sim R_{m} \\text { (Cartesian Tangent Space) and } w \\sim w^{\\wedge}$**\n",
        "\n",
        "* This means that $T_{E} M$ is isomorph to $R_{m}$\n",
        "\n",
        "* Since you can always go to an $R_{m}$ space, any Lie Group will have an cartesian $R_{m}$ tangent space, which is a vector\n",
        "\n",
        "* you can alweays go from one to the other give the isomorphism\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_03.png)"
      ],
      "metadata": {
        "id": "cwmJvdVO47i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SO(3): Lie group of rotation matrices in 3D**\n",
        "\n",
        "* you can write [w]<sub>x</sub> as a linear combination of 3 base matrices $E$<sub>x, y, z</sub>, which facilitates the calculation / it's easier than working directly with [w]<sub>x</sub>\n",
        "\n",
        "* and since $T_{E} M \\sim R_{m}$ sowie $w \\sim w^{\\wedge}$ (Isomorphism), it's also an allowed (exact) operation\n",
        "\n",
        "* Let's write a tangent vector as a regular cartesian vector with 3 coordinates\n",
        "\n",
        "> $[\\omega]_{\\times}=\\omega_{x} \\mathbf{E}_{x}+\\omega_{y} \\mathbf{E}_{y}+\\omega_{z} \\mathbf{E}_{z}$\n",
        "\n",
        "* The matrix product $[\\omega]_{\\times}$ is equivalent to cross product of vectors $\\omega =\\omega_{x} \\mathbf +\\omega_{y} \\mathbf +\\omega_{z}$, isomorph, two ways of representing the same elemtn of the tangent space\n",
        "\n",
        "* looking at the previous slide: cartesian R<sup>m</sup> is easier to work with than Lie Algebra T<sub>E</sub>M\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_04.png)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_05.png)\n",
        "\n",
        "**An elementary example is the space of three dimensional vectors $\\mathfrak{g}=\\mathbb{R}^{3}$** with the bracket operation defined by the cross product $[x, y]=x \\times y$. This is skew-symmetric since $x \\times y=-y \\times x$, and instead of associativity it satisfies the Jacobi identity:\n",
        "\n",
        "> $\n",
        "x \\times(y \\times z)=(x \\times y) \\times z+y \\times(x \\times z)\n",
        "$\n",
        "\n",
        "* **This is the Lie algebra of the Lie group of rotations of space**, and each vector $v \\in \\mathbb{R}^{3}$ may be pictured as an infinitesimal rotation around the axis $v$, with velocity equal to the magnitude of $v$.\n",
        "\n",
        "* The Lie bracket is a measure of the noncommutativity between two rotations: since a rotation commutes with itself, we have the alternating property $[x, x]=x \\times x=0$."
      ],
      "metadata": {
        "id": "ds8k2eji4-9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Multilinear Algebra*"
      ],
      "metadata": {
        "id": "Nj_F9wX47fTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Algebra*"
      ],
      "metadata": {
        "id": "ZiAK4jblJhPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear algebra**\n",
        "\n",
        "Multilinear Algebra, as the name suggests, deals with the properties and applications of multilinear maps, which are functions of several vector variables that are linear in each argument. In other words, it generalizes linear algebra, which deals with linear transformations between vector spaces. The key concepts in multilinear algebra include tensors, tensor products, and various types of multilinear forms, including bilinear, trilinear forms, and so on.\n",
        "\n",
        "* [Multilinear algebra](https://en.m.wikipedia.org/wiki/Multilinear_algebra) extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, **multilinear algebra builds on the concepts of**\n",
        "\n",
        "  * [Multivectors (p-vectors)](https://en.m.wikipedia.org/wiki/Multivector)\n",
        "\n",
        "  * [Exterior algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra) bzw. [Grassmann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra).\n",
        "\n",
        "* Fundamental objects of study in multilinear algebra are\n",
        "\n",
        "  * [Multilinear maps](https://en.m.wikipedia.org/wiki/Multilinear_map) (Multilineare_Abbildung: Abbildung, die f√ºr jedes ihrer Argumente linear ist)\n",
        "\n",
        "  * [Multilinear forms](https://en.m.wikipedia.org/wiki/Multilinear_form)\n",
        "\n",
        "* Abbildung von Modul in einen Ring (also Verallgemeinerung der K-Algebra von Vektorraum in den Korper, zB bei Integration oder Differential)\n",
        "\n",
        "  * Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "  * Jede lineare Abbildung ist eine 1-lineare Abbildung.\n",
        "\n",
        "  * Jede bilineare Abbildung ist eine 2-lineare Abbildung. (S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "    * Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen. Bei diesen ist der Wertebereich G mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch.)\n",
        "\n",
        "  * Sparprodukt ist eine 3-lineare Abbildung\n",
        "\n",
        "*  Multilinearform: wie linear- oder bilinearform, nur mehr argumente.\n"
      ],
      "metadata": {
        "id": "lKfW4ry1JlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gra√ümann-Algebra (Exterior Algebra)**\n",
        "\n",
        "Grassmann Algebra, on the other hand, is a branch of mathematics developed by Hermann Grassmann in the mid-19th century. It is an exterior (or \"wedge\") algebra that extends the concepts of vectors to higher dimensions through the creation of new mathematical entities called \"multivectors\". These multivectors can be interpreted geometrically as oriented areas, volumes, and higher-dimensional analogues. The wedge product operation in Grassmann Algebra is antisymmetric and multilinear, making it a critical component of the study of differential forms and integral calculus on manifolds.\n",
        "\n",
        "* There is a connection between Gra√ümann-Algebra and Multilinear Algebra in the sense that the exterior algebra (Grassmann Algebra) of a vector space can be built from the tensor algebra, a concept arising in multilinear algebra. The exterior algebra is essentially a quotient of the tensor algebra by a certain ideal, which gives rise to the antisymmetry property of the wedge product.\n",
        "\n",
        "* It's important to mention that Grassmann Algebra is a particular instance of multilinear algebra, as the operations it defines (such as the wedge product) are multilinear. However, not all multilinear algebraic structures fall within Grassmann Algebra, because they might not possess the same antisymmetry properties. This distinction is what differentiates general multilinear algebra from the more specific Grassmann Algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multivector\n",
        "\n",
        "Die Gra√ümann-Algebra $\\Lambda V$ eines reellen Vektorraumes $V$ ist die Clifford-Algebra $Cl(V,0)$ mit der trivialen quadratischen Form $Q=0$.\n",
        "\n",
        "Diese Beziehung ist unter anderem f√ºr die Quantisierung supersymmetrischer Feldtheorien wichtig.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Clifford-Algebra#Gra√ümann-Algebra\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) bzw. [Exterior Algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra): Algebra der Differentialformen.\n",
        "\n",
        "* Exterior Algebra eines Vektorraums V ist eine assoziative, schiefsymmetrisch-graduierte Algebra mit Einselement.\n",
        "\n",
        "* The exterior algebra provides an algebraic setting in which to answer geometric questions.\n",
        "\n",
        "* **Exterior algebra, geometric algebra, and clifford algebra are linear algebras** [Quora](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* Sie ist ‚Äì je nach Definition ‚Äì **Unteralgebra oder eine Faktoralgebra einer antisymmetrisierten** [**Tensoralgebra**](https://de.m.wikipedia.org/wiki/Tensoralgebra) von V und wird durch $\\Lambda V$ dargestellt.\n",
        "\n",
        "* Die Multiplikation wird als **√§u√üeres Produkt, Keilprodukt, Dachprodukt oder Wedgeprodukt** bezeichnet. Ein Spezialfall dieses Produkts ist mit dem Kreuzprodukt verwandt.\n",
        "\n",
        "* Anwendung: linearen Algebra (Theorie der Determinanten), Differentialgeometrie (Algebra der Differentialformen)"
      ],
      "metadata": {
        "id": "PaJipKZ_JnRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Vectors ($k$-blades)*"
      ],
      "metadata": {
        "id": "WqR3v9EaJrvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exterior Product (Wedge product u $\\wedge$ v) in Exterior Algebra (Gra√ümann-Algebra)**\n",
        "\n",
        "* **The exterior product (also called the Wedge product u $\\wedge$ v) used to construct bivectors and multivectors (is multilinear)**\n",
        "\n",
        "* the [exterior product or wedge product of vectors](https://en.m.wikipedia.org/wiki/Exterior_algebra) is an algebraic construction used in geometry to study areas, volumes, and their higher-dimensional analogues.\n",
        "\n",
        "> **The exterior product (also called the wedge product) used to construct bivectors and multivectors (is multilinear)**: The wedge product of two vectors is another object commonly called a bivector, and these lie in their own vector space. [Quora](https://www.quora.com/Is-the-wedge-product-the-same-thing-as-the-general-outer-product-If-not-why-is-the-wedge-symbol-used-for-both)\n",
        "\n",
        "* The exterior product of two vectors $u$ and $v$, denoted by $u\\wedge v$, is called a [bivector](https://en.m.wikipedia.org/wiki/Bivector) and lives in a space called the exterior square, a vector space that is distinct from the original space of vectors.\n",
        "\n",
        "* The exterior product, commonly called the wedge product, acts on tangent vectors and is an important operation in differential geometry that generalizes the cross product of 3-vectors.\n",
        "\n",
        "* See also: https://towardsdatascience.com/exterior-product-ecd5836c28ab\n",
        "\n",
        "* Add-on Differentiation:\n",
        "\n",
        "  * The **exterior product** is related to the tensor product in that the exterior product of two forms (a form is a skew-symmetric tensor of type (0,ùëù)) is just the antisymmetrization of the tensor product.\n",
        "\n",
        "  * The **cross product** is a speciality of the three-dimensional space; here the space of 2-forms has the same dimension as the space of 1-forms; indeed, given a metric, the hodge star maps between them. Since the metric also allows to associate vectors and 1-forms, you can define the cross product of v and ùë§ by the following procedure: Determine the 1-forms corresponding to ùë£ and ùë§, calculate their exterior product (which is a 2-form), apply the Hodge star to the result (which, given that we are in three dimensions, again results in a 1-form), and finally determine the vector corresponding to that 1-form.\n",
        "\n",
        "  * [Cross product as an external product](https://en.m.wikipedia.org/wiki/Cross_product#Cross_product_as_an_external_product)\n",
        "\n",
        "  * https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "9yW5RW7IJtlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$k$-blade ($k$-vector)**\n",
        "\n",
        "* a [$k$-blade](https://en.m.wikipedia.org/wiki/Blade_(geometry)) or a simple $k$-vector is a generalization of the concept of scalars and vectors to include simple bivectors, trivectors, etc.\n",
        "\n",
        "* Specifically, a $k$ blade is a $k$-vector that can be expressed as the exterior product (informally wedge product) of 1-vectors, and is of grade $k$. In detail:\n",
        "\n",
        "  * A 0-blade is a scalar.\n",
        "\n",
        "  * A 1-blade is a vector. Every vector is simple.\n",
        "\n",
        "  * A 2-blade is a simple bivector. Sums of 2-blades are also bivectors, but not always simple. A 2-blade may be expressed as the wedge product of two vectors $a$ and $b$ : $a \\wedge b$.\n",
        "\n",
        "  * A 3-blade is a simple trivector, that is, it may be expressed as the wedge product of three vectors $a, b$, and $c$ : $a \\wedge b \\wedge c \\text {. }$\n",
        "\n",
        "  * In a vector space of dimension $n$, a blade of grade $n-1$ is called a [pseudovector](https://en.m.wikipedia.org/wiki/Pseudovector) or an [antivector](https://en.m.wikipedia.org/wiki/Antivector)"
      ],
      "metadata": {
        "id": "ffxYeT3eJvhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Antisymmetry**\n",
        "\n",
        "> $\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2}=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}$\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} &=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1} \\wedge \\mathbf{e}_{3} \\\\\n",
        "&=\\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} \\wedge \\mathbf{e}_{1} \\\\\n",
        "&=-\\mathbf{e}_{3} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra"
      ],
      "metadata": {
        "id": "2ouXP-bDJxWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivector (Clifford number)**\n",
        "\n",
        "* In multilinear algebra, a [multivector](https://en.m.wikipedia.org/wiki/Multivector), sometimes called **Clifford number**,  is an element of the exterior algebra $\\Lambda(V)$ of a vector space $V$.\n",
        "\n",
        "* The exterior product $\\wedge$ (Wedge product) used to construct [bivectors](https://en.m.wikipedia.org/wiki/Bivector) and [multivectors](https://en.m.wikipedia.org/wiki/Multivector) (is multilinear)\n",
        "\n",
        "* This algebra is graded, associative and alternating, and consists of linear combinations of simple $k$ -vectors, also known as decomposable $k$ -vectors  or [$k$-blades](https://en.m.wikipedia.org/wiki/Blade_(geometry)), of the form\n",
        "\n",
        "> $v_{1} \\wedge \\cdots \\wedge v_{k}$\n",
        "\n",
        "where $v_{1}, \\ldots, v_{k}$ are in $V$.\n",
        "\n",
        "* A $k$ -vector is such a linear combination that is homogeneous of degree $k$ (all terms are\n",
        "$k$ -blades for the same $k$ ). Depending on the authors, a \"multivector\" may be either a $k-$\n",
        "vector or any element of the exterior algebra (any linear combination of $k$ -blades with potentially differing values of $k$ ).\n",
        "\n",
        "* In differential geometry, a $k$ -vector is a vector in the exterior algebra of the tangent\n",
        "vector space; that is, it is an antisymmetric tensor obtained by taking linear\n",
        "combinations of the exterior product of $k$ tangent vectors, for some integer $k \\geq 0 .$\n",
        "\n",
        "* A differential $k$ -form is a $k$ -vector in the exterior algebra of the dual of the tangent space,\n",
        "which is also the dual of the exterior algebra of the tangent space.\n",
        "\n",
        "> **For $k=0,1,2$ and $3, k$ -vectors are often called respectively scalars, vectors, bivectors and trivectors; they are respectively dual to 0 -forms, 1 -forms, 2 -forms and 3 forms.**"
      ],
      "metadata": {
        "id": "JbnIY2DPJzO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation for the exterior product of n 1-forms (Œµ, Œ∑, œâ) to obtain an n-form (\"mesh\" of coordinate surfaces, here planes), for n = 1, 2, 3. The \"circulations\" show orientation*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0938.png)"
      ],
      "metadata": {
        "id": "yyX6pjvrJ1J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation of grade n elements in a real exterior algebra for n = 0 (signed point), 1 (directed line segment, or vector), 2 (oriented plane element), 3 (oriented volume). The exterior product of n vectors can be visualized as any n-dimensional shape (e.g. n-parallelotope, n-ellipsoid); with magnitude (hypervolume), and orientation defined by that of its (n ‚àí 1)-dimensional boundary and on which side the interior is.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0939.png)"
      ],
      "metadata": {
        "id": "vcYQLc-WJ3H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parallel plane segments with the same orientation and area corresponding to the same bivector a ‚àß b:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Wedge_product.JPG/471px-Wedge_product.JPG)"
      ],
      "metadata": {
        "id": "7-GwR_RyJ40H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Maps (Operators)*"
      ],
      "metadata": {
        "id": "JYooOdCtJ76C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear Map**\n",
        "\n",
        "> [*A bilinear map*](https://en.m.wikipedia.org/wiki/Bilinear_map) *is a function combining elements of two vector spaces to yield an element of a third vector space, and is linear in each of its arguments. Matrix multiplication is an example as visible in $B : V √ó W ‚Üí X$. In contrast: linear map war eine matrix als map von vector zu vector, das hier ist eine matrix als map zw matrix und matrix!* **In mathematics, a bilinear operator is a generalized \"multiplication\" which satisfies the distributive law.**\n",
        "\n",
        "> **[Multilinear Map](https://en.m.wikipedia.org/wiki/Multilinear_map): A function that is linear when all inputs except one are held constant** (they are linear in each input variable)\n",
        "\n",
        "* when we scale the input variable by $n$ (and all other are held constant) that's the same as scaling the ouput of the function by $n$\n",
        "\n",
        "* when we hold all input constant except one, and we do a sum in the input slot, that's the same thing as doing the sum of the outputs (in image below)\n",
        "\n",
        "* Riemann Curvature Tensor is a multilinear map. Wie findet man heraus, ob eine Surface flach oder gekruemmt ist?\n",
        "\n",
        "* **If the codomain of a multilinear map is the field of scalars, it is called a multilinear form**\n",
        "\n",
        "* For multilinear maps used in cryptography, see [Cryptographic multilinear map](https://en.m.wikipedia.org/wiki/Cryptographic_multilinear_map).\n",
        "\n",
        "* https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Matrix_multiplication\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 15: Tensor Product Spaces](https://www.youtube.com/watch?v=M-OLmxuLdbU&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=18)\n",
        "\n"
      ],
      "metadata": {
        "id": "LhNgU-EEMp8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_71.png)\n",
        "\n",
        "And all tensors are **multilinear maps** which means they are functions that take some number of inputs and they are linear in each input variable while all other input variables are held constant (kind of ceteris paribus in economics!).\n",
        "\n",
        "* If you make all inputs constant except one, we can scale the input before or scale the output after and we get the same result\n",
        "\n",
        "* And also if we replace these input vector components with a sum of two sets of vector components I can just distribute these out and get this sum here (red line under equation in image), so basically I can add the inputs or I can add the outputs.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_73.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OjmCCbxNnzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Special: Differential Operator*\n",
        "\n",
        "[Differential operator](https://en.m.wikipedia.org/wiki/Differential_operator) is an operator defined as a function of the differentiation operator. Consider differentiation as an abstract operation that **accepts a function and returns another function**.\n",
        "\n",
        "*Differential- & Nabla-Operator*\n",
        "\n",
        "Der [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, **um kontextabh√§ngig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren**. Es ist zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei Differentialoperatoren [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) (Anwendung im [Gradientenverfahren](https://de.wikipedia.org/wiki/Gradientenverfahren) in der Numerik), [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes) oder [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes)\n",
        "\n",
        "https://www.youtube.com/watch?v=YW-bUVIOpB0&t=51s\n",
        "\n",
        "* **Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "  * [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "* **Diese drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei r√§umlicher Drehung des urspr√ºnglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden. Diese Eigenschaft folgt aus den **koordinatenunabh√§ngigen** Definitionen.\n",
        "\n",
        "https://www.youtube.com/watch?v=rB83DpBJQsE\n",
        "\n",
        "*Integraloperator*\n",
        "\n",
        "* **Integral**: Der [**Volterraoperator**](https://de.wikipedia.org/wiki/Integraloperator#Volterraoperator) $\\int_{0}^{t}$ zur Bildung des [bestimmten Integrals](https://de.wikipedia.org/wiki/Integralrechnung) (ist ein Beispiel fur einen [Integraloperator](https://de.wikipedia.org/wiki/Integraloperator). Operatoren wie diese, die einer Funktion eine Zahl zuordnen, nennt man [Funktional](https://de.wikipedia.org/wiki/Funktional).\n",
        "\n",
        "  * Die Fourier-Transformation ${\\mathcal {F}}$ ist z.B. ein **linearer Operator** (siehe unten).\n",
        "\n",
        "* Ein [linearer Integraloperator](https://de.wikipedia.org/wiki/Integraloperator) ist ein mathematisches Objekt aus der Funktionalanalysis. Dieses Objekt ist ein linearer Operator, der mit einer bestimmten Integralschreibweise mit einem Integralkern dargestellt werden kann."
      ],
      "metadata": {
        "id": "Y0ofnEZLKsn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Forms (Functionals, Covectors, Differential Forms)*"
      ],
      "metadata": {
        "id": "MMxCx0fZKDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**0-form = Function**:\n",
        "\n",
        "* is a special case and is simply a smooth function. **A 0-forms eats a point and returns a number**\n",
        "\n",
        "* [Glatte Funktionen](https://de.m.wikipedia.org/wiki/Glatte_Funktion) sind 0-Formen. Is a skalar from a function.\n",
        "\n",
        "From 0-form to 1-form:\n",
        "\n",
        "> **Siehe auch [Level Sets](https://en.wikipedia.org/wiki/Level_set) bzw. Niveaumenge** (= die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist.)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1585.png)\n",
        "\n",
        "Visualizing One-Forms:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1586.png)\n"
      ],
      "metadata": {
        "id": "dr3LzbeZhgpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**1-form = Linear Form** (Pfaffsche Formen, Covector, Functional). **Eats a vector and returns a number (scalar):. Is a Covector.**\n",
        "\n",
        "> $\\mathcal{B}: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* *Linear Form = 1-Form = Functional (incl Distribution) = Covector = Dualvector = Differentialformen*\n",
        "\n",
        "* This scalar can be a integrand (Wegintegral) or Differential, Temperature, Speed for 1-Form\n",
        "\n",
        "* [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind 1-Formen (= zB Wegintegral). [1-Formen](https://en.m.wikipedia.org/wiki/One-form) bilden die Grundlage f√ºr die Einf√ºhrung von Differentialformen.\n",
        "Pfaffsche Formen sind die nat√ºrlichen Integranden f√ºr Wegintegrale. Kovektorfeld oder kurz 1-Form ein Objekt, das in gewisser Weise dual zu einem Vektorfeld ist.\n",
        "\n",
        "* Exkurs **Dualraum**: Die Menge aller Linearformen (= stetigen, linearen Abbildungen) √ºber einem gegebenen Vektorraum $V$ bildet dessen [Dualraum](https://de.wikipedia.org/wiki/Dualraum) $V^{*}$ und damit selbst wieder in nat√ºrlicher Weise einen $K$ -Vektorraum. Die Menge aller Funktionale ist wiederum in nat√ºrlicher Form ein Vektorraum uber dem gleichen K√∂rper $\\mathbb{K}$, indem man f√ºr zwei Funktionale $f$ und $g$ √ºber $V$ die Addition und Skalarmultiplikation punktweise definiert. Zu einem Vektorraum $V$ √ºber einem K√∂rper $K$ bezeichnet $V^{*}$ den zu $V$ geh√∂rigen [Dualraum](https://de.m.wikipedia.org/wiki/Dualraum), das hei√üt die Menge aller linearen Abbildungen von $V$ nach $K$. All covectors can be written as the linear combination of the dual basis vectors!\n",
        "\n",
        "\n",
        "* Es ist eine Differentialform vom Grad 1. A one-form on a differentiable manifold is a smooth section of the cotangent bundle (= differential form).\n",
        "\n",
        "* A [linear form](https://de.m.wikipedia.org/wiki/Linearform) is a function that takes one or more vectors as input and outputs a number (= lineare Abbildung von einem Vektorraum in den zugrundeliegenden K√∂rper)\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* One Form and Line Integral: the expression $f(x) d x$ from one-variable calculus is an example of a [$l-$ form](https://en.m.wikipedia.org/wiki/One-form) (Pfaff'sche Form), and can be integrated over an oriented interval $[a, b]$ in the domain of $f: \\int_{a}^{b} f(x) d x$. See Video: [Line Integrals in Differential Forms](https://youtu.be/dhZgGIYzPUU).\n",
        "\n",
        "* [Video: How to visualise a one-form](https://www.youtube.com/watch?v=dxz9JZPewu8): family of surfaces that it pierces. Integrating a one-form, we are just counting the number of surfaces that the line actually passes through\n",
        "\n",
        "* Each term contains one differential: $\\begin{array}{a}\n",
        "\\omega=2 x d x+3 y d y-d z, \\\\\n",
        "\\omega=x d y, \\\\\n",
        "\\omega=d x .\n",
        "\\end{array}$\n",
        "\n",
        "* Example from quantum mechanics:\n",
        "\n",
        "> **A column vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "x \\\\\n",
        "y\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>vector</u>.** i.e. Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state'\n",
        "\n",
        "> **A row vector $\\begin{equation}\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>covector (linear form, functional)</u>.** i.e. Bra $[b_{0} \\quad b_{1}]$\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti‚Äìisomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra‚Äìket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© = 1.\n",
        "\n",
        "* In Funktionalanalyse ist der untersuchte Vektorraum $V$ zumeist ein Funktionenraum, wo diesen durch Funktionale Skalare zugeordnet werden. Beispiel: [Lebesgue-Integral](https://de.m.wikipedia.org/wiki/Lebesgue-Integral).\n",
        "\n",
        "* Siehe auch [Funktionaldeterminante](https://de.wikipedia.org/wiki/Funktionaldeterminante) oder Jacobi-Determinante fur Koordinatentransformationen zB von kartesisches zu Polarkoordinaten in der mehrdimensionalen Integralrechnung, also der **Berechnung von Oberfl√§chen- und Volumenintegralen**  an.\n",
        "\n",
        "* Siehe auch [Functional integration](https://en.m.wikipedia.org/wiki/Functional_integration): Richard Feynman used functional integrals as the central idea in his sum over the histories formulation of quantum mechanics. This usage implies an integral taken over some function space.\n",
        "\n",
        "* [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 4: What are Covectors?](https://www.youtube.com/watch?v=LNoQ_Q5JQMY)"
      ],
      "metadata": {
        "id": "EbzZ6TUChpNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**2-form = Bilinear Form (Surface Integral)**: eats two vectors and returns a number  (scalar): $k=2$,  $f:V\\times V\\to K$\n",
        "\n",
        "> $\\mathcal{B}: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* Algebraic rules - linearity properties:\n",
        "\n",
        "  * $a \\mathcal{B}(\\vec{v}, \\vec{w})=\\mathcal{B}(a \\vec{v}, \\vec{w})=\\mathcal{B}(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}+\\vec{u}, \\vec{w})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}, \\vec{w}+\\vec{t})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{v}, \\vec{t})$\n",
        "\n",
        "> $\\omega=8 d y \\wedge d z$\n",
        "\n",
        "* Similarly, the expression $f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z$ is a 2 -form that has a [surface integral](https://en.m.wikipedia.org/wiki/Surface_integral) over an oriented surface $S$ : $\\int_{S}(f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z) .$\n",
        "\n",
        "* The symbol $\\wedge$ denotes the exterior product, sometimes called the wedge product, of two differential forms.\n",
        "\n",
        "* [Bilinear Forms](https://en.m.wikipedia.org/wiki/Bilinear_form) sind ein Spezialfall der bilinearen Abbildungen (Wertebereich G ist mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch). **Winkel sind wichtiger Anwendungsfall dafur: man kann Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht.**\n",
        "\n",
        "  * A familiar and important example of a (symmetric) bilinear form is the standard [inner product (dot product)](https://en.m.wikipedia.org/wiki/Dot_product) of vectors. Jedes Skalarprodukt ist wiederum eine spezielle Bilinearform (es gelten noch weitere Eigenschaften: symmetrisch <v,w> = <w,v>, und positiv definit). Genauso Integral.\n",
        "\n",
        "  * [Bilinearform](https://de.wikipedia.org/wiki/Bilinearform), Bilinearform: cross product of two vectors, normal and tangent, see [Frenet‚ÄìSerret_formulas](https://en.m.wikipedia.org/wiki/Frenet‚ÄìSerret_formulas).\n",
        "\n",
        "* Die [Sesquilinearform](https://en.m.wikipedia.org/wiki/Sesquilinear_form) ist eine Generalizations der Bilinear Form auf den Koerper der komplexen Zahlen\n",
        "\n",
        "* Der [Metric Tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) ist ein Spezialfall einer Bilinear Form. **A metric tensor is two-form because it takes 2 vectors to output one scalar, which is length or angle**. The metric tensor has 2 additional properties that other bilinear forms might not have: components are symmetric, output must be positive:\n",
        "\n",
        "  * Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  * Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Just like the metric tensor <font color=\"red\">bilinear forms are (0,2) tensors</font> (so they transform using 2 covariant rules when we change coordinate systems):\n",
        "\n",
        "  * $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "  * $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 10: Bilinear Forms](https://www.youtube.com/watch?v=jLiBCaBEB3o&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14) und das Video [Bilinearform einfach erkl√§rt :) | Math Intuition](https://www.youtube.com/watch?v=TjAFH6hWg1I)"
      ],
      "metadata": {
        "id": "4dmD8N0tSkWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**3 -form = Trilinear Form (Volume Integral)**: eats three vectors and returns a number:\n",
        "\n",
        "* $\\omega=-7 z d x \\wedge d y \\wedge d z$\n",
        "\n",
        "* Likewise, a $3-$ form $f(x, y, z) d x \\wedge d y \\wedge d z$ represents a [volume element](https://en.m.wikipedia.org/wiki/Volume_element) that can be integrated over an oriented region of space."
      ],
      "metadata": {
        "id": "mtRicmZJUH1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**$k$-form = Multilinear Form**: eats $k$-vectors and returns a number: $\\omega= .. $\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* In general, a $k$-form is an object that may be integrated over a $k$-dimensional oriented manifold, and is homogeneous of degree $k$ in the coordinate differentials.\n",
        "\n",
        "* On an $n$-dimensional manifold, the top-dimensional form $(n$-form $)$ is called a [volume form](https://en.m.wikipedia.org/wiki/Volume_form)\n",
        "\n",
        "* A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) on a vector space $V$ over a field $K$ is a map\n",
        "$f: V^{k} \\rightarrow K$\n",
        "that is separately $K$ -linear in each of its $k$ arguments.\n",
        "\n",
        "* A multilinear $k$ -form on $V$ over $\\mathbf{R}$ is called a (**covariant**) **$k$ -tensor**, and the vector space is usually denoted $\\mathcal{T}^{k}(V)$ or $\\mathcal{L}^{k}(V) \\cdot$\n",
        "\n",
        "* <font color=\"red\">Multilinear forms are (0,k) tensors</font> (so they transform using k covariant rules when we change coordinate systems) (??)\n",
        "\n",
        "* Kovariante Tensoren (Covectors) sind Multilinearformen [Source](https://de.m.wikipedia.org/wiki/Multilinearform)\n",
        "\n",
        "* Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "* https://unapologetic.wordpress.com/2009/10/22/multilinear-functionals/\n",
        "\n",
        "* Understanding the definition of tensors as multilinear maps: https://math.stackexchange.com/questions/2138459/understanding-the-definition-of-tensors-as-multilinear-maps\n",
        "\n",
        "* [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)\n",
        "\n",
        "* **Exkurs: Exterior Derivative (√Ñu√üere Ableitung)**\n",
        "\n",
        "  * Die [√§u√üere Ableitung](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) ist ein Operator, der einer k-Differentialform eine $(k+1)$-Differentialform zuordnet.\n",
        "\n",
        "  * Betrachtet man sie auf der Menge der $0$-Differentialformen, also auf der Menge der glatten Funktionen, so entspricht die √§u√üere Ableitung der √ºblichen Ableitung f√ºr Funktionen.\n",
        "\n",
        "  * Die √§u√üere Ableitung $\\mathrm{d} \\omega$ einer $k$ -Form $\\omega$ wird induktiv mithilfe der [Lie-Ableitung](https://de.m.wikipedia.org/wiki/Lie-Ableitung) (=die Ableitung eines Vektorfeldes oder allgemeiner eines Tensorfeldes entlang eines Vektorfeldes. ) und der Cartan-Formel\n",
        "\n",
        "  > $\n",
        "\\mathcal{L}_{X}=i_{X} \\circ \\mathrm{d}+\\mathrm{d} \\circ i_{X}\n",
        "$\n",
        "\n",
        "  * definiert; dabei ist $X$ ein Vektorfeld, $\\mathcal{L}_{X}$ die Lie-Ableitung und $i_{X}$ die Einsetzung von $X$.\n",
        "\n",
        "* **Exkurs: Go from Differential Operators (grad, div, curl) to Differential Forms with Exterior Derivative:**\n",
        "\n",
        "  * By taking the [exterior derivative](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) of 0-forms, 1-forms and 2-forms we can express the three important operators of vector calculus - grad, curl and div - in the language of differential forms. The gradient of a sclara field f(x,y,z) is given by:\n",
        "\n",
        "  * $\\operatorname{grad} f=\\nabla f=\\frac{\\partial f}{\\partial x} \\hat{e}_{x}+\\frac{\\partial f}{\\partial y} \\hat{e}_{y}+\\frac{\\partial f}{\\partial z} \\hat{\\mathrm{e}}_{z}$\n",
        "\n",
        "  * Using the correspondance:\n",
        "\n",
        "  * $d x \\Leftrightarrow \\hat{\\mathbf{e}}_{x}, d y \\Leftrightarrow \\hat{\\mathbf{e}}_{y}, d z \\Leftrightarrow \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "  * this gradient vector field can be associated with the exterior derivative of the 0-form f(x,y,z), which is a 1-form\n",
        "\n",
        "  * $d f=\\frac{\\partial f}{\\partial x} d x+\\frac{\\partial f}{\\partial y} d y+\\frac{\\partial f}{\\partial z} d z$\n",
        "\n",
        "  * Grad operator: $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$"
      ],
      "metadata": {
        "id": "gtnhb-2MLClh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "> **Types of Differential Forms where the $\\wedge$ symbol denotes a type of multiplication called the wedge product (exterior product)**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors (for Line Integral)\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation (for Surface Integral)\n",
        "* Trivector = 3D - oriented volume (for Volume Integral)\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "uDFxcwyecPgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form (Differential Geometry / Tangent Spaces)**:\n",
        "\n",
        "* **[Differential forms](https://en.m.wikipedia.org/wiki/Differential_form) are linear or, more generally, multilinear alternating functions of tangent vectors**. Eine Differentialform ordnet einem Punkt einer Mannigfaltigkeit eine **alternierende Multilinearform** auf dem zugeh√∂rigen Tangentialraum zu. Differentialformen sind dabei das bekannteste Beispiel von Schnitten.\n",
        "\n",
        "* The differential forms form an [alternating algebra](https://en.m.wikipedia.org/wiki/Alternating_algebra). This implies that:\n",
        "\n",
        "  > $dy\\wedge dx=-dx\\wedge dy$\n",
        "\n",
        "  > $dx\\wedge dx=0.$\n",
        "\n",
        "* This alternating property reflects the orientation of the domain of integration.\n",
        "\n",
        "* Differential forms provide a unified approach to define integrands over curves, surfaces, solids, and higher-dimensional manifolds. ([Differentialformen](https://de.m.wikipedia.org/wiki/Differentialform) erlauben eine koordinatenunabh√§ngige Integration auf allgemeinen orientierten differenzierbaren Mannigfaltigkeiten.)\n",
        "\n",
        "* **Differential forms are a special class of tensors where you can find derivatives even in the absence of connections or metrics**. You can think of differential forms as a generalization of single variable calculus that works on manifolds, as well as curves, solids, and surfaces.\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) ist die Algebra der Differentialformen\n",
        "\n",
        "* Simple Example of a Differential Form: in the expression $\\int_{0}^{1}$ <font color=\"blue\">$ x^{2} d x$</font> the term $x^{2}$ is the **integrand** and <font color=\"blue\">$ x^{2} d x$</font> is the **differential form** [(Source)](https://www.calculushowto.com/differential-operator/). So for identification, differential forms can be generally recognised as things containing differentials such as $d x, d y$ and $d z$.\n",
        "\n",
        "  * [Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes) (Vektoranalysis): sehr grundlegenden Satz √ºber die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "  * [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) (sowie Koordinatentransformationen und Funktionaldeterminante in der Vektoranalysis): Aus mathematischer Sicht ist eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird Riemann'sche Volumenform genannt (**Hodge-Stern-Operator in der Differentialgeometrie**).\n",
        "\n",
        "* Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind. Covectors (functional): Differential Forms = Covector Fields. Video Eigenchris: [Differential Forms and Covectors](https://youtu.be/XGL-vpk-8dU)\n",
        "\n",
        "Definition der Differentialform: Es sei $U$\n",
        "- eine offene Teilmenge des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Untermannigfaltigkeit des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Mannigfaltigkeit.\n",
        "\n",
        "In jedem dieser F√§lle gibt es:\n",
        "- den Begriff der differenzierbaren Funktion auf $U$; der Raum der beliebig oft differenzierbaren Funktionen auf $U$ werde mit $C^{\\infty}(U)$ bezeichnet;\n",
        "- den Begriff des Tangentialraums $\\mathrm{T}_{p} U$ an $U$ in einem Punkt $p \\in U$;\n",
        "- den Begriff der Richtungsableitung $\\frac{\\partial f}{\\partial X}$ f√ºr einen Tangentialvektor $X \\in \\mathrm{T}_{p} U$ und eine differenzierbare Funktion $f$;\n",
        "- den Begriff des differenzierbaren Vektorfeldes auf $U$; der Raum der Vektorfelder auf $U$ sei mit $\\Gamma(\\mathrm{T} U)$ bezeichnet.\n",
        "- Der Dualraum des Tangentialraums $\\mathrm{T}_{p} U$ wird als Kotangentialraum $\\mathrm{T}_{p}^{*} U$\n",
        "bezeichnet.\n",
        "\n",
        "**Definition: Eine Differentialform vom Grad $k$ auf $U$ oder kurz $k$ -Form $\\omega$ ist ein glatter Schnitt in der $k$ -ten √§u√üeren Potenz des Kotangentialb√ºndels von $U$**.\n",
        "\n",
        "* In symbolischer Schreibweise bedeutet dies $\\omega \\in \\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$, wobei $T^{*} U$ das Kotangentialb√ºndel von $U, \\Lambda^{k}\\left(T^{*} U\\right)$ die $k$ -te √§u√üere Potenz von $T^{*} U$ und $\\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$ somit die Menge der glatten Schnitte von $\\Lambda^{k}\\left(T^{*} U\\right)$ bezeichnet.\n",
        "\n",
        "* Dies bedeutet, **dass jedem Punkt $p \\in U$ eine alternierende Multilinearform $\\omega_{p}$ auf dem Tangentialraum $T_{p} U$ zugeordnet wird**; und zwar so, dass f√ºr $k$ glatte Vektorfelder $X_{1}, \\ldots, X_{k}$ die folgende Funktion glatt, also beliebig oft differenzierbar, ist:\n",
        "\n",
        "> $p \\mapsto \\omega_{p}\\left(\\left(X_{1}\\right)_{p}, \\ldots,\\left(X_{k}\\right)_{p}\\right) \\in \\mathbb{R}$\n",
        "\n",
        "* **Alternativ dazu kann man eine $k$ -Form $\\omega$ als eine alternierende, glatte multilineare Abbildung $\\omega:(\\Gamma T U)^{k} \\rightarrow C^{\\infty}(U)$ auffassen**.\n",
        "\n",
        "* Das bedeutet: $\\omega$ ordnet $k$ Vektorfeldern $X_{1}, \\ldots, X_{k}$ eine Funktion $\\omega\\left(X_{1}, \\ldots, X_{k}\\right)$ zu, sodass\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}+X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)=\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}, \\ldots, X_{k}\\right)+\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)$\n",
        "\n",
        "* $\\omega\\left(X_{1}, \\ldots, f \\cdot X_{i}, \\ldots, X_{k}\\right)=f \\cdot \\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$ f√ºr $f \\in C^{\\infty}(U), 1 \\leq i \\leq k$\n",
        "\n",
        "* und\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{j}, \\ldots, X_{k}\\right)=-\\omega\\left(X_{1}, \\ldots, X_{j}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$\n",
        "gilt.\n",
        "\n",
        "* Alternative unter R√ºckgriff auf Tensorfelder: **Eine $k$ -Form ist ein alternierendes, kovariantes Tensorfeld der Stufe $k$.**"
      ],
      "metadata": {
        "id": "dl0fYX1-K_B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Tensor Algebra*"
      ],
      "metadata": {
        "id": "fVQl5cbp7iqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Analysis*"
      ],
      "metadata": {
        "id": "PsodM6QW8wsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Motivation:*** *In our subject of differential geometry, where you talk about manifolds, **one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning**. They are allowed to **undergo transformation**. And in order to handle this kind of situation, an important tool is the so-called **tensor analysis, or Ricci calculus**, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.*\n",
        "\n",
        "* Die [Tensoranalysis](https://de.wikipedia.org/wiki/Tensoranalysis) ist ein Teilgebiet der Differentialgeometrie beziehungsweise der Differentialtopologie.\n",
        "\n",
        "* In der Tensoranalysis wird **das Verhalten von geometrischen Differentialoperatoren auf Tensorfeldern untersucht**.\n",
        "\n",
        "> <font color=\"blue\">**Tensor Analysis ist eine Verallgemeinerung der Vektoranalysis**</font>\n",
        "\n",
        "* Zum Beispiel kann der Differentialoperator Rotation in diesem Kontext auf n Dimensionen verallgemeinert werden.\n",
        "\n",
        "* Zentrale Objekte der Tensoranalysis sind Tensorfelder. Es wird untersucht, wie Differentialoperatoren auf diesen Feldern wirken.\n",
        "\n",
        "* Vektoren und Matrizen, sofern sie geometrische oder physikalische Groessen reprasentieren, koennen unter dem begriff eines tensors subsumiert werden\n",
        "\n",
        "* **Tensorrechnung in 2 teilen:**\n",
        "\n",
        "  * Anschauungsraum fur ingenieure (**kartesische tensoren**).\n",
        "\n",
        "  * Fur **schiefwinklige (also affine) oder krummlinige Koordinaten** sind begriffe wie kovariant und kontravariant wichtig (zur arbeit mit metriken, deren skalarproduktauf einer nicht positiv definiten bilinearform beruht)."
      ],
      "metadata": {
        "id": "rx-E5d4r8zxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoralgebra**\n",
        "\n",
        "> **The [tensor algebra](https://en.m.wikipedia.org/wiki/Tensor_algebra) is important because many other algebras arise as quotient algebras of T(V)**. These include the [exterior algebra (Grassmann algebra of Differential Forms)](https://en.m.wikipedia.org/wiki/Exterior_algebra), the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra), the [Clifford algebras](https://en.m.wikipedia.org/wiki/Clifford_algebra), the [Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra) and [universal enveloping algebras](https://en.m.wikipedia.org/wiki/Universal_enveloping_algebra).\n",
        "\n",
        "* Die [Tensoralgebra](https://de.m.wikipedia.org/wiki/Tensoralgebra) ist ein mathematischer Begriff, der in vielen Bereichen der Mathematik wie der linearen Algebra, der Algebra, der Differentialgeometrie sowie in der Physik verwendet wird.\n",
        "\n",
        "* Sie fasst \"alle Tensoren\" √ºber einem Vektorraum in der Struktur einer graduierten Algebra zusammen.\n",
        "\n",
        "* Es sei $V$ ein Vektorraum √ºber einem K√∂rper $K$ oder allgemeiner ein Modul √ºber einem kommutativen Ring mit Einselement. Dann ist die Tensoralgebra (als Vektorraum) definiert durch die direkte Summe aller Tensorprodukte des Raums mit sich selbst.\n",
        "\n",
        ">$\n",
        "\\mathrm{T}(V):=\\bigoplus_{n \\geq 0} V^{\\otimes n}=K \\oplus V \\oplus(V \\otimes V) \\oplus(V \\otimes V \\otimes V) \\oplus \\ldots\n",
        "$\n",
        "\n",
        "* Mit der Multiplikation, die auf den homogenen Bestandteilen durch das Tensorprodukt gegeben ist, wird $\\mathrm{T}(V)$ zu einer $\\mathbb{N}_{0}$ -graduierten, unit√§ren, assoziativen Algebra.\n",
        "\n",
        "* Quotientenr√§ume der Tensoralgebra: Durch Herausteilen eines bestimmten Ideals kann man aus der Tensoralgebra beispielsweise die symmetrische Algebra, die √§u√üere Algebra oder die **Clifford-Algebra** gewinnen. Diese Algebren sind in der Differentialgeometrie von Bedeutung.\n",
        "\n"
      ],
      "metadata": {
        "id": "MERE084982Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Example for Tensors: Einstein Field Equations in General Relativity:**\n",
        "\n",
        "> $\n",
        "R_{\\mu \\nu}-\\frac{1}{2} R g_{\\mu \\nu}+\\Lambda {g_{\\mu v}}=\\frac{8 \\pi G}{c^{4}} T_{\\mu \\nu}\n",
        "$\n",
        "\n",
        "4x4 rank 2 metric tensor: ${g_{\\mu v}}$ (measure lengths and angles in the curved geometry of spacetime)\n",
        "\n",
        "* Gravitation als geometrische Eigenschaft der gekr√ºmmten vierdimensionalen Raumzeit.\n",
        "\n",
        "* Zur Beschreibung der **Raumzeit und ihrer Kr√ºmmung bedient man sich der Differentialgeometrie**, die die Euklidische Geometrie des uns vertrauten ‚Äûflachen‚Äú dreidimensionalen Raumes der klassischen Mechanik umfasst und erweitert.\n",
        "\n",
        "* Die Differentialgeometrie verwendet zur Beschreibung gekr√ºmmter R√§ume, wie der Raumzeit der ART, sogenannte **Mannigfaltigkeiten**. Wichtige **Eigenschaften werden mit sogenannten Tensoren beschrieben**, die Abbildungen auf der Mannigfaltigkeit darstellen.\n",
        "\n",
        "* Die gekr√ºmmte Raumzeit wird als [Lorentz-Mannigfaltigkeit](https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit) (Pseudo-riemannsche Mannigfaltigkeit) beschrieben.\n",
        "\n",
        "* Eine besondere Bedeutung kommt dem [**metrischen Tensor**](https://de.wikipedia.org/wiki/Metrischer_Tensor) zu. Wenn man in den metrischen Tensor zwei Vektorfelder einsetzt, erh√§lt man f√ºr jeden Punkt der Raumzeit eine reelle Zahl.\n",
        "\n",
        "  * In dieser Hinsicht kann man den metrischen Tensor als ein verallgemeinertes, punktabh√§ngiges Skalarprodukt f√ºr Vektoren der Raumzeit verstehen.\n",
        "\n",
        "  * Mit seiner Hilfe werden Abstand und Winkel definiert und er wird daher kurz als Metrik bezeichnet.\n",
        "\n",
        "* Ebenso bedeutend ist der [riemannsche Kr√ºmmungstensor](https://de.wikipedia.org/wiki/Riemannscher_Kr√ºmmungstensor) zur Beschreibung der Kr√ºmmung der Mannigfaltigkeit,\n",
        "\n",
        "  * der eine Kombination von ersten und zweiten Ableitungen des metrischen Tensors darstellt.\n",
        "\n",
        "  * Wenn ein beliebiger Tensor in irgendeinem Koordinatensystem in einem Punkt nicht null ist, kann man √ºberhaupt kein Koordinatensystem finden, sodass er in diesem Punkt null wird. Dies gilt dementsprechend auch f√ºr den Kr√ºmmungstensor.\n",
        "\n",
        "  * Umgekehrt ist der Kr√ºmmungstensor in allen Koordinatensystemen null, wenn er in einem Koordinatensystem null ist. Man wird also in jedem Koordinatensystem bez√ºglich der Frage, ob eine Mannigfaltigkeit an einem bestimmten Punkt gekr√ºmmt ist oder nicht, zum gleichen Ergebnis gelangen.\n",
        "\n",
        "* Die ma√ügebliche Gr√∂√üe zur Beschreibung von Energie und Impuls der Materie ist der [Energie-Impuls-Tensor](https://de.wikipedia.org/wiki/Energie-Impuls-Tensor). Dieser Tensor bestimmt die Kr√ºmmungseigenschaften der Raumzeit. Siehe auch [Vierertensor](https://de.wikipedia.org/wiki/Vierertensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "1xA1Fe1B83_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Example for Tensors: Quantum Mechanics & Quantum Computing**\n",
        "\n",
        "Quantum Superposition:\n",
        "\n",
        "> $\\vec{\\psi}=a \\vec{v}+b \\vec{w}$\n",
        "\n",
        "* linear combination, physical quantum states are just vectors, using linear combinations to give more complicated states\n",
        "\n",
        "\n",
        "Quantum Entanglement\n",
        "\n",
        "> $\\vec{\\psi} \\otimes \\vec{\\phi}$\n",
        "\n",
        "* two states are 'entangled' together means state these state vectors have been combined together using the 'tensor product' (circle X)\n",
        "\n",
        "* Takes the geometrical space where the first system lives and the second system and combines them together to create a more complicated geometrical space, and that's where the entangled system lives"
      ],
      "metadata": {
        "id": "thzMwgWj86A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu k√∂nnen, ist eine Basis n√∂tig. Im n-dimensionalen Raum besteht diese aus n linear unabh√§ngigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur f√ºr lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) m√∂glich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabh√§ngigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): F√ºr echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabh√§ngigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekr√ºmmt sind, sind die Koordinatenachsen nicht r√§umlich fest, wie es f√ºr kartesische Koordinaten gilt. Dies f√ºhrt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abh√§ngt ‚Äì im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenfl√§chen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgew√§hlten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "oZN7V4P688A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Tangent Space & Kotangentialraum**\n",
        "\n",
        "* **Tangential and Normal Components (Koordinaten)**: Given a vector at a point on a curve (just a vector, not a tangential vector!), that vector can be decomposed uniquely as a sum of two vectors,\n",
        "\n",
        "  * $\\mathbf{v}_{\\|}$ : one tangent to the curve, called the **tangential component** of the vector. $\\mathbf{v}_{\\perp}$  : another one perpendicular to the curve, called the **normal component** of the vector. Zusatzlich gibt es noch: The binormal unit vector B is defined as the cross product of T and N ([Source](https://en.m.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas))\n",
        "\n",
        "  * More formally, let $S$ be a surface, and $x$ be a point on the surface. Let $\\mathbf{v}$\n",
        "be a vector at $x$. Then one can write uniquely $\\mathbf{v}$ as a sum: $\\mathbf{v}=\\mathbf{v}_{\\|}+\\mathbf{v}_{\\perp}$. Similarly a vector at a point on a surface can be broken down the same way.\n",
        "\n",
        "  * More generally, given a submanifold $N$ of a manifold $M,$ and\n",
        "a vector in the tangent space to $M$ at a point of $N$, it can be\n",
        "decomposed into the component tangent to $N$ and the\n",
        "component normal to $N$.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Surface_normal_tangent.svg/248px-Surface_normal_tangent.svg.png)\n",
        "\n",
        "* **Calculate the components**: To calculate the tangential and normal components, consider a [unit normal](https://en.wikipedia.org/wiki/Normal_(geometry)) to the surface, that is, a [unit vector](https://en.wikipedia.org/wiki/Unit_vector) $\\hat{n}$ perpendicular to $S$ at $x$.\n",
        "\n",
        "  * Then, normal component: $\\mathbf{v}_{\\perp}=(\\mathbf{v} \\cdot \\hat{n}) \\hat{n}$ and thus tangential component: $\\mathbf{v}_{\\|}=\\mathbf{V}-\\mathbf{V}_{\\perp}$ where \".\" denotes the [dot product](https://en.wikipedia.org/wiki/Dot_product).\n",
        "\n",
        "  * Another formula for the tangential component is $\n",
        "\\mathbf{v}_{\\|}=-\\hat{n} \\times(\\hat{n} \\times \\mathbf{v})$ where \" $\\times$ \" denotes the [cross product](https://en.wikipedia.org/wiki/Cross_product).\n",
        "\n",
        "  * Note that these formulas do not depend on the particular unit normal $\\hat{n}$ used (there exist two unit normals to any surface at a given point, pointing in opposite directions, so one of the unit normals is the negative of the other one).\n",
        "\n",
        "* Siehe Artikel: [Tangential and normal components](https://en.wikipedia.org/wiki/Tangential_and_normal_components)\n",
        "\n",
        "* **Tangentialvektor**: Sei $\\gamma:(-\\varepsilon, \\varepsilon) \\rightarrow M$ eine differenzierbare Kurve mit $\\gamma(0)=x$ und dem **Kurvenparameter $t$** (siehe 'Parameterdarstellungen' oben), dann ist: $v=\\frac{d \\gamma}{d t}(0) \\in T_{x} M$ ein [Tangentialvektor](https://en.wikipedia.org/wiki/Tangent_vector). Die Tangentialvektoren in einem Punkt $x \\in M$ spannen einen Vektorraum auf, den Tangentialraum $T_{x} M$. Siehe auch [Tangentialb√ºndel](https://de.wikipedia.org/wiki/Tangentialb√ºndel).\n",
        "\n",
        "* **Tangent Space (Tangentialraum)** Ein [Tangentialraum](https://de.wikipedia.org/wiki/Tangentialraum) ist $T_{x} M$ ein Vektorraum, der eine differenzierbare Mannigfaltigkeit $M$ am Punkt $x$ linear approximiert.\n",
        "\n",
        "* **Normal (Vector) Space**: The normal vector space or normal space of a manifold at point P is the **set of vectors which are orthogonal to the tangent space at P**. Normal vectors are of special interest in the case of smooth curves and smooth surfaces.\n",
        "\n",
        "* **Kotangentialraum**: Der [Kotangentialraum](https://de.wikipedia.org/wiki/Kotangentialraum) ist der Dualraum des entsprechenden Tangentialraums. Er ist ein Vektorraum, der einem Punkt einer differenzierbaren Mannigfaltigkeit $M$ zugeordnet wird.\n",
        "\n",
        "  * Sei $M$ eine differenzierbare Mannigfaltigkeit und $T_{p} M$ ihr Tangentialraum am Punkt $p \\in M$. Dann ist der Kotangentialraum definiert als der Dualraum von $T_{p} M$.\n",
        "\n",
        "  * **Das hei√üt, der Kotangentialraum besteht aus allen Linearformen auf dem Tangentialraum $T_{p} M$**.\n",
        "\n",
        "  * In differential geometry, **one can attach to every point $x$ of a smooth (or differentiable) manifold, $\\mathcal{M},$ a vector space called the cotangent space at $x .$** Typically, the cotangent space, $T_{x}^{*} \\mathcal{M}$ is defined as the dual space of the tangent space at $x, T_{x} \\mathcal{M},$ although there are more direct definitions. The elements of the cotangent space are **called cotangent vectors or tangent covectors**.\n",
        "\n",
        "  * Let $\\mathcal{M}$ be a smooth manifold and let $x$ be a point in $\\mathcal{M}$. Let $T_{x} \\mathcal{M}$ be\n",
        "the tangent space at $x$. Then the cotangent space at $x$ is defined as the dual space of $T_{x} \\mathcal{M}$ : $\n",
        "T_{x}^{*} \\mathcal{M}=\\left(T_{x} \\mathcal{M}\\right)^{*}\n",
        "$\n",
        "\n",
        "  * Concretely, **elements of the cotangent space are linear functionals on $T_{x} \\mathcal{M}$**. That is, **every element $\\alpha \\in T_{x}^{*} \\mathcal{M}$ is a linear map** $\n",
        "\\alpha: T_{x} \\mathcal{M} \\rightarrow F\n",
        "$\n",
        "\n",
        "  * where $F$ is the underlying field of the vector space being considered, for example, the field of real numbers. **The elements of $T_{x}^{*} \\mathcal{M}$ are called cotangent vectors.**\n",
        "\n",
        "*Die [Hauptkr√ºmmungen](https://de.wikipedia.org/wiki/Hauptkr%C3%BCmmung) sind [Eigenwerte](https://de.wikipedia.org/wiki/Eigenwertproblem) der [Weingartenabbildung](https://de.wikipedia.org/wiki/Weingartenabbildung)*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/e/eb/Minimal_surface_curvature_planes-en.svg)"
      ],
      "metadata": {
        "id": "E84nx70p9Db6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Koordinatentransformation (Geometric transformation) & Basiswechsel**\n",
        "\n",
        "* [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation), wenn sich ein Problem in einem anderen Koordinatensystem leichter l√∂sen l√§sst, z. B. bei der Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten.\n",
        "\n",
        "* [Liste von Transformationen in der Mathematik](https://de.m.wikipedia.org/wiki/Liste_von_Transformationen_in_der_Mathematik) und [Intro: Koordinatentransformation](http://walter.bislins.ch/physik/index.asp?page=Koordinatentransformation). Beispiel: [Drehung](https://de.wikipedia.org/wiki/Drehung) (Rotation), [Skalierung](https://de.wikipedia.org/wiki/Skalar_(Mathematik)), [Scherung](https://de.wikipedia.org/wiki/Scherung_(Geometrie)) & [Verschiebung](https://de.wikipedia.org/wiki/Parallelverschiebung) (Translation). [Affine Transformationen](https://de.wikipedia.org/wiki/Affine_Abbildung) aus linearen Transformation und Translation. Translation ist ein Spezialfall einer affinen Transformation, bei der A die Einheitsmatrix ist.\n",
        "\n",
        "* In der Regel verwendet man spezielle Transformationen, bei denen diese Funktionen gewissen Einschr√§nkungen ‚Äì z. B. Differenzierbarkeit, Linearit√§t oder Formtreue ‚Äì unterliegen.\n",
        "\n",
        "* Bei **lineare Transformationen** ([Lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)) sind die neuen Koordinaten lineare Funktionen der urspr√ºnglichen. [Matrixmultiplikation](https://de.wikipedia.org/wiki/Matrizenmultiplikation) des alten Koordinatenvektors $\\vec{x} = (x_1, \\dots, x_n)$ mit der Matrix $A \\rightarrow {\\vec {x}}'=A{\\vec {x}}$.\n",
        "\n",
        "    * $x_{1}^{\\prime}=a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n}$\n",
        "    * $x_{2}^{\\prime}=a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n}$\n",
        "    * $\\cdots$\n",
        "    * $x_{n}^{\\prime}=a_{n 1} x_{1}+a_{n 2} x_{2}+\\cdots+a_{n n} x_{n} .$\n",
        "\n",
        "* **Ein Basiswechsel ist ein Spezialfall einer Koordinatentransformation**: [**Basiswechsel im Vektorraum (Transformationsmatrix)**](https://de.wikipedia.org/wiki/Basiswechsel_(Vektorraum)) (lineare Algebra) ist der √úbergang zwischen zwei verschiedenen Basen eines endlichdimensionalen Vektorraums √ºber einem K√∂rper $K$. Beispiele:\n",
        "\n",
        "  * [Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten](https://de.m.wikipedia.org/wiki/Koordinatentransformation#Beispiele)\n",
        "\n",
        "  * [Transformation von Differential-Operatoren](https://de.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Differentialen) (Jacobi-Matrix)\n",
        "\n",
        "  * [Transformation von Vektorfeldern](https://de.m.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Vektorfeldern_und_-Operatoren)\n",
        "\n",
        "  * [Lorentz-Transformationen](https://de.wikipedia.org/wiki/Lorentz-Transformation) zur Beschreibungen von Ph√§nomenen in verschiedenen Bezugssystemen (Spezielle Relativit√§tstheorie). Verbinden in vierdimensionaler Raumzeit die Zeit- und Ortskoordinaten, mit denen verschiedene Beobachter angeben, wann und wo Ereignisse stattfinden. Lorentz-Transformationen erhalten Abst√§nde in nichteuklidischer Raumzeit ([Minkowskiraum](https://de.wikipedia.org/wiki/Minkowski-Raum)), Winkel aber nicht, da der Minkowskiraum kein normierter Raum ist.\n",
        "\n",
        "  * [Galilei-Transformation](https://de.wikipedia.org/wiki/Galilei-Transformation): √Ñquivalent zu Lorentz-Transformationen im dreidimensionalen euklidischen Raum. Anwendbar, wenn sich Bezugssysteme durch geradlinig-gleichf√∂rmige Bewegung, Drehung und/oder eine Verschiebung in Raum oder Zeit unterscheiden. Alle Beobachtungen von Strecken, Winkeln und Zeitdifferenzen stimmen in beiden Bezugssystemen √ºberein; alle beobachteten Geschwindigkeiten unterscheiden sich um die konstante Relativgeschwindigkeit der beiden Bezugssysteme.\n",
        "\n",
        "  * [Eichtransformation](https://de.wikipedia.org/wiki/Eichtransformation) ver√§ndert die Eichfelder einer physikalischen Theorie (z. B. die elektromagnetischen Potentiale oder die potentielle Energie) dergestalt, dass die physikalisch wirksamen Felder (z. B. das elektromagnetische Feld oder ein Kraftfeld) und damit alle beobachtbaren Abl√§ufe dabei die gleichen bleiben, z. B. die Verschiebung des Nullpunkts der potentiellen Energie, die Wahl des Referenzpotentials bei der Messung elektrischer Spannungen, ein konstanter Phasenfaktor an der komplexen Wellenfunktion der Quantenmechanik.\n",
        "\n",
        "* [Youtube Video 1](https://www.youtube.com/watch?v=CR7e7Zc0QLg) und [Youtube Video 2](https://www.youtube.com/watch?v=FFVauAY_FMI)\n",
        "\n",
        "**Eigenchris Video series:**\n",
        "\n",
        "* [Eigenchris: Tensors For Beginners (-1): Motivation](https://www.youtube.com/watch?v=8ptMTLzV4-I&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1: Forward and Backward Transformations](https://www.youtube.com/watch?v=sdCmW5N1LW4&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=3)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1.5: Correction on Forward + Backward Transforms](https://www.youtube.com/watch?v=ipRrCPvftTk&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=4)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 2: Vector definition](https://www.youtube.com/watch?v=uPbBDToXjBw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=5)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 3: Vector Transformation Rules](https://www.youtube.com/watch?v=A1h_eucHFW4&t=177s)\n",
        "\n"
      ],
      "metadata": {
        "id": "k7tUmz4d9FUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ as Kronecker Product, of Tensors and of Tensor Spaces*"
      ],
      "metadata": {
        "id": "zTjh6Dkf9Imc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product** (generalization of the outer product)\n",
        "\n",
        "* the [tensor product V ‚äó W](https://en.m.wikipedia.org/wiki/Tensor_product) of two vector spaces V and W (over the same field) is a vector space which can be thought of as the space of all tensors that can be built from vectors from its constituent spaces using an additional operation which can be **considered as a generalization and abstraction of the outer product**.\n",
        "\n",
        "**Tensor Product: The 2 different tensor product use cases**\n",
        "\n",
        "* the \"little\" tensor product which combines individual tensors\n",
        "\n",
        "* the \"big\" tensor product which combines entire tensor vector spaces\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_62.png)\n",
        "\n",
        "* the Kronecker product, sometimes denoted by ‚äó,[1] is an operation on two matrices of arbitrary size resulting in a block matrix.\n",
        "* It is a generalization of the outer product (which is denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis.\n",
        "* The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n",
        "* The Kronecker product is a special case of the tensor product, so it is bilinear and associative"
      ],
      "metadata": {
        "id": "JRvzAr6v9RWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Tensor Product $\\otimes$ vs Kronecker Product $\\otimes$**\n",
        "\n",
        "* They are technically different things, but highly related to each other.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 13: Tensor Product vs Kronecker Product](https://www.youtube.com/watch?v=qp_zg_TD0qE&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=16)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_40.png)\n",
        "\n",
        "* **Here are examples on how the tensor products works:**\n",
        "\n",
        "* Basis for Vector Space $V$\n",
        "\n",
        "> $\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}} \\in V$\n",
        "\n",
        "Basis for Dual Space $V*$\n",
        "\n",
        "> $\\epsilon^{1}, \\epsilon^{2} \\in V^{*}$\n",
        "\n",
        "* And the covectors are linear functions that are defined by these rules here, where the covector $\\epsilon^{i}$ is acting on the vector $\\overrightarrow{e_{j}}$ gives the Kronecker delta $\\delta_{j}^{i}$ as the result (=we get 1 if i and j are the same and 0 if not).\n",
        "\n",
        "> $\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{j}^{i}=\\left\\{\\begin{array}{l}1, i=j \\\\ 0, i \\neq j\\end{array}\\right.$\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)\n",
        "\n",
        "**To demonstrate that this is a linear map, we can pass in an input vector $\\overrightarrow{v}$**\n",
        "\n",
        "To get the output we can just we pass $v$ to the covector $\\epsilon$ first\n",
        "\n",
        "> $\\left(\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}\\right)(\\vec{v})$\n",
        "\n",
        "Pass v to the covector:\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}(\\vec{v})\\right)$\n",
        "\n",
        "Then we expand v as a linear combination of the basis vectors\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "The we bring the components outside since covectors are linear functions (we can scale before or after):\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "And the covector acting on a vector becomes a Kronecker delta:\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "Then Kronecker index cancellation rule we can remove the $k$ indexes and get j:\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "So this is a function that takes a vector input and produces a vector output\n",
        "\n",
        "\n",
        "**Here are examples on how the Kronecker products works:**\n",
        "\n",
        "* Here you get a row of columns when you take the first array on the left and distribute every element to the second array on the right:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_46.png)\n",
        "\n",
        ".. and multiplying this with another column vector we get the same thing (here you get a column of rows of columns)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_47.png)\n",
        "\n",
        "\n",
        "**Summary**\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "* these coefficient are the entries of a matrix (on the tensor product side)\n",
        "\n",
        "* on the Kronecker delta side you can see this matrix !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_48.png)\n",
        "\n",
        "* Tensor product and Kronecker product are doing basically the same kind of thing\n",
        "\n",
        "* It‚Äôs just the tensor product is combining the abstract vector and the abstract covector in the land of algebraic symbols\n",
        "\n",
        "* And the Kronecker product is combining the the vector array and the covector array  in the land of arrays\n",
        "\n",
        "* But the components that we get from the tensor are just the components of the matrix that we get from Kronecker product\n",
        "\n",
        "* So they are sort of the same operation they just do the work in different contexts\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_49.png)"
      ],
      "metadata": {
        "id": "kz-n4Xvc9TFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 1*\n",
        "\n",
        "We can see a lot of similarities between:\n",
        "* the Kronecker product for matrices and\n",
        "* the tensor product for abstract Hilbert spaces.\n",
        "\n",
        "* If a vector can be expressed as the tensor product of two vectors, we call it a product vector, i.e., $|\\Psi\\rangle$ **is a product vector** if $|\\Psi\\rangle=|\\phi \\otimes \\chi\\rangle$ for some $|\\phi\\rangle$ and $|\\chi\\rangle$. Any nonproduct vector is called entangled. The entangled states of a composite system occupy a distinguished position in the interpretation of quantum mechanics.\n",
        "\n",
        "* We would like to define a <font color=\"blue\">**tensor product for abstract Hilbert spaces**</font>.\n",
        "\n",
        "* The main need for this comes from the necessity of <font color=\"blue\">**describing composite quantum systems**</font>.\n",
        "\n",
        "* If we have two different systems $\\mathrm{A}$ and $\\mathrm{B}$, then **we have separate Hilbert spaces**, say $\\mathcal{H}_{A}$ and $\\mathcal{H}_{B}$, for describing the quantum states of these systems.\n",
        "\n",
        "* Basically a vector $|\\alpha\\rangle_{A}$ in $\\mathcal{H}_{A}$ gives a state of $\\mathrm{A}$ and a vector $|\\beta\\rangle_{B}$ in $\\mathcal{H}_{B}$ gives a state of $\\mathrm{B}$.\n",
        "\n",
        "* In a similar way, we should <font color=\"blue\">**have an entirely different Hilbert space** $\\mathcal{H}_{A B}$ for describing the states of the composite system AB.</font>\n",
        "\n",
        "* In particular, we would like to have a vector in $\\mathcal{H}_{A B}$ that describes the state of $\\mathrm{AB}$ such that $\\mathrm{A}$ is in state $\\mid \\alpha \\rangle_{A}$ and $\\mathrm{B}$ is in state $|\\beta\\rangle_{B}$. We will denote this state as $|\\alpha\\rangle_{A} \\otimes|\\beta\\rangle_{B}$.\n",
        "\n",
        "* We will also want to have this product $\\otimes$ be such that superpositions of states in $\\mathrm{A}$ or in B could also be equivalently described as superpositions of states in $\\mathrm{AB} ;$ hence **distributivity**.\n",
        "\n",
        "http://www.physics.metu.edu.tr/~sturgut/p455/qm-math4.pdf"
      ],
      "metadata": {
        "id": "L_0hUsHx9Wtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 2*\n",
        "\n",
        "* **The two notions represent operations on different objects: Kronecker product on matrices; tensor product on linear maps between vector spaces**.\n",
        "\n",
        "* But there is a connection: Given two matrices, we can think of them as representing linear maps between vector spaces equipped with a chosen basis.\n",
        "\n",
        "* The Kronecker product of the two matrices then represents the tensor product of the two linear maps.\n",
        "\n",
        "* (This claim makes sense because the tensor product of two vector spaces with distinguished bases comes with a distinguish basis.)\n",
        "\n",
        "https://math.stackexchange.com/questions/203947/tensor-product-and-kronecker-product"
      ],
      "metadata": {
        "id": "o08kUOZC9Ym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 3*\n",
        "\n",
        "Sometimes the Kronecker product is also called direct product\n",
        "or tensor product.\n",
        "\n",
        "https://www.worldscientific.com/doi/pdf/10.1142/9789811202520_0002"
      ],
      "metadata": {
        "id": "SdLDqnuT9aid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 4*\n",
        "\n",
        "* |u> <v| is a way of writing the <font color=\"blue\">**tensor product of a vector and a dual vector**</font> (ie, an element of the Hilbert space and an element of its dual, which is usually casually identified with the Hilbert space using the inner product).\n",
        "\n",
        "* This is a linear operator on the Hilbert space, sending |w > to <v|w>|u>.\n",
        "\n",
        "* In general, the tensor product of a vector space and its dual is the space of (finite rank) linear operators on the vector space.\n",
        "\n",
        "* On the other hand, |u>|v> is an element of the tensor product of the vector space with itself, usually used in physics for describing a composite of two identical systems. Again, since there is an isomorphism between the vector space and its dual, there is one between the space of composite states and the space of linear operators. This is interesting, but I've never seen this put to good use.\n",
        "\n",
        "* Finally, **the Kronecker product is just a particular representation of the tensor product, convenient for dealing with tensor products of linear operators**.\n",
        "\n",
        "Source https://www.physicsforums.com/threads/difference-between-the-outer-product.236244/"
      ],
      "metadata": {
        "id": "Xim2BtEb9cXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 5*\n",
        "\n",
        "* the Kronecker product and tensor product essentially have the same mathematical \"actions\" (an expanded matrix with dimensions equal to the product of the two matrices), but the tensor product explicitly applies only to matrices representing linear maps.\n",
        "\n",
        "* Every matrix represents a linear map.\n",
        "\n",
        "* Linear maps are in some sense more general than matrices. They are also different (types of) objects, even though matrices can be used to represent _some_ linear maps. The matrix representation of a particular linear map depends on the choice of basis of the domain and target space, for example, whereas the linear map itself is invariant under such choices. On the other hand a matrix can also represent a bilinear form, not just a linear map. Consider reading Axler's _Linear Algebra Done Right_ for a good explanation of some of such subtleties.\n",
        "\n",
        "https://math.stackexchange.com//questions/203947"
      ],
      "metadata": {
        "id": "iGV-6fNH9eGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covariance (Covector) and Contravariance (Vector): Tensor Notation for Vectors & Covectors in Coordinate Systems*"
      ],
      "metadata": {
        "id": "icJDI9EQ9gc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of a Tensor**\n",
        "\n",
        "> **Tensor = an object that is invariant under a change of coordinates, and... has components that change in a special, predictable way under a change of coordinates**\n",
        "\n",
        "* the direction and the lengths of an object are invariant (=vector)\n",
        "\n",
        "> **The vector components are NOT invariant. They change  depending on the coordinate system they use.**\n",
        "\n",
        "* if you know how to switch between coordinate system, you should be able to know how to switch between vector components (forward and backward)\n",
        "\n",
        "> **Tensor: A collection of vectors and convectors combined together using the tensor product.** (abstract definition)\n",
        "\n",
        "> **Tensors as partial derivatives and gradients that transform with the Jacobian matrix.**\n",
        "\n",
        "* Tensoren sind Gr√∂ssen, mit deren Hilfe man Skalare, Vektoren und weitere Gr√∂ssen analoger Struktur in ein einheitliches Schema zur Beschreibung mathematischer und physikalischer Zusammenh√§nge einordnen kann.\n",
        "\n",
        "* Sie sind definiert durch ihre Transformationseigenschaften gegen√ºber orthogonalen Transformationen wie z.B. Drehungen. Es geht darum, was √§ndert sich, was √§ndert sich nicht, wenn man das Bezugssystem √§ndert?\n",
        "\n",
        "* Die Besonderheit von Tensorgleichungen ist, dass sie transformations-invariant sind. Wenn es gelingt, einen physikalischen Sachverhalt in Tensorschreibweise zu formulieren, so kann man wegen der speziellen Art, wie Tensoren transformieren, sicher sein, dass die Gleichungen in jedem beliebigen Koordinatensystem gelten.\n",
        "\n",
        "* **Rang (oder Stufe) eines Tensors und Indizes**: Tensoren haben **Indizes**. Die Anzahl der Indizes gibt den Rang oder die Stufe des Tensors an. Die Indizes laufen entsprechend der **Dimension** $D$ des Raumes √ºber $1 . . D$. Bei der 4-dimensionalen Raumzeit beginnt die Nummerierung bei $0,$ wobei der Index 0 die zeitliche Komponente betrifft. Beim Arbeiten mit den Tensoren muss die **Reihenfolge der Indices** immer klar sein. Das Element $t_{12}$ eines Tensors ist in der Regel vom Element $t_{21}$ verschieden.\n",
        "\n",
        "  * Der einfachste Tensor ist ein Tensor mit Rang 0 . Dabei handelt es sich einfach um einen **Skalar**. Ein Skalar hat eigentlich keine Komponenten, sondern ist nur ein einzelner Wert und ben√∂tigt somit keinen Index; daher der Rang $0 .$\n",
        "\n",
        "  * Ein Tensor mit nur einem Index nennt man auch **Vektor**. Man sagt, der Vektor ist ein Tensor mit dem Rang 1. Der Index hat so viele Werte, wie die Dimension des Vektors. Bei einem 3 -dimensionalen Vektor hat der Index also 3 Werte (z.B: 1,2,3$),$ weil der Vektor 3 Komponenten hat: $\\vec{V}=\\left(V^{1}, V^{2}, V^{3}\\right)$.\n",
        "\n",
        "  * Ein Tensor vom Rang 2 hat 2 Indizes und stellt eine quadratische **Matrix** dar usw.\n",
        "\n",
        "  * Jede Tensor-Komponente kann eine Funktion oder eine Zahl sein. In der AR (allgemeinen Relativitatstheorie) sind die Komponenten in der Regel Funktionen der Raumzeit.\n",
        "\n",
        "* **Dimension eines Tensors**: Ein Vektor oder Tensor ist ein Objekt, welches Komponenten hat. Die Anzahl der Komponenten eines Vektors enstpricht der Dimension D des Raumes. Ein 3-dimensionaler Vektor besteht somit aus 3 Komponenten. Ein 3-dimensionaler Tensor der Stufe 2 besteht aus 3x3 Komponenten usw.\n",
        "\n",
        "> **Typ eines Tensors**: Wenn **Rang, Dimension und Komponenten-Arten** (Anzahl Indizes oben und unten) von Tensoren √ºbereinstimmen, dann sagt man, die Tensoren sind vom selben Typ. Dies muss bei der Tensor-Arithmetik beachtet werden.\n",
        "\n",
        "* **Tensor-Arithmetik**: Weil Skalare und Vektoren Subklassen von Tensoren sind ist zu erwarten, dass Tensoren denselben bekannten Rechenregeln f√ºr Addition, Subtraktion, Multiplikation und Division folgen.  Dies stimmt meistens, jedoch mit einigen √Ñnderungen und Einschr√§nkungen. Tensoren zeigen zudem neue Eigenschaften, die es bei Skalaren und Vektoren nicht gibt. [Source](http://walter.bislins.ch/physik/index.asp?page=Tensor%2DArithmetik)\n",
        "\n",
        "* **Tensor vs. Matrix**: A tensor of rank n is a mathematical object that has n indices and m$^n$ components. **Matrix is a tensor rank 2, because you need 2 basis vectors. Tensors rank 3 are the boxes with several stacked matrices**.\n",
        "\n",
        "  * Matrix is just and array of numbers, meanwhile a tensor (like stress tensor) obeys specific transformation rules and has a rules physical meaning. We can use a matrix to represent a tensor, but a tensor has a deeper physical significance. Matrix: I can just write down the matrix and its elements, but don‚Äôt have to add anything else\n",
        "\n",
        "  * Tensor: I need to specify the coordinate system, the components, and the basis vectors that each of those components correspond to. A tensor requires more detailed specification than a matrix. And it has these transformation properties where it‚Äôs invariant under a change of coordinate system, it has physical significance.\n",
        "\n",
        "* See video: [Introduction to Tensors](https://www.youtube.com/watch?v=uaQeXi4E7gA&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=1&t=187s)"
      ],
      "metadata": {
        "id": "icw4h44B9uTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoren und Koordinatensysteme**\n",
        "\n",
        "* In conventional math syntax we make **use of covariant indexes when dealing with Cartesian coordinate systems** $(x_{1},x_{2},x_{3})$ frequently without realizing this is a limited use of tensor syntax as covariant indexed components.\n",
        "\n",
        "* [Curvilinear coordinate systems](https://en.wikipedia.org/wiki/Curvilinear_coordinates), such as **cylindrical or spherical coordinates**, are often used in physical and geometric problems. Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and **covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another**.\n",
        "\n",
        "* In der Anwendung steht das Tensorsymbol immer f√ºr eine bestimmte Bedeutung. Zum Beispiel den Ort eines Teilchens. Der Ort kann in verschiedenen Koordinatensystemen gemessen werden. Entsprechend haben die Komponenten des Tensors in jedem Koordinatensystem andere Werte. Doch der Tensor bleibt derselbe (zB L√§nge eines Vektors), egal in welchem Koordinatensystem er gemessen wird.\n",
        "\n",
        "* **Gradient Vector**: Tensor calculus provides a generalization to the gradient vector formula from standard calculus **that works in all coordinate systems**: $\\nabla F=\\nabla_{i} F \\vec{Z}^{i}$ where: $\\nabla_{i} F=\\frac{\\partial F}{\\partial Z^{i}}$. In contrast, for standard calculus, the gradient vector formula is dependent on the coordinate system in use (example: Cartesian gradient vector formula vs. the polar gradient vector formula vs. the spherical gradient vector formula, etc.). In standard calculus, each coordinate system has its own specific formula, unlike **tensor calculus that has only one gradient formula that is equivalent for all coordinate systems**. This is made possible by an understanding of the metric tensor that tensor calculus makes use of."
      ],
      "metadata": {
        "id": "XavAxkWY9wMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covariance and Contravariance**\n",
        "\n",
        "> In multilinear algebra and tensor analysis, [covariance and contravariance](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors) describe how the quantitative description of certain geometric or physical entities changes with a [change of basis](https://en.wikipedia.org/wiki/Change_of_basis).\n",
        "\n",
        "* **The reason of having covariant or contravariant tensors is because you want to represent the same thing in a different coordinate system. Such a new representation is achieved by a transformation using a set of partial derivatives. In tensor analysis, a good transformation is one that leaves invariant the quantity you are interested in.**\n",
        "\n",
        "* Man unterscheidet in der [Tensor notation](https://en.wikipedia.org/wiki/Tensor_calculus#Syntax) bei Tensoren zwei Arten von Komponenten (*Jeder Tensor kann sowohl in kontravarianten, als auch in kovarianten Komponenten dargestellt werden*):\n",
        "\n",
        "  * **contravariant** upper component index: $v^{i}$\n",
        "\n",
        "  * **covariant** lower component index: $v_{i}$\n",
        "\n",
        "* [**Mixed Tensors**](https://en.wikipedia.org/wiki/Mixed_tensor) with both covariant and contravariant (for Tensors rang 2 or higher): $T^{m}{ }_{n}$. Consider related tensors: $T_{\\alpha \\beta \\gamma}, T_{\\alpha \\beta}^{\\gamma}, T_{\\alpha}{ }^{\\beta}{ }_{\\gamma}, T_{\\alpha}{ }^{\\beta \\gamma}, T_{\\beta \\gamma}^{\\alpha}, T^{\\alpha}{ }_{\\beta}^{\\gamma}, T^{\\alpha \\beta}{ }_{\\gamma}, T^{\\alpha \\beta \\gamma}$. The first one is covariant, the last one contravariant, and the remaining ones mixed.\n",
        "\n",
        "  * A mixed tensor of type or valence $\\left(\\begin{array}{l}M \\\\ N\\end{array}\\right),$ also written \"type $(M, N)$\" which has $M$ contravariant indices and $N$ covariant indices can be defined as a linear function which maps an $(M+N)$ -tuple of $M$ one-forms and $N$ vectors to a scalar.\n",
        "\n",
        "  * a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the [tangent bundle](https://en.wikipedia.org/wiki/Tangent_bundle) as well as the [cotangent bundle](https://en.wikipedia.org/wiki/Cotangent_bundle)\n",
        "\n",
        "* **Pushforwards (covariant) & Pullbacks (contravariant)**:\n",
        "\n",
        "  * Vectors with upper-index objects have [pushforwards](https://de.m.wikipedia.org/wiki/Pushforward), which are covariant.\n",
        "\n",
        "  * Covectors with lower-index objects have [pullbacks](https://de.m.wikipedia.org/wiki/R√ºcktransport), which are contravariant.\n",
        "\n",
        "  * In category theory covariance and contravariance are properties of [functors](https://en.m.wikipedia.org/wiki/Functor). Sometimes, contravariant functors are called [\"cofunctors\"](https://en.m.wikipedia.org/wiki/Functor).\n",
        "\n",
        "* Der Wechsel von einem Tensor in kontravarianter Darstellung zu einem Tensor in kovarianter Darstellung, wird [**\"Index ziehen\" oder Index Manipulation**](http://walter.bislins.ch/physik/index.asp?page=Index%2DManipulation+per+Metrik%2DTensor) genannt. Die Umrechnung geht durch Multiplikation mit dem sog. **Metrischen Tensor**.\n",
        "\n",
        "  * A given **contravariant index of a tensor can be lowered using the [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor)** $g_{\\mu v}$ and a given covariant index can be raised using the inverse metric tensor $g^{\\mu v}$. Thus, $g_{\\mu v}$ could be called the index lowering operator and $g^{\\mu v}$ the index raising operator.\n",
        "\n",
        "* On a manifold, a tensor field will typically have multiple, upper and lower indices, where [Einstein notation](https://en.m.wikipedia.org/wiki/Einstein_notation) is widely used. * Tensors notation allows a vector $(\\vec{V})$ to be decomposed into an [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation) representing the [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) of a [basis vector](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) $\\left(\\vec{Z}_{i}\\right.$ or $\\left.\\vec{Z}^{i}\\right)$ with a component vector $\\left(V_{i}\\right.$ or $\\left.V^{i}\\right)$: $\\vec{V}=V^{i} \\vec{Z}_{i}=V_{i} \\vec{Z}^{i}$.\n",
        "\n",
        "* *Exkurs: In einem engeren Wortsinn bezeichnet kovariant in der mathematischen Physik Gr√∂√üen, **die wie Differentialformen transformieren**. Diese kovarianten Gr√∂√üen $P$ bilden einen Vektorraum $\\mathcal{V}$, auf dem eine Gruppe von linearen Transformationen wirkt. Die Menge der linearen Abbildungen der kovarianten Gr√∂√üen in die reellen Zahlen $Q: P \\mapsto Q(P) \\in \\mathbb{R}, \\quad Q(a P+b \\tilde{P})=a Q(P)+b Q(\\tilde{P})$ bildet den zu $\\mathcal{V}$ dualen Vektorraum $\\mathcal{V}^{*}$. Schreiben wir die transformierten, kovarianten Gr√∂√üen $P^{\\prime}$ mit einer Matrix $\\Lambda$ als $P^{\\prime}=\\Lambda P$ dann definiert $Q^{\\prime}\\left(P^{\\prime}\\right)=Q(P)$ das kontravariante oder kontragrediente Transformationsgesetz des Dualraumes $Q^{\\prime}=\\Lambda^{-1 \\mathrm{~T}} Q$. Wegen $\\left(\\Lambda_{2}\\right)^{-1 \\mathrm{~T}}\\left(\\Lambda_{1}\\right)^{-1 \\mathrm{~T}}=\\left(\\Lambda_{2} \\Lambda_{1}\\right)^{-1 \\mathrm{~T}}$ gen√ºgt die kontravariante Transformation derselben Gruppenverkn√ºpfung wie die kovariante Transformation.*\n",
        "\n",
        "* Quellen: [Ko- und kontravariante Darstellung](https://www.math.tugraz.at/~ganster/lv_vektoranalysis_ss_10/20_ko-_und_kontravariante_darstellung.pdf), [Kovarianz und Kontravarianz von Vektoren](http://walter.bislins.ch/physik/index.asp?page=Kovarianz+und+Kontravarianz+von+Vektoren), [Kovariante und Kontravariante Komponenten](http://walter.bislins.ch/physik/index.asp?page=Kovariante+und+Kontravariante+Komponenten). Siehe auch Video: [Tensorrechnung, 3.1 : Was bedeutet Kovariant und kontravariant?](https://www.youtube.com/watch?v=PNRoBOzije8) und [Kovarianz, Kontravarianz, Tensoren](https://av.tib.eu/media/19916)\n"
      ],
      "metadata": {
        "id": "g1CsjpEU90rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein beliebiger Vektor kann als Linearkombination entweder der kovarianten Basisvektoren oder der kontravarianten Basisvektoren dargestellt werden** (die Transformationen invers zueinander).\n",
        "\n",
        "* **Kovariante Basisvektoren** $\\vec{b}_{u}$ sind kombiniert mit **kontravariante Koordinaten** $a_{u_{i}}$. Heisst auch kontravarianter Vektor (besser: kontravarianter Koordinatenvektor) und sind Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "* **Kontravariante Basisvektoren** $\\vec{b}_{u_{i}}^{*}$ kombiniert mit **kovarianten Koordinaten** $a_{u_{i}}^{*}$. Heisst auch kovarianter Vektor bzw. Covector (Kovektoren als Linearformen von Normalenvektoren) und sind normal zu den Koordinatenfl√§chen.\n",
        "\n",
        "* **Vektordarstellungen**: $\\vec{a}=\\sum_{i=1}^{n} a_{u_{i}} \\vec{b}_{u_{i}}=\\sum_{i=1}^{n} a_{u_{i}}^{*} \\vec{b}_{u_{i}}^{*}$\n",
        "\n",
        "* Wenn $\\vec{g}_{i} \\cdot \\vec{g}^{j}=\\delta_{i}^{j}$ (**Kronecker Delta / Skalarprodukt ist Null**) dann heissen $\\left\\{\\overrightarrow{g_{i}}\\right\\}$ und $\\left\\{\\overrightarrow{g^{j}}\\right\\}$ **reziprok** zueinander. Die beiden Klassen von Basisvektoren sind dual bzw. reziprok zueinander (siehe [duale Basis](https://de.wikipedia.org/wiki/Duale_Basis)). Diese beiden Basen bezeichnet man als **holonome Basen**.\n",
        "\n",
        "* Diese kreuzweise Paarung (kontra-ko bzw. ko-kontra) sorgt daf√ºr, dass der Vektor $\\vec{a}$ unter Koordinatentransformation invariant ist (zB Geschwindigkeit eines Teilchens), da die Transformationen von Koordinaten und Basisvektoren invers zueinander sind und sich gegenseitig aufheben."
      ],
      "metadata": {
        "id": "51tAOvER93PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Vectors): Basiswechsel und Vector-(Coordinate)-Transformation (Contravariant)*"
      ],
      "metadata": {
        "id": "VkuVX0zf959A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vektorkoordinaten verhalten sich kontravariant**\n",
        "\n",
        "* **Problem**: For example, we consider the transformation from one coordinate system $x^{1}, \\ldots, x^{n}$ to another $x^{\\prime}, \\ldots, x^{\\prime n}$\n",
        "$x^{i}=f^{i}\\left(x^{\\prime}, x^{\\prime 2}, \\ldots, x^{\\prime n}\\right)$ where $f^{i}$ are certain functions.\n",
        "Take a look at a couple of specific quantities. How do we transform coordinates? The answer is \"**coordinate differentials**\":\n",
        "\n",
        "> $d x^{i}=\\frac{\\partial x^{i}}{\\partial x^{\\prime k}} d x^{\\prime k}$\n",
        "\n",
        "* **Darstellung**: $c^{1}$ (upper index, hochgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Vektorkoordinaten verhalten sich kontravariant. (wahrscheinlich: kovariante Basisvektoren (= \"kontravarianten Vektor\", weil Koordinaten kontravariant): Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da, wie oben beschrieben, die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "  * Every quantity which under a transformation of (vector) coordinates, transforms like the coordinate differentials is called a contravariant tensor.\n",
        "\n",
        "  * **Vectors exhibit a behavior of changing scale inversely to changes in scale to the reference axes are called contravariant** (see second example below). As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).\n",
        "\n",
        "* **Examples**: (of contravariant vectors / vectors with contravariant components)\n",
        "\n",
        "  * position of an object relative to an observer, or any derivative of position with respect to time: velocity, acceleration, jerk, displacement, momentum, force.\n",
        "\n",
        "  * For instance, by changing scale from meters to centimeters (that is, **dividing the scale of the reference axes by 100**), the components of a measured velocity vector are **multiplied by 100**.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * In physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem.\n",
        "\n",
        "  * A contravariant vector or tangent vector (often abbreviated simply as vector, such as a direction vector or velocity vector) has components that contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be contravariant.\n",
        "\n",
        "  * If the **reference axes** were rotated in one direction, the **component representation** of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way. In Einstein notation, contravariant components are denoted with upper indices as in $\\mathbf{v}=v^{i} \\mathbf{e}_{i}$ (note: implicit summation over index \"i\")\n",
        "\n",
        "https://math.stackexchange.com/questions/8170/intuitive-way-to-understand-covariance-and-contravariance-in-tensor-algebra\n",
        "\n",
        "*Kontravariantes Transformationsverhalten*\n",
        "\n",
        "* Grundfrage: Wie werden Normalenvektoren in den alten Basisvektoren zu Normalenvektoren in neuen Basivektoren?\n",
        "\n",
        "* Die normalen Vektoren gehen gegen die Basis, d.h. kontravariant\n",
        "\n",
        "* Gegeben einen Normalenvektor a aus dem Vektorraum V, und diesen zerlegen in die Basisvektoren in alter Basis und dann transformieren in neue Basis:\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_03.png)\n",
        "\n",
        "Nun Kovektor angewandt auf Summe von Zahlen mal Vektoren (Vektoren sind kontravariant):\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_04.png)\n",
        "\n",
        "> **Man nennt a einen \"kontravarianten\" Vektor, aber eigentlich muss man sagen, dass seine Komponenten kontravariant transformieren. Die eigentlichen Vektoren aus dem Originalvektorraum sind kontravariant.**\n",
        "\n",
        "\n",
        "* and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates\n",
        "\n",
        "* The inverse of a covariant transformation is a **contravariant transformation**.\n",
        "\n",
        "  * Whenever a vector should be invariant under a change of basis, that is to say **it should represent the same geometrical or physical object <u>having the same magnitude and direction as before</u>, its components must transform according to the contravariant rule**.\n",
        "\n",
        "  * Conventionally, indices identifying the components of a vector are placed as upper indices and so are all indices of entities that transform in the same way.\n",
        "\n",
        "* The sum over pairwise matching indices of a product with the same lower and upper indices are invariant under a transformation.\n",
        "\n",
        "* A vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector $\\mathbf{v}$ is given, say, in components $V^{\\prime}$ on a chosen basis $\\mathbf{e}_{i}$. On another basis, say $\\mathbf{e}^{\\prime}{ }_{j}$, the same vector $\\mathbf{v}$ has different components $v^{\\prime j}$ and\n",
        "\n",
        "> $\n",
        "\\mathbf{v}=\\sum_{i} v^{i} \\mathbf{e}_{i}=\\sum_{j} v^{\\prime j} \\mathbf{e}_{j}^{\\prime}\n",
        "$\n",
        "\n",
        "* As a vector, $\\mathbf{v}$ should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its \"real world\" direction and magnitude should appear the same regardless of the basis vectors.\n",
        "\n",
        "* If we perform a change of basis by transforming the vectors $\\mathbf{e}_{i}$ into the basis vectors $\\mathbf{e}_{j}$, we must also ensure that the components $v^{i}$ transform into the new components $v$ to compensate.\n",
        "\n",
        "* The needed transformation of $\\mathbf{v}$ is called the contravariant\n",
        "transformation rule."
      ],
      "metadata": {
        "id": "4bkcyHaG-NV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation: Forward and Backward Transform between Basis Vectors in 2 dimensions**\n",
        "\n",
        "*Forward: build the new basis vectors (not any vector!) from the old basis vectors*:\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "\\text { Old Basis: }\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\\\\n",
        "\\text { New Basis: }\\left\\{\\widetilde{e_{1}}, \\widetilde{e_{2}}\\right\\}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_08.png)\n",
        "\n",
        "* Forward transformation (create new basis vectors):\n",
        "\n",
        "$\n",
        "\\begin{array}{ll}\n",
        "\\widetilde{e_{1}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Which gives us for the image above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}=-1 / 2 \\overrightarrow{e_{1}}+1 / 4 \\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Result is in the forward matrix (Notice how the coefficient from the first equation end up in the first column (2 and 1), and how the coefficients from the second equation end up in the second column)\n",
        "\n",
        "$\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\overrightarrow{e_{1}}=1 / 4 \\widetilde{e_{1}}+(-1) \\widetilde{\\overrightarrow{e_{2}}} \\\\\n",
        "\\overrightarrow{e_{2}}=1 / 2 \\widetilde{e_{1}}+2 \\widetilde{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Which gives as the backward matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in 2 dimensions?**\n",
        "\n",
        "* How do forward and backward matrices relate to each other?\n",
        "\n",
        "* They are inverses, and matrix multiplication (dot product) leads to the identity matrix:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "F B &=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "(2*1/4)+(-1/2*-1) & (2*1/2)+(-1/2*2) \\\\\n",
        "(1*1/4)+(1/4*-1) & (1*1/2)+(1/4*2)\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "> This means: $\n",
        "B=F^{-1}\n",
        "$\n",
        "\n",
        "*Reminder: Matrix Multiplication Rules:*\n",
        "\n",
        "$\\begin{aligned}\n",
        "&=\\left[\\begin{array}{cc}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "e & f \\\\\n",
        "g & h\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "ae+bg & af+bh \\\\\n",
        "ce+dg & cf+dh\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Summary of two transformation matrices between old basis and new basis:**\n",
        "\n",
        "> Forward Matrix: $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> Backward Matrix: $\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_09.png)"
      ],
      "metadata": {
        "id": "3Xgqkmin-PcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and Backward Transform to n dimensions (Generalization)**\n",
        "\n",
        "First construct the new basis vectors from the old ones with right coefficients:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=F_{11} \\overrightarrow{e_{1}}+F_{21} \\overrightarrow{e_{2}}+\\cdots+F_{n 1} \\overrightarrow{e_{n}} \\\\\n",
        "\\widetilde{e_{2}}={F_{12} \\overrightarrow{e_{1}}+F_{22} \\overrightarrow{e_{2}}+\\cdots+F_{n 2} \\overrightarrow{e_{n}}}\\\\\n",
        "{\\ldots} \\\\\n",
        "\\widetilde{e_{n}}=F_{1 n} \\overrightarrow{e_{1}}+F_{2 n} \\overrightarrow{e_{2}}+\\cdots+F_{n n} \\overrightarrow{e_{n}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $F$ (notice again **how it's transposed to the coefficients above**!)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "F_{11} & F_{12} & \\ldots & F_{1 n} \\\\\n",
        "F_{21} & F_{22} & \\ldots & F_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "F_{n 1} & F_{n 2} & \\ldots & F_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "**How to facilitate it without writing all these equations above?**\n",
        "\n",
        "Let‚Äôs take an example: in the following equation taken from the table above (not the coefficient matrix, because there the values are transposed!):\n",
        "\n",
        "$F_{12}$ tells us how much of $\\overrightarrow{e_{1}}$ is in $\\widetilde{e_{2}}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{e_{2}}=F_{12} \\overrightarrow{e_{1}}\n",
        "\\end{equation}$\n",
        "\n",
        "**So what I want is $\\sum F_{k j}$ that tells us how much of $\\overrightarrow{e_{k}}$ makes up $\\widetilde{{e}_{j}}$**:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Exkurs: later we will use the Einstein notation and drop the summation sign to make it easier:**\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        " Now we do the same for the backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=B_{11} \\widetilde{e_{1}}+B_{21} \\widetilde{e_{2}}+\\cdots+B_{n 1} \\widetilde{e_{n}} \\\\\n",
        "\\overrightarrow{e_{2}} &=B_{1 2} \\widetilde{e_{1}}+B_{22} \\widetilde{e_{2}}+\\cdots+B_{n 2} \\widetilde{e_{n}} \\\\\n",
        "{\\ldots} \\\\\n",
        "\\overrightarrow{e_{n}} & =B_{1 n} \\widetilde{e_{1}}+B_{2 n} \\widetilde{e_{2}}+\\cdots+B_{n n} \\widetilde{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $B$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "B_{11} & B_{12} & \\ldots & B_{1 n} \\\\\n",
        "B_{21} & B_{22} & \\ldots & B_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "B_{n 1} & B_{n 2} & \\ldots & B_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "Summarize the backward transformation as well:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Now both together forward and backward transformation:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "* The index that we do the summation over (here: k = 1 and j = 1 under the summation sign) is the first letter of the forward or backward transform and **corresponds to the index of the basis vector that we‚Äôre transforming**.\n",
        "\n",
        "* The second index of the transform (j and i at F, B and e) **corresponds to the output basis vector**\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in n dimensions?**\n",
        "\n",
        "How are they related to each other? (once again proving)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Replacing with following at the last term above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\stackrel{\\sim}{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "And you get:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging summation signs:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k}\\left(\\sum_{j} F_{k j} B_{j i}\\right) \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "So now we build the old basis vectors $\\overrightarrow{e_{i}}$ using the summation of the old basis vectors $\\overrightarrow{e_{k}}$\n",
        "\n",
        "Now the following middle part should be an identity matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ = $I$\n",
        "\n",
        "so that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Which means then that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=\\overrightarrow{e_{1}} \\\\\n",
        "\\overrightarrow{e_{2}} &=\\overrightarrow{e_{2}} \\\\\n",
        "& \\vdots \\\\\n",
        "\\overrightarrow{e_{n}} &=\\overrightarrow{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "In order to achieve that we want for this part the following conditions:\n",
        "\n",
        "for this $\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ we want:\n",
        "\n",
        "* it is 1 when i = k\n",
        "\n",
        "* it is 0 when i ‚â† k\n",
        "\n",
        "because that will give us the identity matrix (because 1 is where row is equal to the columns, so i = k, and all else 0):\n",
        "\n",
        "$\\begin{equation}\n",
        "I_{n}=\\left[\\begin{array}{ccccc}\n",
        "1 & 0 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 0 & 1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\cdots & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "So for this operation we have the Kronecker Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\sum_{j} F_{k j} B_{j i}=\\delta_{i k}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=k \\\\\n",
        "0 \\text { if } i \\neq k\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n"
      ],
      "metadata": {
        "id": "7HbfIgCM-RXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Transformation Rules (Contravariant)**\n",
        "\n",
        "* **How can you transform from one basis to the other using the vector components (and not the basis vectors)?**\n",
        "\n",
        "* Now one could apply the forward matrix to the vector components of the old basis $\\overrightarrow{e_{i}}$ and should get the vector components of the new basis $\\widetilde{e_{i}}$ with\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{\\overrightarrow{e_{i}} (old vector components)} \\quad\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]_{\\widetilde{e_{i}} (new  vector components)}\n",
        "\\end{equation}$\n",
        "\n",
        "* We do Forward Transform in Basis Vectors and Backward Transform in Vector Components to get from old to new vector basis!\n",
        "\n",
        "* Achtung: In this case (=for vector components) you need to apply the backward transformation to get from the old components to the new components!.\n",
        "**The reason is: for basis vectors, forward brings us from old to new, and backward from new to old.** But with vector components it‚Äôs the opposite.\n",
        "This will be important for understanding covariance & contravariance!\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "  * So, we take the vector components of the old basis: $\\begin{equation}\n",
        "B\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "  * Then we multiply it with our backward transformation matrix: $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "0.25 & 0.5 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "  * Making a few transformations: $\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "{\\left[\\begin{array}{l}\n",
        "0.25(1)+0.5(1.5) \\\\\n",
        "(-1)(1)+2(1.5)\n",
        "\\end{array}\\right]} \\\\\n",
        "{\\left[\\begin{array}{c}\n",
        "0.25+0.75 \\\\\n",
        "-1+3\n",
        "\\end{array}\\right]}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "  * And the result are the correct vector components of the new basis: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> **Be aware of difference between changes with basis vectors (forward from old to new, backward from new to old) and vector components (forward from new to old, backward from old to new):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_10.png)\n",
        "\n",
        "**Why does this make perfect sense?**\n",
        "\n",
        "* Look at this example: the vector components on the left (original) are [1, 1],\n",
        "\n",
        "* and the size of the new basis vectors $\\widetilde{e_{1}}$ is just double of the old one: $\\begin{equation}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}\n",
        "\\end{equation}$ as well as $\\begin{equation}\n",
        "\\widetilde{e_{2}}=2 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$.\n",
        "\n",
        "* This corresponds to a forward transformation matrix for the basis vector $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ to get the new basis vector\n",
        "\n",
        "* the vector components of the vector $\\vec{v}$ are then in the new coordinate system half: [0.5, 0.5], which makes sense.\n",
        "\n",
        "* **This is because (as said in the beginning) the <u>length and the direction of a vector should never change</u> (it's invariant!), but the vector component can change and in this case they change opposite to the change of the basis vectors to keep the vectors stable as they were in both bcoordinate systems!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_11.png)\n",
        "\n",
        "> **Learning: Vector components behave the opposite way that basis vectors do !**\n",
        "\n",
        "* Similar, **if you rotate the basis vector clockwise, the vector components of the vector $\\vec{v}$ rotate anti-clockwise**.\n",
        "\n",
        "* But remember: $\\vec{v}$ has not moved, just its components changed opposite to the basis vector components!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_12.png)"
      ],
      "metadata": {
        "id": "hPW8cblf-TWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So we have two ways of writing a vector $\\vec{v}$:**\n",
        "\n",
        "  * **a linear combination of the old basis vectors with the coefficient being the old components,**\n",
        "\n",
        "  * **or we can write it as a linear combination of the new basis vectors with the coefficient being the new components**.\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=v_{1} \\overrightarrow{e_{1}}+v_{2} \\overrightarrow{e_{2}}+\\cdots+v_{n} \\overrightarrow{e_{n}}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "as well as:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\widetilde{v_{1}} \\widetilde{{e_{1}}}+\\widetilde{v_{2}} \\widetilde{e_{2}}+\\cdots+\\widetilde{v_{n}} \\widetilde{e_{n}}=\\sum_{j=1}^{n} \\widetilde{v_{j}} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "So both if these summations are equal to $\\vec{v}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "Let‚Äôs bring out the forward and backward transformations at this point.\n",
        "\n",
        "* Forward: $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{i=1}^{n} F_{i j} \\overrightarrow{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward: $\\begin{equation}\n",
        "\\overrightarrow{e_{j}}=\\sum_{i=1}^{n} B_{i j} \\stackrel{\\sim}{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "If we take the old basis and the old components, what we can do is we can use the backward transformation B to replace the old basis vectors\n",
        "\n",
        "> $\\overrightarrow{e_{j}}$ with the new basis vectors from B: $\\begin{equation}\n",
        "=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{j=1}^{n} v_{j}\\left(\\sum_{i=1}^{n} B_{i j} \\widetilde{{e}_{i}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging the summation we get this:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right) \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Now we have $\\vec{v}$ written as a summation of the new basis vectors**.\n",
        "\n",
        "Nut now, let‚Äôs have a look at the middle part of the equation above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "If we compare that to our summation in the step before when we wrote $\\vec{v}$ as a summation of new basis vectors:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "the coefficients have to be the new components $\\widetilde{v}_{i}$. And indeed $\\widetilde{v}_{i}$ equals:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{v}_{i}=\\sum_{j=1}^{n} B_{i j} v_{j}\n",
        "\\end{equation}$\n",
        "\n",
        "we actually used the backward transformation! So, **this is the proof that to move from the old components to the new components we actually used the backwards transformation!**\n",
        "\n",
        "**Summary**: We know how basis vectors transform and how vector components transform (opposite direction).\n",
        "\n",
        "* Now because vector components behave contrary to the basis vectors we say that vector components are contra variant!\n",
        "\n",
        "* And we see later that vectors are contra variant tensors!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_13.png)\n",
        "\n",
        "**This means also we make a small but important change:**\n",
        "\n",
        "Here you can see we wrote the vector $\\vec{v}$ as a linear combination of the old and the new basis:\n",
        "\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v_{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "But since the vector components $v_{i}$ and $\\widetilde{v}_{i}$ behave contra variant we put the index up $v^{i}$ and $\\widetilde{v}^{i}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v^{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}^{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "t8PMvuxn-Vdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Linear Forms): Basiswechsel und Covector-(Coordinate)-Transformation (Covariant)*"
      ],
      "metadata": {
        "id": "Eg2UlbUr-YCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Rules:**\n",
        "\n",
        "* Vector Space $V$ to get from old to new: **Forward Transform in Basis Vectors (covariant) and Backward Transform in Vector Components (contravariant)**\n",
        "\n",
        "* Vector Space $V*$ to get from old to new: **Backward Transform in Basis Vectors (contravariant) and Forward Transform in Vector Components (covariant)**\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "\\overrightarrow{e_{i}} \\text{Old Basis V with Vectors} \\quad \\vec{v} &  \\widetilde{e_{i}} \\text{New Basis V with Vectors} \\quad \\widetilde{v}\\\\\n",
        "\\overrightarrow{\\epsilon_{i}} \\text{Old Dual Basis V* with Covectors} \\quad \\vec{\\alpha} & \\widetilde{\\epsilon_{i}} \\text{New Dual Basis V* with Covectors} \\quad \\widetilde{\\alpha}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "s9ikXvt0-iN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* covectors / linear forms, like scalars (for example differentials), change with basis (=covariant)\n",
        "\n",
        "* change of coordinate system does not change scalar (like temperature), hence linear forms =scalars) bechave covariant with basis (meanwhile direction in vectors change contravariant to keep the original position of a vector)\n",
        "\n",
        "* **All covectors can be written as the linear combination of the dual basis vectors!**\n",
        "\n",
        "* **Covector components can be obtained by counting how many covector lines that the basis vector pierces**\n",
        "\n",
        "* **Covector components transform in the opposite way that vector components do**\n",
        "\n",
        "* Videos:\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 5: Covector Components ](https://www.youtube.com/watch?v=rG2q77qunSw&t=10s)\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 6: Covector Transformation Rules](https://www.youtube.com/watch?v=d5da-mcVJ20&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=9)\n",
        "\n",
        "\n",
        "**Fun facts**\n",
        "\n",
        "* Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!** We need Epsilon $\\epsilon^{n}$ as dual basis.\n",
        "\n",
        "* **Basis Vector in $V$ - Upper-left**: we started with the transformation rules for basis vectors = covariant: from old to new with the forward transform.\n",
        "\n",
        "* **Vector Components in $V$ - Bottom-left**: The transformation rules for vector components are the opposite compared to the basis vectors (from old to new with the backward transform). So they are contra variant.\n",
        "\n",
        "* **Basis Covector in $V*$ - Upper-right (Dual Space)**: Transformation rules for basis covectors are also opposite compared to basis vectors. So basis covectors also transform by the contra variant rule.\n",
        "\n",
        "\n",
        "* **Covector Components in $V*$ - Bottom-right (Dual Space)**: Covector components transform in the same way that basis vectors do = This means that covectors transform covariantly. (and contravariant to vectors components)\n",
        "\n",
        "> **Wie man sieht: man konstruiert (neue) basisvektoren aus (alten) basisvektoren und (neue) vektorkomponenten aus (alten) vektorkomponenten**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_18.png)"
      ],
      "metadata": {
        "id": "WATiQxEx-kZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does it mean when we say the covector has components?**\n",
        "\n",
        "* Look at this example: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}} \\text { = } 2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$\n",
        "\n",
        "* So when we write a **column** vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* What we mean is: This vector is given by the **linear combination of** $2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}$\n",
        "\n",
        "> **It tells you how much of each basis vector is needed to make the vector**\n",
        "\n",
        "**So what exactly are we measuring with when we write [2 1] ?** - **2 of what and 1 of what?**\n",
        "\n",
        "* Remember: Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!**\n",
        "\n",
        "\n",
        "**Epsilon $\\epsilon^{n}$ as dual basis**\n",
        "\n",
        "Take the basis $\\begin{equation}\n",
        "\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\text { for } V \\text { . }\n",
        "\\end{equation}$\n",
        "\n",
        "We introduce two special covectors: $\\epsilon^{1},\\epsilon^{2}: V \\rightarrow \\mathbb{R}$, which are both functions from vectors to numbers (notice how the labels 1 and 2 are now above instead of below).\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0 \\\\\n",
        "\\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)=0 & \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=1\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "(Hint: kovariant basis from $V$ with kontravariant basis from $V*$ equals the identity)\n",
        "\n",
        "this means we can use the Kronecker-Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{i j}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=j \\\\\n",
        "0 \\text { if } i \\neq j\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n",
        "\n",
        "**So what does this epsilon $\\epsilon$ mean?**\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{1}(\\vec{v})=\\epsilon^{1}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "We can rewrite this like this (this addition and scaling can be brought outside the function since it's a linear combination):\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} \\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "From the previous part we know that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "So we can rewrite the equation above as:\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} * 1 +v^{2} * 0\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{1} = \\epsilon^{1}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "And the same goes for $\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})=\\epsilon^{2}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)=v^{1} \\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=v^{2}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{2} = \\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "> The epsilons are projecting out vector components\n",
        "\n",
        "*Directions of epsilon basis vectors*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_15.png)\n",
        "\n",
        "See image below: Write a general covector alpha $\\alpha$ (which can be any covector of our choice) as a linear combination of the epsilon covectors.\n",
        "\n",
        "**This means that epsilon covectors form a basis for the set of all covectors!**\n",
        "\n",
        "> **And for that reason we call these epsilons the dual basis, because they are basis for the dual space $V*$.**\n",
        "\n",
        "* **We can't just flip column vectors on their side to get the row vectors! This works only in an orthonormal basis**.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_16.png)"
      ],
      "metadata": {
        "id": "1Fkz_0Ra-mfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Die Koeffizienten der Linearformen verhalten sich kovariant**\n",
        "\n",
        "* **Problem**: How do we transform some scalar $\\Phi$ ? The answer is \"**derivatives of a scalar**\":\n",
        "\n",
        "> $\\frac{\\partial \\Phi}{\\partial x^{i}}=\\frac{\\partial \\Phi}{\\partial x^{\\prime k}} \\frac{\\partial x^{\\prime k}}{\\partial x^{i}}$\n",
        "\n",
        "* **Darstellung**: $x_{1}$ (lower index, tiefgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Koeffizienten der Linearformen (=zB Skalare) verhalten sich kovariant. (wahrscheinlich: kontravariante Basisvektoren (= \"kovarianten Vektor\", weil Koordinaten kovariant): Normal zu den Koordinatenfl√§chen)\n",
        "\n",
        "  * [Skalar](https://de.m.wikipedia.org/wiki/Skalar_(Mathematik)): In der Physik werden Skalare verwendet zur Beschreibung physikalischer Gr√∂√üen, die **richtungsunabh√§ngig** sind. Beispiele f√ºr skalare physikalische Gr√∂√üen sind die **Masse eines K√∂rpers, seine Temperatur, seine Energie und auch seine Entfernung von einem anderen K√∂rper** (als Betrag der Differenz der Ortsvektoren). Anders gesagt: Eine skalare physikalische Gr√∂√üe √§ndert sich bei √Ñnderungen der Lage oder Orientierung nicht. Wird hingegen f√ºr die vollst√§ndige Beschreibung der Gr√∂√üe eine Richtung ben√∂tigt, wie bei der Kraft oder der Geschwindigkeit, so wird ein Vektor verwendet, bei Abh√§ngigkeit von mehreren Richtungen ein Tensor (genauer: Tensor 2. oder noch h√∂herer Stufe).\n",
        "\n",
        "  * Every quantity which under a coordinate transformation, transforms like the derivatives of a scalar is called a covariant tensor.\n",
        "\n",
        "  * Covectors (also called **dual vectors**) typically have units of the inverse of distance or the inverse of distance with other units. The components of covectors **change in the same way as changes to scale of the reference axes and consequently are called covariant**.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * Koeffizienten der [Linearformen](https://de.wikipedia.org/wiki/Linearform) (=zB Skalare)\n",
        "\n",
        "  * An example of a covector is the gradient, which has units of a spatial derivative, or distance<sup>‚àí1</sup>.\n",
        "\n",
        "  * Examples of covariant vectors generally appear when taking a gradient of a function.\n",
        "\n",
        "* **Beispiele fur Kovarianz [in der Physik](https://de.wikipedia.org/wiki/Kovarianz_(Physik))**\n",
        "\n",
        "  * Unter Galilei-Transformationen transformieren sich die Beschleunigung und die Kraft in den newtonschen Bewegungsgleichungen im gleichen Sinne wie die Ortsvektoren. Daher sind die Newtonschen Bewegungsgleichungen und damit die klassische Mechanik kovariant bzgl. der Gruppe der Galilei-Transformationen.\n",
        "\n",
        "  * Im gleichen Sinne sind die Einstein-Gleichungen der Gravitation in der allgemeinen Relativit√§tstheorie kovariant unter beliebigen (nichtlinearen glatten) Koordinatentransformationen.\n",
        "\n",
        "  * Ebenso ist die Dirac-Gleichung der Quantenelektrodynamik kovariant unter der Gruppe der linearen Lorentz-Transformationen.\n",
        "\n",
        "  * Die linke Seite der Klein-Gordon-Gleichung f√ºr ein Skalarfeld √§ndert sich unter Lorentz-Transformationen nicht, sie ist spezieller invariant oder skalar.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * A covariant vector or cotangent vector (often abbreviated as covector) has components that co-vary with a change of basis.\n",
        "\n",
        "  * That is, the **components must be transformed by the same matrix as the change of basis matrix**. The components of covectors (as opposed to those of vectors) are said to be covariant.  In Einstein notation, covariant components are denoted with lower indices as in $\\mathbf{e}_{i}(\\mathbf{v})=v_{i}$\n",
        "\n",
        "  * A covariant vector has **components (=coordinates) that change oppositely to the coordinates or, equivalently,\n",
        "transform like the reference axes**.\n",
        "\n",
        "  * For instance, the components of the gradient vector of a function $\\nabla f=\\frac{\\partial f}{\\partial x^{1}} \\widehat{x}^{1}+\\frac{\\partial f}{\\partial x^{2}} \\widehat{x}^{2}+\\frac{\\partial f}{\\partial x^{3}} \\widehat{x}^{3}$\n",
        "transform like the reference axes themselves.\n",
        "\n",
        "  * See also [Covariant Transformation](https://en.wikipedia.org/wiki/Covariant_transformation)\n",
        "\n",
        "\n",
        "Die Komponenten eines Kovektors, eines Vektors aus dem Dualraum, transformieren kovariant.\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_05.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "amwdLqv5-oX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The derivative of a function transforms covariantly!** (and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates)\n",
        "\n",
        "* In physics, a [covariant transformation](https://en.wikipedia.org/wiki/Covariant_transformation) is a rule that specifies how certain entities, such as vectors or tensors, change under a change of basis.\n",
        "\n",
        "* The transformation that describes the new basis vectors as a linear combination of the old basis vectors is defined as a covariant transformation.\n",
        "\n",
        "* Conventionally, indices identifying the basis vectors are placed as lower indices and so are all entities that transform in the same way.\n",
        "\n",
        "*Covariant Derivative*\n",
        "\n",
        "* the [covariant derivative](https://en.wikipedia.org/wiki/Covariant_derivative) is a way of specifying **a derivative along tangent vectors of a manifold**.\n",
        "\n",
        "* The name is motivated by the importance of changes of coordinate in physics: the covariant derivative transforms covariantly under a general coordinate transformation, that is, linearly via the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the transformation.\n",
        "\n",
        "* The covariant derivative is a generalization of the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative) / [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) from vector calculus (ps: Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale R√§ume ist das [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential).)\n",
        "\n",
        "* The covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule,\n",
        "\n",
        "  * $\\nabla_{\\mathbf{u}} \\mathbf{v},$ which takes as its inputs:\n",
        "\n",
        "  * (1) a vector, $\\mathbf{u},$ defined at a point $P$, and\n",
        "\n",
        "  * (2) a vector field, $\\mathbf{v}$, defined in a neighborhood of $P$\n",
        "\n",
        "  * The output is the vector $\\nabla_{\\mathbf{u}} \\mathbf{v}(P)$, also at the point $P$.\n",
        "\n",
        "*Covariant derivative and covariant transformation*\n",
        "\n",
        "* The primary difference from the usual directional derivative is that $\\nabla_{\\mathrm{u}} \\mathrm{v}$ must, in a certain precise sense, **be independent of the manner in which it is expressed in a coordinate system**.\n",
        "\n",
        "* A vector may be described as a list of numbers in terms of a basis, **but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis**.\n",
        "\n",
        "* This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation.\n",
        "\n",
        "> **The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: <u>the covariant derivative must change by a covariant transformation</u>.**\n",
        "\n",
        "*In the Euclidean Space*\n",
        "\n",
        "* In the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points. In such a system one translates one of the vectors to the origin of the other, keeping it parallel.\n",
        "\n",
        "* **With a Cartesian (fixed orthonormal) coordinate system \"keeping it parallel\" amounts to keeping the components constant**.\n",
        "\n",
        "* Euclidean space provides the simplest example: a covariant derivative which is obtained by taking the ordinary directional derivative of the components in the direction of the displacement vector between the two nearby points.\n",
        "\n",
        "*In other (more general) Spaces*\n",
        "\n",
        "* In the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in **polar coordinates** in a two dimensional Euclidean plane, **then it contains extra terms that describe how the coordinate grid itself \"rotates\"**.\n",
        "\n",
        "* **In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves**, etc.\n",
        "\n",
        "* **In this case \"keeping it parallel\" does NOT amount to keeping components constant under translation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "u2amkyDK-qT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/covariant_transformation.png)\n",
        "\n",
        "* In the shown example, a vector $\\mathbf{v}=\\sum_{i \\in\\{x, y\\}} v^{i} \\mathbf{e}_{i}=\\sum_{j \\in\\{r, \\phi\\}} v^{\\prime j} \\mathbf{e}_{j}^{\\prime} .$ is described by two different coordinate\n",
        "systems:\n",
        "  * a rectangular coordinate\n",
        "system (the black grid),\n",
        "  * and a radial\n",
        "coordinate system (the red grid).\n",
        "\n",
        "* Basis vectors have been chosen for both\n",
        "coordinate systems: $\\mathbf{e}_{x}$ and $\\mathbf{e}_{\\mathbf{y}}$ for the rectangular coordinate system, and $\\mathbf{e}_{\\mathrm{r}}$\n",
        "and $\\mathbf{e}_{\\phi}$ for the radial coordinate system.\n",
        "\n",
        "* The radial basis vectors $\\mathbf{e}_{\\mathrm{r}}$ and $\\mathbf{e}_{\\phi}$ appear\n",
        "rotated anticlockwise with respect to the\n",
        "rectangular basis vectors $\\mathbf{e}_{\\mathrm{x}}$ and $\\mathbf{e}_{\\mathrm{y}}$.\n",
        "\n",
        "> **The covariant transformation, performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors**.\n",
        "\n",
        "* The coordinates of $v$ must be transformed into the new coordinate system, **but the vector $v$ itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates**.\n",
        "\n",
        "* The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view $v$ from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors $\\mathbf{e}_{\\mathbf{r}}$ and $\\mathbf{e}_{\\Phi}$. compared to how it appeared relative to the rectangular basis vectors $\\mathbf{e}_{x}$ and $\\mathbf{e}_{y}$.\n",
        "\n",
        "* **Thus, the needed contravariant transformation to $v$ in this example is a clockwise rotation.**\n",
        "\n",
        "Example\n",
        "\n",
        "* Consider the example of moving along a curve $\\gamma(t)$ in the Euclidean plane. In polar coordinates, $\\gamma$ may be written in terms of its radial and angular coordinates by $\\gamma(t)=(r(t), \\theta(t))$.\n",
        "\n",
        "* A vector at a particular time $t$ (for instance, the acceleration of the curve) is expressed in terms of $\\left(\\mathbf{e}_{r}, \\mathbf{e}_{\\theta}\\right),$ where $\\mathbf{e}_{r}$ and $\\mathbf{e}_{\\theta}$ are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and [tangential components](https://en.wikipedia.org/wiki/Tangential_and_normal_components).\n",
        "\n",
        "* At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the [Christoffel symbols](https://en.wikipedia.org/wiki/Christoffel_symbols)) serve to express this change.\n",
        "\n",
        "*Another Example*\n",
        "\n",
        "* In a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.\n",
        "\n",
        "* A vector e on a globe on the equator at point Q is directed to the north. Suppose we parallel transport the vector first along the equator until at point P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q.\n",
        "\n",
        "* **Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector**; instead, it has another orientation. **This would not happen in Euclidean space and is caused by the curvature of the surface of the globe**. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature."
      ],
      "metadata": {
        "id": "mpNqLFw7-sbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Bilinear Forms) $\\mathcal{B}_{i i} \\epsilon^{i} \\epsilon^{j} \\rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$*"
      ],
      "metadata": {
        "id": "GCJMH1Oj-vBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor product $\\otimes$ for bilinear maps: Bilinear forms are linear combinations of covector-covector-pairs** (including the metric tensor)\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$\n",
        "\n",
        "* Generalization: Bilinear Forms as Covector-Covector-Pairs\n",
        "\n",
        "* Why not vector-vector-pairs or so? - Bilinear forms take two vector inputs and since covectors take one vector each, a pair of covectors would take two vector inputs.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 12: Bilinear Forms are Covector-Covector pairs](https://www.youtube.com/watch?v=uDRzJIaN2qw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=15)\n",
        "\n",
        "**Advantages of the tensor product for bilinear maps:**\n",
        "\n",
        "* We can write bilinear forms as linear combinations of covector covector pairs and this:\n",
        "\n",
        "  * immediately gives us the transformation rules $\\begin{aligned} \\widetilde{\\epsilon}^{i} &=B_{j}^{i} \\epsilon^{j} & \\widetilde{\\mathcal{B}_{i j}} &=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l} \\\\ \\epsilon^{i} &=F_{j}^{i} \\widetilde{\\epsilon}^{j} & \\mathcal{B}_{i j} &=B_{i}^{k} B_{j}^{l} \\widetilde{B_{k l}} \\end{aligned}$\n",
        "\n",
        "  * the component multiplication formula: $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\Rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$\n",
        "\n",
        "  * and the correct array shape\n",
        "\n",
        "* **Combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors (and not a row and a column vwector like for the Kronecker delta) associated with the covectors**\n",
        "\n",
        "* Meanwhile the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_43.png)\n"
      ],
      "metadata": {
        "id": "svrOb-ni_IT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "ZdLtfLao_KKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 1: write out any bilinear form as a linear combination of covector-covector pairs**\n",
        "\n",
        "*Look at our classic transformation rules*:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{i j}=B_{i}^{k} B_{j}^{l} \\widetilde{\\mathcal{B}_{k l}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{e_{j}}=F_{j}^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "> $\\overrightarrow{e_{j}}=B_{j}^{i} \\widetilde{e_{i}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{\\epsilon}^{i}=B_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\epsilon^{i}=F_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "If we can assume we can write out any bilinear form as a linear combination of covector-covector pairs **to get the transformation rule for these components** we just transform the basis covectors individually:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l} \\epsilon^{k} \\epsilon^{l}$\n",
        "\n",
        "Basis covectors are contra variant, so to build old from the new we use the forward transform $F$\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l}\\left(F_{i}^{k} \\widetilde{\\epsilon}^{i}\\right)\\left(F_{j}^{l} \\widetilde{\\epsilon}^{j}\\right)$\n",
        "\n",
        "and putting these in front gives us this:\n",
        "\n",
        "> $\\mathcal{B}=\\left(F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}\\right) \\widetilde{\\epsilon}^{i}\\widetilde{\\epsilon}^{j}$\n",
        "\n",
        "which as we can see is the correct one:\n",
        "\n",
        "> $\\widetilde{B_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$"
      ],
      "metadata": {
        "id": "-1G-vCul_L_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 2: We can also get the correct component multiplication formula when a bilinear form acts on two vector inputs:**\n",
        "\n",
        "Given:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "> $\\vec{w}=w^{l} \\overrightarrow{e_{l}}$\n",
        "\n",
        "**Proof:** replace components with equations above:\n",
        "\n",
        "> $s=\\mathcal{B}(\\vec{v}, \\vec{w})$\n",
        "\n",
        "Replace the bilinear form and the vectors with their linear combination expansions in some basis\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}, w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Now we pass each of these vector inputs to their corresponding covectors\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i}\\left(v^{k} \\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "\n",
        "$v$ and $w$ come out in front:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\epsilon^{i}\\left(\\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(\\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Replace ($\\overrightarrow{e_{k}}$) and ($\\overrightarrow{e_{l}}$) with Kronecker deltas\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\delta_{k}^{i} \\delta_{l}^{j}$\n",
        "\n",
        "Finally using the index cancellation rules for $k$ and $l$ we get the correct component multiplication formula ends up giving us a single number as a result:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{i} w^{j}$\n"
      ],
      "metadata": {
        "id": "GaERJjMl_Nua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 3**\n",
        "\n",
        "* Remember with linear maps, the new way of writing with the tensor product makes a lot more tensors (with a row of rows): Even though we had two vector inputs we needed to write one as a column and one as a row  flipped on its side to make the multiplication work correctly. Which is awkward, because vectors should always be written as columns and not as rows!\n",
        "\n",
        "* When we write the bilinear forms as a row of rows the matrix multiplication formula makes a lot more sense. We can write out both vectors as columns\n",
        "\n",
        "* **Above is the old way (row vector & column vector both to describe vectors, and the transition matrix in the middle) and on the bottom is the new better way with tensor products $\\otimes$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_42.png)"
      ],
      "metadata": {
        "id": "94JzCC8X_QsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Metric Tensor) $g(\\vec{\\color{blue}v}, \\vec{\\color{blue}w}) \\mapsto \\color{blue}{v^{i}} \\color{blue}{w^{j}} g_{i j}$ for Index Manipulation (contravariant $Z^{ij}$ and covariant $Z_{ij}$)*"
      ],
      "metadata": {
        "id": "YgcZdTDc_TCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor**\n",
        "\n",
        "* In differential geometry, one definition of a [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) is a type of function which takes as input a pair of tangent vectors $v$ and $w$ at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar $g(v, w)$ in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space.\n",
        "\n",
        "* Define length of and angle between tangent vectors when basis changes (same way like a dot product). Use Cases: How to measure the length of a vector when we change the vector (with a linear map) as well as when the basis changes?\n",
        "\n",
        "* Through integration, the metric tensor allows one to define and **compute the length of curves on the manifold** [Source](http://walter.bislins.ch/physik/index.asp?page=Metrik%2DTensor)\n",
        "\n",
        "**Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$. In a given basis we compute the output of a metric tensor using this formula**:\n",
        "\n",
        "> $g(\\vec{\\color{red}v}, \\vec{\\color{blue}w}) \\mapsto \\color{red}{v^{i}} \\color{blue}{w^{j}} g_{i j}$\n",
        "\n",
        "And this is the formula for computing the output of a metric tensor when it acts on two vectors\n",
        "\n",
        "> $\\left[\\begin{array}{ll}\\color{red}{v^{1}} & \\color{red}{v^{2}}\\end{array}\\right]\\left[\\begin{array}{ll}g_{11} & g_{12} \\\\ g_{21} & g_{22}\\end{array}\\right]\\left[\\begin{array}{l}\\color{blue}{w^{1}} \\\\ \\color{blue}{w^{2}}\\end{array}\\right]$\n",
        "\n",
        "**The algebraic properties of metric tensors are**:\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 9: The Metric Tensor](https://www.youtube.com/watch?v=C76lWSOTqnc&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=12)"
      ],
      "metadata": {
        "id": "_mR6eUfX_WoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)"
      ],
      "metadata": {
        "id": "L2D_3hgX_oLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). **Metric tensors are (0,2) tensors, because it transforms using tow covariant rules**:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "KPlaTJsa_qJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensors for measuring length or angles**\n",
        "\n",
        "* Is a tensor whose components in a given vector basis are given by the **dot products of the basis vectors**:\n",
        "\n",
        "  * <font color=\"blue\">$g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}$</font>\n",
        "\n",
        "* Since the dot products don't care about the order of the input $g_{i j}=g_{j i}$, which means the **tensor is symmetric** along the diagonal line (spur).\n",
        "\n",
        "  * $g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}=\\overrightarrow{e_{j}} \\cdot \\overrightarrow{e_{i}}=g_{j i}$\n",
        "\n",
        "* when we want to get an **angle**, we put both vectors in:\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{v}=\\|\\vec{v}\\|^{2}=$ <font color=\"blue\">$v^{i} v^{j} g_{i j}$</font>\n",
        "\n",
        "  * $\\vec{w} \\cdot \\vec{w}=\\|\\vec{w}\\|^{2}=$ <font color=\"blue\">$w^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* when we want to get a **lengths** we out the same vector twice in\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{w}=\\|\\vec{v}\\|\\|\\vec{w}\\| \\cos \\theta=$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* **In a given basis the Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$**:\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}) \\mapsto$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5NQ7BCXo_r5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric tensor $g$ and inverse metric tensor $\\mathfrak{g}$ for lowering & raising indexes (Contraction)**\n",
        "\n",
        "* In mathematics and mathematical physics, [raising and lowering indices](https://en.m.wikipedia.org/wiki/Raising_and_lowering_indices) are operations on tensors which change their type. Raising and lowering indices are a form of [index manipulation (Ricci calculus)](https://en.m.wikipedia.org/wiki/Ricci_calculus) in tensor expressions.\n",
        "\n",
        "* The metric tensor represents a matrix with scalar elements $\\left(g_{i j}\\right.$ or $\\mathfrak{g}^{i j}$ ) and is a tensor object which is used to raise or lower the index on another tensor object by an operation called [Tensor Contraction](https://en.m.wikipedia.org/wiki/Tensor_contraction), thus **allowing a covariant tensor to be converted to a contravariant tensor, and vice versa**.\n",
        "\n",
        "  * In multilinear algebra, a [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) is an operation on a tensor that arises from the natural pairing of a finite-dimensional vector space and its dual.\n",
        "\n",
        "  * In components, it is expressed as a sum of products of scalar components of the tensor(s) caused by applying the summation convention to a pair of dummy indices that are bound to each other in an expression.\n",
        "\n",
        "  * The contraction of a single mixed tensor occurs when a pair of literal indices (one a subscript, the other a superscript) of the tensor are set equal to each other and summed over. In the Einstein notation this summation is built into the notation. **The result is another tensor with order reduced by 2.**\n",
        "\n",
        "  * Es ist eine Verallgemeinerung der Spur einer linearen Abbildung auf Tensoren, die mindestens einfach kovariant und einfach kontravariant sind.\n",
        "\n",
        "* **For an orthonormal [Cartesian coordinate system](https://en.m.wikipedia.org/wiki/Cartesian_coordinate_system), the metric tensor is just the [kronecker delta](https://en.m.wikipedia.org/wiki/Kronecker_delta) $\\delta_{i j}$ or $\\delta^{i j},$ which is just a tensor equivalent of the identity matrix, and $\\delta_{i j}=\\delta^{i j}=\\delta_{j}^{i}$**. For measuring length of a vector: Pythagoras theorem is only valid in orthonormal bases!\n",
        "\n",
        "* Normally a metric tensor as a function from a pair of vectors to scalars $g: V x V \\rightarrow \\mathbb{R}$. But it can also be a function from a vector in $V$ to a covector in $V^{*}$ $g: V  \\rightarrow V^{*}$ using the metric tensor. Reverse direction: from a covector to its vector partner.\n",
        "\n",
        "* From the metric tensor we know its components have 2 lower indices, so it‚Äôs a member of $V^{*}$ tensor $V^{*}$ bzw. $g \\in V^{*} \\otimes V^{*}$. Now we introduce what's called the inverse metric tensor $\\mathfrak{g} \\in V \\otimes V$. The combination between both in a summation gives you the Kronecker delta: $\\mathfrak{g}^{k i} g_{i j}=\\delta_{j}^{k}$. This is how we define the inverse metric tensor.\n",
        "\n",
        "* The **ordinary metric tensor $g_{i j}$ is covariant** with $g_{i j}=\\vec{g}_{i} \\cdot \\vec{g}_{j}$. It **lowers indexes** and its components are covariant: $T_{i}=g_{i j} T^{j}$\n",
        "\n",
        "* The **inverse metric tensor $\\mathfrak{g}^{k i}$ is contravariant** with $\\mathfrak{g}^{i j}=\\vec{\\mathfrak{g}}^{i} \\cdot \\vec{\\mathfrak{g}}^{j}$. It **raises the indexes** and its components are contravariant (go in the other direction): $T^{i}=\\mathfrak{g}^{i j} T_{j}$.\n",
        "\n",
        "\n",
        "* Both metric tensors are related by the identity: $g_{i k} \\, \\mathfrak{g}^{j k}=\\delta_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_82.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "87h0yKnx_try"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sharp Operator $\\sharp$ and Flat Operator $\\flat$ to generalize raising and lowering indexes on the component of any tensor**\n",
        "\n",
        "* The ordinary metric tensor lowers indexes meanwhile the inverse metric tensor raises indexes.\n",
        "\n",
        "* But these raising and lowering operations don‚Äôt just apply to vector and covector components. We can also raise and lower the indexes on the components of other tensors\n",
        "\n",
        "* $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$ wird zu $\\rightarrow$ $\\flat \\vec{v}=v_{i} \\epsilon^{i}$. The flat operator (on the left side) lowers the indexes (on the right side) from $v^{i}$ to $v_{i}$. **Another way to think of it: the $\\flat$ operator is transforming a vector arrow into a covector stack** (like flattening the pointy arrow into a flat stack)\n",
        "\n",
        "* The covector alpha has components with downstairs indexes: $\\alpha=\\alpha_{i} \\epsilon^{i}$ and the vector alpha sharp has components with upstairs indexes $\\sharp \\alpha=\\alpha^{i} \\overrightarrow{e_{i}}$. The sharp operator is basically raising the index. Also the sharp operator is basically turning a flat covector stack into a sharp pointy arrow vector.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 16: Raising/Lowering Indexes (with motivation, sharp + flat operators)](https://www.youtube.com/watch?v=_z9R7OMpxhY)\n",
        "\n",
        "* See article [Sharp & Flat Operator - Isomorphismus](https://de.wikipedia.org/wiki/√Ñu√üere_Ableitung#Be-_und_Kreuz-_(Flat-_und_Sharp-)_Isomorphismus)\n",
        "\n",
        "* Take tensor $Q$ which is member of vector space $Q \\in V \\otimes V^{*} \\otimes V^{*}$ and has components $Q_{j k}^{I}$:  $Q=Q_{j k}^{i} \\vec{e}_{i} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "* If we multiply it but the inverse metric tensor and sum over $j$ like this $Q^{i}{ }_{j k} \\mathfrak{g}^{j x}$ we can raise the index upward: $=Q^{i x}{ }_{k}$ and we get this new tensor $Q^{\\prime}=Q_{k}^{i x} \\overrightarrow{e_{i} {e}_{x}} \\epsilon^{k}$\n",
        "\n",
        "* This is a member of the new vector space $Q^{\\prime} \\in V \\otimes \\color{red}{V} \\otimes V^{*}$ - we raised the middle index $Q \\in V \\otimes \\color{red}{V^{*}} \\otimes V^{*}$\n",
        "\n",
        "* All these vector spaces can be traveled between using the ordinary metric tensor to lower indexes (blue arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_84.png)\n",
        "\n",
        "* Or we can use the inverse metric tensor to raise indexes (red arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_85.png)"
      ],
      "metadata": {
        "id": "spzpC64M_vis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor as Special Bilinear Form**: Metric Tensor = \"First Fundamental Form\". The metric tensor is a special bilinear form. Generalisation via tensor product: Bilinear Forms as Covector-Covector-Pairs.\n",
        "\n",
        "* So what is the difference between metric tensor and bilinear form? The metric tensor is a bilinear form, but it's a very specific example of a bilinear form. The metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* **Videos**: https://youtu.be/Hf-BxbtCg_A and https://youtu.be/Dn0ZZRVuJcU"
      ],
      "metadata": {
        "id": "oyUSwoxm_xXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "g4uy9SPO_zMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector-Pairs (Linear Maps) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$ to map vector to vector within one basis*"
      ],
      "metadata": {
        "id": "V44oHA2a_1hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear maps**\n",
        "\n",
        "1. maps vectors to vectors $L: V \\mapsto W$, or $L: V \\mapsto V$\n",
        "\n",
        "2. Linearity (addition & scaling)\n",
        "\n",
        "* Both covectors and linear maps are functions. The only difference is that covectors output is a scalar, and linear maps output vectors.\n",
        "\n",
        "> **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* Linear maps are linear combinations of vector-covector-pairs $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ with the tensor product $\\overrightarrow{e_{i}} \\epsilon^{j}$ bzw. $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* **Vector changes, basis not: When we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**.\n",
        "\n",
        "  * While the output vector might be different than the input vector, we are still measuring the output vector using the same basis\n",
        "\n",
        "  * With forward and backward transform we modify the vector components when we change from one basis to another, or from one dual basis to another.\n",
        "\n",
        "  * With linear maps now we modify the vector components when we move a vector around within a given basis!\n",
        "\n",
        "* **Coordinate representations of linear maps end up being matrices!**. Column vectors are coordinate representation of vectors. Row vectors are coordinate representation of covectors.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 7: Linear Maps](https://www.youtube.com/watch?v=dtvM-CzNe50&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=10)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 11: Linear maps are Vector-Covector Pairs](https://www.youtube.com/watch?v=YK2zVcWpROA&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14)\n",
        "\n",
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). Metric tensors are (0,2) tensors, because it transforms using tow covariant rules:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "8erR4QCL_9Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_19.png)"
      ],
      "metadata": {
        "id": "kjVQbCBy__67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Case / Fun fact about how matrices transform vectors:**\n",
        "\n",
        "When you use the column vector (1,0) as an input vector, you get the first column of the matrix as the output (3, -1):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "-1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "When you use the column vector (0,1) as an input vector, you get the second column of the matrix as the output (-4, 2):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "-4 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "The column vectors 1, 0 and 0,1 are like copies of the basis vectors e1 and e2. Because: **Linear maps transform input vectors. But linear maps don't transform the basis!**\n",
        "\n",
        "**So when we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**. While the output vector might be different than the input vector, we are still measuring the output vector using the same basis."
      ],
      "metadata": {
        "id": "xEM1uOD7ACQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_20.png)"
      ],
      "metadata": {
        "id": "IFqMZilJAEFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maps - Case 1: Transformations of vectors within one basis (Linear Maps as Tensor Product $\\otimes$ (Vector-Covector-Pairs)**"
      ],
      "metadata": {
        "id": "pg7lwrp9AJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pure & Impure Matrices & how Vector-Covectors-Pairs help**\n",
        "\n",
        "* When we multiply a row vector and a column vector in this order we get a scalar: $\\begin{aligned} &\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right] \\end{aligned}$ $= (2)(3)+(1)(-4) = 6-4 =2$\n",
        "\n",
        "* But if we reverse the order we get a matrix (which is basically a linear map):  $\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right]\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]$ = $\\left[\\begin{array}{cc}6 & 3 \\\\ -8 & -4\\end{array}\\right]$\n",
        "\n",
        "* **Can we do this the other way around (get the row and column vector from the matrix)?** (Answer: not always so easy)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_36.png)\n",
        "\n",
        "* Here is the proof why it doesn't work for this matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_37.png)\n",
        "\n",
        "* There are some matrices that can be broken up into column vectors and row vectors, and others not. We call them pure and impure matrices!\n",
        "\n",
        "* Pure matrices are boring when they are used as linear maps, because all the output vectors exist along the same direction.\n",
        "\n",
        "* The reason behind is that the columns of the matrices are all scalable multiples of each other (as shown below). Output vectors point all to the same direction.\n",
        "\n",
        "* The set of transformations a pure matrix can do as linear map is really limited\n",
        "\n",
        "* Impure matrices as linear maps are more interesting because they can send basis vectors into different directions.\n",
        "\n",
        "* **Question: Since impure matrices are more interesting, how can we construct impure matrices using column vector, row vector and products**?\n",
        "\n",
        "* **Solution: We define four special vector-covector-pairs using the old e basis $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$  and the old epsilon dual basis $\\left\\{\\epsilon^{1}, \\epsilon^{2}\\right\\}$** (Hint: this will be the transition from 'classic' linear maps to vector-covector-tensor products with another way of writing them!)\n",
        "\n",
        "* For example the 1 on top left can be written using the column and row vector via $\\overrightarrow{e_{1}} \\epsilon_{1}$:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{1}$\n",
        "\n",
        "And we can do the same for all other 3 matrix components:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{1}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{2}$\n",
        "\n",
        "And you‚Äôll notice that these 4 matrices when taking in linear combination, so when we scale each matrix by a different amount and then add them together, we get any general 2x2 matrix $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$ that we like, just by picking the right scaling numbers a, b, c and d:\n",
        "\n",
        "> $a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "So this set of 4 products forms a basis for all matrices that are linear maps from the vector space $V \\mapsto V$ to itself\n",
        "\n",
        "> $\\left\\{\\overrightarrow{e_{1}} \\epsilon^{1}, \\overrightarrow{e_{1}} \\epsilon^{2}, \\overrightarrow{e_{2}} \\epsilon^{1}, \\overrightarrow{e_{2}} \\epsilon^{2}\\right\\}$ is a basis for $V \\rightarrow V$\n",
        "\n",
        "So any general linear map $L$ can be written as this linear combination if we pick the coefficients right:\n",
        "\n",
        "> $L=a \\overrightarrow{e_{1}} \\epsilon^{1}+b \\overrightarrow{e_{1}} \\epsilon^{2}+c \\overrightarrow{e_{2}} \\epsilon^{1}+d \\overrightarrow{e_{2}} \\epsilon^{2}$\n",
        "\n",
        "And we can summarize this using the Einstein notation, any linear map $L$ can be written using these components\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "> **This is a vector and a covector written together - which is a linear map**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_38.png)\n"
      ],
      "metadata": {
        "id": "adsf_EarAL1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Formula for matrix multiplication**\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]\\left[\\begin{array}{l} \\color{red}x \\\\ \\color{blue}y\\end{array}\\right]=\\left[\\begin{array}{l}a \\color{red}x+b \\color{blue}y \\\\ c \\color{red}x+d \\color{blue}y\\end{array}\\right]$\n",
        "\n",
        "This originates in the following abstract algebraic definition:\n",
        "\n",
        "$L: V \\rightarrow W$\n",
        "\n",
        "$L(\\vec{v}+\\vec{w})=L(\\vec{v})+L(\\vec{w})$\n",
        "\n",
        "$L(n \\vec{v})=n L(\\vec{v})$\n",
        "\n",
        "**How to turn the $\\vec{v}$ coefficients into the $\\vec{w}$ coefficients?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_21.png)\n",
        "\n",
        "\n",
        "**Proof: Derive the matrix multiplication formulas just from the abstract linearity properties of linear maps**\n",
        "\n",
        "If we have a linear map $L$ that transforms a vector v into another vector w (where w can be written as a linear combination of basis vectors)\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=w^{1} \\overrightarrow{e_{1}}+w^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "and we know how $L$ transforms basis vectors (via the L coefficients)\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{1}}\\right)=L_{1}^{1} \\overrightarrow{e_{1}}+L_{1}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{2}}\\right)=L_{2}^{1} \\overrightarrow{e_{1}}+L_{2}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "this means we can transform the v components into  the w components using the formulas below\n",
        "\n",
        "$w^{1}=L_{1}^{1} v^{1}+L_{2}^{1} v^{2}$\n",
        "\n",
        "$w^{2}=L_{1}^{2} v^{1}+L_{2}^{2} v^{2}$.\n",
        "\n",
        "And if we repeat this argument for any number of dimensions, so if we have a linear map $L$ in n-dimensions\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=\\sum_{i=1}^{n} w^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "We would get all the $L$ coefficients from this formula below\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{i}}\\right)=\\sum_{j=1}^{n} L_{i}^{j} \\overrightarrow{e_{j}}$\n",
        "\n",
        "we can transform the v components into the w components\n",
        "\n",
        "$w^{i}=\\sum_{j=1}^{n} L_{j}^{i} v^{j}$\n",
        "\n",
        "This is the standard matrix multiplication formula for multiplying matrices and vectors together.\n"
      ],
      "metadata": {
        "id": "whZ3s5JiAh4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit of seeing linear maps as vector-covector-pairs - which are tensor products $\\otimes$**\n",
        "\n",
        "*When we have a linear map acting on a vector, we can get the correct matrix vector component multiplication formula*\n",
        "\n",
        "If you think of some linear map $L$ as a linear combination of these basis linear maps\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "and we have also a vector $\\vec{v}$ which is a linear combination of these basis vectors\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "What would we get in $\\vec{w}$ with $L$ acts on the input $\\vec{v}$?\n",
        "\n",
        "> $\\vec{w}=L(\\vec{v})$\n",
        "\n",
        "To find out we substitute the components $L$ and $\\vec{v}$ accordingly:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "The $\\epsilon^{j}$ dual basis vector is now acting on the input vector! (and that's what covectors do, they act on vectors).\n",
        "\n",
        "So we use the linearity of $\\epsilon^{j}$ to take out the scaling coefficient $v^{k}$ and put it out in front:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "This leaves us with $\\epsilon^{j}$ acting on $\\left(\\overrightarrow{e_{k}}\\right)$. And by definition this is just a Kronecker delta:\n",
        "\n",
        ">  $\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$ = $\\delta_{k}^{j}$\n",
        "\n",
        "So we can replace this in the equation:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "And by the kronecker delta cancellation rule, we can cancel out the $k$'s and replace it at $v$ with $j$:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}} \\delta^{j}$\n",
        "\n",
        "And this is our output vector written as a linear combination of the $e$ basis vectors:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "And we get these coefficients:\n",
        "\n",
        "> $L_{j}^{i} v^{j}$\n",
        "\n",
        "from the standard matrix multiplication rule:\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\vec{w}=L(\\vec{v}) \\\\\n",
        "w^{i}=L_{j}^{i} v^{j}\n",
        "\\end{array}$\n",
        "\n",
        "So from our equation above:\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "**we can see that this product pair is really a linear map**:\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "It took an input vector, transformed it, and gave us an output vector.\n",
        "\n",
        "> **Proof: vector-covector-pairs are linear maps - which are tensor products $\\otimes$  !**\n",
        "\n",
        "* These are pure matrices, so you get the boring linear maps! To get the more interesting linear maps (the impure matrices!), we need to combine a bunch of pure linear maps together in linear combination, which helps us to get more interesting impure linear maps like $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] = a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]$\n",
        "\n",
        "* **And finally this vector-covector-pair is actually a tensor product**\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$ normally written like this: $\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_39.png)"
      ],
      "metadata": {
        "id": "bv6NmiXzAOBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two ways of multiplying a row vector with a column vector: classic way (left) and tensor product (right - much better)**\n",
        "\n",
        "\n",
        "* The $\\otimes$ operator tells us to take the array on the left and distribute it to each of the components inside the array on the right.\n",
        "\n",
        "* This means take the column vector on the left and distribute a copy to each element inside the second array. And you get a row of columns (bottom right), which is basically like a matrix.\n",
        "\n",
        "* **This means that linear maps are rows of columns!**\n",
        "\n",
        "* Picture below: On the left is the old way (row vector & column vector both to describe vectors) and on the right side is the new better way with tensor products $\\otimes$\n",
        "\n",
        "* the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "* (meanwhile combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors associated with the covectors)\n",
        "\n",
        "* **The tensor product for linear maps gave a great change of perspective**. This idea of distributing arrays into each other will turn out to be very useful later on!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_41.png)"
      ],
      "metadata": {
        "id": "bsHsugHiAQoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector Pairs (Linear Maps) + Forward and Backward Transforms (Vectors) $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$ to map vector to vector across two bases*"
      ],
      "metadata": {
        "id": "VcEin6DXApdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **We need transformation rules for linear maps when we are moving a vector AND going from one basis to another**, because linear maps are only working within one basis (move vectors around within one basis), we need to find the coefficients of the linear map in the new basis when we change basis.\n",
        "\n",
        "* Task: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "**How do we do this?**\n",
        "\n",
        "If you take vector components $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ from the vector $\\vec{v}$ in the original basis $\\overrightarrow{e_{i}}$\n",
        "\n",
        "1. **apply the linear map** $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$, you get the new vector components $L(\\vec{v})$ $=\\left[\\begin{array}{c}1 / 2 \\\\ 2\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ that is still **in the same original basis** $\\overrightarrow{e_{i}}$.\n",
        "\n",
        "2. then **apply the backward transform** with the matrix $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$, you get the new vector components for $\\vec{v}$ $\\left[\\begin{array}{l}3/4 \\\\ 1\\end{array}\\right]_{\\widetilde{e_{i}}}$ **in the new basis** ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "**We use the backward transform from old to new, since vector components behave contravariant !!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_22.png)"
      ],
      "metadata": {
        "id": "nrdE_XbCA5ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you get the new vector components $L(\\vec{v})$ in the new basis ${\\widetilde{e_{i}}}$?**\n",
        "\n",
        "* Answer: We need to find the coefficients of the linear map ${\\widetilde{L_{j}^{q}}}$ in the new basis ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "* Again: how do you get the new vector components $L(\\vec{v})$ (the transformed vector with the linear map $L$) in the new basis ${\\widetilde{e_{i}}}$, and not only the components of $\\vec{v}$ (the original vector) in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* Said differently: what are the components of the output vector in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* we cannot apply the linear map $L$ $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$ since it's only valid in the original basis\n",
        "\n",
        "* **We need to find a new matrix ${\\widetilde{L}}$ in the new basis ${\\widetilde{e_{i}}}$ that tells us how to build output vectors using the ${\\widetilde{e_{1}}}$ and ${\\widetilde{e_{2}}}$ basis vectors.**\n",
        "\n",
        "* This means we need to find the ${\\widetilde{L_{j}^{q}}}$ coefficients:\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{1}}}\\right)=\\widetilde{L_{1}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{1}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{2}}}\\right)=\\widetilde{L_{2}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{2}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "**How to get the components of the output vector in the new basis ${\\widetilde{e_{i}}}$:**\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\color{red}{\\widetilde{e_{i}}}\\right)$\n",
        "\n",
        "Let's first use the forward transform $\\widetilde{e_{i}}=\\sum_{j=1}^{n} F_{i}^{j} \\overrightarrow{e_{j}}$ to rewrite the new basis vectors in terms of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\sum_{j=1}^{n} F_{i}^{j} \\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use the linearity of $L$ to take the scale and sum coefficients outside the function:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use this definition $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)=\\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$ to write the output $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$ as linear combination of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} \\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Then we re-arrange the sums a bit:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Now we write the old basis in terms of the new basis vectors with $\\color{blue}{\\overrightarrow{e_{k}}}=\\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{\\overrightarrow{e_{l}}}}$ using the bckward transform:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "We re-arrange the sums again:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "* So on the left we have a linear combination of ${\\widetilde{e}}$ basis vectors with the summation index $q$.\n",
        "\n",
        "* So on the right we have a linear combination of ${\\widetilde{e}}$ basis vectors again but with the summation index $l$.\n",
        "\n",
        "* So we have a linear combination of ${\\widetilde{e}}$ basis vectors on both sides.\n",
        "\n",
        "But with different summation indexes. But the choice doesn't really matter. So we change all the $q$ with $l$:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\widetilde{L_{i}^{l}} \\color{red}{\\widetilde{e_{l}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "Now we see that the $\\widetilde{L_{i}^{l}}$ coefficients on the left side are equal to this part $\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} B_{k}^{l}$ in the middle of the right equation:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\color{pink}{\\widetilde{L_{i}^{l}}} \\widetilde{e_{l}}=\\sum_{l=1}^{n} \\color{pink}{\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}} \\widetilde{e_{l}}$\n",
        "\n",
        "**This is saying that to transform the matrix coordinates from the old basis to the new basis we multiply the old matrix $L_{j}^{k}$ by the backward transform $B$ on the left and by the forward transform $F$ on the right:**\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "**This is what we just did (explanation below):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)"
      ],
      "metadata": {
        "id": "l4kEzo39A7c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUMMARY: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)**\n",
        "\n",
        "* So, matrices, or linear maps, transform with both the forward transform and the backward transform. Here is why:\n",
        "\n",
        "* You have two ways to get the new basis vector component: you can use $\\widetilde{L}$ matrix n the bottom to go from left to right directly $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$\n",
        "\n",
        "* But it's the same thing as going the other way around:\n",
        "\n",
        "  1. To transform the new vector components into the old vector components you use the forward transform\n",
        "\n",
        "  2. And here to transform the components of the input vector into components of the output vector in the old basis, we just use the matrix $L$\n",
        "\n",
        "  3. And finally to get from the old vector components to the new vector components for the output vector we use the backward transform\n",
        "\n",
        "  4. Now we have the components of the new basis vector $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$ with which you can describe any vector in the new vector space.\n",
        "\n",
        "* So the idea of transforming matrix components using both the forward and backward transformations makes sense.\n",
        "\n",
        "**Let's check this on our example from above:**\n",
        "\n",
        "This is the equation again to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis):\n",
        "\n",
        "> $\\widetilde{L}_{i}^{l}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "* Backward Transform $B$: $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$\n",
        "\n",
        "* Linear Map $L$ in old basis ${\\overrightarrow{e_{j}}}$: $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\overrightarrow{e_{j}}}$\n",
        "\n",
        "* Forward Transform $F$: $\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "Now let's place them all in the equation to get the linear map in the new basis:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 8 & 1 \\\\ -1 / 2 & 4\\end{array}\\right]\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "We get this matrix the:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}5 / 4 & 3 / 16 \\\\ 3 & 5 / 4\\end{array}\\right]$\n",
        "\n",
        "And this matrix above tells us how to write the outputs of the linear map as linear combination of the new basis:\n",
        "\n",
        "> $L\\left(\\widetilde{e_{1}}\\right)=5 / 4 \\widetilde{e_{1}}+3 \\widetilde{e_{2}}$\n",
        "\n",
        "> $L\\left(\\widetilde{e_{2}}\\right)=3 / 16 \\widetilde{e_{1}}+5 / 4 \\widetilde{e_{2}}$\n",
        "\n",
        "**Results are correct, the new matrix / linear map in the new basis is converting the vector properly from its original position to the new position (measuring both at the new basis vectors):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_24.png)"
      ],
      "metadata": {
        "id": "qozTbjNIA9Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward & Backward Transform of Linear Maps between different basis**\n",
        "\n",
        "* A tensor is an object that is invariant under a change of coordinates, and **has components that change in a special, predictable way under a change of coordinates**. The way we transform them is by a applying a series of forward and backward transforms!\n",
        "\n",
        "**Forward Transform (going from old $T$ to new $\\widetilde{T}$)**\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **backward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **forward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Backward Transformation (going from new $\\widetilde{T}$ to old $T$)**\n",
        "\n",
        "> $T_{r s t \\ldots}^{i j k \\ldots}=\\left(F_{\\color{red}a}^{i} F_{\\color{red}b}^{j} F_{\\color{red}c}^{k} \\cdots\\right) \\widetilde{T_{\\color{blue}{x y z} \\ldots}^{\\color{red}{a b c} \\ldots}}\\left(B_{r}^{\\color{blue}x} B_{s}^{\\color{blue}y} B_{t}^{\\color{blue}z} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **forward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **backward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Illustration from the linear maps part that helps to understand the $FTB$ = $\\widetilde{T}$ (oben) bzw. $FLB$ = $\\widetilde{L}$ (unten) transform:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$ (forward transform)\n",
        "\n",
        "**How many contravariant and covariant rules we need to follow during a transformation?** - Core Question when it gets more complicated with more basis & dual basis !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_32.png)"
      ],
      "metadata": {
        "id": "5cOVS7o9BBX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "_AUtbuZDBDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Tensors (Any Vector-Covector combination) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$*"
      ],
      "metadata": {
        "id": "457rg_0-BFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Maps vs Bilinear Forms**\n",
        "\n",
        "* **Bilinear Forms: Linear combinations of covector-covector-pairs** $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$ (including metric tensor)\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 14: Tensors are general vector/covector combinations](https://www.youtube.com/watch?v=9R4vhqvE_jw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=17)\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)"
      ],
      "metadata": {
        "id": "m-CmxPR5BUhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take two new tensor**\n",
        "\n",
        "This is a (2,0) tensor\n",
        "\n",
        "> $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "This is a (1,2) tensor\n",
        "\n",
        "> $Q=Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "**Now we can ask the same questions:**\n",
        "\n",
        "1. What are the coordinate transform rules?\n",
        "2. What is the multiplication formula for $Q$ ($D$)?\n",
        "3. What are the array shapes?\n",
        "\n",
        "**1. How to $D$ and $Q$ under a change of basis?**\n",
        "\n",
        "* Transforming tensor components is not hard, long as you have the transformation rules for basis vectors and covectors (plug backward transform in here for example twice on the left side)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_50.png)\n",
        "\n",
        "**2. What is the formula for $Q$ acting on the input $D$ = $Q(D)$**\n",
        "\n",
        "* This is tricky because there is no one way to do that.\n",
        "\n",
        "* **The challenge is now that: As we make these tensors bigger and bigger with more and more covariant and contravariant parts, we end up with more and more ways to do the summations, and more and more ways to compute functions.**\n",
        "\n",
        "* Writing $D$ = $Q(D)$ is ambuguous as it doesn't tell us exactly what to do\n",
        "\n",
        "* so we need to write it out in the Einstein notation like on the left side, for example $Q_{j k}^{i} D^{j k}$\n",
        "\n",
        "* Some examples are:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_51.png)\n",
        "\n",
        "**3. What is the array shape?**\n",
        "\n",
        "* It's like the Kronecker product between two column vectors: like this part $\\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$  in this here: $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "* We can do this for the first example from above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_52.png)\n",
        "\n",
        "* For the other example it would look like this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_53.png)\n",
        "\n",
        "* One could use the 3d visualisation, but it's better in the matrix array way. Because looking at this you can see it‚Äôs a (1,2) tensor with one column aspect and two row aspects\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_54.png)"
      ],
      "metadata": {
        "id": "rXmd5aWCBWWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Array Multiplication**\n",
        "\n",
        "* Easy for smaller tensors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_55.png)\n",
        "\n",
        "* That's not so easier for larger tensors that have high type numbers, because there are several possible multiplication rules (as discussed earlier):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_56.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_57.png)\n",
        "\n",
        "* For much more complex tensors, it's the easiest to just stick with the Einstein notation and also not do the array representation.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_58.png)\n",
        "\n",
        "* So with high type tensors the abstract notation and the array notation have their limitations. When trying to express tensor multiplication formulas it‚Äôs usually easier just to stick with the Einstein component notation. For this reason a lot of sources just write tensors like this and leave out basis vector and covectors completely:\n",
        "\n",
        "> $Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ $\\mapsto$ $Q_{j k}^{i}$\n",
        "\n",
        "> $D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$ $\\mapsto$ $D=D^{a b}$\n",
        "\n",
        "* **But it's important to remember that tensor components always come from a choice of basis, and the same tensor can have different components if we choose to represent it in a different basis.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_59.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_60.png)"
      ],
      "metadata": {
        "id": "mGPcQ4POBYNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector Spaces (Hilbert Spaces)*"
      ],
      "metadata": {
        "id": "vq-kSppvBbU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "These vectors are element of the vector space ${V}$\n",
        "\n",
        "> $\\vec{v}, \\vec{w}, \\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}$ $\\in {V}$\n",
        "\n",
        "These covectors are element of the dual vector space ${V^{*}}$\n",
        "\n",
        "> $\\alpha, \\beta, \\epsilon^{1}, \\epsilon^{2}$ $\\in {V^{*}}$\n",
        "\n",
        "We can make following vector-covector-pairs:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "And we can add them and scale with them with the tensor product rules which forms a vector space:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "So we know that these must be vectors in a vectors space:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "But in which vector space do they live in? In this one:\n",
        "\n",
        "> $\\in V \\otimes V^{*}$\n",
        "\n",
        "**Now this is a new use case of $\\otimes$, because so far we used it for combining vectors or tensors. Here we use it to combine entire vector spaces.** (more about it below in a short separat chapter)"
      ],
      "metadata": {
        "id": "gr-jRSA3I5Ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V \\otimes V^{*}$?** (1,1)-tensors\n",
        "\n",
        "\n",
        "> **(1,1)-Tensors**: $L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\in V \\otimes V^{*}$\n",
        "\n",
        "Remember: vectors have an upstairs index (because they transform contravariant):\n",
        "\n",
        "> $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "and covectors habe a downstairs index (because they transform covariant):\n",
        "\n",
        "> $\\alpha=\\alpha_{i} \\epsilon^{i}$\n",
        "\n",
        "\n",
        "\n",
        "So if we do a summation with vector components like this over $j$, we end up with vector components as the output (because we have one upstairs $i$ index that's left):\n",
        "\n",
        "> $L_{j}^{i} v^{j}=w^{i}$\n",
        "\n",
        "In this case $L$ ist acting as a map from $V$ to $V$, or essentially a linear map.\n",
        "\n",
        "> $V \\mapsto V$\n",
        "\n",
        "> **This is a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a summation with covector components like this with sum over $i$, and our outout would have $j$ as the dowstairs index:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i}=\\beta_{j}$\n",
        "\n",
        "We would end up with covector components as the output. So covector in, covector out:\n",
        "\n",
        "> $V^{*} \\mapsto V^{*}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "Also we can provide $L$ with both vector components and covector components and we do two summations over $i$ and $j$, and that would give us a scalar as the output since there are no indices left to sum over.\n",
        "\n",
        "> $L_{j}^{i} v^{j} \\alpha_{i}=s$\n",
        "\n",
        "So in this case $L$ can be viewed as a function from a pair of vectors and covectors to scalars.\n",
        "\n",
        "> $V \\times V^{*} \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "And finally we can do the same thing but reverse the order of the inputs:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i} v^{j}=s$\n",
        "\n",
        "And in this case $L$ is a function from a covector-vector-pair to scalar:\n",
        "\n",
        "> $V^{*} \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "4bn8DJW0I9YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V^{*} \\otimes V^{*}$?** (0,2)-tensors\n",
        "\n",
        "**Now what if we use the tensor product to combine two covectors together?**\n",
        "\n",
        "Those covectors life in $V^{*}$:\n",
        "\n",
        "> $\\alpha, \\beta, \\gamma, \\delta, \\epsilon^{1}, \\epsilon^{2}$ $\\in V^{*}$\n",
        "\n",
        "So when we have covector-covector-pairs like following it turns out that they all life in the vector space **$V^{*} \\otimes V^{*}$**\n",
        "\n",
        "> $\\alpha \\beta, \\alpha \\gamma, \\delta \\beta, \\epsilon^{1} \\epsilon^{2}, \\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$ **$\\in V^{*} \\otimes V^{*}$**\n",
        "\n",
        "**So elements of the $V^{*} \\otimes V^{*}$are (0,2)-tensors**\n",
        "\n",
        "> $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\in V^{*} \\otimes V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "(Covector-covector-pairs and their linear combinations)\n",
        "\n",
        "If we take these $B$ components and do two summations with two sets of vector components (via $i$ and $j$), then we end up with a scalar:\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i} w^{j}=s$\n",
        "\n",
        "And this of course is a bilinear form (which takes a pair of vectors and outputs a scalar):\n",
        "\n",
        "> $V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a single summation over $i$ with a set of vector components and we‚Äôd be left with the index $j$ downstairs. So the output would be a set of covector components\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i}=\\alpha_{j}$\n",
        "\n",
        "So in this case $B$ is a map from vectors to covectors:\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "Also we could choose to do summation with vector components over the $j$ index and then end up with covector components $i$\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{j}=\\beta_{i}$\n",
        "\n",
        "This would be another map from $V$ to $V^{*}$, but it would be a different map than the previous one, because we‚Äôre doing the summation differently.\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "AZ-VNTnoI_JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create even larger vector spaces from basic building blocks $V$ and $V^{*}$**\n",
        "\n",
        "**So we can create larger and larger vector spaces out of the two basic building blocks $V$ and $V^{*}$**:\n",
        "\n",
        "* starting with two vector spaces $V$ and $V^{*}$ (basic building blocks)\n",
        "\n",
        "  * they contain tensors $v_{i}$ and $\\alpha_{j}$\n",
        "\n",
        "  * with vector components with upstairs index and covector components with downstairs index\n",
        "\n",
        "* we can combine these 2 vector spaces into new vector spaces using the tensor product $V \\otimes V$, $V \\otimes V^{*}$, $V^{*} \\otimes V$ and $V^{*} \\otimes V^{*}$\n",
        "\n",
        "  * and these vectors spaces have following vector components: $\\mathcal{A}^{i j}$, $L_{j}^{I}$, $L_{j}{ }^{i}$ and $\\mathcal{B}_{i j}$\n",
        "\n",
        "  * Indexes from $V$ go upstairs and index from $V^{*}$ go downstairs\n",
        "\n",
        "* And we can continue to make larger and larger vector spaces using the tensor product like $V \\otimes V \\otimes V$, $V^{*} \\otimes V \\otimes V$ etc\n",
        "\n",
        "  * And all these vector spaces contain tensors like $T^{i j k}$, $T_{i}^{j k}$, $T_{j}^{i k}$ etc.\n",
        "\n",
        "  * with components that have different combinations of upstairs and downstairs indexes depending on whether they are constructed using $V$ or $V^{*}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_65.png)"
      ],
      "metadata": {
        "id": "npgLhzSYJA3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So if we have some new tensor $T$ from a vector space we‚Äôve never seen before, we can easily get the correct component indexes just by looking at the vector spaces.**\n",
        "\n",
        "> $T \\in V^{*} \\otimes V \\otimes V^{*} \\otimes V^{*}$\n",
        "\n",
        "Looking at these vector spaces we see that the basis would be made up of a covector, a vector, a covector and another covector in combination\n",
        "\n",
        "> $T={\\epsilon}^{i}  \\overrightarrow{e_{j}} \\epsilon^{k} {\\epsilon}^{l}$\n",
        "\n",
        "And we get the components just by **placing the indexes in the opposite position that we see in the basis**, so that all the summations work out properly.\n",
        "\n",
        "**And now we can ask: How can this tensor $T$ act on other tensors?**\n",
        "\n",
        "> $T_{i}^{j}{ }_{k l}$\n",
        "\n",
        "examples of other tensors to be acted on:\n",
        "\n",
        "> $u^{c} \\quad \\begin{array}{cc}D^{f g} & \\beta_{s} \\\\ Q_{u v}^{t} & L_{y}^{x} & w^{b} & U^{m n o}\\end{array}$\n",
        "\n",
        "**Well, we can basically do any summations we like as long as the upstairs indexes are matched with downstairs indexes and downstairs indexes are matched with upstairs indexes.**\n",
        "\n",
        "We could do something like this with four summations and you can see that all the indexes are positioned properly:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensors_69.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_68.png)"
      ],
      "metadata": {
        "id": "HEU05oWyJCrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is how the result for each would look like** [see explanation](https://youtu.be/M-OLmxuLdbU?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&t=683)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_70.png)\n",
        "\n",
        "The tensors we get from these tensor product form new vector spaces:\n",
        "\n",
        "> $V \\otimes V$\n",
        "\n",
        "> $V \\otimes V^{*}$\n",
        "\n",
        "> $V^{*} \\otimes V$\n",
        "\n",
        "> $V^{*} \\otimes V^{*}$\n"
      ],
      "metadata": {
        "id": "LU6hlHpSJEt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Tensor Products $\\otimes$ in Quantum Mechanics*#"
      ],
      "metadata": {
        "id": "F4gfVMumBdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two systems being described as a joint system:**\n",
        "\n",
        "1. the structure of the two systems is pre- served:\n",
        "\n",
        "2. a measurement on one of the systems does not disturb the other one;\n",
        "\n",
        "3. maximal information obtained on both systems separately gives maximal information on the joint system.\n",
        "\n",
        "With these conditions we show, within the framework of the propositional system formalism, that\n",
        "\n",
        "* if the systems are classical the joint system is described by the cartesian product of the corresponding phase spaces, and\n",
        "\n",
        "* if the systems are quantal the joint system is described by the tensor product of the corresponding Hilbert spaces.\n",
        "\n",
        "[Source: Paper Aerts](https://raw.githubusercontent.com/deltorobarba/papers/master/aerts.pdf)\n",
        "\n",
        "**One should deal with at least two operator products:**\n",
        "\n",
        "* one is given by the composition of two operators defined on the same vector space = product yields an operator de!ned on the common vector space of the factor operators\n",
        "\n",
        "* the other is the direct or tensor product of operators = leads to an operator de!ned on a different vector space: the direct or tensor product of the vector spaces of the factor operators\n",
        "\n",
        "The Kronecker product and some of its physical applications (Francisco M Fern√°ndez)"
      ],
      "metadata": {
        "id": "fZWP-A5kPUTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider two distinguishable particles:\n",
        "\n",
        "* Particle 1: its quantum mechanics is described by a complex vector space V. It has associated operators T1, T2, ..\n",
        "\n",
        "* Particle 2: its quantum mechanics is described by a complex vector space W. It has associated operators S1, S2, ..\n",
        "\n",
        "*This list of operators for each particle may include some or many of the operators you are already familiar with: position, momentum, spin, Hamiltonians, projectors, etc.*\n",
        "\n",
        "**Once we have two particles, the two of them together form our system.**\n",
        "\n",
        "* We are after the description of quantum states of this two-particle system.\n",
        "\n",
        "* On first thought, we may think that any state of this system should be described by giving the state v ‚àà V of the first particle and the state w ‚àà W of the second particle.\n",
        "\n",
        "* This information could be represented by the ordered list (v, w) where the first item is the state of the first particle and the second item the state of the second particle. This is a state of the two-particle system, but it is far from being the general state of the two-particle system. It misses remarkable new possibilities, as we shall soon see.\n",
        "\n",
        "We thus introduce a new notation. Instead of representing the state of the two-particle system with particle one in $v$ and particle two in $w$ as $(v, w)$, **we will represent it as $v \\otimes w$**.\n",
        "\n",
        "* <font color=\"blue\">This element $v \\otimes w$ will be viewed as a vector in a new vector space $V \\otimes W$ that will carry the description of the quantum states of the system of two particles.</font>\n",
        "\n",
        "* <font color=\"blue\">This $\\otimes$ operation is called the \"tensor product.\" In this case we have two vector spaces over $\\mathbb{C}$ **and the tensor product $V \\otimes W$ is a new complex vector space**:</font>\n",
        "\n",
        "> $v \\otimes w \\in V \\otimes W \\quad$ when $\\quad v \\in V, w \\in W$\n",
        "\n",
        "$\\operatorname{In} v \\otimes w$ there is no multiplication to be carried out, we are just placing one vector to the left of $\\otimes$ and another to the right of $\\otimes$.\n",
        "\n",
        "We have only described some elements of $V \\otimes W$, not quite given its definition yet. We now explain two physically motivated rules that define the tensor product completely.\n"
      ],
      "metadata": {
        "id": "3aRUE2W-PW9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Rule 1</u>: If the vector representing the state of the first particle is scaled by a complex number this is equivalent to scaling the state of the two particles. The same for the second particle. So we declare:**</font>\n",
        "\n",
        "> <font color=\"blue\">$(a v) \\otimes w=v \\otimes(a w)=a(v \\otimes w), \\quad a \\in \\mathbb{C}$\n",
        "\n",
        "* like in tensor algebra the linear map bzw. bilinear map, where a factor is attached only to one of two, not both\n",
        "\n",
        "<font color=\"blue\">**<u>Rule 2</u>: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition. We thus demand distributive properties for the tensor product:**</font>\n",
        "\n",
        "> <font color=\"blue\">$\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "\n",
        "> <font color=\"blue\">$v \\otimes\\left(w_{1}+w_{2}\\right)=v \\otimes w_{1}+v \\otimes w_{2}$\n",
        "\n",
        "Example in Quantum Phase Estimation: We first perform a Hadamard gate on the first qubit to get the state and then distribute the superposition (omitted the normalization factor of 1/‚àö2 for clarity):\n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "\n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "\n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "> The tensor product $V \\otimes W$ is thus defined to be the vector space whose elements are **(complex) linear combinations** of elements of the form $v \\otimes w$, with $v \\in V, w \\in W$, with the above rules for manipulation. The tensor product $V \\otimes W$ is the complex vector space of states of the two-particle system!"
      ],
      "metadata": {
        "id": "2hHr5XVJPZLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment: Let $v_{1}, v_{2} \\in V$ and $w_{1}, w_{2} \\in W$. A vector in $V \\otimes W$ constructed by superposition is**\n",
        "\n",
        "> $\n",
        "\\alpha_{1}\\left(v_{1} \\otimes w_{1}\\right)+\\alpha_{2}\\left(v_{2} \\otimes w_{2}\\right) \\in V \\otimes W\n",
        "$\n",
        "\n",
        "<font color=\"blue\">This shows clearly that a general state of the two-particle system cannot be described by stating the state of the first particle and the state of the second particle</font>. The above superpositions give rise to entangled states. An entangled state of the two particles is one that, roughly, **cannot be disentangled into separate states of each of the particles**.\n",
        "\n",
        "*Explanation 1*\n",
        "\n",
        "* **The \"Kronecker product\", better known as the tensor product**, is the natural notion of a product for spaces of states, when these are considered properly:\n",
        "\n",
        "* **A space of states is not a Hilbert space $\\mathcal{H}$, but the projective Hilbert space $\\mathbb{P} \\mathcal{H}$ associated to it**. This is the statement that quantum states are rays in a Hilbert space.\n",
        "\n",
        "* <font color=\"blue\">Now, **why does the physical notion of combining the spaces of states of individual systems into a space of states of the combined system correspond to taking the tensor product?**</font>\n",
        "\n",
        "  * The reason is that <font color=\"blue\">**we want every action of an operator (which are linear maps) on the individual states to define an action on the combined state**</font>\n",
        "\n",
        "  * and the tensor product is exactly that, since, <font color=\"red\">for every pair of linear maps $T_{i}: \\mathcal{H}_{i} \\rightarrow \\mathcal{H}$ (which is a bilinear map $\\left(T_{1}, T_{2}\\right): \\mathcal{H}_{1} \\times \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$ ) there is a unique linear map $T_{1} \\otimes T_{2}: \\mathcal{H}_{1} \\otimes \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$</font>\n",
        "\n",
        "* Alternatively, <font color=\"blue\">**concentrating more on the projective nature of the spaces of states, we observe that $|\\psi\\rangle$ and $a|\\psi\\rangle$ are the same state for any $a \\in \\mathbb{C}$.**</font> Therefore, denoting the sought-for physical product by $\\otimes$ (i.e. not assuming it is the tensor product), we must demand that\n",
        "\n",
        "><font color=\"red\">$\n",
        "|\\psi\\rangle \\otimes|\\phi\\rangle=(a|\\psi\\rangle) \\otimes|\\phi\\rangle=a(|\\psi\\rangle \\otimes|\\phi\\rangle)\n",
        "$</font>\n",
        "\n",
        "* since the states produced by $|\\psi\\rangle$ and $a|\\psi\\rangle$ must yield the same state, i.e. map onto the same projective state. **This obviously fails for the cartesian product, since the pair $(a|\\psi\\rangle,|\\phi\\rangle)$ <u>is not a multiple</u> of the $\\operatorname{pair}(|\\psi\\rangle,|\\phi\\rangle)$, but it is true for the tensor product**.\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/148378/importance-of-kronecker-product-in-quantum-computation\n",
        "\n",
        "*Explanation 2*\n",
        "\n",
        "* In der Mathematik k√∂nnen mit Hilfe des Tensorprodukts (Kronecker-Produkts) von PauliMatrizen (mit Einheitsmatrix) die Darstellungen der h√∂heren Clifford-Algebren √ºber den reellen Zahlen aufgebaut werden.\n",
        "\n",
        "* Pauli-Matrizen k√∂nnen zur Darstellung von Hamilton-Operatoren und zur N√§herung der Exponentialfunktion solcher Operatoren verwendet werden. Sind $\\sigma_{0}, \\sigma_{1}, \\sigma_{2}, \\sigma_{3}$ die vier Pauli-Matrizen, so kann man mit Hilfe des Kronecker-Produkt h√∂herdimensionale Matrizen erzeugen.\n",
        "\n",
        "> $\n",
        "p:=\\sigma_{\\mu_{1}} \\otimes \\sigma_{\\mu_{2}} \\otimes \\ldots \\otimes \\sigma_{\\mu_{n}} \\quad ; \\quad \\mu_{1}, \\mu_{2}, \\ldots, \\mu_{n} \\in\\{0,1,2,3\\} \\quad ; \\quad n \\in \\mathbb{N}\n",
        "$\n",
        "\n",
        "* **Das Kronecker-Produkt von Pauli-Matrizen tritt bei der Beschreibung von Spin-1/2-Systemen auf, die aus mehreren Teilsystemen aufgebaut sind**.\n",
        "\n",
        "* Der Zusammenhang ist dadurch gegeben, dass **das Tensorprodukt zweier Operatoren in der zugeh√∂rigen Matrixdarstellung durch das Kronecker-Produkt der Matrizen gegeben ist** (siehe [Kronecker-Produkt#Zusammenhang mit Tensorprodukten](https://de.wikipedia.org/wiki/Kronecker-Produkt#Zusammenhang_mit_Tensorprodukten)).\n",
        "\n",
        "\n",
        "https://de.wikipedia.org/wiki/Pauli-Matrizen#Kronecker-Produkt_von_Pauli-Matrizen\n",
        "\n",
        "*Explanation 3*\n",
        "\n",
        "* The kronecker product in group theory is widely used, especially with [Wigner D-function](https://de.wikipedia.org/wiki/Wignersche_D-Matrix)\n",
        "\n",
        "* The main purpose of its use in physics is to get the higher dimensional vector space. For example, in atomic physics, when we want to calculate the eigenvalues and eigenvectors of a system of spins 1/2 or spin Hamiltonian.\n",
        "\n",
        "* We analytically or with the help of computer diagonalize spin Hamiltonian and find eigenvectors and eigenvalues with the kronecker product:\n",
        "\n",
        "  * By appling the kronecker product between differnt spins matrices e.g., two matrices (dimensions 2 √ó 2) of spins 1/2, we will get the matrix of dimensions (4 √ó 4).\n",
        "\n",
        "  * This method is very compact, which means we can use the computer to get the eigenvectors and eigenvalues of matrix after applying kronecker product for higher number of spins e.g., for the system of spins 1/2.\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104019\n",
        "\n",
        "*Explanation 4*\n",
        "\n",
        "In quantum theory the analog of a Cartesian product of classical phase spaces is a tensor product of Hilbert spaces.\n",
        "\n",
        "https://quantum.phys.cmu.edu/CQT/chaps/cqt06.pdf\n",
        "\n",
        "*Explanation 5*\n",
        "\n",
        "How do you describe the combined state of two qubits? **Remember that each qubit is a vector space, so they can't just be multiplied**. Instead, you use a tensor product, which is a related operation that creates a new vector space from individual vector spaces, and is represented by the $\\otimes$ symbol.\n",
        "\n",
        "For example, the tensor product of two qubit states $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ and $\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]$ is calculated\n",
        "\n",
        "> $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right] \\otimes\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]=\\left[\\begin{array}{l}a\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right] \\\\ b\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$\n",
        "\n",
        "The result is a four-dimensional matrix, with each element representing a probability.\n",
        "\n",
        "For example, $a c$ is the probability of the two qubits collapsing to 0 and $0, a d$ is the probability of 0 and 1, and so on.\n",
        "\n",
        "Just as a single qubit state $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ must meet the requirement that $|a|^{2}+|b|^{2}=1$ in order to represent a quantum state,\n",
        "\n",
        "a two-qubit state $\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$ must meet the requirement that $|a c|^{2}+|a d|^{2}+|b c|^{2}+|b d|^{2}=1$\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/quantum/overview-algebra-for-quantum-computing\n",
        "\n",
        "*Explanation 6*\n",
        "\n",
        "If we have a set, denoted by 'a', of possible outcomes from one event and a set of outcomes for another event, denoted by 'b', then the possible outcomes for the union event is always the tensor product a‚äób.\n",
        "\n",
        "https://www.quora.com/What-is-a-tensor-product-in-quantum-mechanics\n",
        "\n",
        "*Explanation 7*\n",
        "\n",
        "At some point in the history of quantum mechanics, it was accepted that a single particle is described by a wavefunction which is a function of the position of the particle r, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        ")\n",
        ".\n",
        "\n",
        "At some (possibly later) point it was also accepted that two particles are described by a wavefunction which is a function of the positions of each one of the particles, r1 and r2, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        "1\n",
        ",\n",
        "r\n",
        "2\n",
        ")\n",
        ".\n",
        "\n",
        "In other words, the Hilbert space describing the two-particle system is the tensor product of the Hilbert spaces describing the system of each particle.\n",
        "\n",
        "https://physics.stackexchange.com/questions/53039/when-and-how-did-the-idea-of-the-tensor-product-originate-in-the-history-quantum\n",
        "\n",
        "*Explanation 8*\n",
        "\n",
        "Tensor Products are used to describe systems consisting of multiple subsystems. Each subsystem is described by a vector in a vector space (Hilbert space). For example, let us have two systems I and $/ /$ with their corresponding Hilbert spaces $H_{1}$ and $H_{11}$. Thus, using the bra-ket notation, the vectors $\\left|\\Psi_{1}\\right\\rangle$ and $\\mid \\Psi_{I I}$ ) describe the states of system I and $\\|$ with the state of the total system given by the tensor product $\\left|\\psi_{i}\\right\\rangle \\otimes\\left|\\psi_{1 I}\\right\\rangle$.\n",
        "\n",
        "https://www.quantiki.org/wiki/tensor-product\n",
        "\n",
        "*Explanation 9*\n",
        "\n",
        "The Hilbert space of a composite system is the Hilbert space tensor product of the state spaces associated with the component systems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "Others:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_mechanics\n",
        "\n",
        "http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html\n",
        "\n",
        "https://docs.microsoft.com/de-de/azure/quantum/overview-algebra-for-quantum-computing"
      ],
      "metadata": {
        "id": "QWKNU3sBPbDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary: Tensor Transformations & Einstein Notation*"
      ],
      "metadata": {
        "id": "pBNDxf7EBkj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "JIDiucDuBmiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Tensor Transformation**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_overview.png)"
      ],
      "metadata": {
        "id": "yZth_WAwBoY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)"
      ],
      "metadata": {
        "id": "g_PC1EsqBqK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "UxjiWAuDBsC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vectors and covectors (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)"
      ],
      "metadata": {
        "id": "u4TZq3cvBtyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vector fields and covector fields / differential forms (not single vectors!) (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)"
      ],
      "metadata": {
        "id": "0umDhwJfBvmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einstein Notation**\n",
        "\n",
        "* $\\sum_{i=1}^{3} a_{i} x_{i}=a_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}$  is in this case the same as saying: $a_{i} x_{i}$\n",
        "\n",
        "* See video: [4 rules of Einstein notation](https://www.youtube.com/watch?v=CLrTj7D2fLM&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=3)\n",
        "\n",
        "* This derivation to transform matrix components is pretty complex and can be written in a much easier way:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_25.png)\n",
        "\n",
        "If we have the same index on and top and on the bottom we end up summing over that index letter and can drop the summation sign, in this case: $\\color{red}{\\sum_{k=1}^{n}}$:\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "**Because when we see an index repeated on the top and bottom we know that there is a summation that's going to happen.**\n",
        "\n",
        "Another examples:\n",
        "\n",
        "> $\\color{red}{\\sum_{q=1}^{n}} \\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "can be rewritten by dropping the summation sign because ${\\color{red}{q}}$ (to which the sum sign refers to) is on the top and bottom:\n",
        "\n",
        "> $\\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "And as for the total formula:\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}= \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=  B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_26.png)\n",
        "\n",
        "* According to the [Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation), when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set \\{1,2,3\\} ,\n",
        "\n",
        "> $\n",
        "y=\\sum_{i=1}^{3} c_{i} x^{i}=c_{1} x^{1}+c_{2} x^{2}+c_{3} x^{3}\n",
        "$\n",
        "\n",
        "is simplified by the convention to:\n",
        "\n",
        ">$\n",
        "y=c_{i} x^{i}\n",
        "$\n",
        "\n",
        "* The upper indices are not exponents but are indices of coordinates, coefficients or basis vectors.\n",
        "\n",
        "In [general relativity](https://en.wikipedia.org/wiki/General_relativity), a common convention is that\n",
        "\n",
        "* the Greek alphabet is used for space and time components, where indices take on values 0,1,2 ,\n",
        "or 3 (frequently used letters are $\\mu, v, \\ldots),$\n",
        "\n",
        "* the Latin alphabet is used for spatial components only, where indices take on values $1,2,$ or 3 (frequently used letters are $i, j, \\ldots)$\n",
        "\n",
        "*Die [Penrosesche graphische Notation](https://de.m.wikipedia.org/wiki/Penrosesche_graphische_Notation) ist eine alternative Schreibweise f√ºr die Darstellung von Tensoren!*"
      ],
      "metadata": {
        "id": "OCv-TJ6LBxzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Field*"
      ],
      "metadata": {
        "id": "pAHNvHgpB0r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorfeld**\n",
        "\n",
        "* Tensorfelder sind Funktionen, **die jedem Punkt einen Tensor zuordnen** (Tensor meint in diesem Fall ein rein algebraisches Objekt)\n",
        "\n",
        "* Tensorfelder werden auf ihre analytischen Eigenschaften untersucht (zB differenziert). Man erh√§lt durch Differenzieren eines Tensorfeldes wieder ein Tensorfeld. Tensorfelder sind besondere glatte Abbildungen, die in Tensorb√ºndel hinein abbilden (siehe unten).\n",
        "\n",
        "* Sei $M$ eine differenzierbare Mannigfaltigkeit. Ein [Tensorfeld](https://de.wikipedia.org/wiki/Tensorfeld) vom Typ (r,s) ist ein glatter [Schnitt](https://de.m.wikipedia.org/wiki/Schnitt_(Faserb√ºndel)) im Tensorb√ºndel $T_{s}^{r}(M)$.\n",
        "\n",
        "  * Ein Tensorfeld ist also ein glattes Feld $M \\rightarrow T_{s}^{r}(M),$ welches jedem Punkt der Mannigfaltigkeit einen (r,s)-Tensor zuordnet.\n",
        "\n",
        "  * Die Menge der Tensorfelder wird oft mit $\\Gamma^{\\infty}\\left(T_{s}^{r}(M)\\right)$ bezeichnet."
      ],
      "metadata": {
        "id": "TP8jXL78B271"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensordichte**\n",
        "\n",
        "* [Tensordichte](https://de.wikipedia.org/wiki/Tensordichte) ist die Quantit√§tsgr√∂√üe eines Tensorfeldes (Generalisierung)\n",
        "\n",
        "* die Tensordichte ist eine **Verallgemeinerung der Tensorfelder** in der Tensoranalysis\n",
        "\n",
        "* wurde eingef√ºhrt, um den ‚ÄûUnterschied zwischen Quantit√§t und Intensit√§t, soweit er physikalische Bedeutung hat‚Äú, zu erfassen: ‚Äûdie Tensoren sind die Intensit√§ts-, die Tensordichten die Quantit√§tsgr√∂√üen‚Äú.\n",
        "\n",
        "* eine **Tensordichte** ordnet einem Koordinatensystem ein Tensorfeld derart zu, dass es bei einem Koordinatenwechsel mit dem Absolutbetrag der Funktionaldeterminante multipliziert wird. Eine Tensordichte der Stufe null ist demnach eine skalare Dichte, deren Integral gem√§√ü dem Transformationssatz eine Invariante liefert."
      ],
      "metadata": {
        "id": "WCB2oE9SB4xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field*"
      ],
      "metadata": {
        "id": "r-rllUswCV7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis of Vector Fields**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Tensorfeld\n",
        "\n",
        "> Basis Vectors = Partial Derivatives (Jacobian)\n",
        "\n",
        "> **From Single Vectors to Vector Fields (Transformations)**\n",
        "\n",
        "Energy Momentum Tensor: https://youtu.be/ii7rffG0EwU\n",
        "\n",
        "A (basis) vector can be re-interpreted as a partial derivative (see image below):\n",
        "\n",
        "> $\\overrightarrow{e_{x}} \\equiv \\frac{\\partial \\vec{R}}{\\partial x}$\n",
        "\n",
        "https://www.youtube.com/watch?v=rr5qEb_kT6c&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=3\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_112.png)\n",
        "\n",
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=OMCguyCnTQk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=4\n",
        "\n",
        "* Now if you want to get the coefficients when your old basis in Cartesian and your new basis is Polar coordinates, then you have another forward map at every point in the polar coordinate system\n",
        "\n",
        "* if you now think of the basis vectors as partial derivatives, it makes things much easier. Use mutlivariable chain rules:\n",
        "\n",
        "  * the first old basis vector $\\overrightarrow{e_{x}}$ can be thought of as the partrial derivative in the x direction: $\\frac{\\partial \\vec{R}}{\\partial x}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * the second old basis vector $\\overrightarrow{e_{y}}$ can be thought of as the partrial derivative in the y direction: $\\frac{\\partial \\vec{R}}{\\partial y}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * same goes for the new basis vector (polar coordinates) as partial derivaties into r and $\\theta$ directions\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_90.png)\n",
        "\n",
        "The underlined part are the coefficients to move from one basis (cartesian) to another (polar) (Achtung: not normalised):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_91.png)\n",
        "\n",
        "The forward matrix = the Jacobian matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_92.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_95.png)\n",
        "\n",
        "And you can do the same the other way around to get the Backward transform, which is the inverse Jacobian:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_93.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_94.png)\n",
        "\n",
        "It total it looks likes this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_96.png)\n",
        "\n",
        "And we can store the (forward & backward) coefficients in matrix\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_97.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tYEF_rwU4Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Field Components**\n",
        "\n",
        "> Vectors Components = Derivatives in Vector Fields (vector fields of tangent vectors along curves)\n",
        "\n",
        "> Task: Figure out vector components in a new basis, **but instead of one vector, we consider vector fields**.\n",
        "\n",
        "We consider vectors along a curve (=tangent vectors)\n",
        "\n",
        "https://www.youtube.com/watch?v=9yOb9gHnLUk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=5\n",
        "\n",
        "\n",
        "The cases are both the same:\n",
        "\n",
        "* on we have a single vector in a new basis constructed from the basis vectors in the old basis and its components\n",
        "\n",
        "* on the bottom we have a whole vector field constructed from the old basis (using the chain rule!) and the vectir components ar derivates!\n",
        "\n",
        "* Framed in red is the vector we want to expand, in blue framed the basis vectors and in green framed the vector components\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_98.png)\n",
        "\n",
        "Example: in the cartesian coordinate system the basis vectors are every where the same. Just components are everywhere different. But it‚Äôs easy using the multivariable chain rule when you take the vector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_99.png)\n",
        "\n",
        "Other example iof tangents on a circle in a cartesian coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_100.png)\n",
        "\n",
        "\n",
        "Do these component make sense? Let's check with an example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_101.png)\n",
        "\n",
        "Also works in the polar coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_103.png)\n",
        "\n",
        "Checking if it's true, and it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMCdUWicVfKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contravariance of Vector Field Components**\n",
        "\n",
        "https://www.youtube.com/watch?v=zKuyaQ4JRs8&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=6&t=154s\n",
        "\n",
        "* Vector components are contravariant\n",
        "\n",
        "* We can follow the same reasoning for vector fields of tangent vectors along curves\n",
        "\n",
        "In this example, the polar components have two be equal to the Cartesian components multiplied by the backward transform. So we get this transformation formula for the components of the tangent vectors (the box on the bottom right):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "Partial derivative basis vectors transform one way and the vector component derivatives transform the other way, the vector components are contravariant. (Dervative coefficients are opposites) But don't memorize them! Simply use multivariable chain rules for the four formulas on the bottom and you get the transformation (chain rules over cartesian frames in purple, chain rule sover polar coordinates framed in green)::\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_105.png)\n",
        "\n",
        "Remember when we used for forward and backward transforms with single vectors the following formula:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_107.png)\n",
        "\n",
        "**This is similar for Vector Fields: basis vectors transform one way, and the vector components transform the other wa, using the Jacobian $J$ and the Jacobian inverse $J^-1$ as the forward and backward transforms:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_106.png)\n",
        "\n",
        "..and this is for the specific example we had for the circular curve with radius 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_108.png)\n",
        "\n",
        "**One important last thing: we remove the position vector $R$ and leave the <u>derivative operators = (basis) vector</u>, and they will be considered basis vectors from now on:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_109.png)\n",
        "\n",
        "And this makes sense: The partial derivative with respect to x points in the x direction (and so on):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_110.png)\n",
        "\n",
        "This is useful becasue position vectors $R$ rely on an origin, and as we will see later, on manifolds on curved surfaces we cannot rely on the existnce of an origin point:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_111.png)"
      ],
      "metadata": {
        "id": "J152JTPfWWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covector Field (1-form-Differential Form)*"
      ],
      "metadata": {
        "id": "8h7LKwE9CYwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector Field (= 1-form-Differential Form)**\n",
        "\n",
        "> <font color=\"blue\">**Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind**\n",
        "\n",
        "> **Covectors: Differential Forms = Covector Fields**\n",
        "\n",
        "Das Differential eines Skalarfeldes (differentialform) ist ein Covector field, weil differentiale von skalaren covectoren sind??\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_119.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8&t=201s\n",
        "\n",
        "Re-interprete $d$ from $df$ as being an operator that takes a scalar field and outputs a covector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_120.png)\n",
        "\n",
        "Example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)\n",
        "\n",
        "The way to get the covector fields is by tracing out the level sets of the scalar function: **Skalarfeld links mit Temperaturen, Kovektorfeld rechts mit den Konturen, wo √ºberall die gleichen Konturen existieren (√Ñquivalenzen)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)\n",
        "\n",
        "Another example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_124.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_122.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)\n",
        "\n",
        "Example: If we think of x as a scalar field, it would look like this: it‚Äôs a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [Level Set (Niveaumenge)](https://de.wikipedia.org/wiki/Niveaumenge): **die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist**\n",
        "\n",
        "Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)\n",
        "\n",
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)\n",
        "\n",
        "**How to calculate now? - How do covector fields like $df$ act on $\\vec{v}$ to give us output values?**\n",
        "\n",
        "> Count the number of tangent lines to the curve at p (where the vector originates at point p)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_130.png)\n"
      ],
      "metadata": {
        "id": "mjpXmcIJSASJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But what's the geometrical meaning of $df$ ($\\vec{v}$)?**\n",
        "\n",
        "On a map we can draw out curves of constant elevation (which is the same thing as level sets). We can think of this level set drawing of a mountain as a covector field associated with the mountain.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_131.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_132.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_133.png)\n",
        "\n",
        "> $d f(\\vec{v})$ tells us the rate of change of $f$ when moving at velocity $\\vec{v}$. **$d f(\\vec{v})$ is the directional derivative of $f$ in direction $\\vec{v}$**.\n",
        "\n",
        "**Covector Field Components: The following are the basis covectors**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_134.png)\n",
        "\n",
        "So just as we can expand individual covectors into linear combinations of dual basis vectors (and of course we get different components depending on which basis we use (all on top), we can also expand differential forms - also callee covector fields - into linear combinations of other covector fields where we get different components depending o which basis we use (on the bottom).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_135.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=r_20yXBdhJk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=9\n",
        "\n"
      ],
      "metadata": {
        "id": "05_SpyG3SEPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation Rules of Differential Forms (Covector Fields)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)\n",
        "\n",
        "Same logic applies for covector fields: if we for example want to build the Cartesian basis covector fields our of the polar basis covector fields (from new to old), we use the following coefficients, which are the entries of the Jacobian matrix (=forward transform, because covector basis components act contravariant).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_137.png)\n",
        "\n",
        "**Summary of all transformations (between 2 basis of covector fields, for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=4doR1XCXzKU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=10"
      ],
      "metadata": {
        "id": "FffzqkWvSGIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration with Differential Forms**\n",
        "\n",
        "* With the following interpretation of differential forms we can create the integration and it doesn't depend on coordinate systms at all\n",
        "\n",
        "* Also we **just need start point and end point** and count the number of pierced covector components instead of computing the integral at each point on the line\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_147.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_149.png)\n",
        "\n",
        "Since the covector fields and the paths are the same in both following cases, the result of the integral which is negative 4, is also the same in both cases:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_148.png)\n",
        "\n",
        "> **Covector fields are invariant of the choice of coordinates. Covector field components depend on the choice if coordinate.*And that's why when we change the variable in an integral we still get the same answer.**\n",
        "\n",
        "https://www.youtube.com/watch?v=kyzSofggsqg&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=11\n",
        "\n",
        "https://www.youtube.com/watch?v=PzrGGbX-_54&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=12"
      ],
      "metadata": {
        "id": "rAtqpzXZSH87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field vs Covector Field*"
      ],
      "metadata": {
        "id": "ihDto5NgCay8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gradient ‚àá vs ùëë operator (exterior derivative/differential)\n",
        "\n",
        "https://www.youtube.com/watch?v=nJpONHO_X5o&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=15\n",
        "\n",
        "https://www.youtube.com/watch?v=Do5vzLJRWRE&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=16\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_139.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_zLHtahCwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Extrinsic (Exterior) Geometry vs Intrinsic Geometry*"
      ],
      "metadata": {
        "id": "BJaouQGICdBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=VHkL5HpL0HY&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=7\n",
        "\n",
        "* Let's take a 2D surface like earth, and someone drives along it\n",
        "\n",
        "* we want to get the velocity of the car. We need position vectors, where we need an origin point (center of earth), then we get 2 vectors, take their limit to compute velocity\n",
        "\n",
        "* There are 2 issues with this:\n",
        "\n",
        "  1. Firstly the origin point that we've chosen doesn't live o the earth's 2D surface. We picked an origin point that was exterior to the surface.\n",
        "\n",
        "  2. Secondly the target velocity actually leaves the surface that we are studying and goes off into the outside space - **Remember here Lie algreba and Lie group**: the tangent is outside the 2D surface and the tangent point is the Lie algebra. All on the 2D surface however is part of the Lie group.\n",
        "\n",
        "* So here, both the origin point and the velocity vector are both defined using the 3 dimensional space, even though we are only studying 2 dimensional spherical surface\n",
        "\n",
        "* **This is why it's all called exterior geometrty and exterior product** - studying something outside\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_113.png)"
      ],
      "metadata": {
        "id": "zH_Dj6_LEh_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to find the distance between 2 points on a map of earth, it's called \"intrinsiv geometry\" when we are not allowed to leave the surface (and hence cannot draw a straight line, but need to find a geodesic on the flat map).\n",
        "\n",
        "And that gets even more complicated in General Relativity:\n",
        "\n",
        "1. 4D spacetime is curved space, so we cannot draw a straight line int he curved space. So that means we cant draw poisiton vectors.\n",
        "\n",
        "2. And we cannot pick an origin outside the 4D spacetime, because that would mean picking a point outside of the universe\n",
        "\n",
        "That's why its called intrinsic geometry: you need to stay inside! **But how do we study velocity if we are not allowed to use position vectors?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_114.png)"
      ],
      "metadata": {
        "id": "Y8Mr4AgTEjud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution: we cannot draw straight lines, but we can draw curved paths on a 2D on a map for exmaple**.\n",
        "\n",
        "> **So we can't use normal position vectors to talk about directions on a surface, but we can use derivative operators to talk about different directions:**\n",
        "\n",
        "* If have have some path that's traveling around in our curved space $(x(\\lambda), y(\\lambda))$, we can still consider the direction that the path is pointing in using the derivative with respect to the curve parameter $\\frac{d}{d \\lambda}$\n",
        "\n",
        "* And we can break this direction up into its x and y components using the multivariable chain rules that we have for derivative operators $\\frac{d}{d \\lambda}=\\frac{d x}{d \\lambda} \\frac{\\partial}{\\partial x}+\\frac{d y}{d \\lambda} \\frac{\\partial}{\\partial y}$\n",
        "\n",
        "* But Achtung: you shouldn't think of this direction vector as actually connecting the two points on the earth (origin and destination of yellow arrow). The derivative just gives the general direction that the curve is traveling in at a given point.\n",
        "\n",
        "* So derivatives with us a way to talk about directions on a curved surface in a way that is complete intrinsic to the surface and doesn't require an outside space in any way.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_115.png)"
      ],
      "metadata": {
        "id": "cabaMULeElaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same approach in 4 D space: we can't draw straight lines, but we can still draw curved paths. And that means we can take derivatives with respect to the paths parameters to get a sense of different directions.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_116.png)"
      ],
      "metadata": {
        "id": "PFhrRHMWEnRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> So the old notation uses actual vectors from the **vector spaces R2 and R3** to define directions. But the new notation use the **vector space of derivative operators**.\n",
        "\n",
        "> **The vector space of derivative operators is formally known the \"Tangent Vector Space\" and is denoted: $T_{p}M$**, which is the vector space of derivatives at some point p on a surface $M$. And keep in mind that the vectors on the left and the vectors on the right are in fact from different vector spaces!!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_117.png)\n",
        "\n",
        "And they do form a vector space because we can scale and add them linearly! So these partial derivatives are vectors in the tangent space $T_{p}M$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_118.png)"
      ],
      "metadata": {
        "id": "dvn3ZKheEpKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Metric Tensor Field*"
      ],
      "metadata": {
        "id": "QHq7RwnYCe8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Metric Tensor to compute arc length of a curve in Flat & Curved Space**\n",
        "\n",
        "**In Flat Space**\n",
        "\n",
        "So in summary the equation for calculating the arc length of a curve is an integral that depends on the magnitude of the curves tangent vectors $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|$ in this:\n",
        "\n",
        "> $\\operatorname{arclength}=\\int\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\| d \\lambda$\n",
        "\n",
        "And to calculate the squared magnitude of the tangent vectors we need to use the dot product\n",
        "\n",
        "> $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|^{2}=\\frac{d \\vec{R}}{d \\lambda} \\cdot \\frac{d \\vec{R}}{d \\lambda}$\n",
        "\n",
        "And we end up with this equation in Cartesian coordinates:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$\n",
        "\n",
        "And this equation in polar coordinates:\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p^{j}}\\right)$\n",
        "\n",
        "And the basis vector dot products $\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$ and $\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p j}\\right)$ give us the components of the metric tensor:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda} g_{i j}$\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda} \\widetilde{g_{i j}}$\n",
        "\n",
        "\n",
        "**So the key to getting the arc length of a curve is the tangent vector magnitude. And the key to getting the tangent vector magnitude is the metric tensor components $g_{i j}$ and $\\widetilde{g_{i j}}$**.\n",
        "\n",
        "And remember: the metric tensor is a (0,2) tensor because its components obey 2 covariant transformation laws:\n",
        "\n",
        "> $\\begin{aligned} \\widetilde{g_{i j}} &=\\frac{\\partial c^{k}}{\\partial p^{i}} \\frac{\\partial c^{l}}{\\partial p^{j}} g_{k l} \\\\ g_{k l} &=\\frac{\\partial p^{i}}{\\partial c^{k}} \\frac{\\partial p^{j}}{\\partial c^{l}} \\widetilde{g_{i j}} \\end{aligned}$\n",
        "\n",
        "In flat space there is only 1 metric tensor with which you can calculate the arc length of any curve as long as we can do this integral.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_150.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=BbQmTmSzUCI&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=13\n"
      ],
      "metadata": {
        "id": "L3N2cBc6FoEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Curved Space**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_142.png)\n",
        "\n",
        "In curved space: Vector field: vector changes from point to point. Covector fields have covectors that change from point to point. And now a metric tensor field involves a different metric tensor being placed everywhere in space that change from point to point.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_151.png)\n",
        "\n",
        "Every curved space has its own metric tensor field that gives you the rules for measuring distance on it:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_152.png)\n",
        "\n",
        "Metric tensors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_143.png)\n",
        "\n",
        "Simplify them:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_144.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_145.png)\n",
        "\n",
        "For the intrinsic view (left side only) we can remove $R$ vectors completely and treat derivative operators themselves as the vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_146.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=SmjbpIgVKFs&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=14"
      ],
      "metadata": {
        "id": "yS56QI6iGRym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOpIknui8ULm"
      },
      "source": [
        "###### ***Differential Geometry of Smooth Surfaces***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential geometry of smooth surfaces**\n",
        "\n",
        "* Wikipedia: [First and second fundamental forms, the shape operator, and the curvature](https://en.m.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature)\n",
        "\n",
        "* Artikel: [A-quick-and-dirty-introduction-to-the-curvature-of-surfaces](http://wordpress.discretization.de/geometryprocessingandapplicationsws19/a-quick-and-dirty-introduction-to-the-curvature-of-surfaces/)\n",
        "\n",
        "* [Principal curvatures](https://en.m.wikipedia.org/wiki/Principal_curvature)\n",
        "\n",
        "* [Gauss map](https://en.wikipedia.org/wiki/Gauss_map) provides a mapping from every point on a curve or a surface to a corresponding point on a unit sphere\n",
        "\n",
        "* [Shape operator](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature) or Weingarten map\n",
        "\n",
        "* [Normal plane](https://en.wikipedia.org/wiki/Normal_plane_(geometry)): a normal vector that is at right angles to the surface\n",
        "\n",
        "* [Normal section](https://en.wikipedia.org/wiki/Normal_plane_(geometry)#Normal_section): Intersection of normal plane and  surface (*For most points on most surfaces, different normal sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures, call these Œ∫1, Œ∫2*)\n",
        "\n",
        "* **Extrinsic Properties**: Rely on an embedding of a surface in Euclidean space. Illustrated by the non-linear [Euler‚ÄìLagrange equations](https://en.m.wikipedia.org/wiki/Euler‚ÄìLagrange_equation) in [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations). Lagrange: [minimal surfaces](https://en.m.wikipedia.org/wiki/Minimal_surface) (can only be defined in terms of an embedding).\n",
        "\n",
        "* **Intrinsic Properties / [Gaussian curvature](https://en.m.wikipedia.org/wiki/Gaussian_curvature)**: Geometric properties determined by geodesic distances (not embeddings).\n",
        "\n",
        "  * Gaussian curvature of a surface = how curves on the surface change directions in three dimensional space\n",
        "\n",
        "  * An important role play Lie groups, namely the [symmetry groups](https://en.m.wikipedia.org/wiki/Symmetry_group) of the Euclidean plane, the sphere and the hyperbolic plane.  **These Lie groups can be used to describe surfaces of constant Gaussian curvature**; they also provide an essential ingredient in the modern approach to intrinsic differential geometry through [connections](https://en.m.wikipedia.org/wiki/Connection_(mathematics)).\n",
        "\n",
        "  * [Gaussian curvature](https://en.wikipedia.org/wiki/Gaussian_curvature) is an intrinsic invariant, i.e. invariant under local [isometries](https://en.m.wikipedia.org/wiki/Isometry). This point of view was extended to higher-dimensional spaces by Riemann and led to what is known today as [Riemannian geometry](https://en.m.wikipedia.org/wiki/Riemannian_geometry).\n",
        "\n",
        "* [First fundamental form](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature): Gaussian curvature can be calculated from the first fundamental form => curvature and metric properties of a surface such as length and area, called [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) of surface.\n",
        "\n",
        "> $\\mathrm{I}(x, y)=\\langle x, y\\rangle .$\n",
        "\n",
        "* [Second fundamental form](https://en.wikipedia.org/wiki/Second_fundamental_form) encodes how lengths and angles of curves on the surface are distorted when the curves are pushed off of the surface. Together with First fundamental form, it serves to define extrinsic invariants of the surface, **its principal curvatures**.\n",
        "\n",
        ">$L d x^{2}+2 M d x d y+N d y^{2}$ (quadratic from ins 3D)\n",
        "\n",
        "* [Riemann curvature tensor](https://en.m.wikipedia.org/wiki/Riemann_curvature_tensor): Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VJjblo9jZJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1443.png)\n"
      ],
      "metadata": {
        "id": "dcgQer6SUB0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1444.png)\n"
      ],
      "metadata": {
        "id": "TZkbzLiaUAzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jj](https://raw.githubusercontent.com/deltorobarba/repo/master/gaussiancurvature.png)\n"
      ],
      "metadata": {
        "id": "WHI-KoMUT_dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Definition of second fundamental form*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/5/5e/Second_fundamental_form.svg)\n",
        "\n",
        "[Source](https://www.researchgate.net/figure/Measures-of-surface-curvature-a-The-principal-curvatures-are-calculated-from-the_fig6_321165318)\n"
      ],
      "metadata": {
        "id": "xCMvunOBT9v1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTS59KF68ULu"
      },
      "source": [
        "**Levi-Civita-Zusammenhang (Connection)**\n",
        "\n",
        "* Der [Levi-Civita-Zusammenhang](https://de.wikipedia.org/wiki/Levi-Civita-Zusammenhang#Hauptsatz_der_riemannschen_Geometrie) ist ein wesentliches Hilfsmittel zum Aufbau der riemannschen Kr√ºmmungstheorie. Denn der Kr√ºmmungstensor wird mit Hilfe eines Zusammenhangs definiert, daher bietet es sich an, in der riemannschen Geometrie den eindeutig ausgezeichneten Levi-Civita-Zusammenhang f√ºr die Definition des riemannschen Kr√ºmmungstensors zu verwenden.\n",
        "\n",
        "* Der [Zusammenhang](https://de.wikipedia.org/wiki/Zusammenhang_(Differentialgeometrie)) (Connection) ist in der Differentialgeometrie ein Hilfsmittel, um Richtungs√§nderungen im Laufe einer Bewegung zu quantifizieren und Richtungen in verschiedenen Punkten miteinander in Beziehung zu setzen.\n",
        "\n",
        "* Dieser Artikel behandelt im Wesentlichen den Zusammenhang auf einer differenzierbaren Mannigfaltigkeit beziehungsweise auf einem [Vektorb√ºndel](https://de.wikipedia.org/wiki/Vektorb√ºndel). Ein ausgezeichneter Zusammenhang auf einem Tensorb√ºndel, einem besonderen Vektorb√ºndel, hei√üt kovariante Ableitung.\n",
        "\n",
        "* Allgemeiner existieren auch [Zusammenh√§nge auf Prinzipalb√ºndeln](https://de.wikipedia.org/wiki/Zusammenhang_(Prinzipalb√ºndel)) mit analogen definierenden Eigenschaften.\n",
        "\n",
        "* In der Differentialgeometrie interessiert man sich f√ºr die Kr√ºmmung von Kurven, insbesondere von Geod√§ten. In euklidischen R√§umen ist die Kr√ºmmung einfach durch die zweite Ableitung gegeben.\n",
        "\n",
        "* **Auf differenzierbaren Mannigfaltigkeiten ist die zweite Ableitung nicht direkt zu bilden. Ist $\\gamma$ eine Kurve, so muss man f√ºr die zweite Ableitung dieser Kurve den Differenzenquotienten mit den Vektoren $\\gamma^{\\prime}(t)$ und $\\gamma^{\\prime}\\left(t_{0}\\right)$ bilden. Diese Vektoren befinden sich jedoch in unterschiedlichen Vektorr√§umen, daher kann man nicht einfach die Differenz der beiden bilden**.\n",
        "\n",
        "* **Um das Problem zu l√∂sen, hat man eine Abbildung definiert, welche man Zusammenhang nennt. Diese Abbildung soll einen Zusammenhang zwischen den beteiligten Vektorr√§umen bereitstellen und tr√§gt daher auch diesen Namen**.\n",
        "\n",
        "In diesem Abschnitt bezeichnet $M$ eine glatte Mannigfaltigkeit, $T M$ das Tangentialb√ºndel und $\\pi: E \\rightarrow M$ ein Vektorb√ºndel. Mit $\\Gamma(E)$ wird die Menge der glatten Schnitte im Vektorb√ºndel $E$ notiert.\n",
        "\n",
        "* **Indem man sagt, was die Richtungsableitung eines Vektorfeldes in Richtung eines Tangentialvektors ist, erh√§lt man einen Zusammenhang auf einer differenzierbaren Mannigfaltigkeit $M$**. Demgem√§√ü definiert man einen Zusammenhang auf einem Vektorb√ºndel als eine Abbildung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "\\nabla: \\Gamma(T M) \\times \\Gamma(E) & \\rightarrow \\Gamma(E) \\\\\n",
        "(X, s) & \\mapsto \\nabla_{X} s\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "* die einem Vektorfeld $X$ auf $M$ und einem Schnitt $s$ im Vektorb√ºndel $E$ wieder einen Schnitt in $E$ zuordnet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-V3Q8YM8ULv"
      },
      "source": [
        "**Vektorb√ºndel & Tensorb√ºndel**\n",
        "\n",
        "* [Vektorb√ºndel](https://de.wikipedia.org/wiki/Vektorb√ºndel) oder manchmal auch Vektorraumb√ºndel sind Familien von Vektorr√§umen, die **durch die Punkte eines topologischen Raumes parametrisiert sind**.\n",
        "\n",
        "* Vektorb√ºndel geh√∂ren damit auch zu den [Faserb√ºndeln](https://de.m.wikipedia.org/wiki/Faserb√ºndel). Remind: Faser ist ein Urbild von einem Element (\"Faser der Abbildung √ºber einem Element\") - surjektiv! kann also mehrere Elemente im Urbild haben. Daher Faser $\\mathbb{R}$<sup>2</sup> zu Punkt auf $\\mathbb{R}$ (siehe [hier](https://de.m.wikipedia.org/wiki/Vektorb√ºndel) die Illustration:\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Vectorbundle.svg/320px-Vectorbundle.svg.png)\n",
        "\n",
        "*Illustration des Vektorb√ºndels $(E, B, \\pi)$. Hier ist der **Totalraum** $E=\\mathbb{R}^{2}$ und der **Basisraum** $B=\\mathbb{R} .$ Die Abbildung $\\pi: E \\rightarrow B$ projiziert jede Gerade $E_{x}$ auf den Punkt $x$. Der Raum $E_{x}=\\{p \\in E \\mid \\pi(p)=x\\}$ wird **Faser √ºber $x$** genannt. Au√üerdem ist der Totalraum $E$ die Vereinigung aller Fasern.* (Comment: also Totalraum E ist Urbild mit Faser und Basisraum B ist Zielbild mit Element das von der Faser stammt)\n",
        "\n",
        "* [Tangentialb√ºndel](https://de.wikipedia.org/wiki/Tangentialb√ºndel)\n",
        "\n",
        "* [Tensorb√ºndel](https://de.m.wikipedia.org/wiki/Tensoranalysis#Tensorb√ºndel) ist ein bestimmtes Vektorb√ºndel. Tensorfelder sind dann besondere glatte Abbildungen, die in dieses Vektorb√ºndel hinein abbilden.\n",
        "\n",
        "* **Schnitt (Faserb√ºndel)**\n",
        "\n",
        "  * [Schnitte](https://de.m.wikipedia.org/wiki/Schnitt_(Faserb√ºndel)) sind Abbildungen, welche in der algebraischen Topologie, insbesondere in der Homotopietheorie, untersucht werden. Insbesondere interessiert man sich daf√ºr, unter welchen Bedingungen solche Abbildungen existieren.\n",
        "\n",
        "  * Das bekannteste Beispiel von Schnitten sind die [**Differentialformen**](https://de.m.wikipedia.org/wiki/Differentialform).\n",
        "\n",
        "  * Ein Schnitt kann als **Verallgemeinerung des Graphen einer Funktion** aufgefasst werden.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bundle_section.svg/187px-Bundle_section.svg.png)\n",
        "\n",
        "*Die Abbildung s ist ein Schnitt in einem Faserb√ºndel $p: E \\rightarrow B$. Dieser Schnitt s erlaubt es, den Basisraum $B$ mit dem Teilraum $s(B)$ von $E$ zu identifizieren.*\n",
        "\n",
        "* Die [Schnittkr√ºmmung](https://de.wikipedia.org/wiki/Schnittkr√ºmmung) ist eine Gr√∂√üe der riemannschen Geometrie, eines Teilgebiets der Mathematik. Mit ihrer Hilfe kann man die Kr√ºmmung einer n-dimensionalen riemannschen Mannigfaltigkeit beschreiben.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUvl0oGH8ULh"
      },
      "source": [
        "###### *Geodesics in curved space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEwWL-tS8ULh"
      },
      "source": [
        "Video: [Tensor Calculus 15: Geodesics and Christoffel Symbols (extrinsic geometry)](https://www.youtube.com/watch?v=1CuTNveXJRc)\n",
        "\n",
        "Video: [Tensor Calculus 16: Geodesic Examples on Plane and Sphere](https://www.youtube.com/watch?v=8sVDceI70HM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1576.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1577.png)"
      ],
      "metadata": {
        "id": "OC5VDhg_jVPS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjFey3T8ULx"
      },
      "source": [
        "###### *Covariant Derivative*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVN1fZ2C8ULx"
      },
      "source": [
        "**Covariant Derivative in Flat Space**\n",
        "\n",
        "> **Covariant Derivative = understanding the rate of change of vector (tensor) fields that takes changing basis vectors into account**\n",
        "\n",
        "* Video: [Tensor Calculus 17: The Covariant Derivative (flat space)](https://www.youtube.com/watch?v=U5iMpOn5IHw&t=4s)\n",
        "\n",
        "Challenge: different sources define covariant derivative in different ways.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1571.png)\n",
        "\n",
        "**Consider following two examples:**\n",
        "\n",
        "* the in the first picture the vector field is constant everywhere. hence the deriative of this vector field is zero in the x and y direction.\n",
        "\n",
        "* in the second image below the vector field is moving. So the vector field is NOT zero in the $r$ and $\\theta$ direction. The components 2 and 1 are constant, but the basis vectors $e_r$ and $e_\\theta$ are changing from point to point that causes the vectors in the vector field to change length and direction.\n",
        "\n",
        "> **Constant Components ‚â† Constant Vector Field**\n",
        "\n",
        "*The covariant derivative components with the semicolon form a (1,1) tensor: one contravatiant transformation rule and one covariant transformation rule (where the inverse Jacobian transforms the contravariant index, and the Jacobian transforms the covariant index.)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVok3GM8ULy"
      },
      "source": [
        "**Covariant Derivative in Curved Space Definition (extrinsic)**\n",
        "\n",
        "Video: [Tensor Calculus 18: Covariant Derivative (extrinsic) and Parallel Transport](https://youtu.be/Af9JUiQtV1k)\n",
        "\n",
        "* Parallel Transport plays an important role\n",
        "\n",
        "* Objective: Dealing with rates of change of vector fields on curved surfaces\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1572.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1573.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1574.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g21QIodZ8UL3"
      },
      "source": [
        "**Covariant Derivative in Curved Space Definition (Intrinsic)**\n",
        "\n",
        "Video: [Tensor Calculus 19: Covariant Derivative (Intrinsic) and Geodesics](https://www.youtube.com/watch?v=EFKBp52LtDM)\n",
        "\n",
        "* Geodesics play an important role!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1575.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qS8lKVO8UL5"
      },
      "source": [
        "**Covariant Derivative: Abstract Definition with Levi-Civita Connection (Fundamental Theorem of Riemannian Geometry)**\n",
        "\n",
        "Video: [Tensor Calculus 20: The Abstract Covariant Derivative (Levi-Civita Connection)](https://www.youtube.com/watch?v=cEEahoUUGyc)\n",
        "\n",
        "*What parallel transport is doing it is connecting the tangent vector space at red point p to tangent vector space at blue point q. So parallel transport gives us a way to map vectors from TpS to TqS. And since parallel transport is defined using the covariant derivative, it's really the covariant derivative that's providing the connection between the tangent spaces in this curved space*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1578.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1579.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1580.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPdLE5H8UL-"
      },
      "source": [
        "###### *Lie Bracket (Commutator in Flow Curves)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoq7AWyk8UL-"
      },
      "source": [
        "**Flow Curve (Integral Curve)**\n",
        "\n",
        "https://www.youtube.com/watch?v=SfOiOPuS2_U&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=24&t=69s\n",
        "\n",
        "In the following image:\n",
        "\n",
        "* The partial derivatives of the coordinate variables are basically like basis vectors (framed in blue)\n",
        "\n",
        "* the derivatives with respect to lambda (framed in red) are the components\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1582.png)\n",
        "\n",
        "**Lie Bracket (the commutator of 2 vector fields $\\vec{u}$ & $\\vec{v}$)**\n",
        "\n",
        "* Lie Bracket takes 2 vector fields and tells us if the rectangle of flow curves closes properly\n",
        "\n",
        "* Flow curves do not close properly if Lie bracket is non zero\n",
        "\n",
        "Task of calculating Lie bracket:\n",
        "\n",
        "1. Find change of $\\vec{u}$ in the direction of $\\vec{v}$\n",
        "\n",
        "2. Find change of $\\vec{v}$ in the direction of $\\vec{u}$\n",
        "\n",
        "> **Lie Bracket (Commutator) = measures how much vector field flow curves fail to close.**\n",
        "\n",
        "> $[\\vec{u}, \\vec{v}]=\\vec{u}(\\vec{v})-\\vec{v}(\\vec{u})$\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1583.png)\n",
        "\n",
        "**Coordinate lines are just flow curves along basis vectors**\n",
        "\n",
        "Because coordinate curves always close without a curve, so Lie bracket always has to be zero for them\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1584.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu1qRXiA8UL-"
      },
      "source": [
        "###### *Torsion Tensor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJTRAij8UL_"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Torsion_tensor\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_259.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZZEEgj8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_262.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iQ321N18UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIJL6tVt8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_275.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiU5eKGE8UMH"
      },
      "source": [
        "###### *Detect Curvatur: Holonomy + Geodesic Deviation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4B8QnY8UMI"
      },
      "source": [
        "So we need a new tool, that works everyhwere:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_282.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuQX8qW8UMI"
      },
      "source": [
        "* With a correct parallel transport we can see if a surface is curved (when vectors at starting and at end point point in different directions)\n",
        "\n",
        "* the is holonomy! It's the twisting of a vector when transport it around in a loop\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_279.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ErFwUxZ8UMI"
      },
      "source": [
        "> $R(\\vec{u}, \\vec{v}) \\vec{w}=\\nabla_{\\vec{u}} \\nabla_{\\vec{v}} \\vec{w}-\\nabla_{\\vec{v}} \\nabla_{\\vec{u}} \\vec{w}-\\nabla_{[\\vec{u}, \\vec{v}]} \\vec{w}$\n",
        "\n",
        "* $R(\\vec{u}, \\vec{v})$ is an operator which acts on vector $\\vec{w}$ and produces change vector - This is the change vector after is is parallel transported around a small parallelogram (the blue handwritten arrow is this result !!!!)\n",
        "\n",
        "* this means the Riemann curvature tensor takes 3 vector inputs: 2 vectors defining the parallelogram and one starting input vector, and it outputs the change vector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_276.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnkSoEwu8UMN"
      },
      "source": [
        "###### *Riemann Curvature Tensor, Sectional Curvature, Ricci-Tensor & Ricci-Scalar*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethu1_YS8UMK"
      },
      "source": [
        "Video: [Tensor Calculus 23: Riemann Curvature Tensor Components and Symmetries](https://www.youtube.com/watch?v=optrC-0HhMI)\n",
        "\n",
        "**Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1581.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Riemann curvature tensor (Kr√ºmmungstensor), Ricci-Tensor, Ricci-Scalar**\n",
        "\n",
        "* Kr√ºmmung wird beschrieben, indem man Vektoren auf geschlossenen Wegen durch die Raumzeit schiebt und feststellt, ob sie ver√§ndert zur√ºckkommen. Nehmen Sie vielleicht einen Globus und einen Bleistift in die Hand. Der Bleistift stellt einen Vektor dar. Legen Sie den Bleistift am Nordpol tangential an den Globus an. Verschieben Sie ihn nun l√§ngs irgendeines L√§ngenkreises (Meridian) zum √Ñquator. Danach verschieben Sie den Bleistift l√§ngs des √Ñquators ein St√ºck nach Osten oder Westen und beachten dabei, dass der Bleistift tangential zum Globus bleiben muss. Dann verschieben Sie den Bleistift wieder l√§ngs eines Meridians zum Nordpol zur√ºck. Wenn er dort ankommt, wird er in eine andere Richtung als zu Beginn seines Weges zeigen.\n",
        "\n",
        "* Der Unterschied zwischen den Richtungen zu Beginn und am Ende des Weges ist ein Ma√ü f√ºr die Kr√ºmmung des Globus. Dass der Bleistift w√§hrend des gesamten Weges tangential zum Globus verschoben werden muss, dr√ºckt aus, dass er die Oberfl√§che des Globus w√§hrend der Verschiebung nicht verlassen darf.\n",
        "\n",
        "* Genau diese Operation, die kennzeichnet, wie sich ein Vektor ver√§ndert, wenn man ihn l√§ngs eines geschlossenen Weges durch die Raumzeit verschiebt, wird mathematisch durch den so genannten Kr√ºmmungstensor ausgedr√ºckt.\n",
        "\n",
        "* **Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar**.\n",
        "\n",
        "* Der Ricci-Skalar ordnet jedem Punkt der Raumzeit einen einzelnen Zahlenwert zu, der die lokale Kr√ºmmung der Raumzeit an diesem Punkt kennzeichnet.\n",
        "\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "8XJQVKeDBC-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmHevWQ8UMO"
      },
      "source": [
        "**Our goal: detect curvature in space, but spreading of geodesics can happen also in flat space and there is no curvature. That's why we use the second devriative**.\n",
        "\n",
        "> We choose the second derivative because it will tell us about the spreading due to the curvature of space and not the spreading that could happen in flat space where the geodesics are angled in specific directions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_316.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_317.png)\n",
        "\n",
        "**Ricci Tensor: Sectional Curvature**\n",
        "\n",
        "The terming the numerator (on top) gives us the sign we need to determine how the geodesics travel in direction v are either converging or diverging. And the noralization in the denominator (bottom) keeps the result the same regardless of the length of s and v.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_322.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Tensor Calculus 25 - Geometric Meaning Ricci Tensor/Scalar (Volume Form)](https://www.youtube.com/watch?v=oQZTYt_Pxcc&t=570s)\n",
        "\n",
        "**Ricci Tensor**: Track \"volume change\" along geodesics\n",
        "\n",
        "1. Sectional Curvature: Orthonormal basis\n",
        "\n",
        "2. Volume element derivative: any basis\n",
        "\n",
        "> Ricci tensor tells us how volumes change as we move around in space along geodesics\n",
        "\n",
        "Video: [Tensor Calculus 24: Ricci Tensor Geometric Meaning (Sectional Curvature)](https://www.youtube.com/watch?v=ZhDNijOEw0Y&t=332s)\n",
        "\n",
        "**Ricci Tensor: Track volume change along geodesics**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_310.png)\n",
        "\n",
        "**Ricci Tensor**\n",
        "\n",
        "$R_{\\mu \\nu}$ = Einstein Field equation\n",
        "\n",
        "* The change of the size of the circle, given by the Ricci tensor, represents how quickly these two people get drawn together\n",
        "\n",
        "* The more spacetime is curved, the more quickly bodies will get drawn together\n",
        "\n",
        "* In general relativity gravitational attraction is just the natural result of curved spacetime (it doesn't require any forces like we see with Newtonian)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_309.png)\n",
        "\n",
        "**The first term measures changing from curved space, meanwhile the second term measures changes coming from flat space:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_369.png)\n",
        "\n",
        "> **Ricci scalar**: Keeps track of how the size of a ball deviates\n",
        "from standard flat-space size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_372.png)"
      ],
      "metadata": {
        "id": "Vwm86DvIdeD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Geometric Algebra*"
      ],
      "metadata": {
        "id": "q09fihf25FPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Noncommutative Geometry*"
      ],
      "metadata": {
        "id": "oEn5oZesJVdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ocw.mit.edu/courses/18-238-geometry-and-quantum-field-theory-fall-2002/"
      ],
      "metadata": {
        "id": "82SPYo_nXk8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.mpim-bonn.mpg.de/node/111"
      ],
      "metadata": {
        "id": "c9gNGxhhXfN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Noncommutative_geometry\n",
        "\n",
        "Noncommutative Algebra: Weyl algebra, Clifford algebra, Superalgebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Noncommutative_ring"
      ],
      "metadata": {
        "id": "iUaQzHb9JY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences between commutative and noncommutative algebra**\n",
        "\n",
        "Because noncommutative rings of scientific interest are more complicated than commutative rings, their structure, properties and behavior are less well understood. A great deal of work has been done successfully generalizing some results from commutative rings to noncommutative rings. A major difference between rings which are and are not commutative is the necessity to separately consider [right ideals and left ideals](https://en.m.wikipedia.org/wiki/Ideal_(ring_theory)#Definitions_and_motivation). It is common for noncommutative ring theorists to enforce a condition on one of these types of ideals while not requiring it to hold for the opposite side. For commutative rings, the left‚Äìright distinction does not exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "pwIYqOy9JnTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Clifford Algebra*"
      ],
      "metadata": {
        "id": "HrtIDW9V0p96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Hierarchy of Clifford Algebra $C‚Ñì_{p,q}$](https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras)**\n",
        "\n",
        "*Clifford algebra over the reals is also called Geometric algebra*\n",
        "\n",
        "Algebra $\\rightarrow$ [Signature](https://en.m.wikipedia.org/wiki/Metric_signature) $\\rightarrow$ Equation\n",
        "\n",
        "${Cl}_{4,2} (\\mathbb {R})$ - [Conformal geometric algebra (CGA)](https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra) $\\rightarrow$ +,+,+,+,-,-, $\\rightarrow$ [Twistor](https://en.m.wikipedia.org/wiki/Twistor_space) $\\rightarrow$ [twistor theory](https://en.m.wikipedia.org/wiki/Twistor_theory)\n",
        "\n",
        "\n",
        "\n",
        "${Cl}_{1,3} (\\mathbb {R})$ - [Spacetime algebra\n",
        "](https://en.m.wikipedia.org/wiki/Spacetime_algebra) and ${Cl}_{1,3} (\\mathbb {C})$ - [Dirac algebra\n",
        "](https://en.m.wikipedia.org/wiki/Dirac_algebra) $\\rightarrow$ +,-,-,-, $\\rightarrow$ [Dirac equation](https://en.m.wikipedia.org/wiki/Dirac_equation) $\\rightarrow$ [relativistic spin-1/2](https://en.m.wikipedia.org/wiki/Relativistic_wave_equations#Spin_1.2F2) $\\rightarrow$ [Gamma matrices](https://en.m.wikipedia.org/wiki/Gamma_matrices)\n",
        "\n",
        "${Cl}_{3,0} (\\mathbb {R})$ - [Algebra of physical space (Pauli algebra)\n",
        "](https://en.m.wikipedia.org/wiki/Algebra_of_physical_space) $\\rightarrow$ +,+,+ $\\rightarrow$ [Pauli equation](https://en.m.wikipedia.org/wiki/Pauli_equation) $\\rightarrow$ [spin-1/2](https://en.m.wikipedia.org/wiki/Spin-1/2) $\\rightarrow$ [Pauli matrices](https://en.m.wikipedia.org/wiki/Pauli_matrices)\n",
        "\n",
        "${Cl}_{0,3} (\\mathbb {R})$ - [i.e. Quaternions](https://en.m.wikipedia.org/wiki/Clifford_algebra#Quaternions)\n",
        "\n",
        "\n",
        "${Cl}_{0,1} (\\mathbb {R})$ - [Clifford_algebra#Real_numbers](https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers) $\\rightarrow$ - $\\rightarrow$ [Schr√∂dinger equation](https://en.m.wikipedia.org/wiki/Schr%C3%B6dinger_equation) $\\rightarrow$ spin-0\n",
        "\n",
        "> *See also: For a complete classification of these algebras see [Classification of Clifford algebras](https://en.m.wikipedia.org/wiki/Classification_of_Clifford_algebras).*\n"
      ],
      "metadata": {
        "id": "77his5x9yBCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Clifford Algebra $Cl_{p,q} (\\mathbb{C})$ (Incl. Weyl Algebra)*\n",
        "\n",
        "Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)\n",
        "\n",
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "https://clifford.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_gates\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/representations-and-spinors/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Majorana_equation\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/pauli-and-dirac-matrices/\n"
      ],
      "metadata": {
        "id": "ginSvag_5QfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Properties**\n",
        "\n",
        "* In Geometric Algebra the **basis vectors for the space are typically real valued vectors**. Complex valued vectors have uses in GA (i.e. frequency domain representation of vectors in electrodynamics), but the underlying basis for the vector space is still real valued (i.e. span{ùêû1,ùêû2,ùêû3}.\n",
        "\n",
        "* Clifford algebras provide a further generalization, **allowing those basis vectors to reside in a complex vector space**, with suitable modifications of the vector product rules.\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra\n",
        "\n",
        "* Clifford Algebra focuses on abstract mathematical and algebraic properties\n",
        "\n",
        "* Geometric Algebra focuses on geometric and physical applications\n",
        "\n",
        "> A [Clifford algebra](https://en.m.wikipedia.org/wiki/Clifford_algebra) is an algebra generated by a vector space with a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form), and is a unital [associative algebra](https://en.m.wikipedia.org/wiki/Associative_algebra) (not commutative, but anti-commutative!).\n",
        "\n",
        "* The best known example of a quadratic form is the square of the amount of a vector: $|\\vec{v}|^{2}=x^{2}+y^{2}+z^{2}+\\ldots$ (like used in quantum mechanics on Bloch sphere)\n",
        "\n",
        "* As K-algebras, they generalize the real numbers, complex numbers, quaternions and several other hypercomplex number systems.\n",
        "\n",
        "> **The theory of Clifford algebras is intimately connected with the theory of [quadratic forms](https://en.m.wikipedia.org/wiki/Quadratic_form) and [orthogonal transformations](https://en.m.wikipedia.org/wiki/Orthogonal_group) (Orthogonal group, like SO(2), SO(3) and SO(4)).**\n",
        "\n",
        "* Clifford algebras have important applications in a variety of fields including geometry, theoretical physics and digital image processing.\n",
        "\n",
        "A Clifford algebra is a unital associative algebra that contains and is generated by a vector space $V$ over a field $K_{1}$ where $V$ is equipped with a quadratic form $Q: V \\rightarrow K$. The Clifford algebra $\\mathrm{Cl}(V, Q)$ is the \"freest\" algebra generated by $V$ subject to the condition\n",
        "\n",
        "> $v^{2}=Q(v) 1$ for all $v \\in V$\n",
        "\n",
        "where the product on the left is that of the algebra, and the 1 is its [multiplicative identity](https://en.m.wikipedia.org/wiki/Identity_element#Definitions) (neutral element).\n",
        "\n",
        "See also: https://en.m.wikipedia.org/wiki/Clifford_algebra#Universal_property_and_construction"
      ],
      "metadata": {
        "id": "f7RjlHat54zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extensions & Generalizations**\n",
        "\n",
        "More precisely, **Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra**, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "Weyl algebras and Clifford algebras admit a further structure of a [*-algebra](https://en.m.wikipedia.org/wiki/*-algebra), and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "Source: https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "[Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra), a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra) by a [symplectic form](https://en.m.wikipedia.org/wiki/Symplectic_vector_space)\n",
        "\n",
        "Clifford algebra, a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the exterior algebra by a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form).\n",
        "\n",
        "The Weyl algebra is also referred to as the symplectic Clifford algebra. Weyl algebras represent the same structure for symplectic bilinear forms that Clifford algebras represent for non-degenerate symmetric bilinear forms.\n",
        "\n",
        "*Clifford Algebra & Free Algebra*\n",
        "\n",
        "\n",
        "* for Clifford Algebra: The idea of being the \"freest\" or \"most general\" algebra subject to this identity can be formally expressed through the notion of a [universal property](https://en.m.wikipedia.org/wiki/Universal_property) (from category theory).\n",
        "\n",
        "* in the area of abstract algebra known as ring theory, a [free algebra](https://en.m.wikipedia.org/wiki/Free_algebra) is the noncommutative analogue of a polynomial ring since its elements may be described as \"polynomials\" with non-commuting variables. Likewise, the polynomial ring may be regarded as a free commutative algebra.\n",
        "\n",
        "*Clifford Algebra & Exterior Algebra*\n",
        "\n",
        "* Clifford Algebra as a quantization of the exterior algebra\n",
        "\n",
        "* Clifford algebras are closely related to exterior algebras. Indeed, if Q = 0 then the Clifford algebra Cl(V, Q) is just the exterior algebra ‚ãÄ(V). For nonzero Q there exists a canonical linear isomorphism between ‚ãÄ(V) and Cl(V, Q) whenever the ground field K does not have characteristic two.\n",
        "\n",
        "* **Clifford multiplication together with the distinguished subspace is strictly richer than the exterior product since it makes use of the extra information provided by Q.**\n",
        "\n",
        "* The Clifford algebra is a [filtered algebra](https://en.m.wikipedia.org/wiki/Filtered_algebra), the [associated graded algebra](https://en.m.wikipedia.org/wiki/Associated_graded_ring) is the exterior algebra.\n",
        "\n",
        "* More precisely, Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, **in the same way that the Weyl algebra is a quantization of the symmetric algebra.**\n",
        "\n",
        "* Weyl algebras and Clifford algebras admit a further structure of a *-algebra, and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Clifford_algebra#Relation_to_the_exterior_algebra\n",
        "\n",
        "*Clifford Algebra & Geometric Algebra*\n",
        "\n",
        "* Geometric Algebra is a special form of the more general Clifford algebra\n",
        "\n",
        "* Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.\n",
        "\n",
        "*Other notes*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "...the geometric algebra for this quadratic space is the Clifford algebra... [Source](https://en.m.wikipedia.org/wiki/Geometric_algebra)\n",
        "\n",
        "Clifford algebras were born of a synthesis of inner product spaces and Grassmann's exterior algebras, both of which have geometric applications.\n",
        "\n",
        "A Clifford algebra is constructed from an inner product space (ùëâ,ùëÑ) by generating an associative algebra (whose product is a descendant of the tensor product in the tensor algebra for ùëâ). These are compatible in a sense made clear in the Wiki.\n",
        "\n",
        "https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "P3d4_VAH6tFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*Applications*\n",
        "\n",
        "* One of the principal applications of the exterior algebra is in differential geometry where it is used to define the bundle of differential forms on a smooth manifold. In the case of a (pseudo-)Riemannian manifold, the tangent spaces come equipped with a natural quadratic form induced by the metric. Thus, one can define a Clifford bundle in analogy with the exterior bundle. This has a number of important applications in Riemannian geometry. Perhaps more importantly is the link to a spin manifold, its associated spinor bundle and spinc manifolds.\n",
        "\n",
        "* More: https://en.m.wikipedia.org/wiki/Clifford_algebra#Applications\n",
        "\n",
        "*Examples of Clifford Algebras $C‚Ñì_{p,q}$*\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras\n",
        "\n",
        "Different focusea created based on use cases / application\n",
        "\n",
        "Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "See also hierrchy of clifford algebra: https://en.m.wikipedia.org/wiki/Basil_Hiley#\n",
        "\n"
      ],
      "metadata": {
        "id": "FHJ0usg67LAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Geometric Algebra*"
      ],
      "metadata": {
        "id": "YWqVgPOwPCIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=60z_hpEAtD8&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=19)\n",
        "\n",
        "Video: [Addendum to A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=0bOiy0HVMqA&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)\n",
        "\n",
        "Video: [From Zero to Geo Introduction (Geometric Algebra Series)](https://www.youtube.com/watch?v=2hBWCCAiCzQ&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)"
      ],
      "metadata": {
        "id": "P5GPkVzEPEgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometric Algebra $Cl_{p,q} (\\mathbb{R})$ or $G(M^n)$ (Clifford algebra over reals)\n",
        "\n",
        "> In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does [Source](https://en.m.wikipedia.org/wiki/Vector_calculus)\n",
        "\n",
        "> Geometric Algebra is the Clifford Algebra over the field of real numbers.\n",
        "\n",
        "[**Geometric Product**](https://en.m.wikipedia.org/wiki/Geometric_algebra) *Sum of inner product (dot) and outer (not exterior??) product (wedge). Is the geometric product of any two vectors a and b as the sum of a symmetric product and an antisymmetric product:*\n",
        "\n",
        "> $a b=\\frac{1}{2}(a b+b a)+\\frac{1}{2}(a b-b a) =a \\cdot b+a \\wedge b$\n",
        "\n",
        "> $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n"
      ],
      "metadata": {
        "id": "cHOe2QmAPN5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Geometric Product: Sum of inner product (dot) and outer product (wedge)**: $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n",
        "* Magnitude of u $\\wedge$ v (=bivector) is the area of a parallelogram - similar to cross product, but not restricted to R3 like [cross product](https://en.m.wikipedia.org/wiki/Cross_product) (including magnitude and orientation)\n",
        "\n",
        "* Geometric algebra (GA) is an extension or completion of vector algebra (VA)\n",
        "\n",
        "* **Geometric Algebra: special form of the more general Clifford algebra**\n",
        "\n",
        "* the [geometric algebra](https://en.m.wikipedia.org/wiki/Geometric_algebra) (GA) of a vector space with a quadratic form (usually the Euclidean metric or the Lorentz metric) is an algebra over a field, the Clifford algebra of a vector space with a quadratic form with its multiplication operation called the geometric product.\n",
        "\n",
        "> **The algebra elements are called multivectors, which contains both the scalars $F$ and the vector space $V$.**\n",
        "\n",
        "* Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "* Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis and differential geometry, **e.g. by using the Clifford algebra instead of differential forms**.\n",
        "\n",
        "* Geometric algebra has been advocated as the **preferred mathematical framework for physics**. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity. GA has also found use as a computational tool in computer graphics and robotics.\n"
      ],
      "metadata": {
        "id": "tE1L0QYwPPsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "AuG47_giPSx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences to other algebras**\n",
        "\n",
        "* **Geometric Algebra vs Clifford algebra**: [Source](https://www.quora.com/How-are-geometric-algebra-and-Clifford-algebra-different)\n",
        "\n",
        "  * **Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.**\n",
        "\n",
        "  * A Clifford algebra is a unital associative algebra that contains and is generated by a vector space V over a field K, where V is equipped with a quadratic form Q.\n",
        "\n",
        "  * A Geometric algebra is a Clifford algebra of a vector space over the field of real numbers endowed with a quadratic form.\n",
        "\n",
        "  * **So a Geometric Algebra is a special form of the more general Clifford algebra**. In particular it is a CA over the reals. Additionally rather than being just an abstract object it has specific geometric meaning. For instance the 3+1 dimensional spacetime algebra is a Geometric algebra.\n",
        "\n",
        "* **Geometric Algebra vs Tensor Algebra**: Every element of a geometric algebra can be identified with a tensor, but not every tensor can be identified with an element of a geometric algebra [Source](\n",
        "https://math.stackexchange.com/questions/725350/is-geometric-algebra-isomorphic-to-tensor-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Exterior Algebra**: Exterior algebra defines an antisymmetric wedge product. In an exterior algebra, one can add k-forms to other k-forms, but would not add forms of different rank. This restriction is relaxed in geometric algebra (GA). [Source](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Vector Algebra**: [Source](\n",
        "https://en.m.wikipedia.org/wiki/Comparison_of_vector_algebra_and_geometric_algebra):\n",
        "\n",
        "  * Geometric algebra is an extension of vector algebra, providing additional algebraic structures on vector spaces, with geometric interpretations. Vector algebra uses all dimensions and signatures, as does geometric algebra, notably 3+1 spacetime as well as 2 dimensions.\n",
        "\n",
        "  * For example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. **The torque or curl is then a normal vector field in this 3rd dimension**.\n",
        "\n",
        "  * By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product."
      ],
      "metadata": {
        "id": "HkxCUeosPWmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: The cross product in relation to the exterior product. In red are the orthogonal unit vector, and the \"parallel\" unit bivector.*\n",
        "\n",
        "* $\\mathbf u \\times \\mathbf v$ is perpendicular to the plane containing $\\mathbf {u}$ and $\\mathbf {v}$\n",
        "\n",
        "* $\\mathbf u \\wedge \\mathbf v$ is an oriented representation of the same plane.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Exterior_calc_cross_product.svg/260px-Exterior_calc_cross_product.svg.png)"
      ],
      "metadata": {
        "id": "GLlRNs2BPZpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "9tCNN2HDPcAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "example: **Conformal geometric algebra $C‚Ñì_{4,2}$ - Twistor equation**"
      ],
      "metadata": {
        "id": "nt89GGCOPeSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Paravector"
      ],
      "metadata": {
        "id": "85mklZLQPgRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versor"
      ],
      "metadata": {
        "id": "GHAYUzVUPiWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C_{0,1} (\\mathbb{R})$ - Schr√∂dinger Equation*"
      ],
      "metadata": {
        "id": "4Q18zgBcxYJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras"
      ],
      "metadata": {
        "id": "hqHXXeogxwel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{3,0}(\\mathbb{R})$ - Pauli Algebra and Algebra of Physical Space*"
      ],
      "metadata": {
        "id": "ur1A1K3aOl3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebra of physical space $C‚Ñì_{3,0}$ - Pauli equation**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebra_of_physical_space\n",
        "\n",
        "used in special relativity, classical electrodynamics and relativistic quantum mechanics (Dirac equation in quantum field theory)\n",
        "\n",
        "Whereas the mathematicians do not give special attention to the case  ùëõ=2, the physicists, dealing with four-dimensional space-time, have every reason to do so, and it turns out to be most rewarding to develop procedures and proofs for the special case rather than refer to the general mathematical theorems. The technique for such a program has been developed some years ago [Source](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)"
      ],
      "metadata": {
        "id": "C_DZg7k4OpnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli Algebra $A_2$**\n",
        "\n",
        "See article [The Pauli Algebra - Mathematics LibreTexts](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)\n",
        "\n",
        "$\\mathcal{A}_{2}$ is called the Pauli algebra. The basis matrices are\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\sigma_{0}=I=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{1}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{2}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{3}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "The [Pauli matrices](https://de.m.wikipedia.org/wiki/Pauli-Matrizen) are the following four 2 √ó 2 matrices:\n",
        "\n",
        "> $\\sigma_{0}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right), \\sigma_{1}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right), \\sigma_{2}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right), \\sigma_{3}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)$\n",
        "\n",
        "\n",
        "* The three Pauli matrices satisfy the well known multiplication rules\n",
        "\n",
        "* All of the basis matrices are Hermitian, or self-adjoint\n",
        "\n",
        "**Pauli Matrices (Pauli Operators)**\n",
        "\n",
        "The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse), meaning that the square of a Pauli matrix is the identity matrix.\n",
        "\n",
        ">$\n",
        "I^{2}=X^{2}=Y^{2}=Z^{2}=-i X Y Z=I\n",
        "$\n",
        "\n",
        "The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example $Z X=i Y=-X Z$.\n",
        "\n",
        "*Anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. **Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments**. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation, such as addition.*\n",
        "\n",
        "Single spin one half particle, focus on spin degrees of freedom:\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad 1 \\downarrow=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$\n",
        "\n",
        "**X, Y and Z axis on Bloch sphere:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_077.PNG)\n",
        "\n",
        "Source: https://www.researchgate.net/figure/The-Bloch-sphere-representation-of-a-qubit-The-basis-states-are-located-at-the-north_fig2_284259345\n",
        "\n",
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Bloch_Sphere.svg/423px-Bloch_Sphere.svg.png)\n",
        "\n",
        "*Bloch sphere*"
      ],
      "metadata": {
        "id": "Vi6h1_4V3hsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{C})$ - Dirac Equation*"
      ],
      "metadata": {
        "id": "QBuL0NbxO6cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dirac equation for spin-¬Ω particles with a matrix representation of the **gamma matrices, which represent the generators of the algebra**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Dirac_algebra"
      ],
      "metadata": {
        "id": "js5NF8RU528q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{4,2}(\\mathbb{R})$ - Twistor (Conformal geometric algebra)*"
      ],
      "metadata": {
        "id": "Ym5Ax5nCOtuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra"
      ],
      "metadata": {
        "id": "g1zDxtzDOvrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{R})$ - Spacetime Algebra*"
      ],
      "metadata": {
        "id": "M5NxmpI85zgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Spacetime Algebra](https://www.youtube.com/watch?v=e7aIVSVc8cI&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=21)\n",
        "\n",
        "> Spacetime Algebra: Geometric Algebra + Special Relativity\n",
        "\n",
        "**Spacetime Algebra $C‚Ñì_{1,3}$ - Dirac equation**\n",
        "\n",
        "> Spacetime algebra concerns the Clifford algebra Cl1,3(R) of the four-dimensional [Minkowski spacetime](https://en.m.wikipedia.org/wiki/Minkowski_space)\n",
        "\n",
        "* In mathematical physics, [spacetime algebra (STA)](https://en.m.wikipedia.org/wiki/Spacetime_algebra) is a name for the Clifford algebra Cl1,3(R), or equivalently the geometric algebra G(M4).\n",
        "\n",
        "* According to David Hestenes, spacetime algebra can be particularly closely associated with the geometry of special relativity and relativistic spacetime.\n",
        "\n",
        "* It is a vector space that allows not only vectors, but also bivectors (directed quantities associated with particular planes, such as areas, or rotations) or blades (quantities associated with particular hyper-volumes) to be combined, as well as rotated, reflected, or [Lorentz boosted](https://en.m.wikipedia.org/wiki/Lorentz_transformation#boost).\n",
        "\n",
        "* It is also the natural parent algebra of spinors in special relativity."
      ],
      "metadata": {
        "id": "T8yu7WqUO26x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{0,0}, C‚Ñì_{0,1}, C‚Ñì_{0,2}(\\mathbb{R})$*"
      ],
      "metadata": {
        "id": "SN4YJmrE6HF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few low-dimensional cases are:\n",
        "\n",
        "* Cl0,0(R) is naturally **isomorphic to R** since there are no nonzero vectors.\n",
        "* Cl0,1(R) is a two-dimensional algebra generated by e1 that squares to ‚àí1, and is algebra-isomorphic to C, the field of **complex numbers**.\n",
        "* Cl0,2(R) is a four-dimensional algebra spanned by {1, e1, e2, e1e2}. The latter three elements all square to ‚àí1 and anticommute, and so the algebra is isomorphic to the **quaternions** H.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers\n"
      ],
      "metadata": {
        "id": "nywgP0rp6VF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Analysis**"
      ],
      "metadata": {
        "id": "b8EVGNJmEfDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Spaces*"
      ],
      "metadata": {
        "id": "gmN2IqZ4FDsw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkLBUH1hb36"
      },
      "source": [
        "###### *Spaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqI_Wvvlu5y7"
      },
      "source": [
        "**What constitutes a 'space'?**\n",
        "\n",
        "* **A space is a [set](https://en.m.wikipedia.org/wiki/Set_(mathematics)) - (sometimes called a [universe - Grundmenge](https://en.m.wikipedia.org/wiki/Universe_(mathematics))) with some added structure.**\n",
        "\n",
        "* A space consists of selected **mathematical objects that are treated as points**, and selected **relationships between these points** (nature of the points can vary widely: for example, the **points can be elements of a set, functions on another space, or subspaces of another space.**)\n",
        "\n",
        "\n",
        "* **[Taxonomy of Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces)**: While each type of space has its own definition, the general idea of \"space\" evades formalization (modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, but it does not define the notion of \"space\" itself)\n",
        "\n",
        "Quelle: [Einordnung in die Hierarchie mathematischer Strukturen](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Einordnung_in_die_Hierarchie_mathematischer_Strukturen) sowie [Topologische R√§ume](https://de.m.wikipedia.org/wiki/Topologischer_Raum#Beispiele)\n",
        "\n",
        "Topologischer Raum |  | dazugeh√∂rige Struktur\n",
        "--- | --- | ---\n",
        "[Euklidischer Raum](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) | hat | Skalarprodukt\n",
        "[Normierter Raum](https://de.m.wikipedia.org/wiki/Normierter_Raum) | hat | Norm\n",
        "[Metrischer Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) | hat | Metrik\n",
        "[Uniformer Raum](https://de.m.wikipedia.org/wiki/Uniformer_Raum) | hat | Uniforme Struktur\n",
        "[Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum) | hat | Topologie\n",
        "\n",
        "![xxx](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Beziehungen_zwischen_mathematischen_R√§umen.svg/220px-Beziehungen_zwischen_mathematischen_R√§umen.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ostHcxEvNZ5r"
      },
      "source": [
        "![Normed Vector Space](https://upload.wikimedia.org/wikipedia/commons/7/74/Mathematical_Spaces.png)\n",
        "\n",
        "Quelle: [Mathematical Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYW5u_0Oi7l5"
      },
      "source": [
        "###### *Inner Product Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3TsP_aDsFR"
      },
      "source": [
        "**Inner Product**\n",
        "\n",
        "> * **Inner product $\\rightarrow$ measures distances, lengths, angles**\n",
        "\n",
        "* Das innere Produkt (Skalarprodukt / Produktpunkt called when applied to functions - the alternate name of inner product in linear algebra is 'dot product') stellt quasi eine Geometrie im Vektorraum her, wir k√∂nnen dadurch definieren, welche Vektoren orthogonal, und welche parallel zueinander sind.\n",
        "\n",
        "* **Das Skalarprodukt ben√∂tigt man**,\n",
        "  * um die Lange von Vektoren zu berechnen,\n",
        "  * den Winkel zwischen Vektoren zu berechnen (uber Cosinus von Alpha) und\n",
        "  * ob zwei Vektoren senkrecht zueinander stehen.\n",
        "\n",
        "* Das **Dot Product / Scalar Product / [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt)** (auch inneres Produkt oder Punktprodukt) ist eine [mathematische Verkn√ºpfung](https://de.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik)), die zwei Vektoren eine Zahl (Skalar) zuordnet.\n",
        "\n",
        "* **Scalar vs Scalar Product**: A [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics)) is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector. The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically, the determinant can be viewed as the volume scaling factor of the linear transformation described by the matrix.\n",
        "\n",
        "* Geometrisch berechnet man das Skalarprodukt zweier Vektoren $\\vec{a}$ und $\\vec{b}$ nach der Formel:\n",
        "\n",
        "> $\n",
        "\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})\n",
        "$\n",
        "\n",
        "* Null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben.\n",
        "\n",
        "* **A common special case of the inner product is the scalar product or dot product, is written with a centered dot a ‚ãÖ b.**\n",
        "\n",
        "* In [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) the inner product is the dot product, also known as the scalar product. (They generalize Euclidean spaces to vector spaces of any (possibly infinite) dimension.)\n",
        "\n",
        "* Ist das Skalarprodukt von zwei Vektoren $\n",
        "\\vec{a} \\cdot \\vec{b}= 0$, dann folgt daraus, dass diese orthogonal zueinander stehen.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * A simple example is the real numbers $\\mathbb{R}$ with the standard multiplication as the inner product $\\langle x, y\\rangle:=x y$\n",
        "\n",
        "  * **Inner Product of Functions** says how similar two functions are (how much they align with each other). **Inner product of function are used a lot in Fourier Transform**\n",
        "\n",
        "    * i.e. if they are orthogonal, then zero. if they are very similar, then they have a large inner product\n",
        "\n",
        "    > $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "    * You can also take samples from both functions and calculate the inner product between both. Up to infinity, you get at the integral like written above (Riemann approximation of the continuuos integral above):\n",
        "\n",
        "    > $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcOpkYQjFKx"
      },
      "source": [
        "**Inner Product Space**\n",
        "\n",
        "* Der **[Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space)** (Pr√§hilbertraum bzw. Skalarprodukt) ist ein **Vektorraum**, auf dem ein **inneres Produkt definiert ist**.\n",
        "\n",
        "* An [inner product space](https://en.m.wikipedia.org/wiki/Inner_product_space) is a normed space, **where the norm of a vector is the square root of the inner product of the vector by itself**: $\\sqrt{\\vec{x} \\cdot \\vec{x}} = \\sqrt{{x}^{2}}$\n",
        "\n",
        "* [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) generalize **Euclidean spaces (in which the inner product is the dot product, also known as the scalar product**) to vector spaces of any (possibly infinite) dimension.\n",
        "\n",
        "An **inner product space** is a vector space $V$ over the field $\\mathbb{F}$ together with a map\n",
        "\n",
        "$\n",
        "\\langle\\cdot, \\cdot\\rangle: V \\times V \\rightarrow \\mathbb{F}\n",
        "$\n",
        "\n",
        "called an inner product that satisfies the following conditions $(1),(2),$ and $(3)$ for all vectors $x, y, z \\in V$ and all scalars $a \\in \\mathbb{F}:$ [see here](https://en.m.wikipedia.org/wiki/Inner_product_space)\n",
        "\n",
        "* In linear algebra, **an inner product space is a vector space with an additional structure called an inner product**. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Geometric interpretation of the angle between two vectors defined using an inner product.\n",
        "\n",
        "> **Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors**. They also provide the means of defining orthogonality between vectors (**zero inner product**).\n",
        "\n",
        "> An inner product **naturally induces an associated norm**, (|x| and |y| are the norms of x and y, in the picture) thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Normed Vector Space*"
      ],
      "metadata": {
        "id": "0JfDpLK4KBqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "\n",
        "classical probability theory: 1-Norm: ‚àë pi = ‚àë | p | = || p || 1 = 1\n",
        "\n",
        "quantum probabiliyt theory: 2-Norm: || | œà > || 2 = 1\n",
        "\n",
        "**Reeller Vektor**\n",
        "\n",
        "Die 1-, 2-, 3- und $\\infty$-Normen des reellen Vektors $x=(3,-2,6)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3|^3+|-2|^3+|6|^3}=\\sqrt[3]{251} \\approx 6,308 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Komplexer Vektor**\n",
        "\n",
        "Die 1-, 2-, 3-und $\\infty$-Normen des komplexen Vektors $x=(3-4 i,-2 i)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3-4 i|^3+|-2 i|^3}=\\sqrt[3]{5^3+2^3}=\\sqrt[3]{133} \\approx 5,104 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "1Ui-wrEg177C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhAD7kvd3uuL"
      },
      "source": [
        "> **Eine Norm gibt die <u>Gr√∂√üe / L√§nge (Betrag) eines Elements</u> in einem Vektorraum an**\n",
        "\n",
        "* Eine Metrik gibt hingegen die Distanz zwischen <u>zwei Vektoren (Punkten)</u> an. A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ (but Instead of distance between points, a norm gives us the length of a vector, as measured from the origin)\n",
        "\n",
        "* Eine **[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) ist eine Abbildung (Funktion)** $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$, welche einem Element von einem reellen oder komplexen Vektorraum eine **nicht-negative reelle Zahl** $\\mathbb{R}^{\\geq \\ 0 }$ zuordnet und folgende Eigenschaften besitzt (f√ºr alle $x, y$ aus dem $\\mathbb{K}$ Vektorraum und alle $\\lambda$ aus $\\mathbb{K}$):\n",
        "\n",
        "1. **[Definitheit](https://de.m.wikipedia.org/wiki/Definitheit)**:\n",
        "  * It is **nonnegative**, that is for every vector x, one has ‚Äñx‚Äñ ‚â• 0.\n",
        "  * It is **positive on nonzero vectors**, that is, ‚Äñx‚Äñ = 0 ‚ü∫ x = 0.\n",
        "\n",
        "2. **[Absolute Homogenit√§t](https://de.m.wikipedia.org/wiki/Homogene_Funktion)**: For every vector x, and every **scalar Œ±**, one has ‚Äñ Œ± x ‚Äñ = | Œ± | ‚Äñ x ‚Äñ.\n",
        "\n",
        "3. **[Subadditivit√§t, Dreiecksungleichung](https://de.m.wikipedia.org/wiki/Additive_Funktion#Sub-_und_Superadditivit√§t)**: for every vectors x and y, one has ‚Äñ x+y ‚Äñ ‚â§ ‚Äñ x ‚Äñ + ‚Äñ y ‚Äñ.\n",
        "\n",
        "* From Inner Products to Norms: Eine Norm kann (muss aber nicht) von einem [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (eine sogenannte ['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall is **the norm of a vector the square root of the inner product of the vector by itself**. A complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "\n",
        "* **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. If such a space is complete, we call it a [Banach space](https://de.m.wikipedia.org/wiki/Banachraum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygj95-xb7Eu"
      },
      "source": [
        "[**1. Normen auf endlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "\n",
        "* [**Zahlnorm**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): ein Beispiel ist die [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion).\n",
        "\n",
        "  * ist **induziert vom Standardskalarprodukt** (erf√ºllt die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t) zweier reeller bzw. komplexen Zahlen. Die Betragsnorm ist. der Betrag einer reellen Zahl $z \\in \\mathbb{R}$:\n",
        "  > $\n",
        "\\|z\\|=|z|=\\sqrt{z^{2}}=\\left\\{\\begin{array}{cl}\n",
        "z & \\text { f√ºr } z \\geq 0 \\\\\n",
        "-z & \\text { f√ºr } z < 0\n",
        "\\end {array}\\right.$\n",
        "\n",
        "* [**Matrixnorm**](https://de.m.wikipedia.org/wiki/Matrixnorm): Siehe [Norm -> Matrixnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Matrixnormen) sowie [https://de.m.wikipedia.org/wiki/Matrixnorm](Matrixnorm)\n",
        "  * [Nat√ºrliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): Eine nat√ºrliche Matrixnorm entspricht anschaulich dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor entsteht\n",
        "  * [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm):  Anschaulich entspricht die Spektralnorm damit dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor der L√§nge Eins entsteht = Die Spektralnorm einer Matrix entspricht ihrem maximalen Singul√§rwert, also der Wurzel des gr√∂√üten Eigenwerts des Produkts der adjungierten (transponierten) Matrix mit dieser Matrix. Spektralnorm die von der euklidischen Norm abgeleitete nat√ºrliche Matrixnorm. Von: [Singul√§rwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "\n",
        "* [**Vektornormen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen): ein Beispiel sind die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm)\n",
        "\n",
        "  * Die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) sind eine Klasse von [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen), die f√ºr reelle Zahlen $p \\geq 1$ definiert sind.\n",
        "\n",
        "  * Die $p$ -Norm eines Vektors $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{K}^{n}$ mit $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{C}$ ist f√ºr reelles\n",
        "$1 \\leq p<\\infty$ definiert durch:\n",
        "\n",
        "  > **$\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$**\n",
        "\n",
        "  * p-Normen **erf√ºllen die Minkowski-Ungleichung sowie die H√∂lder-Ungleichung**. For all p ‚â• 1, the p-norms erf√ºllen die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t.\n",
        "\n",
        "  * Die wichtigsten Vektornormen sind die [**Summennorm**](https://de.m.wikipedia.org/wiki/Summennorm) , Euklidische Norm und [**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**2. Normen auf unendlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorr√§umen)\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**[Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum)**\n",
        "\n",
        "* **The sequence space (Folgenraum) is a special case of the function space (Funktionenraum): $\\ell_{\\infty}=L_{\\infty}(\\mathbb{N})$ where the natural numbers are equipped with the counting measure.**\n",
        "\n",
        "* Die $\\ell^{p}$ -Normen sind die Verallgemeinerung der $p$ -Normen auf Folgenr√§ume, wobei lediglich die endliche Summe durch eine unendliche ersetzt wird. Die $\\ell^{p}$ -Norm einer in $p$ -ter Potenz betragsweise summierbaren Folge ist f√ºr reelles $1 \\leq p<\\infty$ dann definiert als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Versehen mit diesen Normen werden die $\\ell$ - R√§ume jeweils zu vollst√§ndigen normierten R√§umen. ${ }^{[6]}$ F√ºr den Grenzwert $p \\rightarrow \\infty$ ergibt sich der Raum der beschr√§nkten Folgen $\\ell^{\\infty}$ mit der Supremumsnorm.\n",
        "\n",
        "**[Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum**\n",
        "\n",
        "* Siehe auch L-p-Raum: https://de.m.wikipedia.org/wiki/Lp-Raum\n",
        "\n",
        "* Die $\\mathcal{L}^{p}$ -Normen einer in $p$ -ter Potenz **Lebesgue-integrierbaren Funktion** mit $1<p<\\infty$ sind in Analogie zu den $\\ell^{p}$ -Normen definiert als\n",
        "\n",
        "> $\n",
        "\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}\n",
        "$\n",
        "\n",
        "* **wobei die Summe durch ein Integral ersetzt wurde**. Ebenso wie bei der wesentlichen Supremumsnorm sind diese Narmen zun√§chst nur Halbnormen, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Ma√ü Null II von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man wieder die Menge der √Ñauivalenzklassen unnn Funktionen $[f] \\in L^{p}(\\Omega)$, die fast √ºberall gleich sind, und definiert auf diesen $L^{p}$ -R√§umen die $L^{p}$ -Normen durch\n",
        "$\\|[f]\\|_{L P(\\Omega)}=\\|f\\|_{\\mathcal{L}^{p}(\\Omega)}$\n"
      ],
      "metadata": {
        "id": "EC3JVQRj2H01"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27wE3ZTM_154"
      },
      "source": [
        "**Beispiele fur Normed Vector Spaces**\n",
        "\n",
        "* **A [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) or normed space <u>is a vector space</u> over the real or complex numbers, on which a norm is defined**.\n",
        "\n",
        "  * Ist $V$ ein [Vektorraum](https://de.m.wikipedia.org/wiki/Vektorraum) √ºber dem K√∂rper $\\mathbb{K}$ der reellen oder der komplexen Zahlen und $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$ eine Norm auf $V,$ dann nennt man das Paar $(V,\\|\\cdot\\|)$ einen **normierten Vektorraum**.\n",
        "\n",
        "  * **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. (If such a space is complete, we call it a Banach space.)\n",
        "\n",
        "  * **Be careful**: A [vector space](https://en.m.wikipedia.org/wiki/Vector_space) is an [algebraic structure](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures), meanwhile a [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) is a type of [abstract (topological) space](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces) (a normed vector space is a vector space over the real or complex numbers, on which a norm is defined). (See also [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space)\n",
        "\n",
        "* **Banach Space**: **$\\mathbb{R}$<sup>n</sup> together with the p-norm is a [Banach space](https://de.wikipedia.org/wiki/Banachraum) = ein vollst√§ndiger normierter Vektorraum**. This Banach space is the Lp-space over Rn. Viele **Folgenr√§ume $\\ell$** oder **Funktionenr√§ume $L$** sind unendlichdimensionale Banachr√§ume. Function Spaces $L$ are a type of infinite vector space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces**, whose vectors are functions.\n",
        "\n",
        "* **Hilbert Space**: Ein Banachraum, dessen Norm durch ein Skalarprodukt induziert ist, hei√üt **[Hilbertraum](https://de.wikipedia.org/wiki/Hilbertraum)**. (z.B. p2-Norm Euklidische Norm, aber nicht p1-Summennorm). L√§sst man die Bedingung der Vollst√§ndigkeit fallen, spricht man von einem Pr√§hilbertraum).\n",
        "  * Die Struktur eines Hilbertraums ist eindeutig festgelegt durch seine Hilbertraumdimension. Diese kann eine beliebige Kardinalzahl sein. Ist die Dimension endlich und betrachtet man als K√∂rper die reellen Zahlen, so handelt es sich um einen euklidischen Raum.\n",
        "  * Hilbertr√§ume tragen durch ihr Skalarprodukt eine topologische Struktur. Dadurch sind hier im Gegensatz zu allgemeinen Vektorr√§umen Grenzwertprozesse m√∂glich.\n",
        "\n",
        "* **Hardy-Space**: Untersucht man statt der messbaren Funktionen nur die holomorphen beziehungsweise die harmonischen Funktionen auf Integrierbarkeit, so werden die entsprechenden $L^{p}$-R√§ume [Hardy-R√§ume](https://de.m.wikipedia.org/wiki/Hardy-Raum) genannt, [L_p Space-Hardy](https://de.m.wikipedia.org/wiki/Lp-Raum#Hardy-R%C3%A4ume)\n",
        "\n",
        "* **F-Space**: The **space Lp for 0 < p < 1 is an [F-space](https://en.m.wikipedia.org/wiki/F-space)**: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ‚â• 1. Some authors use the term [Fr√©chet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space) rather than F-space, but usually the term \"Fr√©chet space\" is reserved for locally convex F-spaces.\n",
        "\n",
        "* **Sobolev-Raum**: Ein [Sobolev-Raum](https://de.wikipedia.org/wiki/Sobolev-Raum), ist ein Funktionenraum von schwach differenzierbaren Funktionen, der zugleich ein Banachraum ist. Der Sobolev-Raum ist der Raum derjenigen reellwertigen Funktionen $u \\in L^{p}(\\Omega),$ deren gemischte partielle schwache Ableitungen bis zur Ordnung $k$ im Lebesgue-Raum $L^{p}(\\Omega)$ liegen.\n",
        "\n",
        "  * Aus der Variationsrechnung: Diese minimiert Funktionale √ºber Funktionen - Sobolev-R√§ume bilden dabei die Grundlage der L√∂sungstheorie partieller Differentialgleichungen.\n",
        "\n",
        "  * A Sobolev space is a vector space of functions equipped with a norm that is a **combination of Lp-norms of the function together with its derivatives up to a given order**.\n",
        "\n",
        "  * The derivatives are understood in a suitable weak sense to make the space complete, i.e. a Banach space.\n",
        "\n",
        "  * **A Sobolev space is a space of functions**\n",
        "\n",
        "    * **possessing sufficiently many derivatives** for some application domain, such as partial differential equations,\n",
        "\n",
        "    * and **equipped with a norm** that measures both the size and regularity of a function.\n",
        "\n",
        "    * Their importance comes from the fact that **weak solutions of some important partial differential equations exist in appropriate Sobolev spaces**, even when there are no strong solutions in spaces of continuous functions with the derivatives understood in the classical sense.\n",
        "\n",
        "  * Partielle Differentialgleichungen betrachtet man meistens auf Sobolew-R√§umen. In diesen R√§umen werden Funktionen, die bis auf Nullmengen √ºbereinstimmen, als gleich angesehen. Da der Rand eines Gebietes √ºblicherweise eine Nullmenge ist, ist der Begriff der Randbedingung problematisch. L√∂sungen f√ºr dieses Problem sind sobolewsche Einbettungss√§tze oder ‚Äì allgemeiner ‚Äì [**Spuroperatoren**](https://de.wikipedia.org/wiki/Sobolev-Raum#Spuroperator).\n",
        "\n",
        "* **$L^p$ (Lebesgue) Space** are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. **They are sometimes called Lebesgue spaces**. Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * [L<sup>p</sup>-Raum](https://de.m.wikipedia.org/wiki/Lp-Raum) sind spezielle R√§ume, die aus allen p-fach integrierbaren Funktionen bestehen. Das $p$ in der Bezeichnung ist ein reeller Parameter: F√ºr jede Zahl $0 < p \\leq \\infty$ ist ein $L^{p}$ -Raum definiert. Die Konvergenz in diesen R√§umen wird als Konvergenz im $p$ -ten Mittel bezeichnet.\n",
        "\n",
        "  * diese R√§ume werden √ºber das Lebesgue-Integral definiert\n",
        "\n",
        "  * Im Fall Banachraum-wertiger Funktionen bezeichnet man sie auch als Bochner-Lebesgue-R√§ume.\n",
        "\n",
        "  * [**Konvergenz_im_p-ten_Mittel**](https://de.m.wikipedia.org/wiki/Konvergenz_im_p-ten_Mittel)\n",
        "\n",
        "  * ${\\mathcal {L}}^{p}$ mit Halbnorm und ${\\mathcal {L}}^{p}$ mit Norm\n",
        "\n",
        "  * Hilbertraum ${\\mathcal {L}}^{2}$\n",
        "\n",
        "  * Der **normierte Vektorraum** $L^{p}$ ist [vollst√§ndig](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum) und damit ein [Banachraum](https://de.m.wikipedia.org/wiki/Banachraum), die Norm $\\|\\cdot\\|_{L} p$ wird **$L^{p}$ Norm** genannt.\n",
        "\n",
        "  * Auch wenn man von sogenannten $L^{p}$ -Funktionen spricht, handelt es sich dabei um die gesamte √Ñquivalenzklasse einer klassischen Funktion. Allerdings liegen im Falle des Lebesgue-Ma√ües auf dem $\\mathbb{R}^{n}$ zwei verschiedene stetige Funktionen nie in der gleichen √Ñquivalenzklasse, so dass der $L^{p}$-Begriff eine nat√ºrliche Erweiterung des Begriffs stetiger Funktionen darstellt.\n",
        "\n",
        "  * The [**Lp spaces**](https://de.m.wikipedia.org/wiki/Lp-Raum) are [function spaces](https://en.m.wikipedia.org/wiki/Function_space) defined using a natural **generalization of the p-norm for finite-dimensional vector spaces**. They are sometimes called **Lebesgue spaces**.\n",
        "\n",
        "  * A normed vector space is automatically a metric space, by defining the metric in terms of the norm in the natural way. But a metric space may have no algebraic (vector) structure ‚Äî i.e., it may not be a vector space ‚Äî so the concept of a **metric space is a generalization of the concept of a normed vector space**.\n",
        "\n",
        "  * Lp spaces form an important class of [Banach spaces](https://en.m.wikipedia.org/wiki/Banach_space) in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as [solutions to variational problems](https://en.m.wikipedia.org/wiki/Central_tendency#Solutions_to_variational_problems)\n",
        "\n",
        "  * An Lp space may be defined as a space of measurable functions for which the p-th power of the absolute value is Lebesgue integrable, where functions which agree almost everywhere are identified.\n",
        "\n",
        "  * More generally, let 1 ‚â§ p < ‚àû and (S, Œ£, Œº) be a [measure space](https://en.m.wikipedia.org/wiki/Measure_space). Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, that\n",
        "\n",
        "  * $\\|f\\|_{p} \\equiv\\left(\\int_{S}|f|^{p} \\mathrm{d} \\mu\\right)^{1 / p}<\\infty$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SpYIJ602L38"
      },
      "source": [
        "###### *Metric Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gcLfqnPX33L"
      },
      "source": [
        "> **Metric $\\rightarrow$ measures distances**\n",
        "\n",
        "* *Difference Metric to Norm: Instead of distance between points, a norm gives us the length of a vector, as measured from the origin*\n",
        "\n",
        "* **Eine Metrik definiert Abst√§nde zwischen Elementen des Vektorraumes.**\n",
        "\n",
        "* Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n",
        "\n",
        "* Eine Metrik (auch Abstandsfunktion) ist eine Funktion, die je zwei Elementen des Raums einen nicht negativen reellen Wert zuordnet, der als Abstand der beiden Elemente voneinander aufgefasst werden kann.\n",
        "\n",
        "* Sei M eine Menge. **<u>Eine Metrik ist eine Abbildung</u>** d: $M \\times M \\rightarrow \\mathbb{R}$ auf $M \\times M$ wenn folgende drei Axiome erf√ºllt sind:\n",
        "\n",
        "1. Beide zusammen bilde Positive Definitheit (**positive definiteness**):\n",
        "  * $d(x, y) \\geq 0$ (**non-negativity**) sowie\n",
        "  * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) f√ºr alle $x, y \\in M$.\n",
        "\n",
        "2. $d(x, y)=d(y, x)$ (**symmetry**) Symmetrie\n",
        "$d(x, y)=d(y, x) \\forall x, y \\in M$\n",
        "\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "\n",
        "**Metriken geben einem Raum eine globale und eine lokale mathematische Struktur**:\n",
        "  * Die globale Struktur kommt in **geometrischen Eigenschaften wie der Kongruenz** von Figuren zum Ausdruck.\n",
        "  * Die lokale metrische Struktur, also die Definition kleiner Abst√§nde, erm√∂glicht unter bestimmten zus√§tzlichen Voraussetzungen die Einf√ºhrung von **Differentialoperationen**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdrlGVRmlmx"
      },
      "source": [
        "**Aus Normen erzeugte Metriken**\n",
        "\n",
        "* **A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ.**\n",
        "\n",
        "* Jede Norm auf einem Vektorraum induziert durch die Festlegung $d(x, y) \\equiv\\|x-y\\|$ eine Metrik. Somit ist jeder normierte Vektorraum (und erst recht jeder Innenproduktraum, Banachraum oder Hilbertraum) ein metrischer Raum.\n",
        "\n",
        "* **Aber Achtung**: nicht jede Metrik ist durch eine Norm induziert! Jede Norm induziert eine Metrik, aber nicht umgekehrt.\n",
        "\n",
        "* **Eine Metrik, die aus einer $p$ -Norm abgeleitet ist, hei√üt auch [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances)**. It is a metric in a normed vector space. p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold (which is possible, but then it's not a metric anymore). Wichtige Spezialf√§lle sind:\n",
        "  * [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "  * [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$\n",
        "  * [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "\n",
        "* Als eine [**Fr√©chet-Metrik**](https://de.m.wikipedia.org/wiki/Fr√©chet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. **Sie stellt eine Verbindung zwischen Metrik und Norm her.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUnISc3anLSR"
      },
      "source": [
        "**Nicht aus Normen erzeugte Metriken**\n",
        "\n",
        "* Auf jeder Menge l√§sst sich eine triviale Metrik, die sogenannte gleichm√§√üig diskrete Metrik (die sogar eine Ultrametrik ist) definieren: $d(x, y)=\\left\\{\\begin{array}{ll}0 & \\text { f√ºr } x=y \\\\ 1 & \\text { f√ºr } x \\neq y\\end{array}\\right.$\n",
        "\n",
        "* Im Allgemeinen **nicht durch eine Norm induziert ist die riemannsche Metrik**, die aus einer differenzierbaren Mannigfaltigkeit eine [riemannsche Mannigfaltigkeit](https://en.m.wikipedia.org/wiki/Riemannian_manifold) macht. (zB  Die k√ºrzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geod√§ten) sind nicht zwingend Geradenst√ºcke, sondern k√∂nnen gekr√ºmmte Kurven sein. **Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch gr√∂√üer (z. B. Kugel) oder kleiner (hyperbolische R√§ume) als 180¬∞ sein**.\n",
        "\n",
        "* Die [franz√∂sische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Franz√∂sische_Eisenbahnmetrik).\n",
        "\n",
        "* Die [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man k√∂nnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zur√ºck.\n",
        "\n",
        "* Der [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) ist eine Metrik auf dem Coderaum, die die Unterschiedlichkeit von (gleich langen) Zeichenketten angibt. Die [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) kann als Erweiterung des Hamming-Abstands angesehen werden. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro‚ÄìWinkler distance](https://en.m.wikipedia.org/wiki/Jaro‚ÄìWinkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "\n",
        "* Mehr Beispiele von nicht aus Normen erzeugten Metriken [hier](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-8s7XOx_Mxn"
      },
      "source": [
        "**Metric Space**\n",
        "\n",
        "* Unter einem [metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space) versteht man in der Mathematik eine Menge, auf der eine Metrik definiert ist.\n",
        "\n",
        "  * **Das Paar $(M, d)$ nennt man einen <u>[metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space)</u>**.\n",
        "\n",
        "  * Beispiel fur einen metrischen Raum: Die Menge der reellen Zahlen $\\mathbb{R}$ mit der Abstandsmetrik $d(x, y):=|x-y|$ bilden einen metrischen Raum.\n",
        "\n",
        "* Jeder metrische Raum ist ein [Hausdorff-Raum](https://de.m.wikipedia.org/wiki/Hausdorff-Raum).\n",
        "\n",
        "\n",
        "* **[Isometrie](https://de.m.wikipedia.org/wiki/Isometrie)**\n",
        "\n",
        "  * Isomorphismen zwischen metrischen R√§umen hei√üen Isometrien. Ein metrischer Raum hei√üt vollst√§ndig, falls alle Cauchy-Folgen konvergieren. Eine Isometrie ist eine Abbildung, die zwei metrische R√§ume aufeinander abbildet und dabei die Metrik ‚Äì also die Abst√§nde zwischen je zwei Punkten ‚Äì erh√§lt.\n",
        "\n",
        "  * Sind zwei metrische R√§ume $\\left(M_{1}, d_{1}\\right),\\left(M_{2}, d_{2}\\right)$ gegeben, und ist $f: M_{1} \\rightarrow M_{2}$ eine Abbildung mit der Eigenschaft\n",
        "\n",
        "  * $d_{2}(f(x), f(y))=d_{1}(x, y)$ f√ºr alle $x, y \\in M_{1}$\n",
        "\n",
        "  * dann hei√üt $f$ Isometrie von $M_{1}$ nach $M_{2}$. Eine solche Abbildung ist stets injektiv.\n",
        "\n",
        "  * Ist $f$ sogar bijektiv, dann hei√üt $f$ **isometrischer Isomorphismus**, und die R√§ume $M_{1}$ und $M_{2}$ hei√üen is isometrische Einbettung von $M_{1}$ in $M_{2}$. [Isometrische Isomorphie](https://de.m.wikipedia.org/wiki/Isometrische_Isomorphie) beschreibt in der Funktionalanalysis einen Zusammenhang zwischen zwei unterschiedlichen R√§umen, die geometrisch identisch sind.\n",
        "\n",
        "* [**Vollst√§ndiger Raum**](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum)\n",
        "\n",
        "  * Ein vollst√§ndiger Raum ist in der Analysis ein metrischer Raum, in dem jede Cauchy-Folge von Elementen des Raums auf eine Zahl (element) innerhalb desselben raumes konvergiert.\n",
        "\n",
        "  * ZB alle rationalen Zahlen sollten nach einsetzung in die cauchy-folge mit einer bestimmten metrik (zB betragsmetrik) auch wieder als ziel in rationalen zahlen muenden. tun sie das nicht, ist es ein unvollst√§ndiger Raum.\n",
        "\n",
        "  * Andererseits ist der Raum der rationalen Zahlen mit der Betragsmetrik nicht vollst√§ndig, weil etwa die Zahl 2‚Äì‚àö nicht rational ist, es jedoch Cauchy-Folgen rationaler Zahlen gibt, die bei Einbettung der rationalen Zahlen in die reellen Zahlen gegen 2‚Äì‚àö und somit gegen keine rationale Zahl konvergieren. Es ist aber stets m√∂glich, die L√∂cher auszuf√ºllen, also einen unvollst√§ndigen metrischen Raum zu vervollst√§ndigen. Im Fall der rationalen Zahlen erh√§lt man dadurch den Raum der reellen Zahlen.\n",
        "\n",
        "  * Die Menge Q der rationalen Zahlen ist mit der Betragsmetrik\n",
        "\n",
        "  * $\n",
        "d(x, y)=|x-y|\n",
        "$\n",
        "\n",
        "  * nicht vollst√§ndig, denn die Folge rationaler Zahlen $x_{1}=1, x_{n+1}=\\frac{x_{n}}{2}+\\frac{1}{x_{n}}$ ist eine Cauchy-Folge, deren Grenzwert (siehe Heron-Verfahren) die irrationale Zahl $\\sqrt{2}$ ist, die nicht in $\\mathbb{Q}$ liegt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exkurs: Divergence, Distance and Metric*"
      ],
      "metadata": {
        "id": "SZkKviQcpLi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jensen-Shannon divergence is a symmetrization of the Kullback-Leibler divergence"
      ],
      "metadata": {
        "id": "GT7Rt3vQAB-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ‚â• 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ‚â§ d(x, y) + d(y, z)     (subadditivity / triangle inequality).\n",
        "\n",
        "**Begriffsabgrenzungen**\n",
        "\n",
        "* **Divergence** fullfills property of positive definiteness (1 + 2)\n",
        "\n",
        "* **Distance** fullfills property of positive definiteness and symmetrie (1 + 2+ 3)\n",
        "\n",
        "* **Metric** fullfills property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3 + 4). H√§ufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion) bezeichnet. Metric Space: Together with the set, a metric makes up a metric space."
      ],
      "metadata": {
        "id": "EmfRw-AgpSZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergences in Machine Learning**\n",
        "\n",
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold.\n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback‚ÄìLeibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ],
      "metadata": {
        "id": "oryOJsv-pW2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Meaning of 'no symmetry' in divergences**\n",
        "\n",
        "*\n",
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)\n",
        "\n",
        "* The Kullback-Leibler divergence is not symmetric. Roughly speaking, it's because you should think of the two arguments of the KL divergence as different kinds of things: the first argument is empirical data, and the second argument is a model you're comparing the data to.\n",
        "\n",
        "* Take a bunch of independent random variables $X_{1}, \\ldots, X_{n}$ whose possible values lie in a finite set.\n",
        "\n",
        "* Say these variables are identically distributed, with $\\operatorname{Pr}\\left(X_{i}=x\\right)=p_{x}$. Let $F_{n, x}$ be the number of variables whose values are equal to $x$. The list $F_{n}$ is a random variable, often called the \"empirical frequency distribution\" of the $X_{i} .$ What does $F_{n}$ look like when $n$ is very large?\n",
        "\n",
        "* More specifically, let's try to estimate the probabilities of the possible values of $F_{n} .$ since the set of possible values is different for different $n$, take a sequence of frequency distributions $f_{1}, f_{2}, f_{3}, \\ldots$ approaching a fixed frequency distribution $f$. It turns out $^{* *}$ that\n",
        "\n",
        "> $\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\ln \\operatorname{Pr}\\left(F_{n}=f_{n}\\right)=-\\mathrm{KL}(f, p)$\n",
        "\n",
        "* In other words, the Kullback-Leibler divergence of $f$ from $p$ lets you estimate the probability of getting an empirical frequency distribution close to $f$ from a large number of independent random variables with distribution $p$.\n"
      ],
      "metadata": {
        "id": "vxh1T80ovldM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1l9_Erf5T5S"
      },
      "source": [
        "###### *Topological Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy5f4vGA-Llx"
      },
      "source": [
        "**General topology (point-set topology)**\n",
        "\n",
        "> Topology $\\rightarrow$ Lagebeziehungen wie ‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú werden verallgemeinert\n",
        "\n",
        "The fundamental concepts in point-set topology are continuity, compactness, and connectedness:\n",
        "\n",
        "* **Continuous functions**, intuitively, take nearby points to nearby points.\n",
        "\n",
        "* **Compact sets** are those that can be covered by finitely many sets of arbitrarily small size.\n",
        "\n",
        "* **Connected sets** are sets that cannot be divided into two pieces that are far apart.\n",
        "\n",
        "**Once a choice of open sets is made, the properties of continuity, connectedness, and compactness, which use notions of nearness, can be defined using these open sets.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Trennungsaxiom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNzMCPIBjYm"
      },
      "source": [
        "**Topological Vector Space**\n",
        "\n",
        "> **Man nennt $T$ eine Topologie auf $X$, und das Paar ($X$,$T$) einen topologischen Raum.**\n",
        "\n",
        "* a topological space ([Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum)) may be defined as a set of points, along with a set of **neighbourhoods** for each point, satisfying a set of **axioms** relating points and neighbourhoods.\n",
        "\n",
        "* The definition of a topological space **relies only upon set theory** and is the most general notion of a mathematical space that allows for the definition of concepts such as **[continuity](https://en.m.wikipedia.org/wiki/Continuous_function#Continuous_functions_between_topological_spaces), [connectedness](https://en.m.wikipedia.org/wiki/Connected_space), and [convergence](https://en.m.wikipedia.org/wiki/Limit_of_a_sequence)**.\n",
        "\n",
        "* **Other spaces, such as manifolds and metric spaces, are specializations of topological spaces with extra structures or constraints.**\n",
        "\n",
        "* Topological spaces are **studied in Point-Set Topology** (General Topology)\n",
        "\n",
        "* ein topologischer Raum ist ein elementarer Gegenstand der Topologie\n",
        "\n",
        "* Durch die Einf√ºhrung einer topologischen Struktur auf einer Menge lassen sich intuitive Lagebeziehungen wie **‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú** aus dem [Anschauungsraum (Euklidischer Raum)](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) auf sehr viele und sehr allgemeine Strukturen √ºbertragen und mit pr√§ziser Bedeutung versehen.\n",
        "\n",
        "* Ein **[topologischer Vektorraum](https://de.m.wikipedia.org/wiki/Topologischer_Vektorraum)** / [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space) ist ein Vektorraum, auf dem neben seiner algebraischen auch noch eine damit vertr√§gliche topologische Struktur definiert ist.\n",
        "\n",
        "* Sei $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. Ein $\\mathbb{K}$ -Vektorraum $E$, der zugleich topologischer Raum ist, hei√üt topologischer Vektorraum, wenn folgende Vertr√§glichkeitsaxiome gelten:\n",
        "  * Die Vektoraddition $E \\times E \\rightarrow E$ ist stetig,\n",
        "  * Die Skalarmultiplikation $\\mathbb{K} \\times E \\rightarrow E$ ist stetig.\n",
        "\n",
        "* **Beispiele:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erkl√§rt, dass wir eine Menge $\\Omega$ C $\\mathbb{R}$ offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen l√§sst.\n",
        "\n",
        "* **Separation Axioms**: Topologische R√§ume k√∂nnen [klassifiziert werden nach Kolmogorov](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms).\n",
        "\n",
        "*What are some examples of topological spaces which are not a metric space?*\n",
        "\n",
        "A set with a single element  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  only has one topology, the discrete one (which in this case is also the indiscrete one‚Ä¶) So that‚Äôs not helpful.\n",
        "\n",
        "A set with two elements, however, is more interesting.  ùëã={‚àô,‚àò}\n",
        "X\n",
        "=\n",
        "{\n",
        "‚àô\n",
        ",\n",
        "‚àò\n",
        "}\n",
        "  can be topologized by having all subsets open (discrete), only the empty set and the whole space open (indiscrete), and also the topology in which  ‚àÖ,{‚àò}\n",
        "‚àÖ\n",
        ",\n",
        "{\n",
        "‚àò\n",
        "}\n",
        "  and  ùëã\n",
        "X\n",
        " are open (and the symmetric one with  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  open).\n",
        "\n",
        "A metric on a two-element set forces us to declare that there‚Äôs a certain distance between the two points, rendering both singleton sets open (and closed). Indeed, a finite metric space has a discrete topology. But we just found three topologies on  ùëã\n",
        "X\n",
        "  which are not the discrete one, giving us three examples of a non-metrizable topological space.\n",
        "\n",
        "Note that the indiscrete topology on  ùëã\n",
        "X\n",
        "  can be regarded as coming from the pseudo-metric where the distance between the points is zero. However, the other two topologies, where one singleton set is open and the other is not, cannot be interpreted in this way. A metric is by definition symmetric, and these two topologies are not.\n",
        "\n",
        "*Source: [Quora](https://www.quora.com/What-are-some-examples-of-topological-spaces-which-are-not-a-metric-space)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mx_opXBBr-r"
      },
      "source": [
        "**Frechet-R√§ume**\n",
        "\n",
        "* [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Verallgemeinerungen des Banachraums und topologische Vektorraum mit speziellen Eigenschaften. Die Hauptvertreter von Fr√©chet-R√§umen sind Vektorr√§ume von glatten Funktionen. Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten.\n",
        "\n",
        "* Es handelt sich um einen **topologischen Vektorraum** mit speziellen Eigenschaften, die ihn als **Verallgemeinerung des Banachraums** charakterisieren.\n",
        "\n",
        "* Die Hauptvertreter von [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Vektorr√§ume von [glatten Funktionen](https://de.wikipedia.org/wiki/Glatte_Funktion).\n",
        "  * Eine glatte Funktion ist eine mathematische Funktion, die unendlich oft differenzierbar (insbesondere stetig) ist.\n",
        "\n",
        "* Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUGnGb0RB0L-"
      },
      "source": [
        "**Uniform Spaces**\n",
        "\n",
        "* **[Uniforme R√§ume](https://de.m.wikipedia.org/wiki/Uniformer_Raum) erlauben es zwar nicht Abst√§nde einzuf√ºhren**, aber trotzdem Begriffe wie gleichm√§√üige Stetigkeit, Cauchy-Folgen, Vollst√§ndigkeit und Vervollst√§ndigung zu definieren. Jeder uniforme Raum ist auch ein topologischer Raum.\n",
        "\n",
        "* **Jeder topologische Vektorraum (egal ob metrisierbar oder nicht) ist auch ein uniformer Raum**. Allgemeiner ist jede kommutative topologische Gruppe ein uniformer Raum. Eine nichtkommutative topologische Gruppe tr√§gt jedoch zwei uniforme Strukturen, eine links-invariante und eine rechts-invariante. Topologische Vektorr√§ume sind in endlichen Dimensionen vollst√§ndig, in unendlichen Dimensionen im Allgemeinen aber nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_1$-Norm und $L^1$-Metrik*"
      ],
      "metadata": {
        "id": "6quxRLo3U9Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) im endlichdimensionalen Raum** (p=1, Lasso, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|$\n",
        "\n",
        "* zB Summennorm des reellen Vektors $x=(3,-2,6) \\in \\mathbb{R}^{3}$ ist $\\|x\\|_{1}=|3|+|-2|+|6|=11$\n",
        "\n",
        "* Von Summennorm abgeleitete Metrik ist [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik). Siehe auch [Taxicab_geometry](https://en.m.wikipedia.org/wiki/Taxicab_geometry).\n",
        "\n",
        "* Die Summennorm ist im Gegensatz zur euklidischen Norm (2-Norm) nicht von einem Skalarprodukt induziert.\n",
        "\n",
        "* Die Einheitssph√§re der reellen Summennorm ist ein Kreuzpolytop mit minimalem Volumen √ºber alle p-Normen. **Daher ergibt die Summennorm f√ºr einen gegebenen Vektor den gr√∂√üten Wert aller p-Normen**. (zB 3 + (-2) + 6 = 11 in Summennorm, aber = 7 in euklidischer Norm fur p=2)\n",
        "\n",
        "* Techniques which use an L1 penalty, like [LASSO](https://en.m.wikipedia.org/wiki/Lasso_(statistics)), encourage solutions where many parameters are zero\n",
        "\n",
        "\n",
        "*Summennorm in zwei Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Vector-1-Norm_qtl1.svg/316px-Vector-1-Norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "LdVhJHBAUSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summennorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "$\\ell^{1}-$ Norm (**Folgenraum**)\n",
        "\n",
        "* Die $\\ell^{1}$ -Norm ist die Verallgemeinerung der Summennorm auf den Folgenraum $\\ell^{1}$ der **betragsweise summierbaren Folgen** $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{N} .$\n",
        "\n",
        "* Hierbei wird lediglich **die endliche Summe durch eine unendliche ersetzt** und die $\\ell^{\\text {t }}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{1}}=\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|$\n",
        "\n",
        "$L^{1}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* Weiter kann die Summennorm auf den Funktionenraum $L^{1}(\\Omega)$ der auf einer Menge $\\Omega$ betragsweise integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{1}$ Norm einer **betragsweise Lebesgue-integrierbaren Funktion** $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}=\\int_{\\Omega}|f(x)| d x$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{1}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden.\n",
        "\n",
        "* Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{1}(\\Omega)$, die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{1}$ -Raum die $L^{1}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{1}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}$"
      ],
      "metadata": {
        "id": "MIi62WcSUgP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 - Manhattan Distance (Lasso)**\n",
        "\n",
        "* The Manhattan norm gives rise to the [Manhattan distance](https://de.m.wikipedia.org/wiki/Manhattan-Metrik), where the distance between any two points, or vectors, is the sum of the differences between corresponding coordinates.\n",
        "\n",
        "* **Die Manhattan-Metrik ist die von der Summennorm (1-Norm) eines Vektorraums erzeugte Metrik.**\n",
        "\n",
        "* ***Aber: Die Summennorm ist nicht von einem Skalarprodukt induziert.***\n",
        "\n",
        "* Die Manhattan-Metrik (auch Manhattan-Distanz, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz d zwischen zwei Punkten a und b als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird:\n",
        "\n",
        "> $d(a, b)=\\sum_{i}\\left|a_{i}-b_{i}\\right|$"
      ],
      "metadata": {
        "id": "n9vaUs0sXyOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_2$-Norm und $L^2$-Metrik*"
      ],
      "metadata": {
        "id": "32OR42wtUS3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidische Norm](https://de.m.wikipedia.org/wiki/Euklidische_Norm) im endlichdimensionalen Raum** (p=2, Ridge, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{2}= \\left(x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}\\right)^{1 / 2} = \\sqrt{\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}}$\n",
        "\n",
        "* Ziel: berechnen die L√§nge (Betrag) eines Vektors in der euklidischen Ebene.  The length of a vector $x = (x_1, x_2, ..., x_n)$ in the $n$-dimensional real vector space $\\mathbb{R}^n$ is usually given by the Euclidean norm $||x||_{2}$.\n",
        "\n",
        "* Die euklidische Norm ist eine von einem Skalarprodukt induzierte Norm (im Gegensatz zur p1 Summennorm)\n",
        "\n",
        "* Beispiel: Vektor ${\\vec {v}}$ mit Komponenten $x$, $y$ und $z$ in drei Dimensionen durch ${\\vec {v}}=(x,y,z)$ wird die L√§nge berechnet durch:\n",
        "\n",
        "> $|{\\vec {v}}|={\\sqrt {x^{2}+y^{2}+z^{2}}}$\n",
        "\n",
        "* Die Euclidean Norm besitzt als eine von einem Skalarprodukt [induzierte Norm](https://de.m.wikipedia.org/wiki/Skalarproduktnorm) **neben den [drei Normaxiomen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Definition) eine Reihe weiterer Eigenschaften**:\n",
        "\n",
        "  * die G√ºltigkeit der [Cauchy-Schwarz-Ungleichung](https://de.m.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung)\n",
        "\n",
        "  * der [Parallelogrammgleichung](https://de.m.wikipedia.org/wiki/Parallelogrammgleichung)\n",
        "\n",
        "  * sowie eine Invarianz unter unit√§ren Transformationen (Die euklidische Norm √§ndert sich also unter unit√§ren Transformationen nicht. F√ºr reelle Vektoren sind solche Transformationen beispielsweise Drehungen des Vektors um den Nullpunkt. Diese Eigenschaft wird zum Beispiel bei der numerischen L√∂sung linearer Ausgleichsprobleme √ºber die **Methode der kleinsten Quadrate mittels QR-Zerlegungen genutzt**.)\n",
        "\n",
        "* F√ºr orthogonale Vektoren erf√ºllt die euklidische Norm selbst eine allgemeinere Form des Satzes des Pythagoras.\n",
        "\n",
        "* Sieht man eine Matrix mit reellen oder komplexen Eintr√§gen als entsprechend langen Vektor an, so kann die euklidische Norm auch f√ºr Matrizen definiert werden und hei√üt dann [**Frobeniusnorm**](https://de.m.wikipedia.org/wiki/Frobeniusnorm). Die euklidische Norm kann auch auf unendlichdimensionale Vektorr√§ume √ºber den reellen oder komplexen Zahlen verallgemeinert werden und hat dann zum Teil eigene Namen.\n",
        "\n",
        "*Euklidische Norm in zwei reellen Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Vector-2-Norm_qtl1.svg/316px-Vector-2-Norm_qtl1.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "p1rB9cfOJadt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euklidische Norm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "* Als [Folgennorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum): Die $\\ell^{2}-$ Norm im Folgenraum ist die Verallgemeinerung der euklidischen Norm auf den [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum) $\\ell^{2}$ der quadratisch summierbaren Folgen $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{\\mathrm{N}} .$ Hierbei wird lediglich die endliche Summe durch eine unendliche ersetzt und die $\\ell^{2}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{2}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{2}\\right)^{1 / 2}$\n",
        "\n",
        "* Die ‚Ñì-p -R√§ume sind ein Spezialfall der allgemeineren Lp-R√§ume, wenn man das Z√§hlma√ü auf dem Raum N betrachtet.\n",
        "\n",
        "* Als [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im [Funktionenraum](https://de.m.wikipedia.org/wiki/Funktionenraum) $L^{2}(\\Omega)$-Norm der auf einer Menge $\\Omega$ quadratisch integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{2}$ Norm einer quadratisch Lebesgue-integrierbaren Funktion $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{2} d x\\right)^{1 / 2}$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{2}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{2}(\\Omega),$ die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{2}$ -Raum die $L^{2}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{2}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}$\n",
        "\n",
        "* Der Raum $L^{2}(\\Omega)$ ist der [Hilbertraum fur L2](https://de.m.wikipedia.org/wiki/Lp-Raum#Der_Hilbertraum_L2) mit dem Skalarprodukt zweier Funktionen\n",
        "\n",
        "> $\\langle f, g\\rangle_{L_{2}(\\Omega)}=\\int_{\\Omega} \\overline{f(x)} \\cdot g(x) d x$"
      ],
      "metadata": {
        "id": "mXr0GKzWTwNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euclidian Distance](https://de.m.wikipedia.org/wiki/Euklidischer_Abstand) (Metrik) induziert aus der Euklidischen Norm**  (L2, Ridge)\n",
        "\n",
        "> $d_{2}:(x, y) \\mapsto\\|x-y\\|_{2}=\\sqrt{d_{\\mathrm{SSD}}}=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}$\n",
        "\n",
        "* Ziel: berechnen den Abstand zwischen zwei Vektoren in der euklidischen Ebene\n",
        "\n",
        "* Special case of the [Minkowski distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) with p=2\n",
        "\n",
        "* In Statistik siehe auch [Tikhonov Regularization (Ridge)](https://en.m.wikipedia.org/wiki/Tikhonov_regularization). Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small."
      ],
      "metadata": {
        "id": "3eQ4165xTtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidian (Metric) Space](https://en.m.wikipedia.org/wiki/Euclidean_space)**\n",
        "\n",
        "* Together with the [Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance) the Euclidean space is a metric space (x element R, d). http://theanalysisofdata.com/probability/B_4.html\n"
      ],
      "metadata": {
        "id": "kbrT-D7PTrew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Syw4dL96u31"
      },
      "source": [
        "###### *Special: Euclidean $p_{‚àû}$-Norm und $L^{‚àû}$-Metrik*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm) (p $\\rightarrow \\infty$): $\\rightarrow$\n",
        "\n",
        "> $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Sie ist ein Spezialfall der [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm).\n",
        "\n",
        "* Anschaulich gesprochen ist **der aus der Maximumsnorm abgeleitete Abstand immer dann relevant, wenn man sich in einem mehrdimensionalen Raum in alle Dimensionen gleichzeitig und unabh√§ngig voneinander gleich schnell bewegen kann**. (zB Rochade beim Schach)\n",
        "\n",
        "* Allgemeiner kann die Maximumsnorm benutzt werden, um zu bestimmen, wie schnell man sich in einem zwei- oder dreidimensionalen Raum bewegen kann, wenn angenommen wird, dass die Bewegungen in x-, y- (und z-)Richtung unabh√§ngig, gleichzeitig und mit gleicher Geschwindigkeit erfolgen.\n",
        "\n",
        "* Noch allgemeiner kann man ein System betrachten, dessen Zustand durch n unabh√§ngige Parameter bestimmt wird. An allen Parametern k√∂nnen gleichzeitig und ohne gegenseitige Beeinflussung √Ñnderungen vorgenommen werden. **Dann ‚Äûmisst‚Äú die Maximumsnorm in Rn die Zeit, die man ben√∂tigt, um das System von einem Zustand in einen anderen zu √ºberf√ºhren**. Voraussetzung hierf√ºr ist allerdings, dass man die Parameter so normiert hat, dass gleiche Abst√§nde zwischen den Werten auch gleichen √Ñnderungszeiten entsprechen. Andernfalls m√ºsste man eine gewichtete Version der Maximumsnorm verwenden, die die unterschiedlichen √Ñnderungsgeschwindigkeiten der Parameter ber√ºcksichtigt.\n",
        "\n",
        "* F√ºr einen Vektor $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{R}^{n}$ nennt man $\\|x\\|_{\\max }:=\\max \\left(\\left|x_{1}\\right|, \\ldots,\\left|x_{n}\\right|\\right)$ $\\rightarrow$ $\\|x\\|_{\\infty}=\\max _{i=1, \\ldots, n}\\left|x_{i}\\right|$ die Maximumsnorm von x.\n",
        "\n",
        "*√Ñquivalenz der euklidischen Norm (blau) und der Maximumsnorm (rot) in zwei Dimensionen:* [Source](https://de.m.wikipedia.org/wiki/√Ñquivalente_Normen)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Equiv_2-norm_max-norm_qtl1.svg/240px-Equiv_2-norm_max-norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "rpLxgxJgWGVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximumsnorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "* Im Gegensatz zur Maximumsnorm wird die Supremumsnorm $\\|f\\|_{\\text {sup }}:=\\sup _{t \\in X}|f(t)|$ nicht f√ºr stetige, sondern f√ºr beschr√§nkte Funktionen $f$ definiert.\n",
        "\n",
        "* In diesem Fall ist es nicht notwendig, dass $X$ kompakt ist; $X$ kann eine beliebige Menge sein.\n",
        "\n",
        "* **Da stetige Funktionen auf kompakten R√§umen beschr√§nkt sind, ist die Maximumsnorm ein Spezialfall der Supremumsnorm**.\n",
        "\n",
        "* Die Supremumsnorm (auch Unendlich-Norm genannt) ist in der Mathematik eine Norm auf dem Funktionenraum der beschr√§nkten Funktionen. Im einfachsten Fall einer reell- oder komplexwertigen beschr√§nkten Funktion ist die Supremumsnorm das Supremum der Betr√§ge der Funktionswerte. Allgemeiner betrachtet man Funktionen, deren Zielmenge ein normierter Raum ist, und die Supremumsnorm ist dann das Supremum der Normen der Funktionswerte.\n",
        "\n",
        "* **F√ºr stetige Funktionen auf einer kompakten Menge ist die Maximumsnorm ein wichtiger Spezialfall der Supremumsnorm.**\n",
        "\n",
        "*Die Supremumsnorm der reellen Arkustangens-Funktion ist œÄ/2. Auch wenn die Funktion diesen Wert betragsm√§√üig nirgendwo annimmt, so bildet er dennoch die kleinste obere Schranke.*\n",
        "\n",
        "![alternativer Text](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Graf_arctg.svg/260px-Graf_arctg.svg.png)\n",
        "\n",
        "\n",
        "Supremumsnorm vs Maximumsnorm:\n",
        "\n",
        "* So ist etwa die **Supremumsnorm** der linearen Funktion $f(x)=x$ in diesem Intervall gleich $1 .$ Die Funktion nimmt diesen Wert zwar innerhalb des Intervalls nicht an, kommt inm jedoch beliebig nahe.\n",
        "\n",
        "* W√§hlt man stattdessen das abgeschlossene Einheitsintervall $M=[0,1]$, dann wird der Wert 1 angenommen und die Supremumsnorm entspricht der **Maximumsnorm**.\n",
        "\n",
        "\n",
        "$L^{‚àû}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* L‚àû is a **function space** (Funktionenraum). Its elements are the essentially bounded measurable functions. More precisely, L‚àû is defined based on an underlying measure space, (S, Œ£, Œº). Start with the set of all measurable functions from S to R which are essentially bounded, i.e. bounded up to a set of measure zero. Two such functions are identified if they are equal almost everywhere. Denote the resulting set by L‚àû(S, Œº).\n",
        "\n",
        "\n",
        "* [Normen auf Operatoren](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_Operatoren)\n",
        "\n",
        "$\\ell^{‚àû}$ -Norm (**Folgenraum**)\n",
        "\n",
        "* The vector space ‚Ñì‚àû is a **sequence space** (Folgenraum) whose elements are the bounded sequences. The vector space operations, addition and scalar multiplication, are applied coordinate by coordinate.\n",
        "\n",
        "* $\\ell^{\\infty},$ the (real or complex) vector space of bounded sequences with the **[supremum norm](https://de.m.wikipedia.org/wiki/Supremumsnorm)**, and $L^{\\infty}=L^{\\infty}(X, \\Sigma, \\mu)$, the vector space of essentially bounded measurable functions with the **[essential supremum norm](https://de.m.wikipedia.org/wiki/Wesentliches_Supremum)**, are two closely related Banach spaces.\n",
        "\n",
        "* In fact the former is a special case of the latter. As a Banach space they are the continuous dual of the Banach spaces $\\ell_{1}$ of absolutely summable sequences, and $L^{1}=L^{1}(X, \\Sigma, \\mu)$ of absolutely integrable measurable functions (if the measure space fulfills the conditions of being localizable and therefore\n",
        "semifinite).\n",
        "\n",
        "* Pointwise multiplication gives them the structure of a Banach algebra, and in fact they are the standard examples of abelian Von Neumann algebras."
      ],
      "metadata": {
        "id": "jC0WO_F2VJou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L ‚àû - Chebyshev Distance**\n",
        "\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance) (or Tchebychev distance), maximum metric, or L‚àû metric is a metric defined on a vector space **where the distance between two vectors is the greatest of their differences** along any coordinate dimension.\n",
        "\n",
        "* The maximum norm gives rise to the **Chebyshev distance** or chessboard distance, the minimal number of moves a chess king would take to travel from x to y. The Chebyshev distance is the L‚àû-norm of the difference, a special case of the Minkowski distance where p goes to infinity. It is also known as Chessboard distance.\n",
        "\n",
        "> $d_{\\infty}:(x, y) \\mapsto\\|x-y\\|_{\\infty}=\\lim _{p \\rightarrow \\infty}\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{p}\\right)^{\\frac{1}{p}}=\\max _{i}\\left|x_{i}-y_{i}\\right|$"
      ],
      "metadata": {
        "id": "reDInvHxXs-r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdr6F1zuU4MG"
      },
      "source": [
        "##### <font color=\"blue\">*Variationsanalyse*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AR9PIXysIa4"
      },
      "source": [
        "###### *Variational Principle*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Mathematisch gesehen ist die Wirkung ein Funktional. W√§hrend Funktionen bestimmten Zahlen andere Zahlen zuordnen, ordnen Funktionale bestimmten Funktionen Zahlen zu.\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "7qhLvSkEBgaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXIblPI6_-oP"
      },
      "source": [
        "**Variationsrechnung**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Variationsrechnung\n",
        "\n",
        "* calculus og variation, khan academy https://youtube.com/playlist?list=PLdgVBOaXkb9CD8igcUr9Fmn5WXLpE8ZE_\n",
        "\n",
        "* **Find [stationary points](https://internal.ncl.ac.uk/ask/numeracy-maths-statistics/core-mathematics/calculus/stationary-points.html) (=derivative is zero, local minima or maxima) of a functional, like an integral I[f] (=here for example the path lenghts, or time spent travelling) is minimal between two points a and b.**\n",
        "\n",
        "  * A stationary point of a function $f(x)$ is a point where the derivative of $f(x)$ is equal to 0 .\n",
        "  * These points are called \"stationary\" because at these points the function is neither increasing nor decreasing.\n",
        "  * Graphically, this corresponds to points on the graph of $f(x)$ where the tangent to the curve is a horizontal line.\n",
        " * The stationary points of a function $y=f(x)$ are the solutions to $\n",
        "\\frac{d y}{d x}=0 $. This repeats in mathematical notation the definition given above: \"points where the gradient of the function is zero\".\n",
        "\n",
        "* **The integral is a functional (=function of functions), its stationary point is a fix point / minima of a functional (not function). Solve (usually differential) equations for stationary function f(x) (via calculus of variations)**\n",
        "\n",
        "* from regular calculus to calculus of variations: find stationary functions, not only stationary points, a function becomes a functional.\n",
        "\n",
        "* **Typical problem in variational calculus: find minimal path between points A and B, not necessarily a linear one (in physics for examples check Brachistochrone !)**.\n",
        "\n",
        "* Also consider that velocity depending on position changes the minimum paths or time to travel (later in vector analysis relevant for Kurvenintegral)\n",
        "\n",
        "In general, Calculus of variations seeks to find y = f(x) such that this integral:\n",
        "\n",
        "> $I[f]=\\int_{x_{1}}^{x_{2}} F\\left(x, y, \\frac{d y}{d x}\\right) d x$\n",
        "\n",
        "is stationary (ps: $\\frac{d y}{d x}$ = $y'$)\n",
        "\n",
        "1. Die [Variationsrechnung](https://de.wikipedia.org/wiki/Variationsrechnung) ist eine **Erweiterung der Funktionalanalysis und beschaeftigt sich mit <u>nichtlinearen Funktionalen</u>** (in der Funktionalanalysis sind es linear Funktionale)\n",
        "\n",
        "2. The [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations) is a field that **uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals**: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. <u>**Functions that maximize or minimize functionals may be found using the Euler‚ÄìLagrange equation of the calculus of variations.**</u>\n",
        "\n",
        "* In calculus of variations we are **NOT concerned with finding fix points of functions (like local maxima in a function), but rather fix points of functionals.**\n",
        "\n",
        "\n",
        "* dann f√ºhrt eine Variation der Wirkung: https://de.m.wikipedia.org/wiki/Feldtheorie_(Physik)#Formalismus\n",
        "\n",
        "* Beispiel: https://de.wikipedia.org/wiki/Fluiddynamik\n",
        "\n",
        "* Martin: formulier problem in variationelle formulierung (dann bist du in sobolove r√§ume), und dann Eigenschaften von Testfunktionen ausnutzen\n",
        "\n",
        "* **Variation der Elemente**: die [Variation der Elemente](https://de.wikipedia.org/wiki/Variation_der_Elemente) ist eine im 19. Jahrhundert entwickelte Methode zur genauen Bahnbestimmung von Himmelsk√∂rpern. Sie dient bis heute zur Modellierung von [Bahnst√∂rungen](https://de.wikipedia.org/wiki/Bahnst√∂rung).\n",
        "\n",
        "* **History of variational principles in physics**:\n",
        "https://en.m.wikipedia.org/wiki/History_of_variational_principles_in_physics\n",
        "\n",
        "* [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential) ist eine **Verallgemeinerung des gew√∂hnlichen Differentiationsbegriffes** dar, indem es die Richtungsableitung auch in unendlichdimensionalen R√§umen definiert.\n",
        "\n",
        "* Variational method in quantum mechanics: In quantum mechanics, the [variational method](https://en.m.wikipedia.org/wiki/Variational_method_(quantum_mechanics)) is one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states. This allows calculating approximate wavefunctions such as molecular orbitals. The basis for this method is the variational principle.\n",
        "\n",
        "Die Variationsrechnung besch√§ftigt sich mit der Minimierung bzw. Maximierung von Funktionalen, die als Integral dargestellt werden k√∂nnen. Man k√∂nnte sie daher als ‚Äûnat√ºrliche‚Äú Methode zur L√∂sung physikalischer Probleme bezeichnen, da die Physik ja bekanntlich von Extremalprinzipen regiert wird (k√ºrzeste Bahn, kleinste Wirkung, Gesamtenergie, Hamilton-Funktion). Die ‚ÄûVariation‚Äú dieser Integralbeziehung bez√ºglich einer abh√§ngigen Gr√∂√üe (in der Physik z.B. der Bahnkurve im Zustandsraum) f√ºhrt auf eine Differentialgleichung, deren L√∂sung diesen Integralausdruck minimiert respektive maximiert.\n",
        "\n",
        "https://link.springer.com/chapter/10.1007/978-3-642-83621-3_7\n",
        "\n",
        "Der Name Variationsrechnung bezieht sich dabei auf die Technik der Variation der Argumente. Wesentliches Ziel der Variationsrechnung ist das Finden von Extrema (haufig unter Nebenbedingungen) f√ºr ein gegebenes Funktional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqXlKua2Mj8"
      },
      "source": [
        "**Variational Principle**\n",
        "\n",
        "* a [variational principle](https://en.wikipedia.org/wiki/Variational_principle) is one that enables a problem to be solved using calculus of variations, which concerns finding such functions which optimize the values of quantities that depend upon those functions.\n",
        "\n",
        "* For example, the problem of determining the shape of a hanging chain suspended at both ends‚Äîa catenary‚Äîcan be solved using variational calculus, and in this case, the variational principle is the following: The solution is a function that minimizes the gravitational potential energy of the chain.\n",
        "\n",
        "* Any physical law which can be expressed as a variational principle describes a **self-adjoint operator.** These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/1/10/Total_variation.gif)\n",
        "\n",
        "*As the green ball travels on the graph of the given function, the length of the path travelled by that ball's projection on the y-axis, shown as a red ball, is the total variation of the function.*\n",
        "\n",
        "* the [total variation](https://en.wikipedia.org/wiki/Total_variation) identifies several slightly different concepts, related to the (local or global) structure of the codomain of a function or a measure. For a real-valued continuous function f, defined on an interval [a, b] ‚äÇ ‚Ñù, its total variation on the interval of definition is a measure of the one-dimensional [arclength](https://en.wikipedia.org/wiki/Arc_length) of the curve with parametric equation x ‚Ü¶ f(x), for x ‚àà [a, b].\n",
        "\n",
        "* In der Variationsrechnung und der Theorie der stochastischen Prozesse ist die [Variation](https://de.wikipedia.org/wiki/Variation_(Mathematik)) (auch totale Variation genannt) einer Funktion **ein Ma√ü f√ºr das lokale Schwingungsverhalten der Funktion**.\n",
        "\n",
        "* Bei den stochastischen Prozessen ist die Variation von besonderer Bedeutung, da sie die Klasse der zeitstetigen Prozesse in zwei fundamental verschiedene Unterklassen unterteilt: jene mit endlicher und solche mit unendlicher Variation.\n",
        "\n",
        "Die [erste Variation](https://de.wikipedia.org/wiki/Erste_Variation) ist eine verallgemeinerte Richtungsableitung eines Funktionals. Ihre Eigenschaften sind in der angewandten Mathematik und der theoretischen Physik relevant. Die erste Variation spielt eine zentrale Rolle in der Variationsrechnung und wird in der analytischen Mechanik genutzt. Ein verwandtes Konzept ist die Funktionalableitung.\n",
        "\n",
        "In der Analysis ist eine Funktion von [beschr√§nkter Variation](https://de.wikipedia.org/wiki/Beschr√§nkte_Variation) (beschr√§nkter Schwankung), wenn ihre totale Variation (totale Schwankung) endlich ist, sie also in gewisser Weise nicht beliebig stark oszilliert. Diese Begriffe h√§ngen eng mit der Stetigkeit und der Integrierbarkeit von Funktionen zusammen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anwendungsgebiete**\n",
        "\n",
        "* Die Variationsrechnung ist die mathematische Grundlage aller physikalischen Extremalprinzipien und deshalb besonders in der theoretischen Physik wichtig, so etwa\n",
        "\n",
        "  * im Lagrange-Formalismus der klassischen Mechanik\n",
        "\n",
        "  * bzw. der Bahnbestimmung, in der Quantenmechanik in Anwendung des Prinzips der kleinsten Wirkung\n",
        "\n",
        "  * und in der statistischen Physik im Rahmen der Dichtefunktionaltheorie.\n",
        "\n",
        "  * In der Mathematik wurde die Variationsrechnung beispielsweise bei der riemannschen Behandlung des Dirichlet-Prinzips f√ºr harmonische Funktionen verwendet.\n",
        "\n",
        "  * Auch in der Steuerungs- und Regelungstheorie findet die Variationsrechnung Anwendung, wenn es um die Bestimmung von Optimalreglern geht.\n",
        "\n",
        "* Ein typisches Anwendungsbeispiel ist das Brachistochronenproblem: Auf welcher Kurve in einem Schwerefeld von einem Punkt A zu einem Punkt B, der unterhalb, aber nicht direkt unter A liegt, ben√∂tigt ein Objekt die geringste Zeit zum Durchlaufen der Kurve? Von allen Kurven zwischen A und B minimiert eine den Ausdruck, der die Zeit des Durchlaufens der Kurve beschreibt. Dieser Ausdruck ist ein Integral, das die unbekannte, gesuchte Funktion, die die Kurve von A nach B beschreibt, und deren Ableitungen enth√§lt."
      ],
      "metadata": {
        "id": "A4hJCXFKFMa9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkRUt1ihCd_"
      },
      "source": [
        "**Fundamentallemma der Variationsrechnung**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Fundamentallemma_der_Variationsrechnung\n",
        "\n",
        "**Fundamentalsatz der Variationsrechnung**\n",
        "\n",
        "* Fundamental Theorem of the Calculus of Variations - [Fundamentalsatz der Variationsrechnung](https://de.wikipedia.org/wiki/Fundamentalsatz_der_Variationsrechnung)\n",
        "\n",
        "* eng verwandt mit dem [weierstra√üschen Satz vom Minimum](https://de.wikipedia.org/wiki/Satz_vom_Minimum_und_Maximum)\n",
        "\n",
        "* Er behandelt die in der Variationsrechnung zentrale Frage, unter welchen Bedingungen reellwertige Funktionale ein Minimum annehmen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eloiR80j3LKg"
      },
      "source": [
        "###### *Brachistochrone & Tautochronie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNCf_416SQR"
      },
      "source": [
        "[**Brachistochrone Curve**](https://en.wikipedia.org/wiki/Brachistochrone_curve) (in rot): Der K√∂rper gleitet auf einer solchen Bahn schneller zum Ziel als auf jeder anderen Bahn, beispielsweise auf einer geradlinigen, obwohl diese k√ºrzer ist.\n",
        "\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/6/63/Brachistochrone.gif)\n",
        "\n",
        "* Brachistochrone: Path between 2 points $A$ and $B$ which minimizes the time taken by a particle falling from $A$ to $B$ under the influence of gravity.\n",
        "\n",
        "* Time = distance / speed. Goal: Mix of minimize distance and maximize speed\n",
        "\n",
        "* Johann I Bernoulli hat sich mit dem **Problem des schnellsten Falles** besch√§ftigt. Im Jahre 1696 fand er schlie√ülich die L√∂sung in der **Brachistochrone**. Heute sieht man dies oft als die **Geburtsstunde der Variationsrechnung**.\n",
        "\n",
        "* Video: [The Brachistochrone Problem and Solution | Calculus of Variations](https://www.youtube.com/watch?v=zYOAUG8PxyM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvO3MMS6JFg"
      },
      "source": [
        "**Tautochronie** der Brachistochrone ‚Äì von jedem Startpunkt auf der Kurve erreichen die Kugeln das ‚ÄûZiel‚Äú gleichzeitig.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/b/bd/Tautochrone_curve.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Bewegungsgleichungen (Newton)*"
      ],
      "metadata": {
        "id": "sprbeM9PikBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hintergrund: Bewegungsgleichungen**\n",
        "\n",
        "* Unter einer [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt.\n",
        "\n",
        "* In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung (=Beschleunigung / Acceleration)\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "> **Es gibt drei Ans√§tze f√ºr Bewegungsgleichungen: Newtonian Mechanics, Lagrangian and Hamiltonian. F√ºr letztere beiden gilt the Principle of Least Action.**"
      ],
      "metadata": {
        "id": "hXh4vP1EJIiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newtonsche Gesetze**\n",
        "\n",
        "Die [Newtonschen Gesetze](https://de.m.wikipedia.org/wiki/Newtonsche_Gesetze) (Fundamentalkonzept der klassischen Mechanik, Extremalprinzip des Wirkungsfunktionals) gelten als die Grundlage der klassischen Mechanik, auf der alle weiteren Modelle basieren. Zentrales Konzept dieser Formulierung ist die Einf√ºhrung von Kr√§ften, die eine Beschleunigung $\\ddot{\\vec{x}}$ einer Masse $m$ hervorrufen. Die Bewegungsgleichung dieser Masse wird bestimmt durch die √úberlagerung der Kr√§fte $\\vec{F}_{i}$, die auf die Masse wirken\n",
        "\n",
        "> $\n",
        "m \\ddot{\\vec{x}}=\\sum_{i=1}^{N} \\vec{F}_{i}\n",
        "$\n",
        "\n",
        "1. Ein kr√§ftefreier K√∂rper bleibt in Ruhe oder bewegt sich geradlinig mit konstanter Geschwindigkeit (siehe [Tr√§gheit](https://de.m.wikipedia.org/wiki/Tr√§gheit#Bedeutung_f√ºr_wichtige_Prinzipien_der_Mechanik))\n",
        "\n",
        "2. Kraft gleich Masse mal Beschleunigung. $(\\vec{F}=m \\cdot \\vec{a})$ $\\rightarrow$ Equation of Motion (2. Newtonsches Gesetz)\n",
        "\n",
        "3. Kraft gleich Gegenkraft: Eine Kraft von K√∂rper A auf K√∂rper B geht immer mit einer gleich gro√üen, aber entgegen gerichteten Kraft von K√∂rper B auf K√∂rper A einher.\n",
        "\n",
        ">$\n",
        "\\vec{F}_{A \\rightarrow B}=-\\vec{F}_{B \\rightarrow A}\n",
        "$\n",
        "\n",
        "**Bewegungsgleichungen**\n",
        "\n",
        "* eine [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) ist eine Gleichung, die die Entwicklung eines mechanischen Systems bei √§u√üeren Einfl√ºssen beschreibt\n",
        "\n",
        "* Unter einer Bewegungsgleichung versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt. In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung.\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "* L√∂sung: Die L√∂sung der Bewegungsgleichung ist die [Trajektorie](https://de.m.wikipedia.org/wiki/Trajektorie_(Physik)), auf der sich das System bewegt. Sie ist, abgesehen von einigen einfachen F√§llen (siehe Beispiele unten), meist nicht in analytisch geschlossener Form darstellbar und muss √ºber [numerische Methoden](https://de.m.wikipedia.org/wiki/Numerische_Mathematik) gewonnen werden. Dies ist z. B. zur Ermittlung der Trajektorien dreier Himmelsk√∂rper, die sich gegenseitig gravitativ anziehen, erforderlich (siehe [Dreik√∂rperproblem](https://de.m.wikipedia.org/wiki/Dreik%C3%B6rperproblem)). Zur L√∂sung eines N-Teilchensystems l√§sst sich die [discrete element method](https://de.m.wikipedia.org/wiki/Discrete_element_method) anwenden. In einfachen F√§llen wird die geschlossene L√∂sung als ‚ÄûBahngleichung‚Äú bezeichnet.\n",
        "\n",
        "**Newtonsche Axiome**\n",
        "\n",
        "Prinzipen: Zum Aufstellen von Bewegungsgleichungen in der klassischen Physik wird verwendet:\n",
        "\n",
        "* das 2. Newtonsche Gesetz,\n",
        "* der Lagrange-Formalismus oder\n",
        "* der Hamilton-Formalismus\n",
        "\n",
        "Darauf basierend ergibt sich die Bewegungsgleichung der Quantenmechanik, die Schr√∂dingergleichung.\n",
        "\n",
        "**In der Technischen Mechanik werden verwendet**:\n",
        "\n",
        "* das Prinzip der virtuellen Arbeit (D‚ÄôAlembertsches Prinzip)\n",
        "* das Prinzip der virtuellen Leistung (Prinzip von Jourdain)\n",
        "* das Prinzip des kleinsten Zwanges.\n",
        "\n"
      ],
      "metadata": {
        "id": "mi3ntZBhiiWf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYzlWwuA8hXl"
      },
      "source": [
        "###### *Principle of Least / Stationary Action (Lagrange-Formalismus)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange-Formalismus**\n",
        "\n",
        "Der [Lagrange-Formalismus](https://de.m.wikipedia.org/wiki/Lagrange-Formalismus) beschreibt die Gesetze der klassischen Mechanik durch die Lagrange-Funktion $L$, die f√ºr Systeme mit einem generalisierten Potential und holonomen [Zwangsbedingungen](https://de.m.wikipedia.org/wiki/Zwangsbedingung) als Differenz aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben ist:\n",
        "\n",
        ">$\n",
        "L=T-V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der Euler-Lagrange-Gleichungen, die die Ableitungen nach der Zeit $t$, den Geschwindigkeiten $\\dot{q}_{i}$ und den [generalisierten Koordinaten](https://de.m.wikipedia.org/wiki/Generalisierte_Koordinate) $q_{i}$ miteinander in Verbindung setzt:\n",
        "\n",
        ">$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial L}{\\partial \\dot{q}_{i}}=\\frac{\\partial L}{\\partial q_{i}}\n",
        "$\n",
        "\n",
        "> Action $S$ = Kinetic Energy - Potential Energy = $\\int (T - V) dt$ = $\\int (\\frac{1}{2} mv^2 - mgh) dt$\n",
        "\n",
        "$L$ is the Lagrangian of the particle: $L=E_{u}-U$.\n",
        "\n",
        "*Need to solve Lagrange equations to make $S$ stationary:*\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{1}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{1}}\\right)$\n",
        "\n",
        ">$\\frac{\\partial L}{\\partial q_{2}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{2}}\\right)$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{3}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{3}}\\right) \\quad \\begin{array}{c}\\\\ \\end{array}$\n",
        "\n",
        "We can use a general coordinate system:\n",
        "$\\\\ {\\left[q_{1}(t), q_{2}(t), q_{3}(t)\\right]}$\n",
        "\n",
        "*Lagrange-Gleichungen erster Art*\n",
        "\n",
        "* Mit den Lagrange-Gleichungen erster Art lassen sich die Zwangskr√§fte berechnen.\n",
        "\n",
        "* Wenn man annimmt, dass sich die √§u√üeren Kr√§fte aus einem Potential ableiten lassen, kann man die Bewegungsgleichung schreiben (Lagrange-Gleichung 1. Art):\n",
        "\n",
        "*Lagrange-Gleichungen zweiter Art*\n",
        "\n",
        "* Die Lagrange-Gleichungen zweiter Art ergeben sich als sogenannte Euler-Lagrange-Gleichungen eines Variationsproblems und liefern die Bewegungsgleichungen, wenn die Lagrange-Funktion gegeben ist.\n",
        "\n",
        "* Sie folgen aus der Variation des mit der Lagrange-Funktion gebildeten Wirkungsintegrals im Hamiltonschen Prinzip.\n",
        "\n",
        "Eingef√ºhrte Formulierung der klassischen Mechanik, in der die Dynamik eines Systems durch **eine einzige skalare Funktion, die Lagrange-Funktion**, beschrieben wird. Der Formalismus ist (im Gegensatz zu der newtonschen Mechanik, die a priori nur in Inertialsystemen gilt) auch in beschleunigten Bezugssystemen g√ºltig. Der Lagrange-Formalismus ist invariant gegen Koordinatentransformationen.\n",
        "\n",
        "Der [Langrange Formalismus](https://de.wikipedia.org/wiki/Lagrange-Formalismus) ist eine m√∂gliche (von vielen!) Formulierung der klassischen Mechanik. Hier wird die Dynamik eines Systems durch die Langrange-Funktion L(${\\boldsymbol{q}}$, $\\dot{\\boldsymbol{q}}$, $t$) beschrieben:\n",
        "\n",
        "* ${\\boldsymbol{q}}$ = (q1, q2, ..., qw) - allgemeine Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ = (d / dt) q ... ($\\dot{\\boldsymbol{q}}$1, $\\dot{\\boldsymbol{q}}$2.. $\\dot{\\boldsymbol{q}}$n) - Vektor der Wirkung (allgemeine Geschwindigkeiten)\n",
        "* $t$ - Zeit (Die explizite Zeitabh√§ngige ber√ºcksichtigt externe, zeitabh√§ngige Faktoren (z.B. Magnet-Felder etc). Wird auf Null gesetzt, denn bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "> $L_{x}(q, \\dot{q}, t)$ wird zu: $L_{x}(q, \\dot{q})$ = $T$<sub>kin</sub> - $V$<sub>pot</sub>"
      ],
      "metadata": {
        "id": "BR8DkOZUtAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Principle of Least Action or Stationary Action*\n",
        "\n",
        "> Action  ùëÜ  = Kinetic Energy - Potential Energy\n",
        "\n",
        "* Principle of Least Action = better: [Stationary Action](https://en.m.wikipedia.org/wiki/Stationary-action_principle) = Wirkung\n",
        "\n",
        "https://youtu.be/dPxhTiiq-1A\n",
        "\n",
        "[The Principle of Stationary Action](https://www.youtube.com/watch?v=M05ixbSOY80): If a particle/system $P$ travels from one point to another in the time interval $\\left[t_{1}, t_{2}\\right]$, the path the particle traverses is such that this function:\n",
        "\n",
        "> $\n",
        "S=\\int_{t_{1}}^{t_{2}} \\mathcal{L} \\text { dt}$\n",
        "\n",
        "is stationary.\n",
        "\n",
        "* Total energy = kinetic energy + potential energy.\n",
        "\n",
        "> **Kinetic - potential energy = Lagrangian $L$**\n",
        "\n",
        "> has no physical meaning !! It's a ver useful mathematical tool\n",
        "\n",
        "* Kinetic energy depends on velocity of a particle (detonated with a dot over x,y,z), potential energy depends on position of a particle x,y,z. Hence the Lagrangian of a particle depends on all of the positions and all of their time derivatives.\n",
        "\n",
        "* **Use and solve the three Lagrangian equations in order to determine the equation of motion of a particle (and Lagrange equations are equivalent to Newton's second law)**\n",
        "\n",
        "* And they can easily applied to other coordinate systems than cartesian (i.e cylindric, or spherical)\n",
        "\n",
        "* Also: **Lagrange equations are very similar to Euler-Lagrange Equations!**\n",
        "\n",
        "* Because the three Lagrange equations very strongly resemble the Euler-Lagrange-equation, there must be some functional that's being made stationary by Lagrange equations\n",
        "\n",
        "* **$S$ is the action integral (= a functional!)**\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_01.png)\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_02.png)"
      ],
      "metadata": {
        "id": "rfHQAhk-EqQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Euler-Lagrange Equation*"
      ],
      "metadata": {
        "id": "puX57AODxaUG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_F2rBw5bk4"
      },
      "source": [
        "**Euler-Lagrange Equation**\n",
        "\n",
        "*Why using Euler-Lagrange Equation?*\n",
        "\n",
        "1) lagrangian mechanies gives equation of motion without considering forces at all, only energy\n",
        "\n",
        "2) This is more convenient for complicated systems with multiple forces to be considered\n",
        "\n",
        "3) Great for dealing with multiple coordinate\n",
        "\n",
        "*How to get the Euler-Lagrange Equation?*\n",
        "\n",
        "* **Step 1: Lagrangian equation**: L = T (kinetic energy) - V (potential energy)\n",
        "\n",
        "  * T = $\\frac{1}{2}m \\dot x ^2 $\n",
        "\n",
        "    * with $\\dot x$ short for: $\\frac{dx}{dt}$\n",
        "\n",
        "  * V = $\\frac{1}{2}k x ^2 $\n",
        "\n",
        "  * so: L = $\\frac{1}{2}m \\dot x ^2 $ - $\\frac{1}{2}k x ^2 $ -> This is our Lagrangian for a specific system !\n",
        "\n",
        "* **Step 2: Now take Euler‚ÄìLagrange Equation**: $\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot q}\\right)=\\frac{\\partial L}{\\partial q}$\n",
        "\n",
        "  * we see it contains the Lagrangian\n",
        "\n",
        "  * it's consistent with Newtonian classical mechanics F=ma etc\n",
        "\n",
        "  * we can plug in our Lagrangian for a specific system (replace L with formula above). we get the equation of motion!\n",
        "\n",
        "* **Step 3: Equation of Motion**: $m \\ddot x = -kx$\n",
        "\n",
        "  * mass m of an object multipliplied by acceleration $\\ddot x$ (which is the second derivative of x, the whole left side is same as newton's: f=m*a)\n",
        "\n",
        "  * we are stating something about the forces acting on the system\n",
        "\n",
        "*More about Euler‚ÄìLagrange equation*\n",
        "\n",
        "* Langrangian = Kinetic Energy - Potential Engergy\n",
        "\n",
        "* then insert it into the **Euler-Langrange Equation**:\n",
        "\n",
        "> $\\frac{d}{d t} \\frac{\\partial L}{\\partial \\dot{\\theta}}=\\frac{\\partial L}{\\partial \\theta}$\n",
        "\n",
        "* the Euler-Langrange Equation is the condition of the action $S$ to be minimized, where action is integral of Lagrangian\n",
        "\n",
        "* Means: of all the possible paths a particle could follow, the actual path it chooses is the one that minimizes (or actually \"extremizes\") the action = **principle of least action**\n",
        "\n",
        "* with Lagrangian we don't need any vectors anymore like in Newtonian, we can sue whatever coordinates. Also it makes it easier to deal with constraints and understands symmetries\n",
        "\n",
        "* F = ma in the Euler Lagrange Equation gives us a single second-order differential equation\n",
        "\n",
        "The Euler‚ÄìLagrange equation is an equation satisfied by a function q of a real argument t, which is a stationary point of the functional:\n",
        "\n",
        "> $S(\\boldsymbol{q})=\\int_{a}^{b} L(t, \\boldsymbol{q}(t), \\dot{\\boldsymbol{q}}(t)) \\mathrm{d} t$\n",
        "\n",
        "* ${\\boldsymbol{q}}$ - Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ - Vektor der Wirkung\n",
        "* t - Zeit (wird auf Null gesetzt, den bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "The Euler‚ÄìLagrange equation, then, is given by\n",
        "\n",
        "> $L_{x}(t, q(t), \\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d} t} L_{v}(t, q(t), \\dot{q}(t))=0$\n",
        "\n",
        "* partial derivative of one dimension, then second dimension and then time\n",
        "\n",
        "* the [Euler equation](https://en.m.wikipedia.org/wiki/Euler‚ÄìLagrange_equation) is a **second-order partial differential** equation whose **solutions are the functions for which a given functional is stationary**.\n",
        "\n",
        "* Because **a differentiable functional is stationary at its local extrema**, the Euler‚ÄìLagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing or maximizing it.\n",
        "\n",
        "* This is analogous to [Fermat's theorem](https://en.m.wikipedia.org/wiki/Fermat%27s_theorem_(stationary_points)) in calculus, stating that at any point where a differentiable function attains a local extremum its derivative is zero.\n",
        "\n",
        "* In Lagrangian mechanics, according to [Hamilton's principle](https://en.m.wikipedia.org/wiki/Hamilton%27s_principle) of stationary action, the evolution of a physical system is described by the solutions to the Euler equation for the action of the system. In this context Euler equations are usually called Lagrange equations. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations.\n",
        "\n",
        "  * Hamilton's principle is William Rowan Hamilton's formulation of the [principle of stationary action](https://en.m.wikipedia.org/wiki/Principle_of_least_action) (also called: 'Principle of least action'). It states that the **dynamics of a physical system are determined by a variational problem for a functional based on a single function**, the Lagrangian, which may contain all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hamiltonian Function*"
      ],
      "metadata": {
        "id": "SqYBnfUAD1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamilton'sches Prinzip (Wirkungsfunktional)*\n",
        "\n",
        "> Das [Hamilton'sche Prinzip](https://de.wikipedia.org/wiki/Hamiltonsches_Prinzip) zeichnet tatsachlich durchlaufene Bahnen dadurch aus, **dass bei ihnen die Wirkung (=Funktional) S[q] (verglichen mit anderen Bahnen) ein Minimum annimmt (minimal variation ??)**.\n",
        "\n",
        "* Das Hamiltonsche Prinzip der Theoretischen Mechanik ist ein **Extremalprinzip**. Physikalische Felder und Teilchen nehmen danach f√ºr eine bestimmte Gr√∂√üe einen extremalen (d. h. gr√∂√üten oder kleinsten) Wert an. Diese Bewertung nennt man Wirkung (=Action), mathematisch ist die Wirkung ein Funktional, daher auch die Bezeichnung **Wirkungsfunktional**.\n",
        "\n",
        "* Die Wirkung erweist sich in vielen F√§llen nicht als minimal, sondern nur als **‚Äûstation√§r‚Äú** (d. h. extremal). Deshalb wird das Prinzip von manchen Lehrbuchautoren auch das Prinzip der **station√§ren Wirkung** genannt. Manche Autoren nennen das Hamiltonsche Prinzip auch **'Prinzip der kleinsten Wirkung'**, was jedoch ‚Äì wie oben ausgef√ºhrt ‚Äì nicht pr√§zise ist.\n",
        "\n",
        "* Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation:\n",
        "\n",
        "> $\\frac{\\delta \\mathcal{S}}{\\delta \\mathbf{q}(t)}=0$\n",
        "\n",
        "* That is, the system takes a path in configuration space for which the action is stationary, with fixed boundary conditions at the beginning and the end of the path.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Least_action_principle.svg/500px-Least_action_principle.svg.png)\n",
        "\n",
        "*As the system evolves, q traces a path through configuration space (only some are shown). The path taken by the system (red) has a stationary action (Œ¥S = 0) under small changes in the configuration of the system (Œ¥q).*"
      ],
      "metadata": {
        "id": "aH2WDPHcw04D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamiltonsche Mechanik & Hamilton-Funktion*\n",
        "\n",
        "> Die [**Hamilton-Funktion**](https://de.m.wikipedia.org/wiki/Hamilton-Funktion) eines Systems von Teilchen ist (,wenn keine rheonomen (d. h. zeitabh√§ngigen) Zwangsbedingungen vorliegen), **die Gesamtenergie als Funktion der Orte und Impulse der Teilchen** und gegebenenfalls der Zeit.\n",
        "\n",
        "* Total Energy = Kinetic Energy + Potential Energy\n",
        "\n",
        "* rewrite everything in terms of momentum, and we get the **Hamiltonian**:\n",
        "\n",
        "> $H=\\frac{p^{2}}{2 m l^{2}}-m g l \\cos \\theta$\n",
        "\n",
        "* the Hamiltonian gives us a pair of first-order differential equations for theta and pi\n",
        "\n",
        "* the generalized version is Momentum * Velocity - L (Lagrangian):\n",
        "\n",
        "> $H=P \\dot{\\theta}-L$\n",
        "\n",
        "* we get a new geometric perspective by connecting it with something known as \"flow phase space\": P (momentum) and Theta create a new vector space (pairs of them) called the \"phase space\", where I can see what the particle will do in the future. Energy is constant = particle travels along a line of constant energy\n",
        "\n",
        "Die [Hamiltonsche Mechanik](https://de.m.wikipedia.org/wiki/Hamiltonsche_Mechanik) ist die am st√§rksten verallgemeinerte Formulierung der klassischen Mechanik und Ausgangspunkt der Entwicklung neuerer Theorien und Modelle, wie der Quantenmechanik. Zentrale Gleichung dieser Formulierung ist die Hamilton-Funktion H. Sie ist folgenderma√üen definiert:\n",
        "\n",
        "> $\n",
        "H=\\sum_{i} \\dot{q}_{i} p_{i}-L(\\vec{q}, \\dot{\\vec{q}}, t)\n",
        "$\n",
        "\n",
        "Dabei sind $\\dot{q}_{i}$ die generalisierten Geschwindigkeiten und $p_{i}$ die generalisierten Impulse.\n",
        "\n",
        "> Die hamiltonschen Bewegungsgleichungen folgen aus dem hamiltonschen Prinzip der station√§ren Wirkung.\n",
        "\n",
        "Ist die potentielle Energie unabh√§ngig von der Geschwindigkeit und h√§ngen die TransformationsGleichungen, die die generalisierten Koordinaten definieren, nicht von der Zeit ab, ist die Hamilton-Funktion in der klassischen Mechanik durch die Summe aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben:\n",
        "\n",
        "> $\n",
        "H=T+V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der kanonischen Gleichungen:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\dot{q}_{i} &=\\frac{\\partial H}{\\partial p_{i}} \\\\\n",
        "\\dot{p}_{i} &=-\\frac{\\partial H}{\\partial q_{i}}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Mit dem Hamilton-Jacobi-Formalismus existiert eine modifizierte Form dieser Beschreibung, die die Hamilton-Funktion mit der Wirkung verkn√ºpft."
      ],
      "metadata": {
        "id": "ntdvyhHFuhn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operators in Classical Mechanics**\n",
        "\n",
        "[Operators in classical mechanics](https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics): In classical mechanics, the movement of a particle (or system of particles) is completely determined by the **Lagrangian** $L(q, \\dot{q}, t)$ or equivalently the **Hamiltonian** $H(q, p, t)$, a function of the generalized coordinates $q$, generalized velocities $\\dot{q}=\\mathrm{d} q / \\mathrm{d} t$ and its conjugate momenta:\n",
        "\n",
        ">$\n",
        "p=\\frac{\\partial L}{\\partial \\dot{q}}\n",
        "$\n",
        "\n",
        "If either $L$ or $H$ is independent of a generalized coordinate $q$, meaning the $L$ and $H$ do not change when $q$ is changed, which in turn means the dynamics of the particle are still the same even when q changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate $q$ is a symmetry). **Operators in classical mechanics are related to these symmetries.**\n",
        "\n",
        "More technically, when $H$ is invariant under the action of a certain group of transformations $G$ :\n",
        "\n",
        ">$\n",
        "S \\in G, H(S(q, p))=H(q, p)\n",
        "$\n",
        "\n",
        "the elements of $G$ are physical operators, which map physical states among themselves.\n",
        "\n",
        "**Connections to Quantum Mechanics**\n",
        "\n",
        "* Functions on phase phase in classical mechanics turn into operators in the quantum space of states in quantum mechanics\n",
        "\n",
        "* And if you know the state of a quantum system at time t0, the Schr√∂dinger equation says they at a later time t will be $|\\psi \\rangle$ $\\rightarrow$ $e^{-\\frac{i}{\\hbar} H t}|\\psi\\rangle$ acting on the state,\n",
        "\n",
        "* where $H$ is the operator version of the classical Hamiltonian function\n",
        "\n",
        "**Feynman Path Integral Formulation**\n",
        "\n",
        "> *Principle of Least Action in Quantum Mechanics (Path Integral Formulation)*\n",
        "\n",
        "The [path integral formulation](https://en.m.wikipedia.org/wiki/Path_integral_formulation) is a description in quantum mechanics that **generalizes the action principle of classical mechanics**. It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or functional integral, over an infinity of quantum-mechanically possible trajectories to compute a quantum amplitude.\n",
        "\n",
        "* Richard Feynman zeigte in den 1940ern, dass sich das Hamiltonsche Prinzip in der Quantenfeldtheorie gerade dadurch ergibt, dass alle m√∂glichen Pfade (auch die nicht zielgerichteten) zul√§ssig sind und aufintegriert werden. Dabei √ºberlagern sich Pfade mit extremaler Wirkung konstruktiv und davon abweichende destruktiv, so dass die Natur schlie√ülich zielgerichtet erscheint.\n",
        "\n",
        "* Principle of least action is equivalent to newtonian mechanics (one can derive the on from the other). And both is for large scale objects, **meanwhile the principle of least action is the large scale approximation of the feynman path integral on quantum objects!** source at Min 7:53 here: https://www.youtube.com/watch?v=dPxhTiiq-1A\n",
        "\n",
        "**Bewegungsgleichung der Allgemeinen Relativit√§tstheorie**\n",
        "\n",
        "Die [Bewegung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) eines K√∂rpers wird durch die Geod√§tengleichung der gekr√ºmmten Raumzeit beschrieben, sofern nur gravitative Kr√§fte auf ihn einwirken. Dann bewegt sich der K√∂rper entlang einer Geod√§ten der Raumzeit. Die Geod√§tengleichung lautet\n",
        "\n",
        "> $\n",
        "\\ddot{x}^{\\mu}+\\Gamma_{\\lambda \\nu}^{\\mu} \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=\\ddot{x}^{\\mu}+\\frac{g^{\\mu \\rho}}{2}\\left(\\partial_{\\lambda} g_{\\nu \\rho}+\\partial_{\\nu} g_{\\lambda \\rho}-\\partial_{\\rho} g_{\\lambda \\nu}\\right) \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=0\n",
        "$\n",
        "\n",
        "wobei $\\Gamma_{\\lambda \\nu}^{\\mu}$ ein Christoffelsymbol 2. Art ist, welches die Abh√§ngigkeit des metrischen Tensors vom Raumzeitpunkt (Ereignis), d. h. der Kr√ºmmung der Raumzeit, charakterisiert."
      ],
      "metadata": {
        "id": "anQlQ5k0TzaJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zER_lMlN46"
      },
      "source": [
        "###### *Special: Minimal Surfaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**minimal surface**\n",
        "\n",
        "A [minimal surface](https://en.m.wikipedia.org/wiki/Minimal_surface) is a surface that locally minimizes its area.\n",
        "\n",
        "* Intro Video: https://youtu.be/_t-3lCZXlPM\n",
        "\n",
        "* Siehe auch: https://www.chemie-schule.de/KnowHow/Oberfl√§chenspannung\n",
        "\n",
        "* This is equivalent to having **zero mean curvature = second derivative is always zero at any point**\n",
        "\n",
        "* **Minimal surfaces represent the lowest energy state!** A flat plane is minimalist surface area\n",
        "\n",
        "* Erwin Schr√∂dinger used Minimal Surfaces equations in 1926 to describe the quantum state of real phsysical systems\n",
        "\n",
        "* the apparent horizon of a back whole can be are always minimal surfaces\n",
        "\n",
        "* non trivial minimal surface: [Katenoid](https://en.m.wikipedia.org/wiki/Catenoid) and a [Helicoid (Wendelfl√§che)](https://de.m.wikipedia.org/wiki/Wendelfl√§che)\n",
        "\n",
        "* For a given constraint there may also exist several minimal surfaces with different areas (for example, see [minimal surface of revolution](https://en.m.wikipedia.org/wiki/Minimal_surface_of_revolution)): the standard definitions only relate to a local optimum, not a global optimum.\n",
        "\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Catenoid.svg/600px-Catenoid.svg.png)\n",
        "\n",
        "\n",
        "Minimal surface theory originates with Lagrange who in 1762 considered the variational problem of finding the surface z = z(x, y) of least area stretched across a given closed contour. He derived the Euler‚ÄìLagrange equation for the solution\n",
        "\n",
        "> $\\frac{d}{d x}\\left(\\frac{z_{x}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)+\\frac{d}{d y}\\left(\\frac{z_{y}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)=0$\n",
        "\n",
        "He did not succeed in finding any solution beyond the plane. In 1776 Jean Baptiste Marie Meusnier discovered that the helicoid and catenoid satisfy the equation and that the differential expression corresponds to twice the mean curvature of the surface, concluding that surfaces with zero mean curvature are area-minimizing.\n",
        "\n",
        "By expanding Lagrange's equation to\n",
        "\n",
        "> $\\left(1+z_{x}^{2}\\right) z_{y y}-2 z_{x} z_{y} z_{x y}+\\left(1+z_{y}^{2}\\right) z_{x x}=0$\n",
        "\n",
        "Gaspard Monge and Legendre in 1795 derived representation formulas for the solution surfaces. While these were successfully used by Heinrich Scherk in 1830 to derive his surfaces, they were generally regarded as practically unusable. Catalan proved in 1842/43 that the helicoid is the only ruled minimal surface.\n",
        "\n",
        "Eine Minimalfl√§che ist eine Fl√§che im Raum, die lokal minimalen Fl√§cheninhalt hat. Derartige Formen nehmen beispielsweise Seifenh√§ute an, wenn sie √ºber einen entsprechenden Rahmen (wie etwa einen Blasring) gespannt sind. In mathematischer Sprache sind Minimalfl√§chen die kritischen Punkte des Fl√§cheninhaltsfunktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\int \\sqrt{g(u)} \\mathrm{d}^{n} u$\n",
        "\n",
        "\n",
        "Hierbei sind die Gr√∂√üen $g(u):=\\operatorname{det}\\left(g_{i j}(u)\\right)_{i, j=1, \\ldots, n}$ und $g_{i j}(u)=\\left(\\frac{\\partial \\mathbf{x}}{\\partial u_{i}}\\right)^{T} \\frac{\\partial \\mathbf{x}}{\\partial u_{j}}$ f√ºr $i, j=1, \\ldots, n$  erkl√§rt (vgl. Hesse-Matrix).\n",
        "\n",
        "**Man beachte, dass eine Minimalfl√§che nicht notwendig minimalen Fl√§cheninhalt hat, sondern lediglich ein station√§rer Punkt des Fl√§cheninhaltsfunktionals ist.** Man kann zeigen, dass das Verschwinden der ersten Variation des Fl√§cheninhaltsfunktionals in zwei Raumdimensionen √§quivalent zum Verschwinden der mittleren Kr√ºmmung H ist, falls die betrachtete Mannigfaltigkeit hinreichend regul√§r ist.\n",
        "\n",
        "\n",
        "**Formulierung als Variationsproblem**\n",
        "\n",
        "Eine Fl√§che ist genau dann eine [Minimalfl√§che](https://de.m.wikipedia.org/wiki/Minimalfl√§che), wenn sie an jedem Punkt die mittlere Kr√ºmmung null hat. Damit stellt sich eine Minimalfl√§che als Spezialfall einer Fl√§che vorgeschriebener mittlerer Kr√ºmmung dar. Diese entziehen sich ebenfalls nicht der Variationsrechnung, sie sind Minima des Hildebrandtschen Funktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\iint\\left(\\left|\\mathbf{x}_{u} \\times \\mathbf{x}_{v}\\right|+2\\left(Q(\\mathbf{x}), \\mathbf{x}_{u}, \\mathbf{x}_{v}\\right)\\right) \\mathrm{d} u \\mathrm{~d} v$\n",
        "\n",
        "Die Eulerschen Gleichungen als notwendige Minimalit√§tsbedingungen dieses Funktionals sind das nach Franz Rellich benannte H-Fl√§chen-System\n",
        "\n",
        "Siehe auch: [Schwarz minimal surface](https://en.m.wikipedia.org/wiki/Schwarz_minimal_surface)\n",
        "\n",
        "$\n",
        "\\Delta \\mathbf{x}=2 H \\mathbf{x}_{u} \\times \\mathbf{x}_{v}, \\quad \\mathbf{x}_{u}^{2}-\\mathbf{x}_{v}^{2}=0=\\mathbf{x}_{u} \\mathbf{x}_{v}\n",
        "$\n",
        "\n",
        "Hierbei ist $H=\\operatorname{div} Q$ die mittlere Kr√ºmmung."
      ],
      "metadata": {
        "id": "D5Q9F5n4hpl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Vektoranalysis*"
      ],
      "metadata": {
        "id": "DfJ1ATjNLmjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary*"
      ],
      "metadata": {
        "id": "xcWGR3zwT-Fo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib8wN8Qsy42W"
      },
      "source": [
        "* Die [Vektoranalysis](https://de.m.wikipedia.org/wiki/Vektoranalysis) ist ein **Teilgebiet der Tensoranalysis**, besch√§ftigt sich haupts√§chlich mit **Vektorfeldern in zwei oder mehr Dimensionen**\n",
        "\n",
        "* Die Vektoranalysis **verallgemeinert die Differential- und der Integralrechnung** (z.B. werden Verzerrungen auf Oberflachen bei der Integration berucksichtigt, oder Stroemungen bei Wegen)\n",
        "\n",
        "* Betrachtet werden **Vektorfelder**, die jedem Punkt des Raumes einen Vektor zuordnen, und **Skalarfelder**, die jedem Punkt des Raumes einen Skalar zuordnen.\n",
        "\n",
        "*  Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat.\n",
        "\n",
        "* Zusammenfassung: https://www.maths2mind.com/geometrie/vektorrechnung-ebene-im-raum/vektoranalysis\n",
        "\n",
        "*Grundbegriffe*\n",
        "\n",
        "* **Kurven** in $\\mathbb{R}^{2}$ oder $\\mathbb{R}^{3}$: Wege bzw. **parametrisierte** Kurven in $\\mathbb{R}^{n}$ sind **stetige** Abbildungen $\\gamma$ [a, b] -> $\\mathbb{R}^{n}$\n",
        "\n",
        "* **Regul√§re Wege** bzw. Kurven: **stetig + differenzierbar** und Norm der Ableitung ist die Summe der Komponenten: $\\|\\dot{\\gamma}(t)\\|^{2}=\\left|\\dot{\\gamma}_{1}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{2}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{3}(t)\\right|^{2} \\neq 0$ fur alle t $\\in$ [a,b]. Bedeutet auch: es gibt uberall einen Tangentialvektor.\n",
        "\n",
        "* **Tangentialvektor**: Ableitung / Geschwindigkeitsvektor an einem Punkt, der in eine Richtung zeigt, der tangential zur Kurve zeigt. Den Vektor normiert man (dividiert durch Norm): $T_{\\gamma}(t):=\\frac{\\dot{\\gamma}(t)}{\\|\\dot{\\gamma}(t)\\|}$\n",
        "\n",
        "* **Normalenvektor**: nur definiert in einer Ebene, also in R2. Sollte senkrecht auf der Kurve / senkrecht auf dem Tangentialvektor stehen (man muss also diesen Punkt $\\dot{\\gamma}(t)=\\left(\\begin{array}{l}\\dot{\\gamma}_{1}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$ um 90 Grad drehen, damit er senkrech steht): $N_{\\gamma}(t):=\\frac{1}{\\|\\dot{\\gamma}(t)\\|}\\left(\\begin{array}{c}-\\dot{\\gamma}_{2}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$.\n",
        "\n",
        "* **Die Norm im Normalenvektor (Jacobi-Determinante) gibt zB die Kruemmung / Verzerrung einer Flaeche an, die bei der Integration beruecksichtigt werden muss.**\n",
        "\n",
        "* Eine [**vektorielle Gr√∂√üe**](https://de.wikipedia.org/wiki/Vektorielle_Gr√∂√üe) oder gerichtete Gr√∂√üe ist eine physikalische Gr√∂√üe, die ‚Äì im Gegensatz zu den skalaren Gr√∂√üen ‚Äì einen Richtungscharakter hat.\n",
        "  * Typische vektorielle Gr√∂√üen sind die kinematischen Gr√∂√üen Geschwindigkeit und Beschleunigung, die dynamischen Gr√∂√üen Impuls und Kraft bzw. Drehimpuls und Drehmoment sowie die Feldst√§rken der elektrischen und magnetischen Felder der Elektrodynamik.\n",
        "  * Vektorielle Gr√∂√üen werden sowohl zeichnerisch als auch rechnerisch wie geometrische Vektoren behandelt, wobei einige Besonderheiten zu beachten sind."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu k√∂nnen, ist eine Basis n√∂tig. Im n-dimensionalen Raum besteht diese aus n linear unabh√§ngigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur f√ºr lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) m√∂glich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabh√§ngigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): F√ºr echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabh√§ngigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekr√ºmmt sind, sind die Koordinatenachsen nicht r√§umlich fest, wie es f√ºr kartesische Koordinaten gilt. Dies f√ºhrt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abh√§ngt ‚Äì im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenfl√§chen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgew√§hlten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "AobQvOfI9Az_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Unifying Principle: Integrating a differential operator acting on a field over a domain is the same as adding the field components along the boundary (local to global transition!)**\n",
        "\n",
        "Source: [A unified view of Vector Calculus (Stoke's Theorem, Divergence Theorem & Green's Theorem)](https://m.youtube.com/watch?v=PIoqMNL7tV0&feature=youtu.be)\n",
        "\n",
        "Fundamental Theorem of Calculus: If $f(x)$ differentiable on $[a, b]$\n",
        "\n",
        ">$\n",
        "\\int_a^b f^{\\prime}(x) d x=f(b)-f(a)\n",
        "$"
      ],
      "metadata": {
        "id": "7wCvJGyaUTuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_097.jpg)"
      ],
      "metadata": {
        "id": "Ddm461kwVgkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_095.jpg)"
      ],
      "metadata": {
        "id": "691EMDoJUEQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_096.jpg)"
      ],
      "metadata": {
        "id": "_l0OHdTfVfGz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujcsbmyfEJDt"
      },
      "source": [
        "**Koordinatentransformation**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Differentialgeometrie#Koordinatentransformationen\n",
        "\n",
        "* Fur die Berechnung des Fl√§chen- oder Volumenintegrals eine geeignete Substitutionsfunktion zu finden ist nicht trivial. Sie transformiert das Volumenintegral oft von einem Koordinatensystem in ein anderes, um die Berechnung zu vereinfachen oder √ºberhaupt zu erm√∂glichen.\n",
        "\n",
        "* Bei der Integration √ºber geometrische Objekte ist es sogar oft unpraktisch, √ºber kartesische Koordinaten zu integrieren. So l√§sst sich in der Physik das Integral √ºber ein radialsymmetrisches Potentialfeld, dessen Wert nur von einem Radius r abh√§ngt, wesentlich leichter in Kugelkoordinaten berechnen. Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$  an.\n",
        "\n",
        "* Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$ an. Nach dem [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) gilt dann in diesem Beispiel:\n",
        "\n",
        "> $\n",
        "\\int_{\\Omega} U(\\vec{r}) d V=\\int_{\\Phi^{-1}(\\Omega)} U(\\Phi(r, \\theta, \\varphi)) \\cdot|\\operatorname{det} D \\Phi(r, \\theta, \\varphi)| \\mathrm{d} r \\mathrm{~d} \\theta \\mathrm{d} \\varphi\n",
        "$\n",
        "\n",
        "* Der vektorielle Faktor ist das [Spatprodukt](https://de.wikipedia.org/wiki/Spatprodukt) aller partiellen Ableitungen von $\\vec{\\xi}(u, v, w)$\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}\n",
        "$\n",
        "\n",
        "Generell lassen sich Spatprodukte auch als Determinanten schreiben, so gilt hier:\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}=\\operatorname{det}\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\frac{\\partial \\vec{\\xi}}{\\partial v} \\frac{\\partial \\vec{\\xi}}{\\partial w}\\right)=\\operatorname{det}\\left(J_{\\vec{\\xi}}\\right)\n",
        "$\n",
        "\n",
        "Die aneinandergereihten partiellen Gradienten $(\\vec{\\xi}$ ist eine vektorwertige Funktion)\n",
        "formen gerade die Elemente der $3 \\times 3$ Jacobi-Matrix. Die zugeh√∂rige Jacobi-Determinante, auch als [**Funktionaldeterminante**](https://de.wikipedia.org/wiki/Funktionaldeterminante) bezeichnet, berechnet genau den\n",
        "zus√§tzlichen Faktor f√ºr eine Koordinatentransformation.\n",
        "\n",
        "Ist das Volumenelement skalar, reduziert sich der Faktor auf dessen euklidische Norm $\\|\\vec{N}\\| .$ Nachdem das Volumenintegral parametrisiert ist, kann mit Hilfe des [Satzes von Fubini](https://de.wikipedia.org/wiki/Satz_von_Fubini) das Integral Schritt f√ºr Schritt berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrJh93jH1D-"
      },
      "source": [
        "**Transformationssatz**\n",
        "\n",
        "* Der [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) (auch Transformationsformel) beschreibt in der Analysis das Verhalten von Integralen unter Koordinatentransformationen. Er ist somit die Verallgemeinerung der Integration durch Substitution auf Funktionen h√∂herer Dimensionen.\n",
        "\n",
        "* Der Transformationssatz wird als Hilfsmittel bei der Berechnung von Integralen verwendet, wenn sich das Integral nach √úberf√ºhrung in ein anderes Koordinatensystem leichter berechnen l√§sst.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{R}^{d}$ eine offene Menge und $\\Phi: \\Omega \\rightarrow \\Phi(\\Omega) \\subseteq \\mathbb{R}^{d}$ ein [Diffeomorphismus](https://de.wikipedia.org/wiki/Diffeomorphismus) (=eine bijektive, stetig differenzierbare Abbildung, deren Umkehrabbildung auch stetig differenzierbar ist). Dann ist die Funktion $f$ auf $\\Phi(\\Omega)$ genau dann integrierbar, wenn die Funktion $x \\mapsto f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))|$ auf $\\Omega$ integrierbar ist. In diesem Fall gilt:\n",
        "\n",
        "> $\n",
        "\\int_{\\Phi(\\Omega)} f(y) \\mathrm{d} y=\\int_{\\Omega} f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))| \\mathrm{d} x\n",
        "$\n",
        "\n",
        "* Dabei ist $D \\Phi(x)$ die Jacobi-Matrix und $\\operatorname{det}(D \\Phi(x))$ die Funktionaldeterminante von $\\Phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUDAAXziE4Pj"
      },
      "source": [
        "**Funktionaldeterminante**\n",
        "\n",
        "* Die [Funktionaldeterminante oder Jacobi-Determinante](https://de.wikipedia.org/wiki/Funktionaldeterminante) ist eine mathematische Gr√∂√üe, die in der mehrdimensionalen Integralrechnung, also der Berechnung von **Oberfl√§chen- und Volumenintegralen**, eine Rolle spielt. Insbesondere findet sie in der [Fl√§chenformel](https://de.wikipedia.org/wiki/Fl√§chenformel) und dem aus dieser hervorgehenden Transformationssatz Verwendung.\n",
        "\n",
        "* [Exkurs Determinante](https://de.wikipedia.org/wiki/Determinante): *die Determinante eine Zahl (ein Skalar), die einer quadratischen Matrix zugeordnet wird und aus ihren Eintr√§gen berechnet werden kann. Sie gibt an, wie sich das **Volumen (oder der Fl√§cheninhalt)** bei der durch die Matrix beschriebenen linearen Abbildung √§ndert*\n",
        "\n",
        "* **Lokales Verhalten einer Funktion**: Die Funktionaldeterminante gibt zu einem gegebenen Punkt wichtige Informationen √ºber das Verhalten der Funktion $f$ in der N√§he dieses Punktes.\n",
        "\n",
        "  * Wenn beispielsweise die Funktionaldeterminante einer stetig differenzierbaren Funktion in einem Punkt $p$ ungleich null ist, so ist die Funktion in einer Umgebung von $p$ invertierbar.\n",
        "\n",
        "  * Weiterhin gilt, dass bei positiver Determinante in $p$ die Funktion ihre Orientierung beibeh√§lt und bei negativer Funktionaldeterminante die Orientierung umkehrt.\n",
        "\n",
        "  * Der absolute Wert der Determinante im Punkt $p$ gibt den Wert an, mit dem die Funktion in der N√§he von $p$ expandiert oder schrumpft.\n",
        "\n",
        "* F√ºr eine differenzierbare Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ ist die Funktionaldeterminante definiert als die Determinante der Jacobi-Matrix von $f,$ also als\n",
        "det $D f(x)$\n",
        "mit\n",
        "\n",
        "> $\n",
        "D f(x)=\\left(\\frac{\\partial f_{i}}{\\partial x_{j}}(x)\\right)_{i, j=1, \\ldots, n}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Aix1sXHVnV"
      },
      "source": [
        "**Beispiel: Polarkoordinaten**\n",
        "\n",
        "**1. Koordinatentransformation**: Die Umrechnungsformeln von Polarkoordinaten in kartesische Koordinaten lauten:\n",
        "\n",
        "> $\n",
        "\\begin{array}{l}\n",
        "x=r \\cos \\varphi \\\\\n",
        "y=r \\sin \\varphi\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**2. Funktionaldeterminante** lautet also:\n",
        "\n",
        "> $\n",
        "\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}=\\operatorname{det}\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial \\varphi} \\\\\n",
        "\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial \\varphi}\n",
        "\\end{array}\\right)=\\operatorname{det}\\left(\\begin{array}{cc}\n",
        "\\cos \\varphi & -r \\sin \\varphi \\\\\n",
        "\\sin \\varphi & r \\cos \\varphi\n",
        "\\end{array}\\right)=r \\cdot(\\cos \\varphi)^{2}+r \\cdot(\\sin \\varphi)^{2}=r\n",
        "$\n",
        "\n",
        "**3. Fl√§chen- oder Volumenintegral**: Folglich ergibt sich f√ºr das Fl√§chenelement $\\mathrm{d} A$ (alternativ kann man bei dreidimensionalen Kugelkoordinaten an dieser Stelle auch das Volumenelement $\\mathrm {d} V$ mit der Funktionaldeterminante berechnen):\n",
        "\n",
        "> $\n",
        "\\mathrm{d} A=\\left|\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}\\right| \\mathrm{d} r \\mathrm{~d} \\varphi=r \\mathrm{~d} r \\mathrm{~d} \\varphi\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Felder*"
      ],
      "metadata": {
        "id": "aGvecdZZd-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbKijNH7RjtR"
      },
      "source": [
        "*Skalarfeld*\n",
        "\n",
        "* In der mehrdimensionalen Analysis, der Vektorrechnung und der Differentialgeometrie ist ein [skalares Feld (kurz Skalarfeld)](https://de.wikipedia.org/wiki/Skalarfeld) $\\varphi$ **eine Funktion, die jedem Punkt eines Raumes eine reelle Zahl (Skalar) zuordnet**, z. B. eine Temperatur, Luftdruck.\n",
        "\n",
        "* Wichtige Operationen im Zusammenhang mit Skalarfeldern sind:\n",
        "\n",
        "  * **[Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) eines Skalarfeldes, der ein Vektorfeld ist**.\n",
        "\n",
        "  * [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) eines Skalarfeldes: die Richtungsableitung einer von mehreren Variablen abh√§ngigen Funktion ist die **momentane √Ñnderungsrate dieser Funktion in einer durch einen Vektor vorgegebenen Richtung**. (Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale R√§ume ist das [G√¢teaux-Differential](https://de.m.wikipedia.org/wiki/G%C3%A2teaux-Differential).)\n",
        "  * Ein Skalarfeld ist das einfachste [Tensorfeld](https://de.m.wikipedia.org/wiki/Tensorfeld).\n",
        "\n",
        "* Skalarfelder sind von gro√üer Bedeutung in der Feldbeschreibung der Physik und in der mehrdimensionalen Vektoranalysis.\n",
        "\n",
        "* Man unterscheidet dabei zwischen **reellwertigen** Skalarfeldern $\\varphi\\colon M\\to \\mathbb {R}$ und **komplexwertigen** Skalarfeldern $\\displaystyle \\varphi \\colon M\\to \\mathbb {C} $.\n",
        "\n",
        "* Man spricht von einem **station√§ren Skalarfeld**, wenn die Funktionswerte nur vom Ort abh√§ngen. H√§ngen sie auch von der Zeit ab, handelt es sich um ein **instation√§res Skalarfeld**.\n",
        "\n",
        "* Beispiele f√ºr Skalarfelder in der Physik sind der Luftdruck, die Temperatur, Dichte oder allgemein **Potentiale (= Skalarpotentiale)**.\n",
        "\n",
        "* Im Gegensatz zum Skalarfeld ordnet ein Vektorfeld jedem Punkt einen Vektor zu. Ein Skalarfeld ist das einfachste Tensorfeld\n",
        "\n",
        "* Ein **Niveaufeld** ist ein Feld, wo √ºberall die gleiche skalare Gr√∂sse vorliegt (zB die gleiche Temperatur ohne √Ñnderungen (Steigungen, Senken)) = english: Level Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vektorfeld*\n",
        "\n",
        "If you give a **magnitude and direction** at any given point, then you get a [vector field](https://en.m.wikipedia.org/wiki/Vector_field)\n",
        "\n",
        "Vector Fields in 2D: https://www.geogebra.org/m/cXgNb58T\n",
        "\n",
        "Vector field 3D: https://www.geogebra.org/m/u3xregNW\n",
        "\n",
        "**Der Gradient eines Skalarfeldes ist ein Vektorfeld (zB: Skalar ist die Temperatur in einem Pool, und Temperaturveranderung uber Zeit ist die erste Ableitung dessen und dann ein Vektorfeld), und die zweite Ableitung ist die Beschleunigung der Temperaturveranderung.**\n",
        "\n",
        "* Beispiele von Vektorfeldern: \n",
        "  * **Gradientenfeld** (das von einer Punktquelle nach allen Seiten gleichm√§√üig flie√üende Feld einer Str√∂mung und das elektrische Feld um eine Punktladung - Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde**, bzw. ‚Äì k√ºrzer formuliert ‚Äì der Gradient des Skalarfelds (= **ein Skalarfeld, das nach dem Ort abgeleitet wird, ist ein Gradientenfeld**, mit Skalarpotenzial inkl. angegeben).\n",
        "  * **Zentralfelder**: ein Intervall, welches die Null enth√§lt(Gravitationsfeld), \n",
        "  * **\"Wirbelfelder\"** (das in Kreislinien um den Ausfluss einer , Badewanne\" herumwirbelnde Str√∂mungsfeld, oder das Magnetfeld um einen stromdurchflossenen Draht.)\n",
        "\n",
        "* In der mehrdimensionalen Analysis und der Differentialgeometrie ist ein [Vektorfeld](https://de.m.wikipedia.org/wiki/Vektorfeld) eine Funktion, die jedem Punkt eines Raumes einen Vektor zuordnet.\n",
        "\n",
        "* Eine Abbildung $V: D \\rightarrow \\mathbb{R}^{n}, D \\subseteq \\mathbb{R}^{n}$ mit n = 1,2,3..\n",
        "\n",
        "* Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat.\n",
        "\n",
        "* Meist sind Vektorfelder stetig differenzierbar = Komponenten sind stetig differenzierbar, zB Vektor $v1$ mit (x,y,z) und ihren drei Ableitungen als Komponente des Vektorfeldes $V$\n",
        "\n",
        "* Das duale Konzept zu einem Vektorfeld ist eine Funktion, die jedem Punkt eine [Linearform](https://de.m.wikipedia.org/wiki/Linearform), [zB [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale)] zuordnet, eine solche Abbildung wird [pfaffsche Form](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) (One-Form) genannt. (Pfaffsche Formen sind die nat√ºrlichen Integranden f√ºr Wegintegrale.)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_029.jpg)\n",
        "\n",
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* siehe mehr unten unter \"Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus\"\n",
        "\n",
        "**Vektorfelder, die Gradienten eines Skalarfelds sind**, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als **konservative Vektorfelder** bezeichnet - ihnen allen gemeinsam sind dabei die folgenden drei einander √§quivalenten Eigenschaften:\n",
        "\n",
        "1. **Wegunabh√§ngigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve $S$ innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abh√§ngig, nicht dagegen von ihrer L√§nge.\n",
        "\n",
        "2. **Verschwinden des Ringintegrals f√ºr beliebige Randkurven** $S$ :\n",
        "$\n",
        "\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0\n",
        "$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit** des Feldes:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$\n"
      ],
      "metadata": {
        "id": "VpsrL-yaTDrr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9TPhq5fEux"
      },
      "source": [
        "*Fundamentalzerlegung (Fundamentalsatz der Vektoranalysis)*\n",
        "\n",
        "* der [Helmholtzscher Zerlegungssatz](https://de.m.wikipedia.org/wiki/Helmholtz-Theorem) ist der Fundamentalsatz der Vektoranalysis. Beschreibt den allgemeinen Fall.\n",
        "\n",
        "* Jedes Vektorfeld $\\vec{F}$ l√§sst sich als eine √úberlagerung eines Quellenanteils $\\vec{F}_{Q}$ und eines Wirbelanteils $\\vec{F}_{W}$ beschreiben."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Potentiale*"
      ],
      "metadata": {
        "id": "rc8jFaF5d0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Skalarpotential*\n",
        "\n",
        "> [Skalarpotenzial](https://de.wikipedia.org/wiki/Skalarpotential) ist ein Ma√ü f√ºr die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein K√∂rper entgegen der wirkenden Kraft bewegt, dann erh√∂ht sich seine potenzielle Energie.\n",
        "\n",
        "Das [Skalarpotential](https://de.wikipedia.org/wiki/Skalarpotential), oft einfach auch nur Potential genannt, ist in der Mathematik ein - im Unterschied zum Vektorpotential - skalares Feld $\\Phi(\\vec{r})$, dessen Gradient gem√§√ü folgender Formel\n",
        "\n",
        "> $\n",
        "\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "ein als \"Gradientenfeld\" genanntes Vektorfeld $\\vec{F}(\\vec{r})$ liefert.\n",
        "\n",
        "Kurz: **Die erste Ableitung des Skalarpotentials (= ein Skalarfeld) ergibt das Gradientenfeld (= ein spezielles Vektorfeld).**\n",
        "\n",
        "* Ist $\\vec{F}(\\vec{r})$ ein **konservatives** Kraftfeld, in dem die Kraft $\\vec{F}$ dem Prinzip des kleinsten Zwanges folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Definition\n",
        "$\n",
        "\\vec{F}(\\vec{r})=-\\operatorname{grad} \\Phi(\\vec{r})=-\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Skalarpotentiale bilden u. a. die mathematische Grundlage der Untersuchung konservativer Kraftfelder wie des elektrischen und des Gravitationsfelds, aber auch von wirbelfreien sogenannten Potentialstr√∂mungen.\n",
        "\n",
        "**Ein Skalarfeld $\\Phi: \\vec{r} \\mapsto \\Phi(\\vec{r})$ ist genau dann ein Skalarpotential**, wenn es in einem einfach zusammenh√§ngenden Gebiet\n",
        "\n",
        "1. zweimal stetig differenzierbar ist, das hei√üt keine , Spr√ºnge\", Stufen oder andere\n",
        "Unstetigkeitsstellen enth√§lt;\n",
        "\n",
        "2. zu ihm ein Vektorfeld $\\vec{F}: \\vec{r} \\mapsto \\vec{F}(\\vec{r})$ existiert, so dass gilt:\n",
        "$\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})$\n",
        "\n",
        "$\\vec{F}$ wird daher oft auch das zugeh√∂rige Gradientenfeld genannt, das als Gradient des Skalarpotentials $\\Phi$ seinerseits stets folgende Bedingungen erf√ºllt:\n",
        "\n",
        "1. **Wegunabh√§ngigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve S innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abh√§ngig, nicht dagegen von ihrer L√§nge.\n",
        "\n",
        "2. **Verschwinden des geschlossenen Kurvenintegrals f√ºr beliebige Randkurven S**:\n",
        "$\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit des Feldes**:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/d/d9/GravityPotential.jpg)\n",
        "\n",
        "*Das Gravitationspotential einer homogenen Kugel*"
      ],
      "metadata": {
        "id": "87gT5L69aUhd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2pH152ScafZ"
      },
      "source": [
        "*Vektorpotential*\n",
        "\n",
        "**Ist ein Vektorfeld $v$ das Gradientenfeld einer Funktion $f,$ das hei√üt $v=\\nabla f$  (=Skalarfeld ist abgeleitet nach Ort), so bezeichnet man $f$ als Potential**. [Vektorpotential](https://de.wikipedia.org/wiki/Vektorpotential)\n",
        "\n",
        "* Wirbelfelder, die Rotationen eines anderen Vektorfelds sind, sind stets quellenfrei ‚Äì quellenfreie Vektorfelder k√∂nnen daher umgekehrt immer auch als Rotation eines anderen Vektorfelds interpretiert werden, das man in diesem Fall als ‚ÄûVektorpotential‚Äú des betreffenden quellenfreien Vektorfelds bezeichnet\n",
        "\n",
        "* Mathematisch ist das Vektorpotential (im Unterschied zum Skalarpotential) ein Vektorfeld $\\mathbf{A}(\\mathbf{r}),$ dessen Rotation ein zweites Vektorfeld $\\mathbf{B}(\\mathbf{r})$ liefert gem√§√ü folgender Formel:\n",
        "\n",
        "> $\n",
        "\\mathbf{B}(\\mathbf{r}) \\stackrel{\\text { def }}{=} \\operatorname{rot} \\mathbf{A}(\\mathbf{r})=\\nabla \\times \\mathbf{A}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "Vektorpotentiale lassen sich u. a. dazu verwenden, die zur Beschreibung des elektromagnetischen Felds verwendeten Maxwell-Gleichungen zu entkoppeln und dadurch leichter l√∂sbar zu machen.\n",
        "\n",
        "Obwohl es zun√§chst nur als mathematisches Hilfsmittel eingef√ºhrt wurde, kommt ihm in der Quantenmechanik physikalische Realit√§t zu, wie das [Aharonov-Bohm-Experiment](https://de.wikipedia.org/wiki/Aharonov-Bohm-Effekt) zeigte.\n",
        "\n",
        "**Beziehungen zwischen Vektor- und Skalarpotential**: Gem√§√ü dem helmholtzschen Theorem kann (fast) jedes Vektorfeld $\\mathrm{K}(\\mathrm{r})$\n",
        "\n",
        "* als **Superposition zweier Komponenten $\\mathbf{F}(\\mathbf{r})$ und $\\mathbf{G}(\\mathbf{r})$ aufgefasst werden**,\n",
        "\n",
        "* deren erste der Gradient eines Skalarpotentials $\\Phi(\\mathbf{r})$ ist, die zweite dagegen die Rotation eines Vektorpotentials $\\mathbf{\\Gamma}(\\mathbf{r}):$\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "* Ist $\\mathbf{F}(\\mathbf{r})$ ein konservatives Kraftfeld, in dem die Kraft $\\mathbf{F}$ dem [Prinzip des kleinsten Zwanges](https://de.wikipedia.org/wiki/Prinzip_des_kleinsten_Zwanges) folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Schreibweise\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=-\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=-\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "https://de.wikipedia.org/wiki/Skalarpotential#Beziehungen_zwischen_Skalar-_und_Vektorpotential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vCM9-DX-j2"
      },
      "source": [
        "###### *Differentialoperator*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpGcMJaKroS6"
      },
      "source": [
        "**Differentialoperator**\n",
        "\n",
        "Die drei kovarianten Differentialoperatoren (Covector-Maps!)\n",
        "\n",
        "> **Folgende drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei r√§umlicher Drehung des urspr√ºnglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz **spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden**. Diese Eigenschaft folgt aus den **koordinatenunabh√§ngigen** Definitionen.\n",
        "\n",
        "**Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.).\n",
        "\n",
        "Als Differentialoperator kann er beispielsweise auf ein Skalarfeld angewandt werden und wird in diesem Fall ein Vektorfeld liefern, das Gradientenfeld genannt wird.\n",
        "\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld**, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde, bzw. ‚Äì k√ºrzer formuliert ‚Äì der Gradient des Skalarfelds. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "\n",
        "  * [Rotation (curl)](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. Curl = Circulation Density of a Vector Field (it's a velocity field). Siehe auch [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation#Drehung_(Rotation)) sowie [Drehmatrix](https://de.wikipedia.org/wiki/Drehmatrix). $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "*Depiction of a two-dimensional vector field with a uniform curl.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1555.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nabla-Operator (Del operator)*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Del\n",
        "\n",
        "The differential operator del, also called nabla, is an important vector differential operator. It appears frequently in physics in places like the differential form of Maxwell's equations. In three-dimensional Cartesian coordinates, del is defined as\n",
        "\n",
        "> $\\nabla=\\hat{\\mathbf{x}} \\frac{\\partial}{\\partial x}+\\hat{\\mathbf{y}} \\frac{\\partial}{\\partial y}+\\hat{\\mathbf{z}} \\frac{\\partial}{\\partial z}$\n",
        "\n",
        "Del defines the gradient, and is used to calculate the curl, divergence, and Laplacian of various objects.\n",
        "\n",
        "Koordinatenunabh√§ngige Definition mit dem Nabla-Operator\n",
        "\n",
        "* Der [Nabla-Operator](https://de.wikipedia.org/wiki/Nabla-Operator) ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, um kontextabh√§ngig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren.\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der Nabla-Operator ist auch in anderen Koordinatensystemen definiert und so kann mit ihm zum Beispiel die Rotation [koordinatenunabh√§ngig](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes#Koordinatenunabh√§ngige_Definition_mit_dem_Nabla-Operator) durch\n",
        "\n",
        "> $\\operatorname{rot} \\vec{F}:=\\nabla \\times \\vec{F}$\n",
        "\n",
        "definiert werden. Mit dem Nabla-Operator k√∂nnen auch der Gradient- sowie die Divergenz eines Vektorfeldes dargestellt und Produktregeln hergeleitet werden.\n",
        "\n",
        "Formal ist der Nabla-Operator ein Vektor, dessen Komponenten die partiellen\n",
        "Ableitungsoperatoren $\\frac{\\partial}{\\partial x_{i}}$ sind:\n",
        "\n",
        "> $\n",
        "\\vec{\\nabla}=\\left(\\frac{\\partial}{\\partial x_{1}}, \\ldots, \\frac{\\partial}{\\partial x_{n}}\\right)\n",
        "$"
      ],
      "metadata": {
        "id": "ARBZopZZd73L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSvDHxVmgzEU"
      },
      "source": [
        "*Laplace Operator*\n",
        "\n",
        "Der [**Laplace-Operator**](https://de.wikipedia.org/wiki/Laplace-Operator) ist ein linearer Differentialoperator innerhalb der [mehrdimensionalen Analysis](https://de.wikipedia.org/wiki/Analysis#Mehrdimensionale_reelle_Analysis) ($\\Delta \\colon D(\\Delta )\\to L^{2}(\\mathbb{R} ^{n})$ und ein unbeschr√§nkter Operator). Der Laplace-Operator kommt in vielen Differentialgleichungen vor, die das Verhalten physikalischer Felder beschreiben. Beispiele sind die Poisson-Gleichung der Elektrostatik, die **Navier-Stokes-Gleichungen** f√ºr Str√∂mungen von Fl√ºssigkeiten oder Gasen und die Diffusionsgleichung f√ºr die W√§rmeleitung.\n",
        "\n",
        "Der Laplace-Operator ordnet einem zweimal differenzierbaren Skalarfeld $f$ **die Divergenz seines Gradienten zu**,\n",
        "\n",
        ">$\n",
        "\\Delta f=\\operatorname{div}(\\operatorname{grad} f)\n",
        "$\n",
        "\n",
        "oder mit dem Nabla-Operator notiert\n",
        "\n",
        ">$\n",
        "\\Delta f=\\nabla \\cdot(\\nabla f)=(\\nabla \\cdot \\nabla) f=\\nabla^{2} f\n",
        "$\n",
        "\n",
        "**Laplacian in the Heat Equation**\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flow & Flux*"
      ],
      "metadata": {
        "id": "6Nb2j8QPdg3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\vec{F}=\\langle M, N\\rangle$ be a vector field with continuous components defined on a smooth curve $C$, parameterized by $\\vec{r}(t)=\\langle f(t), g(t)\\rangle$, let $\\vec{T}$ be the unit tangent vector of $\\vec{r}(t)$, and let $\\vec{n}$ be the clockwise $90^{\\circ}$ degree rotation of $\\vec{T}$.\n",
        "\n",
        "$\\vec{F}$ - Vectorfield\n",
        "\n",
        "$\\vec{T}$ - Curve or Path\n",
        "\n",
        "$ds$ - Weg√§nderung / Wegl√§nge\n",
        "\n",
        "**The flow of $\\vec{F}$ along $C$ is**\n",
        "\n",
        "> $\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{T} \\mathrm{~d} s=\\int_{C} \\vec{F} \\cdot \\overrightarrow{d r} .\n",
        "$\n",
        "\n",
        "* Flow - the degree to which my path is aligned with the vectorfield = Tangential to curve. Flow (special case: Circulation).\n",
        "\n",
        "**The [flux](https://en.m.wikipedia.org/wiki/Flux) of $\\vec{F}$ across $C$ is**\n",
        "\n",
        ">$\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{n} \\mathrm{~d} s=\\int_{C} M \\mathrm{~d} y-N \\mathrm{~d} x=\\int_{C}\\left(M g^{\\prime}(t)-N f^{\\prime}(t)\\right) d\n",
        "$\n",
        "\n",
        "* Flow - the degree to which my path is normal to the vectorfield = Normal to curve\n",
        "\n",
        "Vector Fields can represent different things, like force fields. Here we consider velcoity fields, like in some turbulent water or wind system. The vector tells you magnitude and direction water or wind is flowing at that particular point.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1556.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "zBJu_o0zwhge"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolnoE19IyYG"
      },
      "source": [
        "###### *Kurvenintegral (Line Integral)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [What is a LINE INTEGRAL? // Big Idea, Derivation & Formula](https://www.youtube.com/watch?v=WA5_a3C2iqY&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=3)\n",
        "\n",
        "**Now what is the area of this rectangle? How can we calculate that?**\n",
        "\n",
        "* the height is just the function value at whatever particular point you are at $f\\left(x_{k}, y_{k}\\right)$\n",
        "\n",
        "* what is the size of the base? It's $\\Delta s_k$, which is the Arc length\n",
        "\n",
        "Standard definition of the [line integral](https://en.m.wikipedia.org/wiki/Line_integral)\n",
        "\n",
        "> $\\Delta A_{k}=f\\left(x_{k}, y_{k}\\right) \\Delta s_{k}$\n",
        "\n",
        "wird zu:\n",
        "\n",
        "> $\\int_{C} f(x, y) d s$\n",
        "\n",
        "> $= \\lim _{n \\rightarrow \\infty} \\sum_{k=1}^{\\infty} f(x_{k}, y_{k}) \\Delta s_{k}$\n",
        "\n",
        "(standard Riemann integral definition)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1557.png)"
      ],
      "metadata": {
        "id": "remNbl5LQNZE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEoDCIYLOem"
      },
      "source": [
        "* z.B. zur Berechnung des Umfangs eines Objekts. welche Kraft ist noetig, um durch ein Vektorfeld auf einer Kurve entlang zu gehen?\n",
        "\n",
        "* Das [Kurven-, Linien-, Weg- oder Konturintegral](https://de.wikipedia.org/wiki/Kurvenintegral) erweitert den gew√∂hnlichen Integralbegriff f√ºr die Integration\n",
        "\n",
        "  * in der komplexen Ebene (Funktionentheorie) oder\n",
        "\n",
        "  * im mehrdimensionalen Raum (Vektoranalysis).\n",
        "\n",
        "* Den Weg, die Linie oder die Kurve, √ºber die integriert wird, nennt man den Integrationsweg.\n",
        "\n",
        "* Wegintegrale √ºber geschlossene Kurven werden auch als Ringintegral, Umlaufintegral oder Zirkulation bezeichnet und mit dem Symbol\n",
        "‚àÆ bzw. $\\textstyle \\oint$  geschrieben.\n",
        "\n",
        "Auch Linienintegral. Berechne z.B. den kurzesten Weg zwischen zwei Punkten unter Berucksichtigung der Geschwindigkeit (eine gerade Linie ist nicht immer der kurzeste Weg oder ein moglicher Weg).\n",
        "\n",
        "*The line integral over a scalar field f can be thought of as the area under the curve C along a surface z = f(x,y), described by the field*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1558.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_lY1plvJOZ-"
      },
      "source": [
        "**Kurvenintegral 1. Art (√ºber Skalarfelder, nicht-orientiert)**\n",
        "\n",
        "* zB zur Berechnung der Masse eines Drahtes [entlang einer Helix](https://www.youtube.com/watch?v=8XcqTg1NPKg) mit einer gegebenen Dichte\n",
        "\n",
        "Wegintegral erster Art ist das **Wegintegral einer stetigen Funktion**, $\n",
        "f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\n",
        "$ entlang eines st√ºckweise stetig differenzierbaren Weges $\n",
        "\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}\n",
        "$ ist definiert als\n",
        "\n",
        "> $\\int_{\\mathcal{C}} f \\mathrm{~d} s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\|_{2} \\mathrm{~d} t$\n",
        "\n",
        "F√ºr eine stetige Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ und einem regul√§ren Weg $\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}$ definiert man das Kurvenintegral von $f$ l√§ngs $\\gamma$ durch: $\\int_{\\gamma} f d s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\| d t$\n",
        "\n",
        "  * $ds$ sowie $ \\|\\dot{\\gamma}(t)\\| d t$ sind das '**Linienelement**'\n",
        "\n",
        "  * $ \\|\\dot{\\gamma}(t)\\|$ ist die Norm von der Ableitung, die man berucksichtigen muss beim Integrieren wie schnell man durch die Kurve lauft (Gewichtungsfaktor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvuqF-aJDBO"
      },
      "source": [
        "**Kurvenintegral 2. Art (√ºber Vektorfelder, orientiert)**\n",
        "\n",
        "* nicht mehr skalare, sondern vektorielle Funktion integrieren (Vektorfeld)\n",
        "\n",
        "* ps: jedes Kurvenintegral zweiter Art ist auch ein Kurvenintegral erster Art\n",
        "\n",
        "* z.B. um eine Arbeit, Zirkulation oder elektrische Spannung [zu berechnen](https://www.youtube.com/watch?v=HmgkyI_Q0Oo)\n",
        "\n",
        "* nennt man daher auch \"Arbeitsintegral\", wenn man sich zB ein Kraftfeld v vorstellt. Oder Zirkulation mit Integral entlang des Weges berechnen, wenn v ein Geschwindigkeitsfeld ist. Oder elektrische Spannung, wenn es ein elektrisches Feld ist.\n",
        "\n",
        "* Zun√§chst fragen: hat v ein Skalarpotential? Ist die betrachtete Menge einfach zusammenh√§ngend und ist zB die Rotation des Vektorfeldes gleich der Nullvektor? Berechnung des Skalarpotentials: mit der Ansatzmethode oder mit der Kurvenintegralmethode\n",
        "\n",
        "* Danach fragen, ob der Weg geschlossen oder offen ist? (geschlossen: Anfangspunkt = Endpunkt, wie bei Kreis oder Dreieck). Ist er geschlossen, ist der Wert des Kurvenintegrals gleich Null. (**Remember**: Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ f√ºr beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQdL1yhzPGzI"
      },
      "source": [
        "###### *Oberfl√§che (Surface Area)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Introduction (Explicit, Implicit and Parametric)*"
      ],
      "metadata": {
        "id": "Rs9afq_oyhKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Surface_integral"
      ],
      "metadata": {
        "id": "EsFuukjmxCb8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChIebVaPLd6"
      },
      "source": [
        "* z.B. zur Berechnung der Oberfl√§che eines Balls, Funktion der Temperatur an jedem Punkt an einer Oberfl√§che, um die durchschnittliche Temperatur auf der Oberfl√§che zu berechnen (https://youtu.be/AIxiYG-gZ00 at min 3:00)\n",
        "\n",
        "* Durch Parametrisierung wird z.B. die Kruemmung einer Ebene in $R^3$ beruecksichtigt. Man berechnet naemlich die Flaeche, in dem man von $R^3$ auf $R^2$ projiziert und dann das Integral berechnet (das geht, weil Flaeche in $R^3$ ist offen, stetig differenzierbar und bijektiv)\n",
        "\n",
        "* Das Oberfl√§chenintegral oder Fl√§chenintegral ist eine Verallgemeinerung des Integralbegriffes auf ebenen oder gekr√ºmmten Fl√§chen. Das Integrationsgebiet $\\mathcal{F}$ ist also nicht ein eindimensionales Intervall, sondern eine zweidimensionale Menge im dreidimensionalen Raum $\\mathbb{R}^{3}$. F√ºr eine allgemeinere Darstellung im $\\mathbb{R}^{n}$ mit $n \\geq 2$ siehe: Integration auf Mannigfaltigkeiten.\n",
        "\n",
        "Es wird generell zwischen einem skalaren und einem vektoriellen Oberfl√§chenintegral unterschieden, je nach Form des Integranden und\n",
        "des sogenannten Oberfl√§chenelements. Sie lauten\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} f \\mathrm{~d} \\sigma$ mit skalarer Funktion $f$ und skalarem Oberfl√§chenelement $\\mathrm{d} \\sigma$ sowie\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} \\vec{v} \\cdot \\mathrm{d} \\vec{\\sigma}$ mit vektorwertiger Funktion $\\vec{v}$ und vektoriellem Oberfl√§chenelement $\\mathrm{d} \\vec{\\sigma}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcvZH7qLBOzs"
      },
      "source": [
        "Allgemein l√§sst sich eine Fl√§che im $\\mathbb{R}^{3}$ mit zwei Parametern $u$ und $v$ in\n",
        "folgender Form darstellen:\n",
        "\n",
        "> $\n",
        "\\varphi: B \\rightarrow \\mathbb{R}^{3}, \\quad(u, v) \\mapsto \\vec{\\varphi}(u, v)=\\left(\\begin{array}{l}\n",
        "x(u, v) \\\\\n",
        "y(u, v) \\\\\n",
        "z(u, v)\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Auf der Fl√§che $\\vec{\\varphi}(u, v)$ bilden die Kurvenscharen $u=$ const bzw. $v=$ const die Koordinatenlinien. Diese √ºberziehen die Fl√§che mit einem Koordinatennetz.\n",
        "wobei durch jeden Punkt zwei Koordinatenlinien verlaufen. Somit hat ieder Punkt auf der Fl√§che eindeutige Koordinaten $\\left(u_{0}, v_{0}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45jm0deB13y"
      },
      "source": [
        "Mit den Parametrisierungen und den Oberfl√§chenelementen kann man nun die Oberfl√§chenintegrale definieren. Diese mehrdimensionalen Integrale sind Lebesgue-Integrale, k√∂nnen aber in den meisten Anwendungsf√§llen als mehrfache Riemann-Integrale berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yO9AukUtSzo"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Oberfl√§chenintegral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction Video: [Describing Surfaces Explicitly, Implicitly & Parametrically // Vector Calculus](https://youtu.be/jZRqCfi5_Uo)"
      ],
      "metadata": {
        "id": "8vVdeCClvrPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_047.PNG)"
      ],
      "metadata": {
        "id": "2l2A7LKvxN6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_048.PNG)"
      ],
      "metadata": {
        "id": "Vy8LqMptxP8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_049.PNG)"
      ],
      "metadata": {
        "id": "txf7BHR5xRyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_050.PNG)"
      ],
      "metadata": {
        "id": "NOOvynWUxTWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_051.PNG)"
      ],
      "metadata": {
        "id": "shyAhgDcxUy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this a transformation. The parametrization is a way to map the simple rectangle case to the complex cone case:"
      ],
      "metadata": {
        "id": "HbWGt8pbwJIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_052.PNG)"
      ],
      "metadata": {
        "id": "0nkMWGDIxXS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Calculate surface area*"
      ],
      "metadata": {
        "id": "9F9ROmFYzbtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Surface Area = $\\int_{c}^{d} \\int_{a}^{b}\\left|\\vec{r}_{u} \\times \\vec{r}_{v}\\right| d u \\, d v$\n",
        "\n",
        "* ich parametrize zwischen a und b (zB zwischen 0 und 2 $\\pi$ fur $\\theta$) bzw c und d (zB zwischen 0 und $\\pi$ fur $\\phi$)\n",
        "\n",
        "* ich bestimme das partielle Differential von ${r}$ with respect to $_{u}$ und dann nochmal das partielle Differential von ${r}$ with respect to $_{v}$\n",
        "\n",
        "* dann nehme ich beide partiellen Differentiale und berechne das Cross Product"
      ],
      "metadata": {
        "id": "Imn0bOPskCfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/hnjfY9hRIXk"
      ],
      "metadata": {
        "id": "jPhsipagzjxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I can describe a surface parametrically, that means there is somewhere there is a u/v coordinate system with a region. And there is a map / transformation to a more squiggely version in 3D for example:"
      ],
      "metadata": {
        "id": "ZRcm323Cz4qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_053.PNG)"
      ],
      "metadata": {
        "id": "zxlu5_9s2CZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r_u}$ is the partial derivative in the direction where $v$ is constant\n",
        "\n",
        "$\\vec{r_v}$ is the partial derivative in the direction where $u$ is constant\n",
        "\n",
        "And above pointing is the cross product of $\\vec{r_u}$  with $\\vec{r_v}$"
      ],
      "metadata": {
        "id": "DorAizAb1CDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now comes the most important: the length of the cross product is the same as the area of the parallelogram of the two vectors that form the cross product. $\\rightarrow$ With that I can approximate the surface area when I take the limit (and takes the definition of an integral)."
      ],
      "metadata": {
        "id": "Mp_2P1Y_2UAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_054.PNG)"
      ],
      "metadata": {
        "id": "skuzWvoB3IIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here you get the final formula:"
      ],
      "metadata": {
        "id": "2DXWCQfO3iWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_055.PNG)"
      ],
      "metadata": {
        "id": "g57LTd053g-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 1: Calculation for a Sphere*"
      ],
      "metadata": {
        "id": "sOKlhEom4-yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/iTkdDnC0seQ"
      ],
      "metadata": {
        "id": "IE91hyiV3vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see example calculation here for the surface of a sphere:"
      ],
      "metadata": {
        "id": "o_WQFs7q3ndU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_058.PNG)"
      ],
      "metadata": {
        "id": "IbRBNcSD5TG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First we take spherical coordinate, because it's more natural for a sphere"
      ],
      "metadata": {
        "id": "W4da3T_U34-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_057.PNG)"
      ],
      "metadata": {
        "id": "BEYNs3Ty5Q0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_056.PNG)"
      ],
      "metadata": {
        "id": "lOWlLVQL5Pvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_059.PNG)"
      ],
      "metadata": {
        "id": "lqQekDTy5U8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r}_{\\phi}$ und $\\vec{r}_{\\theta}$ sind die partial derivatives with respect ti phi and theta jeweils\n",
        "\n",
        "$\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}$ ist das cross product (das ist die Determinante)\n",
        "\n",
        "Dann nehme ich davon $\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right|$ (square root), um die Fl√§che der Oberfl√§che zu erhalten"
      ],
      "metadata": {
        "id": "9ueegH1T5ttN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_060.PNG)"
      ],
      "metadata": {
        "id": "Zm6soIMU7FIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_061.PNG)"
      ],
      "metadata": {
        "id": "CeifKd7M7HFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I plug it in into $A=\\int_{0}^{2 \\pi} \\int_{0}^{\\pi}\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right| d \\phi \\, d \\theta$\n",
        "\n",
        "* mit den Limits dass phi is zwischen 0 und pi\n",
        "\n",
        "* und theta is zwischen 0 und 2*pi"
      ],
      "metadata": {
        "id": "shp6Hska7QI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_062.PNG)"
      ],
      "metadata": {
        "id": "Ui1roOrb7v0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 2: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "Pf3GY2W7nkkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 1](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "ZZkvuyz8nryU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find surface area of a plane z = -x that lies inside a cylinder $x^2 + y^2 = 4$\n",
        "\n",
        "* First find a parametrization for the plane: i use standard polar coordinate with sin and cosine, and for the z-component (the height of the surface area) I can use -x\n",
        "\n",
        "> $\\vec{r}(r, \\theta)=\\langle r \\cos \\theta, r \\sin \\theta,-r \\cos \\theta \\rangle$\n",
        "\n",
        "* second, i know what my constraints are: $2\\pi$, because the cylinder = 4 so radius = 2\n",
        "\n",
        "> $0 \\leq r \\leq 2 \\quad 0 \\leq \\theta \\leq 2 \\pi$"
      ],
      "metadata": {
        "id": "4W0gn2ven_XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_063.png)"
      ],
      "metadata": {
        "id": "1Mtbl0FwqNG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 3: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "WS0hoJa5wico"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 2](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "mNac0ltVrYme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I make my parameters $\\theta$ and $z$ (height) $\\rightarrow$ die richtige (iS von einfachste) Parametrisierung ist sehr wichtig"
      ],
      "metadata": {
        "id": "4Kxxwv2vwjxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_064.png)"
      ],
      "metadata": {
        "id": "Vo5gBKjswuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_065.png)"
      ],
      "metadata": {
        "id": "42E-WtVtxPNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_066.png)"
      ],
      "metadata": {
        "id": "XvNkFTfcxQae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Area for Implicit Surfaces*"
      ],
      "metadata": {
        "id": "xDFYb8uW7h9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Surface Area for Implicit & Explicit Surfaces](https://www.youtube.com/watch?v=k13kwLzoTpo&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1)"
      ],
      "metadata": {
        "id": "Q2FhPOJS7mFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cases where I don't have a parametrized version (like above), but an implicit version"
      ],
      "metadata": {
        "id": "elSoMeoZ8OBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine two surfaces: one flat on the bottom and one curved above it.\n",
        "\n",
        "* Take the normal vector of both and compare them.\n",
        "\n",
        "* If both surfaces are flat, the inner product = 1 and its integrand of the above surface will give you the surface area of the bottom (since they are equal).\n",
        "\n",
        "* If the inner product is not 1, then there is a difference and it will compute the surface area of the above surface."
      ],
      "metadata": {
        "id": "JGd3syYr7rqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_067.png)"
      ],
      "metadata": {
        "id": "EP-BKu5x8HLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implicit Surface F(x, y, z) = C\n",
        "\n",
        "Assume:\n",
        "\n",
        "* Smooth (i.e. F differentiable, $\\nabla F \\neq \\vec{0}$\n",
        "and continuous)\n",
        "\n",
        "* The surface is 'above' a region in the xy-plane with $\\nabla F \\bullet \\hat{k} \\neq 0$\n",
        "\n",
        "> Surface Area $=\\iint_{R} \\frac{|\\nabla F|}{|\\nabla F \\cdot \\hat{k}|} d A$"
      ],
      "metadata": {
        "id": "XxRR64jL8p5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_069.png)"
      ],
      "metadata": {
        "id": "qyMsoMtf-Okp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The more the plane above is twisted, the more the gradient $|\\nabla F \\cdot \\hat{k}|$ gets smaller, and if the denominator gets smaller, the total surface is bigger overall. We practically get a ratio between the original length of the gradient vector and then this portion of the gradient vector in the $\\hat{k}$ direction -  that is our scaling factor!"
      ],
      "metadata": {
        "id": "l-81RyUh_gBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I change this now with a generic plane, because choosing the $\\hat{k}$ direction was a bit arbitrary:"
      ],
      "metadata": {
        "id": "6GLNqJEd99f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_068.png)"
      ],
      "metadata": {
        "id": "AZrhMZvg95h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Area for Explicit Surfaces*"
      ],
      "metadata": {
        "id": "-TPsQ4p0KZ2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's turn to explicit surfaces**"
      ],
      "metadata": {
        "id": "wcq8cfnW-TM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_070.png)"
      ],
      "metadata": {
        "id": "N4Jpyv30-lYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example video: [Computing the Surface Area of an Implicitly Defined Surface](https://www.youtube.com/watch?v=810yT1zQtxA&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=2)"
      ],
      "metadata": {
        "id": "TAeCA5jt_97i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Oberfl√§chenintegral (Surface Integral)*"
      ],
      "metadata": {
        "id": "CQu9zwVlu7sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Formulas & Examples*\n",
        "\n",
        "Moving on to surface integrals:\n",
        "\n",
        "Now $G(x, y, z)$ defined on a smooth surface\n",
        "\n",
        "> Surface Integral $=\\iint_S G(x, y, z) d \\sigma$\n",
        "\n",
        "* g (x,y,z) is a function that is defined on that surface\n",
        "\n",
        "> the surface integral is adding up the values of that function along each elements of the surface area\n",
        "\n",
        "> It's like some 'height' is added: you have the position x and y. And z can be the temperature at each point on a surface, or the thickness, or else. It's like an abstract 'height'.\n",
        "\n",
        "Changing our Surface Area (SA) definitions to Surface Integral (SI) definitions with cross product and function:\n",
        "\n",
        "*Potential Applications of Surface Integrals:*\n",
        "\n",
        "* Here: add up density * areas = surface integral to compute thickness\n",
        "\n",
        "Repetition of Surface Areas computed only with the cross product:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1559.png)"
      ],
      "metadata": {
        "id": "f02swzD0zHoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Orientable vs Non-Orientable Surfaces*\n",
        "\n",
        "Source: https://youtu.be/S48JsV-pCBo\n",
        "\n",
        "> <font color=\"blue\">**Definition: A smooth surface is orientable if there is a continuous, field of unit normal vectors**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1560.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1561.png)"
      ],
      "metadata": {
        "id": "imvAz-081zR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fundamental Theorem of Calculus (Konservatives bzw. wirbelfreies Vektorfeld)*"
      ],
      "metadata": {
        "id": "0USsGJE7Tfz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus*\n",
        "\n",
        "Ein konservatives Feld kann geschrieben werden als Derivative eines **Potential Function** (siehe Skalarpotenzial):\n",
        "\n",
        "> $\\vec{F} = \\Delta f$\n",
        "\n",
        "> Das Skalarpotenzial ist ein Ma√ü f√ºr die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein K√∂rper entgegen der wirkenden Kraft bewegt, dann erh√∂ht sich seine potenzielle Energie.\n",
        "\n",
        "https://youtu.be/tb3KSeF0WrQ\n",
        "\n",
        "Source: [Conservative Vector Fields // Vector Calculus](https://www.youtube.com/watch?v=76nzOtupeRc&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=13)\n",
        "\n",
        "Like electric (electromagnetic) fields or gravitational fields - the work done (i.e. by the force of gravity on a moving ball in the air) only depends on the endpoints, the differences in the height in this example (now matter which way the ball went).\n",
        "\n",
        "A field $\\vec{F}$ is conservative on an open domain if (the line integral along the curve c):\n",
        "\n",
        ">$\\int_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "is the same for ALL paths $C$ between points $A$ and $B$ in the domain.\n",
        "\n",
        "**The work done is in all 3 cases the same, even though the paths are different:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_009.png)\n",
        "\n",
        "If you compute it you get the integral of the derivative at the end. We know from the **Fundamental Theory of Calculus** that if you integrate the derivative, then you get the endpoints! In the above case the work done is zero.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_010.png)\n",
        "\n",
        "**When is a field conservative?**\n",
        "\n",
        "A continuous field $\\vec{F}$ is conservative * if and only if\n",
        "\n",
        ">$\n",
        "\\vec{F}=\\nabla f\n",
        "$\n",
        "\n",
        "For some differentiable 'potential function' $f$ (=scalar function).\n",
        "\n"
      ],
      "metadata": {
        "id": "xSQIpQcXex8o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56sZZ-KLKsX_"
      },
      "source": [
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ f√ºr beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.\n",
        "\n",
        "* Deutet man $\\vec{X}(\\vec{r})$ als Kraftfeld, so ist das Kurvenintegral die gesamte l√§ngs der Randkurve $S$ gegen die Kraft $\\vec{X}(\\vec{r})$ verrichtete Arbeit.\n",
        "\n",
        "* Wirbelfrei sind z. B. das ruhende elektrische Feld in der Elektrostatik und das Gravitationsfeld, aber auch Felder wie das Geschwindigkeitsfeld einer Potentialstr√∂mung.\n",
        "\n",
        "> Ist $\\vec{X}(\\vec{r})$ wirbelfrei, dann gilt: $\\operatorname{rot} \\vec{X}(\\vec{r})=\\overrightarrow{0}$\n",
        "d. h. die Rotation des Vektorfeldes ist gleich null.\n",
        "\n",
        "* Ist der Definitionsbereich einfach **zusammenh√§ngend, so gilt auch die Umkehrung**.\n",
        "\n",
        "> Wirbelfreie Vektorfelder lassen sich stets als **Gradient eines zugrundeliegenden skalaren Felds** $\\Phi(\\vec{r})$ formulieren (siehe Gradientenfeld): $\n",
        "\\vec{X}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Daraus folgt, dass f√ºr das skalare Feld $\\Phi$ gilt:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\overrightarrow{0}$ (**siehe auch unten auch Gradientfeld & Skalarpotenzial**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLH2-gTPd43O"
      },
      "source": [
        "*Quellenfreie und wirbelfreie Vektorfelder; Zerlegungssatz*\n",
        "\n",
        "Ein mindestens **zweimal stetig differenzierbares Vektorfeld** $\\mathbf{v}(\\mathbf{r})$ im $\\mathbb{R}^{3}$ hei√üt quellenfrei (beziehungsweise wirbelfrel), wenn seine **Quellendichte (Divergenz) beziehungsweise Wirbeldichte (Rotation) dort √ºberall Null ist**. Unter der weiteren\n",
        "Voraussetzung, dass die Komponenten von $\\mathbf{v}$ im Unendlichen hinreichend rasch verschwinden, gilt der sogenannte Zerlegungssatz: Jedes Vektorfeld $\\mathbf{v}(\\mathbf{r})$ ist\n",
        "eindeutig durch seine Quellen bzw. Wirbel bestimmt, und zwar gilt die folgende\n",
        "Zerlegung in einen wirbelfreien beziehungsweise quellenfreien Anteil:\n",
        "\n",
        "> $\n",
        "\\mathbf{v}(\\mathbf{r}) \\equiv-\\operatorname{grad}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{div}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}+\\operatorname{rot}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{rot}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}\n",
        "$\n",
        "\n",
        "Dies entspricht der Zerlegung eines statischen elektromagnetischen Feldes in den elektrischen beziehungsweise magnetischen Anteil (siehe Elektrodynamik). Es sind also genau die Gradientenfelder (d. h. die , elektrischen Feldkomponenten\") wirbelfrei bzw. genau die Wirbelfelder (d. h. die ,magnetischen Feldkomponenten\") quellenfrei. Dabei sind grad $\\phi(\\mathbf{r}):=\\nabla \\phi,$ div $\\mathbf{v}:=\\nabla \\cdot \\mathbf{v}$ und rot $\\mathbf{v}:=\\nabla \\times \\mathbf{v}$ die bekannten, mit dem Nabla-Operator $(\\nabla)$ der Vektoranalysis\n",
        "gebildeten Operationen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do I know if a vector field is conservative?**\n",
        "\n",
        ">$\n",
        "\\vec{F}=M(x, y, z) \\hat{\\imath}+N(x, y, z) \\hat{\\jmath}+P(x, y, z) \\hat{k}\n",
        "$\n",
        "\n",
        "is conservative* if and only if\n",
        "\n",
        ">$\n",
        "\\frac{\\partial M}{\\partial y}=\\frac{\\partial N}{\\partial x} \\quad \\frac{\\partial N}{\\partial z}=\\frac{\\partial P}{\\partial y} \\quad \\frac{\\partial M}{\\partial z}=\\frac{\\partial P}{\\partial x}\n",
        "$\n",
        "\n",
        "*for $M, N, P$ continuous first partials, on an open simply connected domain\n",
        "\n",
        "Source: https://youtu.be/ZGUvyGeNT44"
      ],
      "metadata": {
        "id": "Q8yG2xqwbngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do we find the (scalar) potential function so $\\vec{F}=\\nabla f ?$**\n",
        "\n",
        "* You take M, N and P and integrate them to get $\\vec{F}$, because\n",
        "\n",
        "> $\\vec{F} = \\nabla f=$ $\\langle \\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta z},\\rangle$ = $M, N, P$ from above\n",
        "\n",
        "* then you fill in a beginning and endpoint of the curve (parametrization), and according to the Fundamental theorem of Line Integrals (where only the beginning and endpoints count - no matter which path has been taken - you take the difference of the endpoint minus beginning\n",
        "\n",
        "Source: https://youtu.be/jlza4rEFXKM"
      ],
      "metadata": {
        "id": "-w6SBRW9cbqc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQKZZ-1M-GE"
      },
      "source": [
        "###### *Green's Theorem*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRhCrZ_fK_8D"
      },
      "source": [
        "*Green's Theorem: Curl or Circulation (Flow) Density (Circulation-Curl Form)*\n",
        "\n",
        "[Satz von Green](https://de.wikipedia.org/wiki/Satz_von_Green)\n",
        "\n",
        "* Der Satz von Green (auch Green-Riemannsche Formel oder Lemma von Green, gelegentlich auch Satz von Gau√ü-Green)\n",
        "\n",
        "* **erlaubt es, das Integral √ºber eine ebene Fl√§che durch ein Kurvenintegral auszudr√ºcken**.\n",
        "\n",
        "* Der Satz ist ein Spezialfall des Satzes von Stokes.\n",
        "\n",
        "Integrals Ãàatze (Gauss, Stokes, Greenschen Formeln, Lo Ãàsung der Poissongleichung, Fundamentalsatz der Vektoranalysis II)\n",
        "\n",
        "* We have the **local** Curl Density (seen under Differentialoperator: Rotation) at a specific point\n",
        "\n",
        "> Circulation Density: $\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)$\n",
        "\n",
        "* And we have the **global** flow (seen in Line Integral)\n",
        "\n",
        "> Circulation: $=\\oint_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "* with Green's Theorem we can now bring both together\n",
        "\n",
        "Source Video: https://youtu.be/JB99RbQAilI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1564.png)\n",
        "\n",
        "It's at first a bit strange how the points inside are relevant for the curl on the path at the boundary. Circulation only occurs at the boundary. It doesn't matter what happens on the inside. Circluation is only a measure along the boundary.\n",
        "\n",
        "But we are saying circulation is equal to a double integral. An area integral. An area integral with all our circulation densities where it really does matter what's going along in the middle, what these spinners are doing in the inside. They contribute at the end to the curl that happens at the boundary.\n",
        "\n",
        "It's like in the Fundamental Theorem of Calculus (and Line Integral): you take the integral of a derivative and end up with the boundaries. It represents what happens at the boundary.  of some interval. And somehow the difference of the boundary of some interval is related to integrating over the entire integral.\n",
        "\n",
        "And similarly here we are relating information over the entiore region, theb information of the circulation density and accumulating it all up, and that results in just a circulation along the boundary."
      ],
      "metadata": {
        "id": "IhjyNptYvpah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Green's Theorem: Divergence or Flux Density (Divergence-Flux Form or Normal Form)*\n",
        "\n",
        "Source: https://youtu.be/GsjJs71SBec\n",
        "\n",
        "> The concept of Flux along a boundary: The tendency of the vector field to be aligned with the outward normal.\n",
        "\n",
        "> The Flux Density = is also called the Divergence ! (how much the vector field is spreading away from some point at one specific point)\n",
        "\n",
        "* Flux is a global property\n",
        "\n",
        "* at the picture of can be gas leaving a source in the center\n",
        "\n",
        "* If I want to have some notion of the degree to which my vector field is leaving away from one specific point, around which I draw a rectangle\n",
        "\n",
        "* Let's draw this rectangle (like before, but we analysed whsat is the circlution around it) but this time compute flux:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1565.png)"
      ],
      "metadata": {
        "id": "qQDFOBfOfApb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Green's Theorem: Exercise for 2D (Circulation & Flux)*\n",
        "\n",
        "Source: https://youtu.be/ig62HfJLCG0\n",
        "\n",
        "My field is: $\\vec{F} = x^2 \\hat{i} + xy \\hat{j}$\n",
        "\n",
        "* with M = $x^2$ and N = xy\n",
        "\n",
        "My path around an area is a square: x = $\\pm 1$, y = $\\pm 1$\n",
        "\n",
        "Remember when you compute a double integral: inside first for x and then outside after for y\n",
        "\n",
        "How to get to the terms inside the double integral?\n",
        "\n",
        "* Circulation-Curl Form: $\\oint_{C} \\vec{F} \\cdot d \\vec{r}=\\iint_{R}\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right) d x d y$\n",
        "\n",
        "* Divergence Flux Form: $\\oint_{C} \\vec{F} \\cdot \\vec{n} d s=\\iint_{R}\\left(\\frac{\\partial M}{\\partial x}+\\frac{\\partial N}{\\partial y}\\right) d x d y$\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1566.png)"
      ],
      "metadata": {
        "id": "kmwWODfmpZ2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stoke's Theorem*"
      ],
      "metadata": {
        "id": "521_dSZMrpGC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y64AmnhcNBpb"
      },
      "source": [
        "> Stokes Theorem is a generalization of Green's Theorem on Curl or Circulation (Flow)\n",
        "\n",
        "[Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes)\n",
        "\n",
        "* sehr grundlegenden Satz √ºber die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "*  eine Verbindungslinie von der Differentialgeometrie zur Algebraischen Topologie er√∂ffnet.\n",
        "\n",
        "* Dieser Zusammenhang wird durch den Satz von de Rham beschrieben, f√ºr den der Satz von Stokes grundlegend ist.\n",
        "\n",
        "* Im Folaenden ist $n=3$ und es wird die Schreibweise mit Mehrfachintegralen\n",
        "verwendet.\n",
        "\n",
        "Das qeschlossene Kurvenintegral einer vektoriellen Gr√∂√üe (rechte Seite) kann\n",
        "mittels der Rotation in ein Fl√§chenintegral √ºber eine von dem geschlossenen\n",
        "Integrationsweg $\\Gamma=\\partial A$ berandete, nicht notwendig ebene Fl√§che\n",
        "umgewandelt werden (linke Seite). Dabei werden - wie auch beim\n",
        "Gau√ü'schen Satz - die gew√∂hnlichen Orientierungseigenschaften\n",
        "vorausgesetzt. Es gilt:\n",
        "\n",
        "> $\n",
        "\\iint_{A} \\operatorname{rot} \\vec{F} \\cdot \\mathrm{d} \\vec{A}=\\oint_{\\Gamma=\\partial A} \\vec{F}(\\vec{r}) \\cdot \\mathrm{d} \\vec{r}\n",
        "$\n",
        "\n",
        "Der Vektor $\\mathrm{d} \\vec{A}$ ist gleich dem Betrag der zur betrachteten Fl√§che $A$ bzw. zu\n",
        "$\\partial V$ geh√∂renden infinitesimalen Fl√§chenelemente multipliziert mit dem\n",
        "zugeh√∂rigen Normalenvektor. Auf der rechten Seite wird durch das\n",
        "Kreissymbol im Integralzeichen daran erinnert, dass √ºber eine geschlossene\n",
        "Kurve integriert wird.\n",
        "\n",
        "Explanation Video: https://youtu.be/0UvNF_cfBJ4\n",
        "\n",
        "Example Calculation: https://youtu.be/ms4JjH0BANU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: What is the circulation around the boundary??**\n",
        "\n",
        "* it is influenced by the curling at each point along the boundary\n",
        "\n",
        "* the tendency to curl within the surface is given by (cross product of del with F vector field) and inner product of that with the normal vector at each point\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1562.png)\n",
        "\n",
        "Conditions on Stokes Theorem: For $S$ a piecewise smooth **oriented** surface with piecewise smooth boundary $C$ and $\\vec{F}$ a field with **continuous first partials** for each component on an open region containing $S$"
      ],
      "metadata": {
        "id": "G25WFhk52AjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Divergence Theorem (Volumenintegral / Gau√üscher Integralsatz)*"
      ],
      "metadata": {
        "id": "LxkcVzsECAEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Verallgemeinerung des Green'schen Theorem on Divergence\n",
        "\n",
        "See also Volume Form: https://en.m.wikipedia.org/wiki/Volume_form\n",
        "\n",
        "Deutsch: [Gau√üscher Integralsatz](https://de.m.wikipedia.org/wiki/Gau√üscher_Integralsatz)\n",
        "\n",
        "* In vector calculus, the [divergence theorem](https://en.m.wikipedia.org/wiki/Divergence_theorem), also known as Gauss's theorem or Ostrogradsky's theorem, is a theorem which relates the flux of a vector field through a closed surface to the divergence of the field in the volume enclosed.\n",
        "\n",
        "> More precisely, **the divergence theorem states that the surface integral of a vector field over a closed surface, which is called the flux through the surface, is equal to the [volume integral](https://en.m.wikipedia.org/wiki/Volume_integral) of the divergence over the region inside the surface**.\n",
        "\n",
        "* Intuitively, it states that the sum of all sources of the field in a region (with sinks regarded as negative sources) gives the net flux out of the region.\n",
        "\n",
        "Beispiel: elektrostatische Verbindung (von Ionenbindung): https://de.m.wikipedia.org/wiki/Elektrostatik\n",
        "\n",
        "Stokes and Greens theorem on manifolds (derivative and exterior derivative): https://youtu.be/1lGM5DEdMaw\n",
        "\n",
        "[Integralsatz](https://de.wikipedia.org/wiki/Integralsatz) ist ein Namensbestandteil bestimmter mathematischer S√§tze, in deren Aussage ein Integral vorkommt.\n",
        "\n",
        "Unter dem Begriff der klassischen Integrals√§tze werden der **Satz von Gau√ü, der Satz von Green, der Satz von Stokes** und einige ihrer Spezialf√§lle zusammengefasst. Diese S√§tze der Vektoranalysis h√§ngen eng miteinander zusammen: der Integralsatz von Stokes umfasst die anderen beiden S√§tze als Spezialf√§lle.\n",
        "\n",
        "Au√üerdem gibt es neben den klassischen Integrals√§tzen noch weitere S√§tze, die man kurz als Integrals√§tze bezeichnet. Zu diesen z√§hlt beispielsweise der cauchysche Integralsatz, der ein zentrales Resultat aus der Funktionentheorie ist.\n",
        "\n",
        "[Integralsatz von Gau√ü](https://de.m.wikipedia.org/wiki/Gau√üscher_Integralsatz)\n",
        "\n",
        "* Im Folgenden sei das ‚ÄûIntegrationsvolumen‚Äú V n-dimensional.\n",
        "\n",
        "* Das [Volumenintegral](https://de.m.wikipedia.org/wiki/Volumenintegral) √ºber den Gradienten einer skalaren Gr√∂√üe $\\phi$, kann dann in ein [Oberfl√§chenintegral](https://de.m.wikipedia.org/wiki/Oberfl√§chenintegral) (bzw. Hyperfl√§chenintegral) √ºber den Rand dieses Volumens umgewandelt werden:\n",
        "\n",
        "> $\n",
        "\\int_{V} \\operatorname{grad} \\phi(\\vec{x}) \\mathrm{d} V=\\oint_{\\partial V} \\phi \\mathrm{d} \\vec{A}\n",
        "$\n",
        "\n",
        "Explanation Video: https://youtu.be/pY4t-ikhzhU\n",
        "\n",
        "Example Calculation: https://youtu.be/E7RUu1K8UDM\n",
        "\n",
        "Another Example calculation: [Divergence Theorem for regions bounded by two surfaces](https://youtu.be/RZE7iB71X7o) (Inside and outside surface, like inner sphere and outer sphere)\n",
        "\n",
        "Another Example calculation: [Deriving Gauss's Law for Electric Flux via the Divergence Theorem from Vector Calculus](https://youtu.be/8PmIarVZmc8) (see here some background: https://de.m.wikipedia.org/wiki/Gau√üsches_Gesetz)"
      ],
      "metadata": {
        "id": "JdmqQyXMp5fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1563.png)"
      ],
      "metadata": {
        "id": "JmH0XpY1yU_5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwK1ZsvrPIY"
      },
      "source": [
        "<font color=\"black\">**Exkurs: Volumenintegrale**\n",
        "\n",
        "> [Volume integrals](https://en.m.wikipedia.org/wiki/Volume_integral) are especially important in physics for many applications, for example, to calculate flux densities.\n",
        "\n",
        "* z.B. zur Berechnung des Volumeninhaltes eines Objektes\n",
        "\n",
        "* Das [Volumenintegral](https://de.wikipedia.org/wiki/Volumenintegral) erweitert das Oberfl√§chenintegral auf die Integration √ºber ein beliebiges dreidimensionales Integrationsgebiet, wobei eine Funktion dreimal hintereinander integriert wird, jeweils √ºber eine Richtung eines dreidimensionalen Raumes.\n",
        "\n",
        "* **Dabei muss es sich jedoch nicht notwendigerweise um ein Volumen eines geometrischen K√∂rpers handeln**.\n",
        "\n",
        "* Zur vereinfachten Darstellung wird oft nur ein einziges Integralzeichen geschrieben und die Volumenintegration lediglich durch das Volumenelement $\\mathrm {d} V$ angedeutet:\n",
        "\n",
        "> $\\iiint_{V} f(r) d^{3} r=\\int_{V} f(\\vec{x}) \\mathrm{d} V$\n",
        "\n",
        "* wobei die zu integrierende Funktion zumindest von drei Variablen ${\\vec {x}}=(x,y,z)$ f√ºr eine (kartesische) Beschreibung im dreidimensionalen Raum $\\mathbb{R^3}$ abh√§ngt, **es sind aber auch h√∂herdimensionale R√§ume m√∂glich**.\n",
        "\n",
        "* Es handelt sich um ein **skalares Volumenintegral**, wenn der Integrand $f$ und das Volumenelement $\\mathrm{d} V$ skalar sind. Bei einem **vektoriellen Integranden**, z. B. einem Vektorfeld $\\vec{f}$, ist auch das Volumenelement $\\mathrm{d} \\vec{V}$ ein Vektor, sodass sich ein vektorielles Volumenintegral ergibt.\n",
        "\n",
        "* Um ein Volumenintegral zu berechnen, ist meist eine Parametrisierung des Integrationsgebiets n√∂tig.\n",
        "\n",
        "**Volumenform**\n",
        "\n",
        "* Eine [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) ist ein mathematisches Objekt, welches zur Integration √ºber Raumbereiche ben√∂tigt wird, insbesondere bei der Verwendung spezieller Koordinatensysteme, also ein Spezialfall eines Volumens.\n",
        "\n",
        "* In der Physik und im Ingenieurwesen sind auch Bezeichnungen wie infinitesimales Volumenelement oder Ma√üfaktor gebr√§uchlich.\n",
        "\n",
        "* Beispiele in 3 Dimensionen:\n",
        "\n",
        "  * [Kartesische Koordinaten](https://de.m.wikipedia.org/wiki/Kartesisches_Koordinatensystem): $\\mathrm{d} V=\\mathrm{d} x \\cdot \\mathrm{d} y \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Zylinderkoordinaten](https://de.m.wikipedia.org/wiki/Polarkoordinaten#Zylinderkoordinaten) (ebene Polarkoordinaten um eine dritte Koordinate erg√§nzt): $\\mathrm{d} V=\\rho \\cdot \\mathrm{d} \\rho \\cdot \\mathrm{d} \\varphi \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Kugelkoordinaten](https://de.m.wikipedia.org/wiki/Kugelkoordinaten): $\\mathrm{d} V=r^{2} \\cdot \\sin \\theta \\cdot \\mathrm{d} r \\cdot \\mathrm{d} \\theta \\cdot \\mathrm{d} \\varphi$\n",
        "\n",
        "* Das Volumenelement in drei Dimensionen l√§sst sich nach dem [Transformationssatz](https://de.m.wikipedia.org/wiki/Transformationssatz) mit Hilfe der [Funktionaldeterminante](https://de.m.wikipedia.org/wiki/Funktionaldeterminante) det $J$ berechnen. Die [Jacobi-Matrix](https://de.m.wikipedia.org/wiki/Jacobi-Matrix) f√ºr die Transformation von den Koordinaten $\\left\\{x_{1}, x_{2}, x_{3}\\right\\}$ zu $\\left\\{x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right\\}$ ist hierbei definiert durch\n",
        "\n",
        ">$\n",
        "J=\\frac{\\partial\\left(x_{1}, x_{2}, x_{3}\\right)}{\\partial\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right)}\n",
        "$\n",
        "\n",
        "* Das Volumenelement ist dann gegeben durch\n",
        "\n",
        ">$\n",
        "\\mathrm{d} V^{\\prime}=|\\operatorname{det} J| \\mathrm{d} x_{1}^{\\prime} \\mathrm{d} x_{2}^{\\prime} \\mathrm{d} x_{3}^{\\prime}\n",
        "$\n",
        "\n",
        "* Aus mathematischer Sicht ist **eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$**. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird [Riemann'sche Volumenform](https://de.m.wikipedia.org/wiki/Hodge-Stern-Operator#Riemannsche_Volumenform) genannt (Hodge-Stern-Operator in der Differentialgeometrie)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRiQv6BoMmA"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionalanalysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "ppApZ0cS7_F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green's Function**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Greensche_Funktion\n",
        "\n",
        "Video: [Green's functions: the genius way to solve DEs](https://youtu.be/ism2SfZgFJg)\n",
        "\n",
        "Video: [Green's Functions](https://youtu.be/-riPW1yt_fA)\n",
        "\n",
        "Video: [Introducing Green's Functions for Partial Differential Equations (PDEs)](https://youtu.be/xNqLZnM-PPY)\n",
        "\n",
        "Video: [Green's functions, Delta functions and distribution theory](https://youtu.be/AqfYSNsrnhI)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1334.jpg)\n"
      ],
      "metadata": {
        "id": "qvEaRxI18Q6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4I7IyMVeJte"
      },
      "source": [
        "**Schwache Ableitung & schwache L√∂sung**\n",
        "\n",
        "* Eine [schwache Ableitung](https://de.wikipedia.org/wiki/Schwache_Ableitung) bzw. [weak derivative](https://en.wikipedia.org/wiki/Weak_derivative) ist in der Funktionalanalysis, einem Teilgebiet der Mathematik, eine Erweiterung des Begriffs der gew√∂hnlichen (klassischen) Ableitung.\n",
        "\n",
        "* Er erm√∂glicht es, Funktionen eine Ableitung zuzuordnen, die nicht (stark bzw. im klassischen Sinne) differenzierbar sind.\n",
        "\n",
        "* Schwache Ableitungen spielen eine gro√üe Rolle in der Theorie der partiellen Differentialgleichungen. **R√§ume schwach differenzierbarer Funktionen sind die Sobolev-R√§ume**.\n",
        "\n",
        "* Ein noch allgemeinerer Begriff der Ableitung ist die Distributionenableitung.\n",
        "\n",
        "* In mathematics, a weak derivative is a generalization of the concept of the derivative of a function (strong derivative) **for functions not assumed differentiable, but only integrable**, i.e., to lie in the $L^{p}$ space $L^{1}([a, b])$. See distributions for a more general definition.\n",
        "\n",
        "**This concept gives rise to the definition of [weak solutions](https://en.wikipedia.org/wiki/Weak_solution) in Sobolev spaces, which are useful for problems of differential equations and in functional analysis.**\n",
        "\n",
        "The **absolute value function** u : [‚àí1, 1] ‚Üí [0, 1], u(t) = |t|, which is not differentiable at t = 0, has a weak derivative v known as the [**sign function**](https://de.wikipedia.org/wiki/Vorzeichenfunktion) given by\n",
        "\n",
        "![gg](https://mathepedia.de/img/Abs_x.png)\n",
        "\n",
        "*Signumfunktion als schwache Ableitung der Betragsfunktions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbR-3EnGQ33f"
      },
      "source": [
        "**Anfangswertprobleme (Cauchy-Problem)**\n",
        "\n",
        "* Beispielsweise werden alle schwingenden Pendel durch eine Differentialgleichung beschrieben (siehe: Pendelgleichung), und der generelle Bewegungsablauf folgt immer dem gleichen Prinzip. Der konkrete Bewegungsablauf ist jedoch durch die Rand- oder Anfangsbedingung(en) (wann wurde das Pendel angesto√üen, und wie weit) bestimmt. Die L√∂sbarkeit von Anfangswertproblemen bei gew√∂hnlichen Differentialgleichungen 1. Ordnung wird durch den Satz von Picard-Lindel√∂f beschrieben. https://mathepedia.de/Gewoehnliche_Differentialgleichungen.html\n",
        "\n",
        "* Wer zu einer Differentialgleichung eine [**Anfangsbedingung**](https://de.wikipedia.org/wiki/Anfangsbedingung) hinzuf√ºgt, stellt damit ein **Anfangswertproblem**. Eine besonders spannende Frage lautet dabei, wie eine Anfangsbedingung zu einer gegebenen Differentialgleichung beschaffen sein muss, damit das entstehende Anfangswertproblem **genau eine eindeutig bestimmte L√∂sung zul√§sst**.\n",
        "\n",
        "Der freie Fall (etwa eines Apfels vom Baum) wird beschrieben durch die Bewegungsgleichung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "y^{\\prime \\prime}(t) &=-g \\\\\n",
        "\\Rightarrow y^{\\prime}(t) &=-g \\cdot t+v_{0}\n",
        "\\end{aligned}$\n",
        "\n",
        "mit der Konstanten $g \\approx 9,81 \\mathrm{~m} / \\mathrm{s}^{2}$ (Erdbeschleunigung).\n",
        "Die L√∂sungsmenge dieser Differentialgleichung besteht zun√§chst aus allen Funktionen der Form\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=-\\frac{1}{2} g t^{2}+v_{0} \\cdot t+y_{0}\n",
        "$\n",
        "\n",
        "mit beliebigen Integrationskonstanten $y_{0}$ und $v_{0}$.\n",
        "Eine m√∂gliche Anfangsbedingung sagt z. B. aus, dass der Apfel zu Beginn der Bewegung an einem\n",
        "Ast in drei Metern H√∂he h√§ngt:\n",
        "\n",
        ">$\n",
        "y(0)=y_{0}=3 \\mathrm{~m}\n",
        "$\n",
        "\n",
        "und sich in Ruhe befindet:\n",
        "\n",
        ">$\n",
        "y^{\\prime}(0)=v_{0}=0 \\mathrm{~m} / \\mathrm{s}\n",
        "$\n",
        "\n",
        "Diese Anfangsbedingung zeichnet nun in der L√∂sungsmenge der Differentialgleichung die eine Funktion\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=3 \\mathrm{~m}-\\frac{1}{2} g t^{2}\n",
        "$\n",
        "\n",
        "als die eindeutig bestimmte L√∂sung des Anfangswertproblems aus (Loesung dann zB uber das [Runge-Kutta-Verfahren](https://de.wikipedia.org/wiki/Runge-Kutta-Verfahren) - Einschrittverfahren zur n√§herungsweisen L√∂sung von Anfangswertproblemen in der numerischen Mathematik)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfUxHFgakA7f"
      },
      "source": [
        "**Randwertproblem (Boundary value)**\n",
        "\n",
        "* Bei partiellen Differentialgleichungen, wenn also die gesuchte Funktion nicht nur von einer, sondern von mehreren Variablen abh√§ngt, werden oftmals [Randbedingungen](https://de.wikipedia.org/wiki/Randbedingung) an Stelle von Anfangsbedingungen verwendet. Manchmal wird dann der Spezialfall einer Randbedingung, deren Definitionsbereich eine Hyperebene im vollen Definitionsbereich der Differentialgleichung bildet, Anfangsbedingung genannt.\n",
        "\n",
        "* In der Betriebswirtschaftslehre und der Volkswirtschaftslehre entsprechen die Randbedingungen den kurzfristig oder gar nicht durch den Entscheidungstr√§ger beeinflussbaren Datenparametern wie beispielsweise die Umweltzust√§nde der Witterung oder der Gesetze.\n",
        "\n",
        "Sei die gegebene Differentialgleichung $y^{\\prime \\prime}(x)=-y(x)$. Die L√∂sungsmenge dieser Gleichung ist $a \\sin (x)+b \\cos (x)$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=1$ und $y(\\pi / 2)=0 \\Rightarrow$ Die L√∂sung ist $y=\\cos (x)$.\n",
        "\n",
        "* Periodische Randbedingung: Gesucht≈Çist die L√∂sung mit $y(0)=0$ und $y(\\pi)=0 \\Rightarrow$ Es gibt unendlich viele L√∂sungen der Form $a \\sin (x)$ mit beliebigem $a$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=0$ und $y(2 \\pi)=1 \\Rightarrow$ Es gibt keine L√∂sung.\n",
        "\n",
        "Arten von Randbedingungen (Es gibt unterschiedliche M√∂glichkeiten, auf dem Rand des betrachteten Gebietes Werte vorzuschreiben):\n",
        "\n",
        "* Werte der L√∂sung vorschreiben; im Fall einer auf dem Intervall $[a, b]$ definierten gew√∂hnlichen Differentialgleichung schreibt man also $y(a)$ und $y(b)$ vor und spricht\n",
        "dann von [**Dirichlet-Randbedingungen**](https://de.wikipedia.org/wiki/Dirichlet-Randbedingung).\n",
        "\n",
        "* Bedingungen an die Ableitungen stellen, also $y^{\\prime}(a)$ und $y^{\\prime}(b)$ vorgeben, dann spricht man [**von Neumann-Randbedingungen**](https://de.wikipedia.org/wiki/Neumann-Randbedingung) (bei gew√∂hnlichen Differentialgleichungen, wie oben ausgef√ºhrt, von Anfangsbedingungen).\n",
        "\n",
        "* Ein Spezialfall sind [**periodische Randbedingungen**](https://de.wikipedia.org/wiki/Periodische_Randbedingung), hier muss (im Beispiel einer auf dem Intervall $[a, b]$ betrachteten gew√∂hnlichen Differentialgleichung) gelten: $y(a)=y(b)$ bzw. $y^{\\prime}(a)=y^{\\prime}(b)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partielle Differentialgleichung**\n",
        "\n",
        "> https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419\n",
        "\n",
        "* **[Partielle Differentialgleichungen](\n",
        "https://de.wikipedia.org/wiki/Partielle_Differentialgleichung) werden in erster Linie durch Trennung der Variablen und sp√§tere Integration gel√∂st.**\n",
        "\n",
        "* Sobolevr√§ume sind ein grundlegendes Werkzeug bei der Behandlung von PDE's (rein und angewandt).\n",
        "\n",
        "* See also Variational Formulations - wichtig fur partielle Differentialgleichungen\n",
        "\n",
        "* [Euler-Gleichungen (Str√∂mungsmechanik)](https://de.wikipedia.org/wiki/Euler-Gleichungen_(Str√∂mungsmechanik)) bildet dann ein System von **nichtlinearen partiellen** Differentialgleichungen **erster Ordnung** Wird die Viskosit√§t vernachl√§ssigt ($\\eta =\\lambda =0$), so erh√§lt man die Euler-Gleichungen (hier f√ºr den kompressiblen Fall)\n",
        "\n",
        "> $\\rho \\frac{\\partial \\vec{v}}{\\partial t}+\\rho(\\vec{v} \\cdot \\nabla) \\vec{v}=-\\nabla p+\\vec{f}$\n",
        "\n",
        "* [**Navier-Stokes-Gleichungen**](https://de.wikipedia.org/wiki/Navier-Stokes-Gleichungen) bilden dann ein System von **nichtlinearen partiellen** Differentialgleichungen **zweiter Ordnung**\n",
        "\n",
        "  * Die Gleichungen sind eine **Erweiterung der Euler-Gleichungen** der Str√∂mungsmechanik **um Viskosit√§t beschreibende Terme** (Die Navier-Stokes-Gleichungen beinhalten die Euler-Gleichungen als den Sonderfall, in dem die innere Reibung (Viskosit√§t) und die W√§rmeleitung des Fluids vernachl√§ssigt werden.)\n",
        "\n",
        "  * Die Navier-Stokes-Gleichungen bilden das Verhalten von Wasser, Luft und √ñlen ab und werden daher in diskretisierter Form bei der Entwicklung von Fahrzeugen wie Autos und Flugzeugen angewendet.\n",
        "\n",
        "  * Dies geschieht in N√§herungsform, da keine exakten analytischen L√∂sungen f√ºr diese komplizierten Anwendungsf√§lle bekannt sind.\n",
        "\n",
        "  * Siehe auch https://de.wikipedia.org/wiki/Numerische_Str√∂mungsmechanik\n",
        "\n",
        "> $\\rho \\overrightarrow{\\vec{v}}=\\rho\\left(\\frac{\\partial \\vec{v}}{\\partial t}+(\\vec{v} \\cdot \\nabla) \\vec{v}\\right)=-\\nabla p+\\mu \\Delta \\vec{v}+(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\vec{v})+\\vec{f}$\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Potentialstr√∂mung\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Black-Scholes-Modell\n",
        "\n",
        "* Loesungsverfahren\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Finite-Elemente-Methode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Spektralmethode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Liste_numerischer_Verfahren\n",
        "\n",
        "  * https://en.wikipedia.org/wiki/Method_of_characteristics\n",
        "\n",
        "* Die Methode der Charakteristiken ist eine Methode zur L√∂sung partieller\n",
        "Differentialgleichungen (PDGL/PDE), die typischerweise erster Ordnung und quasilinear sind\n",
        "\n",
        "* Die grundlegende Idee besteht darin, **die PDE durch eine geeignete Koordinatentransformation auf ein System gew√∂hnlicher Differentialgleichungen auf bestimmten Hyperfl√§chen, sogenannten Charakteristiken, zur√ºckzuf√ºhren.**\n",
        "\n",
        "* Die PDE kann dann als Anfangswertproblem in dem neuen System mit Anfangswerten auf den die Charakteristik schneidenden Hyperfl√§chen gel√∂st werden. St√∂rungen breiten sich l√§ngs der Charakteristiken aus.\n",
        "\n",
        "* Charakteristiken spielen eine Rolle in der qualitativen Diskussion der L√∂sung bestimmter PDE und in der Frage, wann Anfangswertprobleme f√ºr diese PDE korrekt gestellt sind.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdOgQKlZAssy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving the heat equation:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}=\\alpha \\nabla^{2} T$\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$\n",
        "\n",
        "* can be solved with Fourier series\n",
        "\n",
        "* [DE2: But what is a partial differential equation?](https://www.youtube.com/watch?v=ly4S0oi3Yz8&list=PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6&index=2)\n",
        "\n",
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$"
      ],
      "metadata": {
        "id": "aO7NYOE50lp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_001.png)"
      ],
      "metadata": {
        "id": "RRBeNGa92iKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small change in time:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_002.png)"
      ],
      "metadata": {
        "id": "EQLycVW52oK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small step in space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_003.png)"
      ],
      "metadata": {
        "id": "RKaZoj0J2pCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What that actually encodes is that we look at the limit of that ratio (temperature/space or time change) for smaller and smaller nudges to the input rather than a specific finitely small value\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_004.png)"
      ],
      "metadata": {
        "id": "YE3q91CB3c03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$ tells us that\n",
        "\n",
        "* the way this function changes with respect to time $\\frac{\\partial T}{\\partial t}(x, t)$\n",
        "\n",
        "* depends on how it changes with respect to space $\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$.\n",
        "\n",
        "* <font color=\"blue\">More specifically it's proportional to the second partial derivative with respect to x.</font>\n",
        "\n",
        "* at a high level, the intuition is that at points where the temperature distribution curves, it tends to change more quickly in the direction of that curvature.\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_005.png)"
      ],
      "metadata": {
        "id": "3cBh_epu3xyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$\n",
        "\n",
        "*It checks how different is a point from the average of its neigbours.* (you use second derivative for it)\n",
        "\n",
        "$\\frac{dT_2}{dt}$ = $\\Delta T_2$ - $\\Delta T_1$ (difference of differences) = $\\Delta \\Delta T_1$ (second difference = second derivative)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_007.png)"
      ],
      "metadata": {
        "id": "w9EjvvKl_LWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "* imagine discrete points. and then check the average of the neighbouring points.\n",
        "\n",
        "* if a point is much higher temperature than its neighbours, it will decrease. If it's lower, it will increase.\n",
        "\n",
        "* the equation reflects this relationship between the points and their differences\n",
        "\n",
        "* and the \"second difference\" is for continuous cases the \"second derivative\":\n",
        "\n",
        "> $\\frac{d T_{2}}{d t}=\\frac{\\alpha}{2} \\Delta \\Delta T_{1}$ $\\rightarrow$ $\\frac{\\partial T}{\\partial t}=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}$\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.** (you use second derivative for it)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_006.png)\n",
        "\n",
        "For higher dimensions than 1 (like 3D) you use also the second derivative, but add the other dimensions as well, and the result is called nabla, the Laplacian:\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ],
      "metadata": {
        "id": "qyKPD8oU7rEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Harmonic Analysis*"
      ],
      "metadata": {
        "id": "O8cFhBpbIFFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fourier Analysis*"
      ],
      "metadata": {
        "id": "7bWObSh9Mh0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Discrete Fourier Transform](https://youtu.be/yYEMxqreA10)"
      ],
      "metadata": {
        "id": "CJhXQ68Ua1uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The maths behind fourier transform](https://youtu.be/FOOQrrOo-II)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1343.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1344.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1346.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1345.jpg)"
      ],
      "metadata": {
        "id": "Clt-PAZNLm0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $f(x)$ is a periodic function with period $2 \\pi$. If we want to approximate this function with a trigonometric polynomial of degree $n$.\n",
        "\n",
        "**Step 1: Approximate periodic functions using infinite sums of sines and cosines:**\n",
        "\n",
        "> <font color=\"blue\">$F_n(x)=a_0+a_1 \\cos (x)+b_1 \\sin (x)+a_2 \\cos (2 x)+b_2 \\sin (2 x)+\\cdots+a_n \\cos (n x)+b_n \\sin (n x)$\n",
        "\n",
        "For $k \\geq 1$ the \"best\" coefficients to use are the following Fourier coefficients:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "&a_0=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(x) d x \\\\\n",
        "&a_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\cos (k x) d x \\\\\n",
        "&b_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\sin (k x) d x\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Step 2: Replace sin and cosine with $e$ using Eulers Formula:**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$\n",
        "\n",
        "\n",
        "we get this euqation:\n",
        "\n",
        "> <font color=\"blue\">$X_k=x_0 e^{-b_0 j}+x_1 e^{-b_1 j}+\\ldots+x_n e^{-b_{N-1} j}$</font>\n",
        "\n",
        "**Step 3: Taking the sum where where ${\\frac{2 \\pi k n}{N}} = b_n$** for the Discrete Fourier Transform (oft hat man nur einige Messpunkte / Samples aus Experiment oder Simulation)\n",
        "\n",
        "> <font color=\"blue\">$X_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-\\frac{j 2 \\pi k n}{N}}$</font>\n",
        "\n",
        "\n",
        "$\\omega=\\frac{2 \\pi n}{T}$ ist die Kreisfrequenz der diskreten Fourier Transform f√ºr nicht-periodische Funktionen, wodurch man auch schreiben kann:\n",
        "\n",
        "> <font color=\"blue\">$F(\\omega)_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-i \\omega k}$</font> $\\quad$ ([Source](https://de.m.wikipedia.org/wiki/Fourierreihe#Zusammenhang_mit_der_Fourier-Transformation_f√ºr_nicht-periodische_Funktionen))\n",
        "\n",
        "$\\omega_{k}=\\frac{k \\pi}{T}$ is the basic frequency and you can expand f(x) as a sum of sines and cosines that are also periodic in 2 T, and **higher and higher harmonics of those basic sines and cosines (and each with a contribution factor)**.\n",
        "\n",
        "$k = \\frac{2 \\pi}{wavelength}$, e.g. wavelength = $4 \\pi$ (2 complete turns within one period of time), then $k = \\frac{1}{2}$.\n",
        "\n",
        "**Step 4: Understand orthogonal projection**: Dabei ist folgendes ein Skalarprodukt / inner product / projection:\n",
        "\n",
        "> $\\langle x_{n}, e^{-i \\omega k}\\rangle$\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$\n",
        "\n",
        "**Step 5: Get Continous Fourier Transform:** For very large N (the resolution with which we can resolve different frequencies becomes infinitesimally small) the sum $\\sum$ is becoming a Riemann integral (continous case) -  If a Fourier series can approximate a function well on an intervall, can it do it also on the whole real line between -$\\infty$ and $\\infty$?\n",
        "\n",
        "> <font color=\"blue\">$X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$</font>\n",
        "\n",
        "where we calculate the coefficients $a_k$ and $b_k$ at each particular frequency with function $x(t)$ and **analyzing function (sinusoids) $e^{-j 2 \\pi F t} d t$**.\n",
        "\n",
        "  * you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "  * wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient\n",
        "\n",
        "  * and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "*Appendix: Instead of getting one complex coefficient per frequency you can also work with two real coefficients per frequency and end up with two integrals:*\n",
        "\n",
        "* one to correlate with signal with the cosine function:\n",
        "\n",
        "> $x_{a}(F)=\\int_{-\\infty}^{\\infty} x(t) \\cos 2 \\pi \\, Ft \\,d t$\n",
        "\n",
        "* and one to correlate the signal with the sine function:\n",
        "\n",
        "> $X_{b}(F)=\\int_{-\\infty}^{\\infty} x(t) \\sin 2 \\pi \\, Ft \\, d t$\n"
      ],
      "metadata": {
        "id": "xto8Ha2DK0Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Whereas a Taylor Series attempts to approximate a function locally about the point where the expansion is taken, a Fourier series attempts to approximate a periodic function over its entire domain. That is, a Tavlor series approximates a function point wise and a Fourier series approximates a function globally.*\n",
        "\n",
        "* [Steve Brunton: The Fourier Transform](https://www.youtube.com/watch?v=jVYs-GTqm5U)\n",
        "\n",
        "* [Looking Glass: Fourier Transform](https://youtu.be/Xxut2PN-V8Q)\n",
        "\n",
        "* https://sites.oxy.edu/ron/math/120/03/labs/lab8.PDF\n",
        "\n",
        "* https://math.stackexchange.com/questions/47430/is-fourier-series-an-inverse-of-taylor-series\n",
        "\n",
        "* http://dev.ipol.im/~coco/website/taylorfourier.html\n",
        "\n",
        "* https://math.stackexchange.com/questions/7301/connection-between-fourier-transform-and-taylor-series"
      ],
      "metadata": {
        "id": "Z6mGDVo2QU7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Start with approximation of f(x) between -$\\pi$ and $\\pi$ with some orthogonal base vectors made by sin and cos and their coefficients A and B calculated via the inner product with f(x) with each frequency of sin and cos (all are orthogonal base vectors)**\n",
        "\n",
        "**Approximate <u>periodic functions</u>: [Fourier Series](https://de.m.wikipedia.org/wiki/Fourierreihe)**\n",
        "\n",
        "You expand f(x) as a sum of sines and cosines (Grundt√∂ne) that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics (Obert√∂ne) of those basic sines and cosines.\n",
        "\n",
        "> <font color=\"blue\">$f(t)=$$\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left[a_{n} \\cdot \\cos \\left(n \\omega_{0} t\\right)+b_{n} \\sin \\left(n \\omega_{0} t\\right)\\right]$\n",
        "\n",
        "mit folgenden Termen:\n",
        "\n",
        "* <font color=\"blue\">$a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$</font>\n",
        "\n",
        "* <font color=\"blue\">$\\omega_{0}=\\frac{2 \\pi}{T}$</font> $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "* $T$ *ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "* <font color=\"blue\">$a_n$ und $b_n$</font>: siehe weiter unten unter 'orthogonale Projektion'\n",
        "\n",
        "\n",
        "We can approximate f(x) with an expansion of sine and cosines of higher and higher frquency (and shorter wavelength):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos (k x)+B_{k} \\sin (k x)\\right)$\n",
        "\n",
        "How do we calculate the coefficients? You calculate the (Hilbert space) inner product between f(x) and cos(kx) (=the particular k-th frequency cosine wave) bzw. sin(kx). I project f(x) into the cosine k-direction (and I need to normalize by this function):\n",
        "\n",
        "> $A_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos (k x) d x$ = $\\frac{1}{||cos(kx)||^2}$ $\\langle f(x), cos(k x)\\rangle$\n",
        "\n",
        "> $B_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin (k x) d x$ = $\\frac{1}{||sin(kx)||^2}$ $\\langle f(x), sin(k x)\\rangle$\n",
        "\n",
        "**Fourier transform is just another representation on another orthogonal basis vectors (sine and cosine with different frequencies).**\n"
      ],
      "metadata": {
        "id": "7NB7Ne-NuLnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Inner product / projection on orthogonal basis vectors to get coefficients**\n",
        "\n",
        "**<u>Orthogonal Projection (Inner Product)</u>: Calculate the coefficients $a_k$ and $b_k$ bzw. $e$ at each particular frequency**\n",
        "\n",
        "*Why do we use inner product / orthogonal projection? - If you`re familiar with calculating correlations, the Fourier transform is essentially the same:*\n",
        "\n",
        "* you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "* <font color=\"blue\">wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient (=inner product large when vectors are not orthogonal)</font>\n",
        "\n",
        "* and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "> <font color=\"blue\">$a_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"blue\">$b_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"orange\">$\\langle f(t) , \\cos (x) \\rangle$</font> = Inner Product / Projection of f(t) on cos(x) to get coefficient (how similar?)\n",
        "\n",
        "*Die Koeffizienten berechnen den Anteil den f(x) jeweils an der analysing function hat. Oben multipliziert man diesen Anteil dann mit der jeweiligen analysing function (sehr √§hnlich zu probabilities of states in quantum mechanics). Das ist eine orthogonale Projektion (inner product f(x) mit analysing function) am unteren Beispiel:*\n",
        "\n",
        "> $a_n$ = $\\frac{2}{2} \\int_{0}^{1}(1-t) \\cdot \\cos (n \\pi t) d t$\n",
        "\n",
        "$\\rightarrow$ dieser Teil ist das inner product / Projektion von $f(x) = 1-t$ auf die analysing function $\\cos (n \\pi t) d t$\n",
        "\n",
        "\n",
        "*For when you use $e$ instead of sin & cos: We want to know the $c_k$ coefficients by projecting f(x) onto orthogonal basis vectors sin and cos*\n",
        "\n",
        "<font color=\"black\">The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$\n",
        "\n",
        "> <font color=\"blue\">$c_{k}$$=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$</font>$=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle$\n",
        "\n",
        "\n",
        "* we want to calculate the coefficients $a_k$ and $b_k$ at each particular frequency (and each frequency is orthogonal to each other, both cos vs sine as well as cos k vs cos j)\n",
        "\n",
        "> <font color=\"blue\">$c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$ (inner product)\n",
        "\n",
        "\n",
        "**Most important is the inner product / projection on orthogonal basis vectors to get coefficients!!**\n",
        "\n",
        "The Fourier series definition is exactly the same as how we write a vector F in an orthogonal basis in $R^2$, in a 2 dimensional vector space.\n",
        "\n",
        "* I pick some basis (picture on top right) for example x and y,\n",
        "\n",
        "  * and then I take the inner product of F in the x-direction and then the inner product of F in the y-direction,\n",
        "\n",
        "  * and I take those coefficients, let's call it $a_1$ and $a_2$, and I take them and multiply them by the X unit vector and the Y unit vector, and I add them up\n",
        "\n",
        "* and in Fourier: sine and cosine functions are orthogonal, just like x and y are orthogonal vectors.\n",
        "\n",
        "  * And then i take my function f and project it on the sine and cosine to see how much of f is in this cosine direction and how much of f is in the sine direction\n",
        "\n",
        "  * from that I get my $A_k$-th coefficient and $B_k$-th coefficient, and then I multiply that by my cosine function (and sine function) and I add all of those up\n",
        "\n",
        "\n",
        "> $a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$\n",
        "\n",
        "> $a_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $b_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $\\omega_{0}=\\frac{2 \\pi}{T}$ $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "*T ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pY8yHtyJwMjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Finetune with approximation of f(x) to get between 0 and $L$, which is periodic in L and repeats beyond 0 and L**\n",
        "\n",
        "* we go now in the domain 0 to L (before above it was -$\\pi$ to $\\pi$)\n",
        "\n",
        "* now we make the sines and cosines periodic between 0 and L (=space of Lebesgue integrable functions)\n",
        "\n",
        "> $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "We the Fourier series for L-periodic functions (and not just 2 $\\pi$ like before):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos \\left(\\frac{2 \\pi k x}{L}\\right)+B_{k} \\sin \\left(\\frac{2 \\pi k x}{L}\\right)\\right)$\n",
        "\n",
        "How do we get A and B?\n",
        "\n",
        "* We take the inner product of f(x) with $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$ (und entsprechend auch fur sin)\n",
        "\n",
        "* this inner product is the projection of f(x) onto the orthogonal basis vector (here each k - $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$)\n",
        "\n",
        "* and then normalized by the norm of the cosine function, which in thhis case the norm squared is $\\frac{2}{L}$:\n",
        "\n",
        "> $A_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "> $B_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\sin \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "* these cosine and sine functions are an orthogonal basis for my function space (Hilbert space of functions f(x))\n",
        "\n",
        "* If I plugged in a cosine k and a sine j, these functions are orthogonal. Like if I plug in two cosines with different k's, their inner product is 0 (because they should be orthogonal). Umgekehrt: if I plug in cos kx and cos kx then I would get a non zero inner product.\n",
        "\n",
        "* btw: the approximation of f(x) between 0 and L will repeat infineily beyond 0 and l."
      ],
      "metadata": {
        "id": "52T4qM5-A2r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Introduce $e^{ikx}$ to combine sin and cos in one complex coefficient**\n",
        "\n",
        "**<u>Analysing Function</u>: Statt sin und cos kann man auch $e$ verwenden**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$ (Eulers formula)\n",
        "\n",
        "$e^{i k x}$ is the analyzing function made of sinusoids (eigentlich: $e^{-j 2 \\pi F t} d t$)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i x \\frac{k \\pi}{L}}$</font> (becoming a Riemann integral) with $\\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$</font> $= \\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "\n",
        "\n",
        "Anderes Beispiel mit einem Integral statt Summe geschrieben:\n",
        "\n",
        "> $X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$\n",
        "\n",
        "* basics: remember the Euler expansion:\n",
        "\n",
        "> $e^{i k x}=\\cos (k x)+i \\sin (k x)$\n",
        "\n",
        "* now we talk about the fourier series in terms of a complex basis:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$\n",
        "\n",
        "(you could expand that in sin and cos: $f(x)=\\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "See also [Inner Products in Hilbert Space](https://www.youtube.com/watch?v=g-eNeXlZKAQ&list=PLMrJAkhIeNNT_Xh3Oy0Y4LTj0Oxo8GqsC&index=4)\n",
        "\n",
        "* inner product of functions is consistent with definition of inner products of vectors\n",
        "\n",
        "* similar functions should have a large inner product\n",
        "\n",
        "* if I go to infinity with delta x (make it finer and finer), then the Riemann approximation $\\langle f,g \\rangle$ becomes the continuous integral formulation $\\int$\n",
        "\n",
        "You can expand f(x) as a sum of sines and cosines that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics of those basic sines and cosines.\n",
        "\n",
        "You can represent this as a complex Fourier series:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k \\pi \\frac{x}{L}} \\quad \\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency / Kreisfrequenz)\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$"
      ],
      "metadata": {
        "id": "dLbwYXpgvwx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√§herungsverfahren f√ºr periodische Funktionen: Fourier Series**\n",
        "\n",
        "* nutzt man weil zB manche periodische Funktionen sehr kompliziert sind, um sie mit einer Funktion f(x) zu beschreiben\n",
        "\n",
        "**N√§herungsverfahren f√ºr nicht-periodische Funktionen: Fourier Transform**\n",
        "\n",
        "**N√§herungsverfahren f√ºr Polynome: Taylor Polynom**\n",
        "\n",
        "https://medium.com/sho-jp/fourier-transform-101-part-1-b69ea3cb4837\n",
        "\n",
        "**Harmonic Analysis**\n",
        "\n",
        "Aus Sicht der [abstrakten harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) sind sowohl die Fourier-Reihen und die Fourier-Integrale als auch die Laplace-Transformation, die Mellin-Transformation oder auch die Hadamard-Transformation Spezialf√§lle einer **allgemeineren (Fourier-)Transformation**."
      ],
      "metadata": {
        "id": "MWiANZ7vSEOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "*We can clearly observe a peak value at 10 Hz with a magnitude of one while all other frequencies hover around zero. We can verify this from the original signal where there are 10 complete cycles in a second with an amplitude of one:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_49.png)\n",
        "\n",
        "*When a similar principle is applied to a more complicated time series as shown in the green plot below, we can deduce from its Fourier transform that the data comprises of 3 different elementary components with 3 different frequencies (2, 5 and 10 Hz) at 3 different amplitudes (0.5, 1 and 2):*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_50.png)\n",
        "\n",
        "Source: https://medium.com/@khairulomar/deconstructing-time-series-using-fourier-transform-e52dd535a44e"
      ],
      "metadata": {
        "id": "1q0EqJnVRvQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetrien**\n",
        "\n",
        "Die Kosinusfunktion ist achsensymmetrisch:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_51.png)\n",
        "\n",
        "In Worten: Ein x-Wert und der negative x-Wert haben denselben Kosinuswert. Als Formel: cos(‚àíùë•)=cosùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        ">$\\cos \\left(\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        ">$\\cos \\left(-\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        "Die Sinusfunktion ist punktsymmetrisch zum Koordinatenursprung. Stelle dir vor, wie du den rechten Arm des Graphen um (0|0) drehst.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_52.png)\n",
        "\n",
        "In Worten: sin(‚àíùë•)\n",
        "sin\n",
        "(\n",
        "-\n",
        "x\n",
        ")\n",
        " ist sinùë•\n",
        "sin\n",
        "x\n",
        " mit umgedrehtem Vorzeichen.\n",
        "Als Formel: sin(‚àíùë•)=‚àísinùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "> $\\sin \\left(\\frac{\\pi}{4}\\right)=0,71$\n",
        "\n",
        "> $\\sin \\left(-\\frac{\\pi}{4}\\right)=-0,71$\n",
        "\n",
        "Source: https://www.kapiert.de/sinus-und-kosinusfunktionen-eigenschaften/\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax6bbzUGSd6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_53.png)\n",
        "\n",
        "*Source: [Mathe mit Nina](https://youtu.be/u6Fqi8596qA)*\n"
      ],
      "metadata": {
        "id": "bdefArXifISa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeO_i6Z9-rLD"
      },
      "source": [
        "###### *Laplace Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhmFbOnmaUS"
      },
      "source": [
        "**Fourier** $\\quad X(\\omega)=\\int_{-\\infty}^{\\infty} x(t) e^{-i \\omega t} d t$\n",
        "\n",
        "**Laplace** $\\quad X(s)=\\int_{0}^{\\infty} x(t) e^{-s t} d t$\n",
        "\n",
        "with s = ${\\alpha + i \\omega}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1193.png)"
      ],
      "metadata": {
        "id": "msM9RjFYPKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1194.png)"
      ],
      "metadata": {
        "id": "qke32YL1QCrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Taylor Series*"
      ],
      "metadata": {
        "id": "xDUD9L1G7HIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor Series Expansion to approximate $e^x$**\n",
        "\n",
        "> <font color=\"blue\">$e^{x} \\approx \\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !} \\approx 1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\ldots$\n",
        "\n",
        "*(N√§herungsverfahren f√ºr Polynome: Taylor Polynom)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1347.jpg)\n",
        "\n",
        "Funktionsvorschrift:\n",
        "\n",
        "$a_{i}=a_{1} \\cdot q^{i-1}$ bzw $a_{i}=a_{0} \\cdot q^{i}$ f√ºr Anfangsglied a1. Oder auch (andere Folge): $a_{i}=a_{0} \\cdot q^{i}$\n",
        "\n",
        "Rekursionsvorschrift (wie Fibonacci):\n",
        "\n",
        "$a_{i+1}=a_{i} \\cdot q$. Oder auch (andere Folge): $a_{i}=q \\cdot a_{i-1}$\n",
        "\n",
        "*The following code examples are taken from ['Python for Undergraduate Engineers'](https://pythonforundergradengineers.com/creating-taylor-series-functions-with-python.html)*"
      ],
      "metadata": {
        "id": "rkbWd2CM6u6w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3QOyKV6llLn"
      },
      "source": [
        "**Taylor Series Expansion to approximate $cos(x)$**\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x) \\approx \\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !} \\approx 1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\frac{x^{8}}{8 !}-\\frac{x^{10}}{10 !}+\\ldots$\n",
        "\n",
        "* We can code this formula into a function that contains a for loop. Note the variable x is the value we are trying to find the cosine of, the variable n is the number of terms in the Taylor Series, and the variable i is the loop index which is also the Taylor Series term number.\n",
        "* We are using a separate variable for the coefficient coef which is equal to (‚àí1)<sup>i</sup>, the numerator num which is equal to x<sup>2i</sup> and the denominator denom which is equal to (2i!). Breaking the Taylor Series formula into three parts can cut down on coding errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Beziehung zwischen Sinus, Kosinus und Exponentialfunktion**\n",
        "\n",
        "Die [trigonometrischen Funktionen](https://de.m.wikipedia.org/wiki/Trigonometrische_Funktion) sind eng mit der [Exponentialfunktion](https://de.m.wikipedia.org/wiki/Exponentialfunktion) verbunden, wie die [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel) zeigt:\n",
        "\n",
        "Eigentlich hat $e$ nichts mit periodischen Funktionen wie $sin$ oder $cos$ zu tun. Aber f√ºgt man $i$ in den exponent von $e$, dann wird daraus eine rotierende, periodische Funktion.*\n",
        "\n",
        "**Part I: Let`s get the Taylor expansions for three common functions**\n",
        "\n",
        "> $e^{x}=1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{x^{5}}{5 !}+\\cdots$\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x)=1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\cdots$\n",
        "\n",
        "> <font color=\"red\">$\\sin (x)=x-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !}-\\frac{x^{7}}{7 !}+\\cdots$\n",
        "\n",
        "* there is some relation between these three:\n",
        "\n",
        "  * all all built upon $\\frac{x^{n}}{n !}$\n",
        "\n",
        "  * cosinus has all even numbers, sinus all odd numbers, and exponential both\n",
        "\n",
        "  * Vorzeichen is alternating for cos and sin, but all posisitv for exponential\n",
        "\n",
        "**Part II: Add $i$ to relate all three functions**\n",
        "\n",
        "* You can't just add up cos and sin to get exp! You need to add the $i = \\sqrt{-1}$\n",
        "\n",
        "* We need to replace all $x$ with an $i x$ in the exp series\n",
        "\n",
        "> $e^{ix}=1+ix+\\frac{(ix)^{2}}{2 !}+\\frac{(ix)^{3}}{3 !}+\\frac{(ix)^{4}}{4 !}+\\frac{(ix)^{5}}{5 !}+\\cdots$\n",
        "\n",
        "* It introduces the following: $i=i \\quad i^{2}=-1 \\quad i^{3}=-i \\quad i^{4}=1$\n",
        "\n",
        "* this turns the exponential into the following:\n",
        "\n",
        "> $e^{i x}=1+i x-\\frac{x^{2}}{2 !}-\\frac{ix^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{i x^{5}}{5 !}+$\n",
        "\n",
        "* finally, let's separate the terms with $i$ (all odd) from those without $i$ (all even):\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$\\left[1-\\frac{x^{2}}{2!}+\\frac{x^{4}}{4!}-\\cdots \\right]$</font> + $i$ <font color=\"red\">$\\left[x-\\frac{x^{3}}{3!}+\\frac{x^{5}}{5!}-\\cdots\\right]$</font>\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$cos(x)$</font> + $i$ <font color=\"red\">$ \\, sin(x)$</font>\n",
        "\n",
        "*$sin(x)$ ist der imagin√§re Anteil, und $cos(x)$ ist der reale Anteil f√ºr $z=e^{i \\varphi}=x+i y$*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/400px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "\n",
        "**Part III: Let`s replace $x$ with $\\pi$**\n",
        "\n",
        "> $e^{i \\pi}=\\cos (\\pi)+i \\sin (\\pi)$\n",
        "\n",
        "* sin and cos repeat and one full period is $2 \\pi$ radians (=360¬∞)\n",
        "\n",
        "* $cos(\\pi)$ is half a rotation = -1 and $sin(\\pi)$ = 0, which means:\n",
        "\n",
        "> $e^{i \\pi}= -1$ $\\,$ and $\\,$ $e^{2\\pi i} = 1$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIb2bU82PFrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Music Theory*"
      ],
      "metadata": {
        "id": "fq5mO57kWnjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A beginner's guide to music theory](https://youtu.be/n2z02J4fJwg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1372.jpg)"
      ],
      "metadata": {
        "id": "enDRiPrwWj_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The mathematical problem with music, and how to solve it](https://youtu.be/nK2jYk37Rlg)\n",
        "\n",
        "* Frequency (in Hertz) = no. of vibrations per second. Typically between 50 to a few thousand Hertz. Higher frequency = higher pitch (i.e. 200. vs 1000)\n",
        "\n",
        "* all musical tones are multiples of pure tones (harmonics) = behave to the mathematical sine function. For example tone f consists of frequencies of several integer multiples of a pure tone f, 2f, 3f.. (first harmonic, second harmonic, third harmonic..).\n",
        "\n",
        "* Melodies = sequences of tones. Strictly speaking these two melodies have not even one frequency in common, but they sound similar. But both melodies have the **same ratios between the frequencies of their tones.** Frequencies are different, but ratios are the same.\n",
        "\n",
        "* Changing from melody 1 to melody 2 is called **transposition from one key to another**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1373.jpg)\n",
        "\n",
        "* Another important concept is the **Musical intervall**, which is the frequency ratio mentioned above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1374.jpg)\n",
        "\n",
        "* One important interval is called the octave, and it corresponds to a frequency ratio of 2:1. The Octave is very pleasent to the ear. **Octave equivalence**: two tones that are an Octave apart sound highly similar: hence have same name and belong to the same pitch class.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1375.jpg)\n",
        "\n",
        "* Another important is called the **Fifth** with ration 3 over 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1376.jpg)\n"
      ],
      "metadata": {
        "id": "A5ewjWi3Xe1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGNbw_qosx7"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionentheorie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-vaCcWrF4t"
      },
      "source": [
        "Die [Funktionentheorie](https://de.wikipedia.org/wiki/Funktionentheorie) befasst sich mit der Theorie differenzierbarer komplexwertiger Funktionen mit komplexen Variablen. Da insbesondere die Funktionentheorie einer komplexen Variablen reichlich Gebrauch von Methoden aus der reellen Analysis macht, nennt man das Teilgebiet auch komplexe Analysis.\n",
        "\n",
        "* F√ºr holomorphe Funktionen gilt, dass Real- und Imagin√§rteil [harmonische Funktionen](https://de.wikipedia.org/wiki/Harmonische_Funktion) sind, also die [Laplace-Gleichung](https://de.wikipedia.org/wiki/Laplace-Gleichung) erf√ºllen. Dies verkn√ºpft die Funktionentheorie mit den partiellen Differentialgleichungen, beide Gebiete haben sich regelm√§√üig gegenseitig beeinflusst.\n",
        "\n",
        "* Das Wegintegral einer holomorphen Funktion ist vom Weg unabh√§ngig. Dies war historisch das erste Beispiel einer Homotopieinvarianz. Aus diesem Aspekt der Funktionentheorie entstanden viele Ideen der algebraischen Topologie, beginnend mit Bernhard Riemann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmbwvHZAec-"
      },
      "source": [
        "**Komplexe Funktion**\n",
        "\n",
        "* Eine [komplexe Funktion](https://de.wikipedia.org/wiki/Funktionentheorie#Komplexe_Funktionen) ordnet einer komplexen Zahl eine weitere [komplexe Zahl](https://de.m.wikipedia.org/wiki/Komplexe_Zahl) zu\n",
        "\n",
        "* Da jede komplexe Zahl durch zwei reelle Zahlen in\n",
        "der Form $x+$ iy geschrieben werden kann, l√§sst sich eine allgemeine Form einer komplexen Funktion darstellen durch:\n",
        "\n",
        "> $\n",
        "x+i y \\mapsto f(x+i y)=u(x, y)+i v(x, y)\n",
        "$\n",
        "\n",
        "* Dabei sind $u(x, y)$ und $v(x, y)$ reelle Funktionen, die von zwei reellen Variablen $x$ und $y$ abh√§ngen.\n",
        "\n",
        "* $u(x, y)$ hei√üt der Realteil und $v(x, y)$ der Imagin√§rteil der Funktion.\n",
        "\n",
        "* **Insofern ist eine komplexe Funktion nichts anderes als eine Abbildung von $\\mathbb{R}^{2}$ nach $\\mathbb{R}^{2}$ (also eine Abbildung, die zwei reellen Zahlen wieder zwei reelle Zahlen zuordnet).**\n",
        "\n",
        "* **Tats√§chlich k√∂nnte man die Funktionentheorie auch mit Methoden der reellen Analysis aufbauen.**\n",
        "\n",
        "* Der Unterschied zur reellen Analysis wird erst deutlicher, wenn man **komplex-differenzierbare Funktionen** betrachtet und dabei die <u>**multiplikative Struktur des K√∂rpers der komplexen Zahlen**</u> ins Spiel bringt, die dem Vektorraum $\\mathbb{R}^{2}$ fehlt.\n",
        "\n",
        "* Wie auch bei reellwertigen und reellen Funktionen ist die Verwendung des Begriffes einer komplexen Funktion in der Literatur aber nicht eindeutig. Teilweise wird er synonym mit einer komplexwertigen Funktion verwendet, teilweise wird er auch nur f√ºr komplexwertige Funktionen einer komplexen Variablen verwendet, also Funktionen\n",
        "\n",
        "> $\n",
        "f: D \\rightarrow \\mathbb{C}\n",
        "$\n",
        "\n",
        "bei denen $D \\subseteq \\mathbb{C}$ ist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDS2jqpHCZov"
      },
      "source": [
        "**Komplexwertige Funktion**\n",
        "\n",
        "* Eine [komplexwertige Funktion](https://de.wikipedia.org/wiki/Komplexwertige_Funktion) ist eine Funktion, deren Funktionswerte komplexe Zahlen sind\n",
        "\n",
        "* Eng damit verwandt ist der Begriff der **komplexen Funktion**, der in der Literatur aber nicht eindeutig verwendet wird\n",
        "\n",
        "* Komplexwertige Funktionen werden in der Analysis und in der Funktionentheorie untersucht und haben vielf√§ltige Anwendungen wie zum Beispiel in der Physik und der Elektrotechnik, wo sie beispielsweise zur Beschreibung von Schwingungen dienen.\n",
        "\n",
        "* Eine komplexwertige Funktion ist eine Funktion\n",
        "$f: D \\rightarrow \\mathbb{C}$ bei der die Zielmenge die Menge der komplexen Zahlen ist.\n",
        "\n",
        "* **An die Definitionsmenge $D$ sind keine Anforderungen gestellt**.\n",
        "\n",
        "* **Aufgrund der Einbettung der reellen Zahlen in die komplexen Zahlen lassen sich alle reellwertigen Funktionen auch als komplexwertige Funktionen auffassen.**\n",
        "\n",
        "* Beispiel: Die Funktion $f: \\mathbb{R} \\rightarrow \\mathbb{C}$ definiert durch $f(x)= \\mathrm{e}^{\\mathrm{i} x}=\\cos (x)+\\mathrm{i} \\sin (x)$ ist eine komplexwertige Funktion einer reellen Variable, und zwar die [Eulersche Formel](https://de.wikipedia.org/wiki/Eulersche_Formel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpqu4CjYvL7"
      },
      "source": [
        "**Holomorphe Funktion**\n",
        "\n",
        "* Holomorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* [Holomorphe Funktionen](https://de.wikipedia.org/wiki/Holomorphe_Funktion) sind **an jedem Punkt komplex differenzierbar**.\n",
        "\n",
        "* Eine Funktion $f\\colon U\\to {\\mathbb  {C}}$ mit einer offenen Menge $ U\\subseteq \\mathbb {C}$ hei√üt holomorph, falls sie in jedem Punkt von $U$ komplex differenzierbar ist.\n",
        "\n",
        "* Holomorphie eine sehr starke Eigenschaft ist, die im Reellen kein Pendant besitzen (z.B. ist jede holomorphe Funktion beliebig oft (stetig) differenzierbar und l√§sst sich lokal in jedem Punkt in eine Potenzreihe entwickeln.)\n",
        "\n",
        "Es sei $U \\subseteq \\mathbb{C}$ eine offene Teilmenge der komplexen Ebene und $z_{0} \\in U$ ein Punkt dieser Teilmenge. Eine Funktion $f: U \\rightarrow \\mathbb{C}$ hei√üt komplex differenzierbar im Punkt $z_{0}$, falls der Grenzwert\n",
        "\n",
        ">$\n",
        "\\lim _{h \\rightarrow 0} \\frac{f\\left(z_{0}+h\\right)-f\\left(z_{0}\\right)}{h}\n",
        "$\n",
        "\n",
        "existiert. Man bezeichnet ihn dann als $f^{\\prime}\\left(z_{0}\\right)$.\n",
        "\n",
        "Die Funktion $f$ hei√üt holomorph im Punkt $z_{0}$, falls eine Umgebung von $z_{0}$ existiert, in der $f$ komplex differenzierbar ist. Ist $f$ auf ganz $U$ holomorph, so nennt man $f$ holomorph.\n",
        "\n",
        "**Ist weiter $U=\\mathbb{C},$ so nennt man $f$ eine <u>ganze Funktion</u>**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeegSuMPXcu"
      },
      "source": [
        "**Biholomorphe Funktionen**\n",
        "\n",
        "* Eine Funktion, die holomorph, bijektiv und deren Umkehrfunktion holomorph ist, nennt man [biholomorph](https://de.wikipedia.org/wiki/Biholomorphe_Abbildung).\n",
        "\n",
        "* Im Fall einer komplexen Ver√§nderlichen ist das √§quivalent dazu, dass die Abbildung bijektiv und konform ist.\n",
        "* Aus Sicht der Kategorientheorie ist eine biholomorphe Abbildung ein Isomorphismus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StjTRqJOsq4"
      },
      "source": [
        "**Cauchy-Riemannsche Differentialgleichungen**\n",
        "\n",
        "* Siehe [Cauchy-Riemannsche Differentialgleichungen](https://de.wikipedia.org/wiki/Cauchy-Riemannsche_partielle_Differentialgleichungen) - Mit einer Einleitung [hier](https://de.wikipedia.org/wiki/Holomorphe_Funktion#Cauchy-Riemannsche_Differentialgleichungen).\n",
        "\n",
        "* Zerlegt man eine Funktion $f(x+i y)=u(x, y)+i v(x, y)$ in ihren Real-und Imagin√§rteil mit reellen Funktionen $u, v,$ so hat die totale Ableitung $L$ als Darstellungsmatrix die **Jacobi-Matrix**\n",
        "\n",
        ">$\n",
        "\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
        "\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Folglich ist die Funktion $f$ genau dann komplex differenzierbar, wenn sie reell differenzierbar ist und f√ºr $u, v$ die Cauchy-Riemannschen Differentialgleichungen\n",
        "\n",
        ">$\n",
        "\\begin{array}{l}\n",
        "\\frac{\\partial u}{\\partial x}=\\frac{\\partial v}{\\partial y} \\\\\n",
        "\\frac{\\partial u}{\\partial y}=-\\frac{\\partial v}{\\partial x}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "erf√ºllt sind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNafAzsW_vP"
      },
      "source": [
        "**Analytische Funktion (=holomorph)**\n",
        "\n",
        "* Als [analytische Funktion](https://de.wikipedia.org/wiki/Analytische_Funktion) bezeichnet man eine Funktion, die lokal durch eine **konvergente Potenzreihe** gegeben ist.\n",
        "\n",
        "* Aufgrund der Unterschiede zwischen reeller und komplexer Analysis spricht man zur Verdeutlichung oft auch explizit von reell-analytischen oder komplex-analytischen Funktionen.\n",
        "\n",
        "* **Im Komplexen sind die Eigenschaften analytisch und holomorph √§quivalent.**\n",
        "\n",
        "* **Ist eine Funktion in der gesamten komplexen Ebene definiert und analytisch, nennt man sie ganz.**\n",
        "\n",
        "\n",
        "Es sei $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{K}=\\mathbb{C} .$ Es sei $D \\subseteq \\mathbb{K}$ eine offene Teilmenge. Eine Funktion $f: D \\rightarrow \\mathbb{K}$ hei√üt analytisch im Punkt $x_{0} \\in D,$ wenn es eine **Potenzreihe**\n",
        "\n",
        ">$\n",
        "\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "gibt, die auf einer Umgebung von $x_{0}$ gegen $f(x)$ konvergiert. Ist $f$ in jedem Punkt von $D$ analytisch, so hei√üt $f$ analytisch.\n",
        "\n",
        "Viele g√§ngige Funktionen der reellen Analysis wie beispielsweise Polynome, Exponential- und Logarithmusfunktionen, trigonometrische Funktionen und rationale Ausdr√ºcke in diesen Funktionen sind analytisch.\n",
        "\n",
        "Unter einer [Potenzreihe](https://de.wikipedia.org/wiki/Potenzreihe) $P(x)$ versteht man in der Analysis eine unendliche Reihe der Form\n",
        "\n",
        ">$\n",
        "P(x)=\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "mit einer beliebigen Folge $\\left(a_{n}\\right)_{n \\in \\mathbb{N}_{0}}$ reeller oder komplexer Zahlen\n",
        "dem Entwicklungspunkt $x_{0}$ der Potenzreihe.\n",
        "\n",
        "Potenzreihen spielen eine wichtige Rolle in der Funktionentheorie und **erlauben oft eine sinnvolle Fortsetzung reeller Funktionen in die komplexe Zahlenebene**. Insbesondere stellt sich die Frage, f√ºr welche reellen oder komplexen Zahlen eine Potenzreihe konvergiert. Diese Frage f√ºhrt zum Begriff des [Konvergenzradius](https://de.wikipedia.org/wiki/Konvergenzradius).\n",
        "\n",
        "* Jede Polynomfunktion l√§sst sich als Potenzreihe auffassen, bei der fast alle Koeffizienten $a_{n}$ gleich 0 sind.\n",
        "\n",
        "* Wichtige andere Beispiele sind **Taylorreihe** und **Maclaurinsche Reihe**.\n",
        "\n",
        "* Funktionen, die sich durch eine Potenzreihe darstellen lassen, werden auch [analytische Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) genannt.\n",
        "\n",
        "**Beispielhaft die Potenzreihendarstellung einiger bekannter Funktionen**:\n",
        "\n",
        "* **Exponentialfunktion**: $e^{x}=\\exp (x)=\\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !}=\\frac{x^{0}}{0 !}+\\frac{x^{1}}{1 !}+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\cdots$ f√ºr alle\n",
        "$x \\in \\mathbb{R},$ d. h., der Konvergenzradius ist unendlich.\n",
        "\n",
        "* **Sinus**: $\\sin (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n+1}}{(2 n+1) !}=\\frac{x}{1 !}-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !} \\mp \\cdots$\n",
        "\n",
        "* **Kosinus**: $\\cos (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !}=\\frac{x^{0}}{0 !}-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !} \\mp \\cdots$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GiFSM9LMFFp"
      },
      "source": [
        "**Ganze Funktion**\n",
        "\n",
        "* In der Funktionentheorie ist eine [ganze Funktion](https://de.wikipedia.org/wiki/Ganze_Funktion) eine Funktion, **die in der gesamten komplexen Zahleneben**e $\\mathbb {C}$  holomorph (also analytisch) ist.\n",
        "\n",
        "* an entire function, also called an integral function, is a complex-valued function that is holomorphic at all finite points over the whole complex plane.\n",
        "\n",
        "* **Every entire function f(z) can be represented as a power series**\n",
        "\n",
        "* Typische Beispiele ganzer Funktionen sind Polynome oder die **Exponentialfunktion** sowie Summen, Produkte und Verkn√ºpfungen davon, etwa die **trigonometrischen Funktionen** und die **Hyperbelfunktionen**.\n",
        "\n",
        "* Jede ganze Funktion kann als eine √ºberall konvergierende Potenzreihe um ein beliebiges Zentrum dargestellt werden. Weder der Logarithmus noch die Wurzelfunktion sind ganz.\n",
        "\n",
        "* Eine ganze Funktion kann eine **isolierte Singularit√§t**, insbesondere sogar eine wesentliche Singularit√§t im komplexen Punkt im Unendlichen (und nur da) besitzen.\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* der Kehrwert der Gammafunktion $1 / \\Gamma(z)$\n",
        "\n",
        "* die Fehlerfunktion $\\operatorname{erf}(z)$\n",
        "\n",
        "* der Integralsinus $\\operatorname{Si}(z)$\n",
        "\n",
        "* die Airy-Funktionen $\\operatorname{Ai}(z)$ und $\\operatorname{Bi}(z)$\n",
        "\n",
        "* die Fresnelschen Integrale $S(z)$ und $C(z)$\n",
        "\n",
        "* die Riemannsche Xi-Funktion $\\xi(z)$\n",
        "\n",
        "* die Besselfunktionen erster Art $J_{n}(z)$ f√ºr ganzzahlige $n$\n",
        "\n",
        "* die Struve-Funktionen $H_{n}(z)$ f√ºr ganzzahlige $n>-2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Era7KDwoxIS"
      },
      "source": [
        "**Laurent-Reihe**\n",
        "\n",
        "**Exkurs**: Die Laurent-Reihe ist eine **unendliche Reihe √§hnlich einer Potenzreihe**, aber zus√§tzlich **mit negativen Exponenten**. Allgemein hat eine Laurent-Reihe in $x$ mit Entwicklungspunkt $c$ diese Gestalt (Dabei sind die $a_{n}$ und $c$ meist komplexe Zahlen):\n",
        "\n",
        ">$\n",
        "f(x)=\\sum_{n=-\\infty}^{\\infty} a_{n}(x-c)^{n}\n",
        "$\n",
        "\n",
        "Es sei $D$ eine nichtleere offene Teilmenge der Menge $\\mathbb{C}$ der komplexen Zahlen und $P_{f}$ eine weitere Teilmenge von $\\mathbb{C}$, die nur aus isolierten Punkten besteht. Eine Funktion $f$ hei√üt meromorph, wenn sie f√ºr Werte aus $D \\backslash P_{f}$ definiert und holomorph ist und f√ºr Werte aus $P_{f}$ Pole hat. $P_{f}$ wird als Polstellenmenge von $f$ bezeichnet.\n",
        "\n",
        "* Zerlegung einer komplex differenzierbaren Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4Q4vb9RlIR"
      },
      "source": [
        "**Meromorphe Funktion**\n",
        "\n",
        "* Meromorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* F√ºr viele Fragestellungen der Funktionentheorie ist der **Begriff der holomorphen Funktion zu speziell**.\n",
        "\n",
        "* Dies liegt daran, dass der Kehrwert $\\frac{1}{f}$ einer holomorphen Funktion $f$ an einer Nullstelle von $f$ eine Definitionsl√ºcke hat und somit $\\frac{1}{f}$ dort auch **nicht komplex differenzierbar ist**.\n",
        "\n",
        "* Man f√ºhrt daher den allgemeineren Begriff der [meromorphen Funktion](https://de.wikipedia.org/wiki/Meromorphe_Funktion) ein, die auch **isolierte Polstellen** besitzen kann.\n",
        "\n",
        "* Meromorphe Funktionen lassen sich lokal als [Laurentreihen](https://de.wikipedia.org/wiki/Laurent-Reihe) mit abbrechendem Hauptteil darstellen. Ist $U$ ein Gebiet von $\\mathbb{C},$ so bildet die Menge der auf $U$ meromorphen Funktionen einen K√∂rper.\n",
        "\n",
        "* Alle holomorphen Funktionen sind auch meromorph, da ihre Polstellenmenge leer ist.\n",
        "\n",
        "Zum Beispiel ist die Gamma-Funktion meromorph, weil sie holomorph ist auf $\\mathbb{C}$, abgesehen von abzahlbar vielen nicht-hebbaren Singularitaeten (hierbei in allen negativen ganzen Zahlen, da der Definitionsbereich einer Gammafunktion $\\mathbb{C}$  \\ -$\\mathbb{N}$ <sub>0</sub> ist).\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* [Gamma Funktion](https://en.wikipedia.org/wiki/Gamma_function)\n",
        "\n",
        "* [Elliptische Funktion](https://de.wikipedia.org/wiki/Elliptische_Funktion)\n",
        "\n",
        "*Der Absolutwert der Gammafunktion geht nach Unendlich an den Polstellen (links). Rechts hat sie keine Polstellen und steigt nur schnell an.*\n",
        "\n",
        "*The [gamma function](https://en.wikipedia.org/wiki/Gamma_function) is meromorphic in the whole complex plane.*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/3/33/Gamma_abs_3D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjZAY8WRubkK"
      },
      "source": [
        "**Isolierte Singularit√§t**\n",
        "\n",
        "* [Isolierte Singularit√§ten](https://de.wikipedia.org/wiki/Isolierte_Singularit√§t) sind besondere [**isolierte Punkte**](https://de.wikipedia.org/wiki/Isolierter_Punkt) in der Quellmenge einer holomorphen Funktion.\n",
        "\n",
        "* Man unterscheidet bei isolierten Singularit√§ten zwischen **hebbaren Singularit√§ten**, **Polstellen** und **wesentlichen Singularit√§ten**.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{C}$ eine offene Teilmenge, $z_{0} \\in \\Omega$. Ferner sei $f: \\Omega \\backslash\\left\\{z_{0}\\right\\} \\rightarrow \\mathbb{C}$ eine holomorphe komplexwertige Funktion. Dann hei√üt $z_{0}$ isolierte Singularit√§t von $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAgJBUpT0Oo"
      },
      "source": [
        "**Klasse 1: Hebbare Singularit√§t (Definitionsl√ºcke)**\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [hebbare Singularit√§t](https://de.wikipedia.org/wiki/Definitionsl√ºcke), wenn $f$ auf $\\Omega$ holomorph fortsetzbar ist.\n",
        "\n",
        "* Hat Grenzwert in Form von einer Zahl bzw. nach dem riemannschen Hebbarkeitssatz, wenn $f$ in einer Umgebung von $z_{0}$ beschr√§nkt ist.\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z)=c$\n",
        "\n",
        "> $f(z)$ beschrankt in $U\\left(z_{0}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAm2tWA_vART"
      },
      "source": [
        "**Klasse 2: Polstelle**\n",
        "\n",
        "* im Prinzip das gleiche wie bei hebbaren Singularitaten, aber mit dem Unterschied, dass die Funktion nah an der Singularitat unbeschraenkt ist = also ins unendliche geht $\\lim _{z \\rightarrow z_{0}}|f(z)|=\\infty$\n",
        "\n",
        "Alternative Definition:\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z) \\cdot\\left(z-z_{0}\\right)^{k}=c \\neq 0(k \\geqslant 1)$\n",
        "\n",
        "Man multipliziert an den unendlichen Funktionswert immer oefter eine fast Null in Form des Polynoms $\\left(z-z_{0}\\right)^{k}$, bis man eine endliche komplexe Zahl als Grenzwert hat, und nicht mehr unendlich (aber nicht Null). Die Anzahl der Polynome k multipliziert mit dem Funktionswert ist der Grad / Ordnung der Polstelle.\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [Polstelle](https://de.wikipedia.org/wiki/Polstelle) oder $P o l$, wenn\n",
        "\n",
        "  * $z_{0}$ **keine hebbare Singularit√§t ist** und\n",
        "  * es eine nat√ºrliche $\\operatorname{Zahl} k$ gibt, sodass $\\left(z-z_{0}\\right)^{k} \\cdot f(z)$ **eine hebbare Singularit√§t bei $z_{0}$ hat**.\n",
        "\n",
        "* Ist das $k$ minimal gew√§hlt, dann sagt man $f$ habe in $z_{0}$ einen Pol $k$ -ter Ordnung.\n",
        "\n",
        "* Man bezeichnet eine einpunktige Definitionsl√ºcke einer Funktion als Polstelle oder auch k√ºrzer als Pol, wenn die Funktionswerte in jeder Umgebung des Punktes (betragsm√§√üig) beliebig gro√ü werden.\n",
        "\n",
        "* Damit geh√∂ren die Polstellen zu den isolierten Singularit√§ten.\n",
        "\n",
        "* Das Besondere an Polstellen ist, dass sich die Punkte in einer Umgebung **nicht chaotisch verhalten, sondern in einem gewissen Sinne gleichm√§√üig gegen unendlich streben**. Deshalb k√∂nnen dort Grenzwertbetrachtungen durchgef√ºhrt werden.\n",
        "\n",
        "* Generell spricht man nur bei [glatten](https://de.wikipedia.org/wiki/Glatte_Funktion) (stetig & differenzierbar) oder [analytischen Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) von Polen.\n",
        "\n",
        "*Fur reelle Funktionen: f(x)=1/x hat einen Pol erster Ordnung an der Stelle x=0**\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/9/90/GraphKehrwertfunktion.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNpaYyHVD4o"
      },
      "source": [
        "**Klasse 3: Wesentliche Singularit√§t**\n",
        "\n",
        "* nicht hebbar und keine Polstelle, dann hei√üt $z_{0}$ eine wesentliche Singularit√§t von $f$\n",
        "\n",
        "* zum Beispiel fur $f(z)=e^{-\\frac{1}{z}}, z_{0}=0$, eine Funktion die von links ins unendliche strebt, und von rechts nach Null\n",
        "  * keine hebbare Singularitaet, weil zwei verschiedene Grenzwerte existieren und eine davon nicht endlich ist.\n",
        "  * keine Polstelle, weil ein Grenzwert gleich Null ist (muss aber unbeschraenkt sein bei Polstellen)\n",
        "\n",
        "\n",
        "*Plot der Funktion $\\exp(1/z)$. Sie hat im Nullpunkt eine wesentliche Singularit√§t (Bildmitte). Der Farbton entspricht dem komplexen Argument des Funktionswertes, w√§hrend die Helligkeit seinen Betrag darstellt. Hier sieht man, dass sich die wesentliche Singularit√§t unterschiedlich verh√§lt, je nachdem, wie man sich ihr n√§hert (im Gegensatz dazu w√§re ein Pol gleichm√§√üig wei√ü).*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/0/0b/Essential_singularity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSHsrLNpJDv"
      },
      "source": [
        "**Cauchysche Integralformel**\n",
        "\n",
        "* Die [cauchysche Integralformel](https://de.wikipedia.org/wiki/Cauchysche_Integralformel) ist eine der fundamentalen Aussagen der Funktionentheorie, eines Teilgebietes der Mathematik.\n",
        "\n",
        "* Sie besagt in ihrer schw√§chsten Form, dass die Werte einer holomorphen Funktion $f$ im Inneren einer Kreisscheibe bereits durch ihre Werte auf dem Rand dieser Kreisscheibe bestimmt sind.\n",
        "\n",
        "* Eine starke Verallgemeinerung davon ist der Residuensatz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVLX7lrIqBtX"
      },
      "source": [
        "**Residuensatz**\n",
        "\n",
        "* Der [Residuensatz](https://de.wikipedia.org/wiki/Residuensatz) ist ein wichtiger Satz der Funktionentheorie. Er stellt eine **Verallgemeinerung des cauchyschen Integralsatzes und der cauchyschen Integralformel** dar. Seine Bedeutung liegt nicht nur in den weitreichenden Folgen innerhalb der Funktionentheorie, sondern auch in der Berechnung von Integralen √ºber reelle Funktionen.\n",
        "\n",
        "* Er besagt, dass das Kurvenintegral l√§ngs einer geschlossenen Kurve √ºber eine bis auf isolierte Singularit√§ten holomorphe Funktion lediglich vom Residuum in den Singularit√§ten im Innern der Kurve und der Umlaufzahl der Kurve um diese Singularit√§ten abh√§ngt. Anstelle eines Kurvenintegrals muss man also nur Residuen und Umlaufzahlen berechnen, was in vielen F√§llen einfacher ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivq0YtFjjOPz"
      },
      "source": [
        "**Cauchyscher Integralsatz**\n",
        "\n",
        "* Der [cauchysche Integralsatz](https://de.wikipedia.org/wiki/Cauchyscher_Integralsatz)  ist einer der wichtigsten S√§tze der Funktionentheorie.\n",
        "\n",
        "* Er handelt von Kurvenintegralen f√ºr holomorphe (auf einer offenen Menge komplex-differenzierbare) Funktionen.\n",
        "\n",
        "* Im Kern besagt er, dass zwei dieselben Punkte verbindende Wege das gleiche Wegintegral besitzen, falls die Funktion √ºberall zwischen den zwei Wegen holomorph ist.\n",
        "\n",
        "* Der Satz gewinnt seine Bedeutung unter anderem daraus, dass man ihn zum Beweis der cauchyschen Integralformel und des Residuensatzes benutzt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHwO7VW3o75X"
      },
      "source": [
        "##### <font color=\"blue\">*Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JijVDdYpeTw"
      },
      "source": [
        "###### *Chaos Theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJSt90Zne8M"
      },
      "source": [
        "**Chaos Theory**\n",
        "\n",
        "* Many phenomena in nature can be described by dynamical systems; [chaos theory](https://en.m.wikipedia.org/wiki/Chaos_theory) makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n",
        "\n",
        "* [Complexity theory](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory. Chaos theory is a method of qualitative and quantitative analysis to investigate the behavior of [dynamical systems](https://en.m.wikipedia.org/wiki/Dynamical_system) that cannot be explained and predicted by single data relationships, but must be explained and predicted by whole, continuous data relationships.\n",
        "\n",
        "* Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random.\n",
        "\n",
        "* The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things:\n",
        "\n",
        "  * how much uncertainty can be tolerated in the forecast (uncertainty in a forecast increases exponentially with elapsed time)\n",
        "  * how accurately its current state can be measured,\n",
        "  * and a time scale depending on the dynamics of the system, called the [Lyapunov time](https://en.m.wikipedia.org/wiki/Lyapunov_time) (chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years)\n",
        "\n",
        "* In practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Lyapunov exponent*"
      ],
      "metadata": {
        "id": "RTdYXBryoZqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lyapunov exponent**\n",
        "\n",
        "* the Lyapunov exponent or Lyapunov characteristic exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories.\n",
        "\n",
        "* Quantitatively, two trajectories in phase space with initial separation vector $\\delta \\mathbf {Z} _{0}$ diverge (provided that the divergence can be treated within the linearized approximation) at a rate given by\n",
        "\n",
        "> ${\\displaystyle |\\delta \\mathbf {Z} (t)|\\approx e^{\\lambda t}|\\delta \\mathbf {Z} _{0}|}$\n",
        "\n",
        "where $\\lambda$ is the Lyapunov exponent.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1197.png)"
      ],
      "metadata": {
        "id": "uoo8xNAEmiam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complex Systems*"
      ],
      "metadata": {
        "id": "fhdD4JnqZHKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on4GTn_0efWt"
      },
      "source": [
        "**Complex Systems**\n",
        "\n",
        "* [Complex Systems](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* [Chaos](https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory) is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "*Properties:*\n",
        "\n",
        "1. **Agentenbasiert**: Komplexe Systeme bestehen aus einzelnen Teilen, die miteinander in Wechselwirkung stehen (Molek√ºle, Individuen, Software-Agenten etc.).\n",
        "2. **Nichtlinearit√§t**: Kleine St√∂rungen des Systems oder minimale Unterschiede in den Anfangsbedingungen f√ºhren oft zu sehr unterschiedlichen Ergebnissen (Schmetterlingseffekt, Phasen√ºberg√§nge). Die Wirkzusammenh√§nge der Systemkomponenten sind im Allgemeinen nichtlinear.\n",
        "3. [**Emergenz**](https://de.wikipedia.org/wiki/Emergenz): Im Gegensatz zu lediglich komplizierten Systemen zeigen komplexe Systeme Emergenz. Entgegen einer verbreiteten Vereinfachung bedeutet Emergenz nicht, dass die Eigenschaften der emergierenden Systemebenen von den darunter liegenden Ebenen vollst√§ndig unabh√§ngig sind. Emergente Eigenschaften lassen sich jedoch auch nicht aus der isolierten Analyse des Verhaltens einzelner Systemkomponenten erkl√§ren und nur sehr begrenzt ableiten.\n",
        "4. **Wechselwirkung** (Interaktion): Die Wechselwirkungen zwischen den Teilen des Systems (Systemkomponenten) sind lokal, ihre Auswirkungen in der Regel global.\n",
        "5. **Offenes System**: Komplexe Systeme sind √ºblicherweise offene Systeme. Sie stehen also im Kontakt mit ihrer Umgebung und befinden sich fern vom thermodynamischen Gleichgewicht. Das bedeutet, dass sie von einem permanenten Durchfluss von Energie bzw. Materie abh√§ngen.\n",
        "6. **Selbstorganisation**: Dies erm√∂glicht die Bildung insgesamt stabiler Strukturen (Selbststabilisierung oder Hom√∂ostase), die ihrerseits das thermodynamische Ungleichgewicht aufrechterhalten. Sie sind dabei in der Lage, Informationen zu verarbeiten bzw. zu lernen.\n",
        "7. **Selbstregulation**: Dadurch k√∂nnen sie die F√§higkeit zur inneren Harmonisierung entwickeln. Sie sind also in der Lage, aufgrund der Informationen und derer Verarbeitung das innere Gleichgewicht und Balance zu verst√§rken.\n",
        "8. **Pfade**: Komplexe Systeme zeigen Pfadabh√§ngigkeit: Ihr zeitliches Verhalten ist nicht nur vom aktuellen Zustand, sondern auch von der Vorgeschichte des Systems abh√§ngig.\n",
        "9. **Attraktoren**: Die meisten komplexen Systeme weisen so genannte Attraktoren auf, d. h., dass das System unabh√§ngig von seinen Anfangsbedingungen bestimmte Zust√§nde oder Zustandsabfolgen anstrebt, wobei diese Zustandsabfolgen auch chaotisch sein k√∂nnen; dies sind die ‚Äûseltsamen Attraktoren‚Äú der Chaosforschung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YzAyotrq51A"
      },
      "source": [
        "* Complexity theory is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIwXOz85zxW_"
      },
      "source": [
        "###### *Fractal Dimensions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzaMSBJmGj_F"
      },
      "source": [
        "**Fractal Geometry**\n",
        "\n",
        "* a [fractal](https://en.m.wikipedia.org/wiki/Fractal) is a subset of Euclidean space with a fractal dimension that strictly exceeds its topological dimension. Fractals appear the same at different scales, as illustrated in successive magnifications of the Mandelbrot set.\n",
        "\n",
        "* Fractals exhibit similar patterns at increasingly smaller scales, a property called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge,it is called affine self-similar.\n",
        "\n",
        "* idealization is that everything is smooth (rebellion against calculus, differentiable, where assumption is things look smooth if you zoom in enough). Mandelbrot: nature is fractal (capture roughness)\n",
        "\n",
        "* self-similar shapes give a basis for modeling the regularity in some forms of roughness. but that doesn't mean all is only perfectly self-similar either!! (Perfect self similar are: Von Koch snowflake, Sierpensky triangle)\n",
        "\n",
        "> <font color=\"blue\">Fractal dimension: Sierpensky triangle is 1,585 dimensional, Von Koch snowflake is 1,262 dimensional, Britain coast line 1,21 dimensional, Norway: 1,52 dimensional, calm sea 2,05 dimensional, waves 2,3 dimensional</font>\n",
        "\n",
        "> **Fractals are shapes with a non-integer dimension, captures idea of roughness, but dimension can vary depending on how much you zoom in**. But a shape is considered a fractal only when the measures dimension stays approximately constant across multiple different scales.\n",
        "\n",
        "> Useful in security: **Is something fractal? Yes - then probably from nature. No - then probably man-made**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deterministic fractals**:\n",
        "  * [Julia set](https://de.m.wikipedia.org/wiki/Julia-Menge) - siehe auch [Newtonfraktal](https://de.m.wikipedia.org/wiki/Newtonfraktal). The Julia set and the Fatou set are two complementary sets (Julia \"laces\" and Fatou \"dusts\").\n",
        "  * [Mandelbrot set](https://en.m.wikipedia.org/wiki/Mandelbrot_set) (is kind of like a map of Julia sets)\n",
        "  * [Logistic map](https://de.m.wikipedia.org/wiki/Logistische_Gleichung) (Feigenbaum Attractor) with [Feigenbaum-Konstante](https://de.m.wikipedia.org/wiki/Feigenbaum-Konstante),\n",
        "    * [Vergesst den Pi-Tag, feiert lieber den Feigenbaum-Tag](https://www.spektrum.de/kolumne/die-feigenbaum-konstante-ist-die-wichtigste-groesse-der-dynamik/2120670)\n",
        "    * [Bifurcation Diagram](https://en.m.wikipedia.org/wiki/Bifurcation_diagram)\n",
        "  * Peano curve, Pentaflake, 3D Hilbert curve etc.\n",
        "  * A [fractal curve](https://en.m.wikipedia.org/wiki/Fractal_curve) is, loosely, a mathematical curve whose shape retains the same general pattern of irregularity, regardless of how high it is magnified, that is, its graph takes the form of a fractal. They do NOT have finite length. A famous example is the boundary of the Mandelbrot set. [Space-filling curves](https://en.m.wikipedia.org/wiki/Space-filling_curve) are special cases of fractal curves. Its range contains the entire 2-dimensional unit square (or more generally an n-dimensional unit hypercube). Space-filling curves in the 2-dimensional plane are sometimes called [Peano curve](https://en.m.wikipedia.org/wiki/Peano_curve). Peano curves are constructed by [Hilbert curves](https://en.m.wikipedia.org/wiki/Hilbert_curve) to form a single continuous loop over the entire sphere.\n",
        "\n",
        "\n",
        "**Random and natural fractals**:\n",
        "  * Zeros of a Wiener process,\n",
        "  * Brownian motion,\n",
        "  * [Coastline of Ireland, Great Britain or Norway](https://en.m.wikipedia.org/wiki/Coastline_paradox)  (Coastline paradox)\n",
        "  * von [Koch curve](https://de.m.wikipedia.org/wiki/Koch-Kurve) with random orientation\n",
        "  * The surface of Broccoli or human brain,\n",
        "  * Distribution of [galaxy clusters](https://en.m.wikipedia.org/wiki/Galaxy_cluster)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1196.png)\n",
        "\n",
        "*Zeros of a Wiener process:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/8/83/Wiener_process_set_of_zeros.gif)"
      ],
      "metadata": {
        "id": "PeB3l9rViknz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fractal Dimensions**: die [fraktale Dimension](https://de.m.wikipedia.org/wiki/Fraktale_Dimension) einer Menge ist eine Verallgemeinerung des Dimensionsbegriffs von geometrischen Objekten wie Kurven (eindimensional) und Fl√§chen (zweidimensional).\n",
        "\n",
        "  * [Hausdorff dimension](https://de.m.wikipedia.org/wiki/Hausdorff-Dimension):  Hausdorff dimension of a single point is zero, of a line segment is 1, of a square is 2, and of a cube is 3. That is, for sets of points that define a smooth shape or a shape that has a small number of corners‚Äîthe shapes of traditional geometry and science‚Äîthe Hausdorff dimension is an integer agreeing with the usual sense of dimension, also known as the [topological dimension (inductive dimension)](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "  * [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "  * [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension)\n",
        "  * [Box-counting dimension](https://en.m.wikipedia.org/wiki/Minkowski‚ÄìBouligand_dimension) (Minkowski‚ÄìBouligand dimension)\n",
        "  * [List of fractals by Hausdorff dimension](https://en.m.wikipedia.org/wiki/List_of_fractals_by_Hausdorff_dimension)\n",
        "\n",
        "*Example of Box Counting dimension:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Great_Britain_Box.svg/640px-Great_Britain_Box.svg.png)\n"
      ],
      "metadata": {
        "id": "b-iO6R8pfzwE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcbvY2o-JyH"
      },
      "source": [
        "###### *Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-OG2Rw8EzzH"
      },
      "source": [
        "**Dynamisches System**\n",
        "\n",
        "> **A dynamical system involves one or more variables that change over time according to autonomous differential equations.**\n",
        "\n",
        "> $\\dot{x}=$ rate of change of $x$ as time changes\n",
        "\n",
        "> $\\dot{y}=$ rate of change of $y$ as time changes\n",
        "\n",
        "They depend on t:\n",
        "\n",
        "$\\frac{d x}{d t}=$ rate of change of $x$ as time changes $\\frac{d y}{d t}=$ rate of change of $y$ as time changes\n",
        "\n",
        "but the differential equation that describe x dot and y dot don‚Äôt actually involve time t:\n",
        "\n",
        "> $\\dot{x}=-y-0.1 x$\n",
        "\n",
        "> $\\dot{y}=x-0.4 y$\n",
        "\n",
        "This makes them autonomous. Each combination of x and y to only corresponds to one combination of x dot and y dot. You can represent it in a dynamical system called the phase space. Each point in space is a unique state of the system. And has its own rate of change shown as a vector.\n",
        "\n",
        "> <font color=\"red\">Siehe auch: [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. STS is interesting because it bridges the two major parts of mathematical physics ‚Äì the [dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory) and [topological (quantum) field theories](https://en.m.wikipedia.org/wiki/Topological_quantum_field_theory). Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.</font>\n",
        "\n",
        "* [Dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory). Ein (deterministisches) [dynamisches System](https://de.m.wikipedia.org/wiki/Dynamisches_System) ist ein Modell eines zeitabh√§ngigen Prozesses, der homogen bez√ºglich der Zeit ist (Verlauf h√§ngt nur vom Anfangszustand, aber nicht von der Wahl des Anfangszeitpunkts)\n",
        "\n",
        "* Wichtige Fragestellungen: Langzeitverhalten (zum Beispiel [Stabilit√§t](https://de.wikipedia.org/wiki/Stabilit√§tstheorie), [Periodizit√§t](https://de.wikipedia.org/wiki/Periodische_Funktion), [Chaos](https://de.wikipedia.org/wiki/Chaosforschung) und [Ergodizit√§t](https://de.wikipedia.org/wiki/Ergodizit√§t)), die [Systemidentifikation](https://de.wikipedia.org/wiki/Systemidentifikation) und ihre [Regelung](https://de.wikipedia.org/wiki/Regelung_(Natur_und_Technik))\n",
        "\n",
        "* Formal betrachte man ein **dynamisches System** bestehend aus einem topologischen Raum $X$ und einer Transformation $f: \\mathcal{T} \\times X \\longrightarrow X$, wobei $\\mathcal{T}$ ein linear geordnetes Monoid ist wie $\\mathcal{T}=\\mathbb{N}, \\mathbb{Z},[0, \\infty[$ oder $\\mathbb{R}$ und $f$ normalerweise stetig oder mindestens messbar ist (oder mindestens wird verlangt, dass $f(t, \\cdot): X \\longrightarrow X$ stetig/messbar ist f√ºr jedes $t \\in \\mathcal{T}$ ) und erf√ºllt $f(t+s, x)=f(t, f(s, x))$ f√ºr alle ¬ªZeiten ¬´ $t, s \\in \\mathcal{T}$ und Punkte $x \\in X$.\n",
        "\n",
        "\n",
        "Ein dynamisches System ist ein Tripel $(T, X, \\Phi)$, bestehend aus\n",
        "\n",
        "* **Zeitraum**: einer Menge $T=\\mathbb{N}_{0}, \\mathbb{Z}, \\mathbb{R}_{0}^{+}$ oder $\\mathbb{R}$,\n",
        "* **Zustandsraum** (dem Phasenraum): einer nichtleeren Menge $X$,\n",
        "* **Operation** $\\Phi: T \\times X \\rightarrow X$ von $T$ auf $X,$\n",
        "\n",
        "so dass f√ºr alle Zust√§nde $x \\in X$ und alle Zeitpunkte $t, s \\in T$ gilt:\n",
        "\n",
        "1. **Identit√§tseigenschaft**: $\\Phi(0, x)=x$\n",
        "\n",
        "2. **Halbgruppeneigenschaft**: $\\Phi(s, \\Phi(t, x))=\\Phi(s+t, x)$\n",
        "\n",
        "* Wenn $T=\\mathbb{N}_{0}$ oder $T=\\mathbb{Z}$ ist, dann hei√üt $(T, X, \\Phi)$ **zeitdiskret** oder kurz diskret, und mit $T=\\mathbb{R}_{0}^{+}$ oder $T=\\mathbb{R}$ nennt man $(T, X, \\Phi)$ **zeitkontinuierlich** oder kontinuierlich.\n",
        "\n",
        "* Beispiele: [Exponentielles Wachstum und Federpendel](https://de.m.wikipedia.org/wiki/Dynamisches_System#Einf√ºhrende_Beispiele)\n",
        "\n",
        "  * das Str√∂mungsverhalten von Fl√ºssigkeiten und Gasen\n",
        "  * Bewegungen von Himmelsk√∂rpern unter gegenseitiger Beeinflussung durch die Gravitation\n",
        "  * Populationsgr√∂√üen von Lebewesen unter Ber√ºcksichtigung der R√§uber-Beute-Beziehung\n",
        "  * die Entwicklung wirtschaftlicher Kenngr√∂√üen unter Einfluss der Marktgesetze.\n",
        "\n",
        "* Das Langzeitverhalten eines dynamischen Systems l√§sst sich durch den globalen Attraktor beschreiben, da bei physikalischen oder technischen Systemen oft **Dissipation vorliegt, insbesondere Reibung**.\n",
        "\n",
        "* Die [Determiniertheit (als Systemeigenschaft)](https://de.wikipedia.org/wiki/Systemeigenschaften#Determiniertheit) ist der Grad der ‚ÄûVorbestimmtheit‚Äú des Systems: Ein System geht von einem Zustand Z1 in den Zustand Z2 √ºber: Z1 ‚Üí Z2. Bei deterministischen Systemen ist dieser √úbergang bestimmt (zwingend), bei stochastischen wahrscheinlich. Deterministische Systeme erlauben prinzipiell die Ableitung ihres Verhaltens aus einem vorherigen Zustand, stochastische Systeme nicht. **Aus der Komplexit√§t eines Systems l√§sst sich keine Aussage √ºber die Vorhersagbarkeit treffen**: Es gibt einfache deterministische Systeme, die chaotisch sind (z. B. Doppelpendel) und komplexe deterministische Systeme (Chloroplasten bei der Photosynthese).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYGXfTQpV2tS"
      },
      "source": [
        "**Phasenraum und Trajektorie**\n",
        "\n",
        "* Der [Phasenraum](https://de.wikipedia.org/wiki/Phasenraum) beschreibt die Menge aller m√∂glichen Zust√§nde eines dynamischen Systems. Ein Zustand wird durch einen Punkt im Phasenraum eindeutig abgebildet.\n",
        "\n",
        "* **Jeder Zustand ist ein Punkt im Phasenraum und wird durch beliebig viele [Zustandsgr√∂√üen](https://de.m.wikipedia.org/wiki/Zustandsgr√∂√üe) dargestellt, welche die Dimensionen des Phasenraums bilden.**\n",
        "\n",
        "* kontinuierliche Systeme werden durch Linien (Trajektorien) repr√§sentiert\n",
        "* diskrete Systeme werden durch Mengen isolierter Punkte repr√§sentiert.\n",
        "\n",
        "* In der Mechanik besteht er aus verallgemeinerten Koordinaten (Konfigurationsraum) und zugeh√∂rigen verallgemeinerten Geschwindigkeiten, siehe [Prinzip der virtuellen Leistung](https://de.m.wikipedia.org/wiki/Prinzip_der_virtuellen_Leistung)\n",
        "\n",
        "* Die zeitliche Entwicklung eines Punktes im Phasenraum wird durch **Differentialgleichungen** beschrieben und durch **Trajektorien** (Bahnkurven, Orbit) im Phasenraum dargestellt. Dies sind **Differentialgleichungen erster Ordnung in der Zeit** und durch einen Anfangspunkt eindeutig festgelegt (ist die Differentialgleichung zeitunabh√§ngig, sind dies **autonome Differentialgleichungen**). Dementsprechend kreuzen sich zwei Trajektorien im Phasenraum auch nicht, da an einem Kreuzungspunkt der weitere Verlauf nicht eindeutig ist. Geschlossene Kurven beschreiben oszillierende (periodische) Systeme.\n",
        "\n",
        "* *Konstruktion eines Phasen(raum)portr√§ts f√ºr ein [mathematisches Pendel](https://de.wikipedia.org/wiki/Mathematisches_Pendel)*\n",
        "\n",
        "![hh](https://upload.wikimedia.org/wikipedia/commons/c/cd/Pendulum_phase_portrait_illustration.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m066sar7fCUi"
      },
      "source": [
        "###### *Attractor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxw1WlN9hhlY"
      },
      "source": [
        "**Attraktor**\n",
        "\n",
        "* [Attraktor](https://de.wikipedia.org/wiki/Attraktor) ist ein Begriff aus der Theorie dynamischer Systeme und beschreibt **eine Untermenge eines Phasenraums** (d. h. eine gewisse Anzahl von Zust√§nden), auf die sich ein dynamisches System im Laufe der Zeit zubewegt und die unter der Dynamik dieses Systems nicht mehr verlassen wird.\n",
        "\n",
        "* Das hei√üt, eine Menge von Variablen n√§hert sich im Laufe der Zeit (asymptotisch) einem bestimmten Wert, einer Kurve oder etwas Komplexerem (also einer Region im n-dimensionalen Raum) und bleibt dann im weiteren Zeitverlauf in der N√§he dieses Attraktors.\n",
        "\n",
        "* Bekannte Beispiele sind der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor), der [R√∂ssler-Attraktor](https://de.wikipedia.org/wiki/R%C3%B6ssler-Attraktor) und die [Nullstellen](https://de.wikipedia.org/wiki/Nullstelle) einer differenzierbaren Funktion, welche Attraktoren des zugeh√∂rigen Newton-Verfahrens sind.\n",
        "\n",
        "> **Die Menge aller Punkte des Phasenraums, die unter der Dynamik demselben Attraktor zustreben, hei√üt Attraktions- oder Einzugsgebiet dieses Attraktors**.\n",
        "\n",
        "\n",
        "Unter einem Attraktor versteht man eine Teilmenge $A \\subseteq X$,\n",
        "die den folgenden Bedingungen gen√ºgt\n",
        "1. $A$ ist vorw√§rts invariant;\n",
        "2. Das Sammelbecken $B(A)$ ist eine Umgebung von $A$;\n",
        "3. $A$ ist eine minimale nicht leere Teilmenge von $X$ mit Bedingungen 1\n",
        "und 2 .\n",
        "\n",
        "Bedingung 1 erfordert eine gewisse Stabilit√§t des Attraktors. Daraus folgt offensichtlich, dass $A \\subseteq B(A)$. Anhand Bedingung 2 wird weiterhin verlangt, dass $A \\subseteq B(A)^{\\circ}$ und bedeutet u. a., jeder Punkt in einer gewissen N√§he von $A$ n√§here sich dem Attraktor beliebig. Manche Autoren lassen Bedingung 2 weg. Bedingung 3 erfordert, dass der Attraktor nicht in weitere Komponenten zerlegt werden kann (ansonsten\n",
        "w√§re bspw. der ganze Raum trivialerweise ein Attraktor).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cai3DPiwy4"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine ganzzahlige Dimension besitzen**\n",
        "\n",
        "*Bei der Untersuchung dynamischer Systeme interessiert man sich ‚Äì ausgehend von einem bestimmten [Anfangszustand (=Anfangsbedingung)](https://de.wikipedia.org/wiki/Anfangsbedingung) ‚Äì vor allem f√ºr das Verhalten f√ºr $t\\to \\infty$ . Der Grenzwert in diesem Fall wird als Attraktor bezeichnet. Typische und h√§ufige Beispiele von Attraktoren sind:*\n",
        "\n",
        "* **asymptotisch stabile Fixpunkte**: Das System n√§hert sich immer st√§rker einem bestimmten Endzustand an, in dem die Dynamik erliegt; ein statisches System entsteht. Typisches Beispiel ist ein ged√§mpftes Pendel, das sich dem Ruhezustand im tiefsten Punkt ann√§hert.\n",
        "\n",
        "* **(asymptotisch) stabile Grenzzyklen**: Der Endzustand ist die Abfolge gleicher Zust√§nde, die periodisch durchlaufen werden (periodische Orbits). Ein Beispiel daf√ºr ist die Simulation der R√§uber-Beute-Beziehung, die f√ºr bestimmte Parameter der R√ºckkoppelung auf ein periodisches Ansteigen und Sinken der Populationsgr√∂√üen hinausl√§uft.\n",
        "\n",
        "* F√ºr ein hybrides dynamisches System mit chaotischer Dynamik konnte im $\\mathbb {R} ^{n}$ die Oberfl√§che eines [n-Simplex](https://de.wikipedia.org/wiki/Simplex_(Mathematik)) als Attraktor identifiziert werden: (asymptotisch stabile) Grenztori: Treten mehrere miteinander inkommensurable Frequenzen auf, so ist die Trajektorie nicht geschlossen, und der Attraktor ist ein Grenztorus, der von der Trajektorie asymptotisch vollst√§ndig ausgef√ºllt wird. Die zu diesem Attraktor korrespondierende Zeitreihe ist quasiperiodisch, d. h., es gibt keine echte Periode, aber das Frequenzspektrum besteht aus scharfen Linien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMS8uZlNlFTx"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine fraktale Dimension besitzen**\n",
        "\n",
        "*Die Existenz von Attraktoren mit komplizierterer Struktur war zwar schon l√§nger bekannt, man betrachtete sie aber zun√§chst als instabile Sonderf√§lle, deren Auftreten nur bei bestimmter Wahl des Ausgangszustands und der Systemparameter beobachtet wird. Dies √§nderte sich mit der Definition eines neuen, speziellen Typs von Attraktor:*\n",
        "\n",
        "* [Seltsamer Attraktor](https://de.wikipedia.org/wiki/Seltsamer_Attraktor): In seinem Endzustand zeigt das System h√§ufig ein chaotisches Verhalten (es gibt jedoch auch Ausnahmen, z. B. quasiperiodisch angetriebene nichtlineare Systeme).  Ein seltsamer Attraktor ist ein Attraktor, also ein Ort im Phasenraum, der den Endzustand eines dynamischen Prozesses darstellt, **dessen fraktale Dimension nicht ganzzahlig und dessen Kolmogorov-Entropie echt positiv ist**. Es handelt sich damit um ein Fraktal, das nicht in geschlossener Form geometrisch beschrieben werden kann. Gelegentlich wird auch der Begriff chaotischer Attraktor bevorzugt, da die ‚ÄûSeltsamkeit‚Äú dieses Objekts sich mit den Mitteln der Chaostheorie erkl√§ren l√§sst. Der dynamische Prozess zeigt ein aperiodisches Verhalten.\n",
        "\n",
        "    * [Lorenz Attraktor](https://de.m.wikipedia.org/wiki/Lorenz-Attraktor)\n",
        "\n",
        "    * [R√∂ssler attractor](https://en.m.wikipedia.org/wiki/R√∂ssler_attractor)\n",
        "\n",
        "  * [Multiscroll attractor](https://en.m.wikipedia.org/wiki/Multiscroll_attractor)\n",
        "\n",
        "  * [H√©non map](https://en.m.wikipedia.org/wiki/H√©non_map)\n",
        "\n",
        "* Der seltsame Attraktor l√§sst sich **nicht in einer geschlossenen geometrischen Form** beschreiben und **besitzt keine ganzzahlige Dimension**. Attraktoren nichtlinearer dynamischer Systeme weisen dann eine **fraktale Struktur** auf.\n",
        "\n",
        "* Wichtiges Merkmal ist das chaotische Verhalten, d. h., jede noch so geringe √Ñnderung des Anfangszustands f√ºhrt im weiteren Verlauf zu signifikanten Zustands√§nderungen. Prominentestes Beispiel ist der **Lorenz-Attraktor, der bei der Modellierung von Luftstr√∂mungen in der Atmosph√§re entdeckt wurde**.\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Attractor#Fixed_point\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Limit_cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAvwOlbmQn8"
      },
      "source": [
        "**Special: Lorenz-Attraktor**\n",
        "\n",
        "Der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor) oder englisch: [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) ist der seltsame Attraktor eines Systems von drei gekoppelten, nichtlinearen **gew√∂hnlichen Differentialgleichungen**:\n",
        "\n",
        "$\\dot{X}=a(Y-X)$\n",
        "\n",
        "$\\dot{Y}=X(b-Z)-Y$\n",
        "\n",
        "$\\dot{Z}=X Y-c Z$\n",
        "\n",
        "* Formuliert wurde das System um 1963 von dem Meteorologen Edward N. Lorenz, der es als Idealisierung eines [hydrodynamischen Systems (Fluiddynamik)](https://de.wikipedia.org/wiki/Fluiddynamik) entwickelte. Basierend auf einer Arbeit von Barry Saltzman (1931‚Äì2001) ging es Lorenz dabei um eine Modellierung der Zust√§nde in der Erdatmosph√§re zum Zweck einer Langzeitvorhersage.\n",
        "\n",
        "* Allerdings betonte Lorenz, dass das von ihm entwickelte System allenfalls f√ºr sehr begrenzte Parameterbereiche von $a,b,c$ realistische Resultate liefert.\n",
        "\n",
        "* Die mathematische Beschreibung des Modells durch die Navier-Stokes-Gleichungen f√ºhrt √ºber verschiedene Vereinfachungen, beispielsweise endlich abgebrochene Reihendarstellungen, zu dem oben angegebenen Gleichungssystem.\n",
        "\n",
        "* Die numerische L√∂sung des Systems zeigt bei bestimmten Parameterwerten deterministisch chaotisches Verhalten, die Trajektorien folgen einem seltsamen Attraktor. Damit spielt der Lorenzattraktor f√ºr die mathematische Chaostheorie eine Rolle, denn die Gleichungen stellen wohl eines der einfachsten Systeme mit chaotischem Verhalten dar.\n",
        "\n",
        "* Die typische Parametereinstellung mit chaotischer L√∂sung lautet: $a=10,b=28$ und $c=8/3$, wobei\n",
        "  * $a$ mit der [Prandtl-Zahl](https://de.wikipedia.org/wiki/Prandtl-Zahl) (=dimensionslose Kennzahl von Fluiden, das hei√üt von Gasen oder tropfbaren Fl√ºssigkeiten. Sie ist definiert als Verh√§ltnis zwischen kinematischer Viskosit√§t und Temperaturleitf√§higkeit. Die Prandtl-Zahl stellt die Verkn√ºpfung des Geschwindigkeitfeldes mit dem Temperaturfeld eines Fluids dar.)\n",
        "  * $b$ mit der [Rayleigh-Zahl](https://de.wikipedia.org/wiki/Rayleigh-Zahl) (=eine dimensionslose Kennzahl, die den Charakter der W√§rme√ºbertragung innerhalb eines Fluids beschreibt) identifiziert werden kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xyzZzNOEhv"
      },
      "source": [
        "###### *Summary: Attractor, Trajectory in phase space, Lyapunov exponent (forecast error)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIujOlBtNqx-"
      },
      "source": [
        "For this specific system shown, the vector field looks like this, and let scatter a bunch of random points around to represent different possible states see how they evolve and move around: They all spiral towards the center.\n",
        "\n",
        "For this exampe the attraction is zero, and the basin is every point in space. Notice that at the origin x dot and y dot also equal zero.\n",
        "\n",
        "$\\dot{x}=-(0)-0.1(0)=0$\n",
        "\n",
        "$\\dot{y}=(0)-0.4(0)=0$\n",
        "\n",
        "The origin is in this case **fixed point (attractor)**, because every point in there will stay forever. It seem like an inevitability, hence determinism (as chaos is).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkfT8XP8QmJd"
      },
      "source": [
        "But there are also other types of attractors, like the **Van der Pol oscillator**:\n",
        "\n",
        "> $\\dot{x}=\\mu\\left(x-\\frac{1}{3} x^{3}-y\\right)$\n",
        "\n",
        "> $\\dot{y}=\\frac{1}{\\mu} x$\n",
        "\n",
        "It creates something called: \"**Limit Cycle Attractor**\":\n",
        "\n",
        "* it doesn't end up in a point\n",
        "\n",
        "* typically appear in physical systems with some sort of oscillation (electrical circuits, tectonic plates, etc).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luo2ipo2RaUH"
      },
      "source": [
        "**Strange Attractor** (has a fractal dimension)\n",
        "\n",
        "Lorenz Attractor: Lorenz system describes convection cycles in the atmoshphere. Very sensitive to changes in initial conditions already after a short time:\n",
        "\n",
        "> $\\dot{x}=\\sigma(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(\\rho-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\beta z$\n",
        "\n",
        "The Lorenz equations have a few parameters that can be tweaked to alter the behavior of a system. This is what is known as ‚Äòstrange attractor‚Äô:\n",
        "\n",
        "> $\\dot{x}=10(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(28-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\frac{8}{3} z$\n",
        "\n",
        "1. No point in the space is ever visited more than once by the same trajectory - it that happens, the trajectory would travel in a predictable loop.\n",
        "2. And no 2 trajectories will ever intersect. If that happened, they would merge into the same path, giving two different sets of initial conditions the same outcome.\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_03.png)\n",
        "\n",
        "A single trajectory will visit an infinite number of points in this limited space, and this limited space will have an infinite number of trajectories.\n",
        "\n",
        "* Trajectories are just curved, so they should be 1-dimensional..normally!\n",
        "* For the strange attractor: no matter how much you zoom in on this attractor, you can always find more and more trajectories everywhere.\n",
        "* That‚Äôs why this attractor is said to have a **non integer dimension** - it‚Äôs **made up of infinite long curves in a finite space, which are so detailed, that they start to partially fill up higher dimensions**. It‚Äôs not 1, 2 or 3 dimensional, its somewhere in between. (=detail at arbitrarily small scales)\n",
        "\n",
        "Conclusion: Lorenz attractor is a fractal space, and hence a strange attractor.\n",
        "\n",
        "Even is both points started at the same point, small initial differences can bring them to complete different paths:\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chros_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOAWTDsoV7-F"
      },
      "source": [
        "The difference in trajectories increases exponentially:\n",
        "\n",
        "**$\\lambda$ stands for Lyapunov exponent**\n",
        "\n",
        "$\\lambda$ > 0, trajectories will increase exponentially (= chaotic)\n",
        "\n",
        "$\\lambda$ = 0, distance will stay constant\n",
        "\n",
        "$\\lambda$ < 0, distance will converge to zero.\n",
        "\n",
        "*Unfortunately there is no way to find the Lyapunov exponent only by looking at the equations. It is measured by running the simulation, keeping track of mainy pairs of trajectories, and find the average rate of change in their distance. But it provides a simple metric to comunicate how chaotic a system is*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_06.png)\n",
        "\n",
        "**For the Lorenz attractor it is $\\lambda$ ‚âà 0.9**\n",
        "\n",
        "Used to find out duration in which the predictions are valid: \"Predictability Horizon\". We get this by re-arranging our previous equation $d_{t}=d_{0} e^{\\lambda t}$:\n",
        "\n",
        "> Predictability horizon $=\\frac{1}{\\lambda} \\ln \\frac{a}{d_{0}}$\n",
        "\n",
        "$d_{0}=$ initial error\n",
        "\n",
        "$a=$ maximum allowed error\n",
        "\n",
        "**For the Lorenz attractor, after 10 time steps, any error would have multiplied by 8,000 already.**\n",
        "\n",
        "Example what that means with this exponential divergence:\n",
        "\n",
        "We have a simulation that predicts where ocean currents flow, and you want to keep the error less than 1,000 km.\n",
        "* If you ran it twice, once an initial error of 1m and once the initial error was a million times smaller than one micrometer (0,000001m), how much longer  the simulation with the smaller error would stay below the margin of error.\n",
        "* The simulation would be valid 9 days instead of 3 days, but for an initial error that is a million times smaller !!\n",
        "\n",
        "Let‚Äôs write the expressions for the two predictability horizons, and put them in a fraction:\n",
        "\n",
        "> $\\frac{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{0.000001}\\right)}{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{1}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(\\frac{1}{\\lambda} \\ln 10^{9}\\right)}{\\left(\\frac{1}{\\lambda} \\ln 10^{3}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(9 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}{\\left(3 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}$\n",
        "\n",
        "> = $\\frac{9}{3}$.\n",
        "\n",
        "= It‚Äôs 3 times longer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Stochastic**"
      ],
      "metadata": {
        "id": "82Lb_gBqUzi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1605.png)\n",
        "\n",
        "https://www.researchgate.net/figure/A-flowchart-showing-different-types-of-stochastic-processes-Processes-lower-in-the-chart_fig2_330251417 \n",
        "\n",
        "http://home.ubalt.edu/ntsbarsh/simulation/sim.htm"
      ],
      "metadata": {
        "id": "m_CrbI1s1u5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Statistical hypothesis testing (Sigma)*"
      ],
      "metadata": {
        "id": "fQEB0NP4Fk57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1 Sigma: 95% confidence (2 Sigma?), 0.683 Wahrscheinlichkeit\n",
        "* 3 Sigma: 1 in 300 or 0,27% false positive, 0.997 Wahrscheinlichkeit\n",
        "* 5 Sigma: 1 in 3,5 Mio\n",
        "* https://en.m.wikipedia.org/wiki/Statistical_hypothesis_testing\n",
        "* https://reference.wolfram.com/applications/eda/ExperimentalErrorsAndErrorAnalysis.html\n",
        "* https://www.studysmarter.de/schule/mathe/stochastik/sigma-regeln/\n",
        "* https://www.zmescience.com/science/what-5-sigma-means-0423423/"
      ],
      "metadata": {
        "id": "9w9iTJlLFpdH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1flpzoQFTVa"
      },
      "source": [
        "###### *Stochastic Analysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Analysis**\n",
        "\n",
        "* [Stochastic calculus](https://en.m.wikipedia.org/wiki/Stochastic_calculus) bzw. [Stochastische Analysis](https://de.m.wikipedia.org/wiki/Stochastische_Analysis) is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. This field is created and started by the Japanese mathematician Kiyoshi It√¥ during World War 2.\n",
        "\n",
        "* [Stochastische_Integration](https://de.m.wikipedia.org/wiki/Stochastische_Integration): Es sind stochastische Prozesse mit unendlicher Variation, insbesondere der Wiener-Prozess, als Integratoren zugelassen.\n",
        "\n",
        "* [Stochastischer Prozess](https://de.m.wikipedia.org/wiki/Stochastischer_Prozess): (auch Zufallsprozess) zeitlich geordnete, zuf√§llige Vorg√§nge. Theorie der stochastischen Prozesse ist Erweiterung der Wahrscheinlichkeitstheorie dar und bildet Grundlage f√ºr stochastische Analysis.\n",
        "\n",
        "* > Stochastic: Statistik + Probability Theory (incl stochastic / random processes)\n",
        "\n",
        "* Bei einem Sparguthaben entspr√§che dies dem **exponentiellen Wachstum** durch Zinseszins. Bei Aktien wird dieses Wachstumsgesetz hingegen **in der Realit√§t offenbar durch eine komplizierte Zufallsbewegung √ºberlagert**.\n",
        "\n",
        "* Bei zuf√§lligen St√∂rungen, die sich aus vielen kleinen Einzel√§nderungen zusammensetzen, **wird von einer Normalverteilung als einfachstem Modell ausgegangen**.\n",
        "\n",
        "* Au√üerdem zeigt sich, dass die Varianz der St√∂rungen proportional zum betrachteten Zeitraum $\\Delta t$ ist. Der Wiener-Prozess $W_t$ besitzt alle diese gew√ºnschten Eigenschaften, eignet sich also als ein Modell f√ºr die zeitliche Entwicklung der Zufallskomponente des Aktienkurses.\n"
      ],
      "metadata": {
        "id": "DC-KHFg7J17I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0iE2bRTLxY"
      },
      "source": [
        "**Stochastic Differential Equation ( ‚Üí  Black Scholes, Spontaneous Symmetry Breaking)**\n",
        "\n",
        "* Eine [stochastischen Differentialgleichung](https://de.wikipedia.org/wiki/Stochastische_Differentialgleichung) ist eine Verallgemeinerung des Begriffs der gew√∂hnlichen Differentialgleichung auf stochastische Prozesse.\n",
        "\n",
        "> Stochastische Differentialgleichungen werden in zahlreichen Anwendungen eingesetzt, **um zeitabh√§ngige Vorg√§nge zu modellieren**, die neben deterministischen Einfl√ºssen zus√§tzlich **stochastischen St√∂rfaktoren (Rauschen)** ausgesetzt sind.\n",
        "\n",
        "* Die formale Theorie der stochastischen Differentialgleichungen wurde erst in den 1940er Jahren durch den japanischen Mathematiker It≈ç Kiyoshi formuliert.\n",
        "\n",
        "* Gemeinsam mit der **stochastischen Integration** begr√ºndet die Theorie der **stochastischen Differentialgleichungen** die **stochastische Analysis**.\n",
        "\n",
        "* **Objective**: Genau wie bei deterministischen Funktionen m√∂chte man auch bei stochastischen Prozessen den Zusammenhang zwischen dem Wert der Funktion und ihrer momentanen √Ñnderung (ihrer Ableitung) in einer Gleichung formulieren.\n",
        "\n",
        "* **Challenge**: Was im einen Fall zu einer gew√∂hnlichen Differentialgleichung f√ºhrt, ist im anderen Fall problematisch, da viele stochastische Prozesse, wie beispielsweise **der Wiener-Prozess, nirgends differenzierbar sind**.\n",
        "\n",
        "* Loesung ist wie bei gewohnlichen Differentialgleichungen plus einen stochastischen Term\n",
        "\n",
        "*Beim Typus der stochastischen Differentialgleichungen treten in der Gleichung stochastische Prozesse auf. Eigentlich sind stochastische Differentialgleichungen keine Differentialgleichungen [im obigen Sinne](https://de.m.wikipedia.org/wiki/Differentialgleichung#Weitere_Typen), sondern lediglich gewisse Differentialrelationen, welche als Differentialgleichung interpretiert werden k√∂nnen.*\n",
        "\n",
        "*Beispiele fur stochastische Differentialgleichungen*\n",
        "\n",
        "* Die SDGL f√ºr die geometrische brownsche Bewegung lautet $\\mathrm{d} S_{t}=r S_{t} \\mathrm{~d} t+\\sigma S_{t} \\mathrm{~d} W_{t} .$ Sie wird beispielsweise im Black-Scholes-Modell zur Beschreibung von Aktienkursen verwendet.\n",
        "\n",
        "* Die SDGL f√ºr einen Ornstein-Uhlenbeck-Prozess ist $\\mathrm{d} X_{t}=\\theta\\left(\\mu-X_{t}\\right) \\mathrm{d} t+\\sigma \\mathrm{d} W_{t} .$ Sie wird unter anderem im Vasicek-Modell zur finanzmathematischen Modellierung von Zinss√§tzen √ºber den Momentanzins\n",
        "verwendet.\n",
        "\n",
        "* Die SDGL f√ºr den Wurzel-Diffusionsprozess nach William Feller lautet $\\mathrm{d} X_{t}=\\kappa\\left(\\theta-X_{t}\\right) \\mathrm{d} t+\\sigma \\sqrt{X_{t}} \\mathrm{~d} W_{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelei85Xp8rU"
      },
      "source": [
        "**Supersymmetric theory of stochastic dynamics & Spontaneous Symmetry breaking**\n",
        "\n",
        "* [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) **is an exact theory of stochastic (partial) differential equations (SDEs)**, the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Chaos_theory#Chaos_as_a_spontaneous_breakdown_of_topological_supersymmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpmflh50O1X"
      },
      "source": [
        "###### *Markov Process & Monte Carlo Simulation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_psDmmPW0P"
      },
      "source": [
        "A Markov chain is a **discrete-time stochastic process** that progresses from one state to another with certain probabilities that can be **represented by a graph and state transition matrix P** as indicated below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/markov.PNG\" alt=\"markov\">\n",
        "\n",
        "* Markov chains may be modeled by **finite state machines, and random walks**\n",
        "\n",
        "* A Stochastic (State Transition) Matrix describes a Markov chain X<sub>t</sub> over a [**finite state space (Probability space) S**](https://en.m.wikipedia.org/wiki/Probability_space) with cardinality S.\n",
        "\n",
        "  * Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties.\n",
        "\n",
        "  * Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* Let P be the transition matrix of Markov chain {X0, X1, ...}.\n",
        "\n",
        "  * **Reducibility**: a Markov chain is said to be irreducible if it is possible to get to any state from any state. In other words, a Markov chain is irreducible if there exists a chain of steps between any two states that has positive probability.\n",
        "\n",
        "  * **Periodicity**: a state in a Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. Thus, starting in state 'i', the chain can return to 'i' only at multiples of the period 'k', and k is the largest such integer. State 'i' is aperiodic if k = 1 and periodic if k > 1.\n",
        "\n",
        "  * **Transience and Recurrence**: A state 'i' is said to be transient if, given that we start in state 'i', there is a non-zero probability that we will never return to 'i'. State i is recurrent (or persistent) if it is not transient. A recurrent state is known as positive recurrent if it is expected to return within a finite number of steps and null recurrent otherwise. Transience and recurrence issues are central to the study of Markov chains and help describe the Markov chain's overall structure. The presence of many transient states may suggest that the Markov chain is absorbing, and a strong form of recurrence is necessary in an ergodic Markov chain.\n",
        "\n",
        "  * **Ergodicity**: a state 'i' is said to be ergodic if it is aperiodic and positive recurrent. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic. Ergodic Markov chains are, in some senses, the processes with the \"nicest\" behavior.\n",
        "\n",
        "  * **Absorbing State**: a state i is called absorbing if it is impossible to leave this state. Therefore, the state 'i' is absorbing if pii = 1 and pij = 0 for i ‚â† j. If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain. Absorbing states are crucial for the discussion of absorbing Markov chains. A common type of Markov chain with transient states is an absorbing one. An absorbing Markov chain is a Markov chain in which it is impossible to leave some states, and any state could (after some number of steps, with positive probability) reach such a state. It follows that all non-absorbing states in an absorbing Markov chain are transient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcP1_FkDDDOv"
      },
      "source": [
        "**Monte Carlo Simulation**\n",
        "\n",
        "* Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response.\n",
        "\n",
        "* Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on **repeated random sampling to obtain numerical results**. The underlying concept is to use randomness to solve problems that might be deterministic in principle.\n",
        "\n",
        "* Monte Carlo methods are useful for **simulating systems with many coupled degrees of freedom**, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean‚ÄìVlasov processes, kinetic models of gases). Other examples include **modeling phenomena with significant uncertainty in inputs** such as the calculation of risk in business\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6jdLMMCTSD"
      },
      "source": [
        "**Markov Chain Monte Carlo (MCMC)**\n",
        "\n",
        "* Markov Chain Monte Carlo refers to a **class of methods** for sampling from a probability distribution in order to **construct the most likely distribution**.\n",
        "\n",
        "> MCMC can be considered as a **random walk** that gradually converges to the true distribution.\n",
        "\n",
        "* We cannot directly calculate the (i.e. logistic) distribution, so instead we generate thousands of values ‚Äî called samples ‚Äî for the parameters of the function (alpha and beta) to **create an approximation of the distribution**.\n",
        "\n",
        "* The idea behind MCMC is that **as we generate more samples, our approximation gets closer and closer to the actual true distribution**.\n",
        "\n",
        "* Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. **Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chCbs1o_Iud"
      },
      "source": [
        "###### *White Noise*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0ShhpknOvG"
      },
      "source": [
        "\n",
        "**A white noise process has following conditions**\n",
        "\n",
        "* Mean (level) is zero (does not change over time - stationary process)\n",
        "* Variance is constant (does not change over time - stationary process)\n",
        "* Zero autocorrelation (values do not correlate with lag values)\n",
        "\n",
        "**White Noise: Independent & Identically Distributed**\n",
        "\n",
        "* Hence, in a time series is white noise if the variables are independent and identically distributed (IID) with a mean of zero.\n",
        "\n",
        "* The term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies\n",
        "\n",
        "* **Necessary Condition**: Independence: variables are statistically uncorrelated = their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element R·µ¢·µ¢ is the variance of component w·µ¢; and the correlation matrix must be the n by n identity matrix.\n",
        "\n",
        "* **Sufficient Condition**: every variable in w has a normal distribution with zero mean and the same variance, w is said to be a Gaussian white noise vector. In that case, the joint distribution of w is a multivariate normal distribution; the independence between the variables then implies that the distribution has spherical symmetry in n-dimensional space. Therefore, any orthogonal transformation of the vector will result in a Gaussian white random vector. In particular, under most types of discrete Fourier transform, such as FFT and Hartley, the transform W of w will be a Gaussian white noise vector, too; that is, the n Fourier coefficients of w will be independent Gaussian variables with zero mean and the same variance.\n",
        "\n",
        "* A random vector (that is, a partially indeterminate process that produces vectors of real numbers) is said to be a white noise vector or white random vector if its components each have a probability distribution with zero mean and finite variance, and are statistically independent: that is, their joint probability distribution must be the product of the distributions of the individual components.\n",
        "\n",
        "* First moment has to be zero and second moment has to be finite though. (iid ) White noise is always an independent process but reverse may not be true.\n",
        "\n",
        "* The technical definition of white noise is that it has equal intensity at all frequencies. This corresponds to a delta function autocorrelation. This is only possible if there is no correlation between any sequential values. So yes, the independence is true both backwards and forwards. Note that the actual distribution is irrelevant.\n",
        "\n",
        "\n",
        "\n",
        "**Types of White Noise Processes**\n",
        "* If the variables in the series are drawn from a Gaussian distribution, the series is called Gaussian white noise\n",
        "* There are also white noise processes, like Levy etc.\n",
        "\n",
        "**Relationship to Stochastic Processes**\n",
        "* White noise is the generalized mean-square derivative of the Wiener process or Brownian motion (so Wiener is an integrated White Noise)\n",
        "\n",
        "**White noise is an important concept in time series analysis and forecasting**\n",
        "\n",
        "* **Predictability**: If your time series is white noise, then, by definition, it is random. You cannot reasonably model it and make predictions.\n",
        "* **Model Diagnostics**: The statistics and diagnostic plots can be uses on time series to check if it is white noise. The series of errors from a time series forecast model should ideally be white noise. If the series of forecast errors are not white noise, it suggests improvements could be made to the predictive model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8idRoUCpsoKR"
      },
      "source": [
        "###### *Random Walk*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdW8o7tCtbPh"
      },
      "source": [
        "**Random Walk**\n",
        "\n",
        "* Random walk is another time series model where the current observation is equal to the previous observation with a random step up or down. Known as a stochastic or random process.\n",
        "\n",
        "> y<sub>(t)</sub> = B<sub>0</sub> + B<sub>1</sub> * X<sub>(t-1)</sub> + e<sub>(t)</sub>\n",
        "\n",
        "* A random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence.\n",
        "\n",
        "* The process used to generate the series forces dependence from one-time step to the next. This dependence provides some consistency from step-to-step rather than the large jumps that a series of independent, random numbers provides. It is this dependency that gives the process its name as a ‚Äúrandom walk‚Äù or a ‚Äúdrunkard‚Äôs walk‚Äù.\n",
        "\n",
        "> **A simple random walk is a martingale**\n",
        "\n",
        "* In higher dimensions, the set of randomly walked points has interesting geometric properties. In fact, one gets a discrete fractal, that is, a set which exhibits stochastic self-similarity on large scales.\n",
        "* Examples include the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler: all can be approximated by random walk models, even though they may not be truly random in reality\n",
        "* Random walks serve as a fundamental model for the recorded stochastic activity / stochastic processes.\n",
        "* As a more mathematical application, the value of œÄ can be approximated by the use of random walk in an agent-based modeling environment.\n",
        "\n",
        "* There are many types of time-dependent processes referred to as random walks - most often refers to a special category of Markov chains or Markov processes. A random walk on the integers (and the gambler's ruin problem) are examples of **Markov processes in discrete time**.\n",
        "\n",
        "* Specific cases or limits of random walks include the L√©vy flight and diffusion models such as Brownian motion. **A Wiener process (~ Brownian motion) is the integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments. A Wiener process is the scaling limit of random walk in dimension 1.\n",
        "\n",
        "* Random walks can take place on a variety of spaces: graphs, on the integers or real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* **Random Walk and Autocorrelation**\n",
        "\n",
        "  * We can calculate the correlation between each observation and the observations at previous time steps. Given the way that the random walk is constructed, we would expect a strong autocorrelation with the previous observation and a linear fall off from there with previous lag values.\n",
        "\n",
        "* **Stationarity**\n",
        "\n",
        "  * A stationary time series is one where the values are not a function of time. Given the way that the random walk is constructed and the results of reviewing the autocorrelation, we know that the observations in a random walk are dependent on time.\n",
        "  * The current observation is a random step from the previous observation. Therefore we can expect a random walk to be non-stationary. In fact, all random walk processes are non-stationary. Note that not all non-stationary time series are random walks.\n",
        "  * Additionally, a non-stationary time series does not have a consistent mean and/or variance over time. A review of the random walk line plot might suggest this to be the case. We can confirm this using a statistical significance test, specifically the Augmented Dickey-Fuller test.\n",
        "\n",
        "**IID**\n",
        "\n",
        "  * This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (‚Äúi.i.d.‚Äù).\n",
        "\n",
        "  * This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied. So, if you begin with a time series that wanders all over the map, but you find that its first difference looks like it is an i.i.d. sequence, then a random walk model is a potentially good candidate.\n",
        "\n",
        "* **Prediction**\n",
        "\n",
        "  * A random walk is unpredictable; it cannot reasonably be predicted. Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step. Simply because we know that the next time step will be a function of the prior time step.\n",
        "  * This is often called the naive forecast, or a persistence model. We can implement this in Python by first splitting the dataset into train and test sets, then using the persistence model to predict the outcome using a rolling forecast method. Once all predictions are collected for the test set, the mean squared error is calculated.\n",
        "\n",
        "* **Drift**\n",
        "\n",
        "  * A random walk model is said to have ‚Äúdrift‚Äù or ‚Äúno drift‚Äù according to whether the distribution of step sizes has a non-zero mean or a zero mean. At period n, the k-step-ahead forecast that the random walk model without drift gives for the variable Y is\n",
        "\n",
        "  > $\\hat{Y}_{n+k}=Y_{n}$\n",
        "\n",
        "  * In others words, it predicts that all future values will equal the last observed value. This doesn‚Äôt really mean you expect them to all be the same, but just that you think they are equally likely to be higher or lower, and you are staying on the fence as far as point predictions are concerned. If you extrapolate forecasts from the random walk model into the distant future, they will go off on a horizontal line, just like the forecasts of the mean model. So, qualitatively the long-term point forecasts of the random walk model look similar to those of the mean model, except that they are always ‚Äúre-anchored‚Äù on the last observed value rather than the mean.of the historical data.\n",
        "\n",
        "  * For the random-walk-with-drift model, the k-step-ahead forecast from period n is:\n",
        "\n",
        "  > $\\hat{\\mathrm{Y}}_{\\mathrm{n}+\\mathrm{k}}=\\mathrm{Y}_{\\mathrm{n}}+\\mathrm{k} \\hat{\\mathrm{d}}$\n",
        "\n",
        "  * where dÀÜ is the estimated drift, i.e., the average increase from one period to the next. So, the long-term forecasts from the random-walk-with-drift model look like a trend line with slope dÀÜ , but it is always re-anchored on the last observed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk3roPmAuCDi"
      },
      "source": [
        "**Applications of Random Walks**\n",
        "\n",
        "* In computer networks, random walks can model the number of transmission packets buffered at a server.\n",
        "* In population genetics, random walk describes the statistical properties of genetic drift.\n",
        "* In image segmentation, random walks are used to determine the labels (i.e., ‚Äúobject‚Äù or ‚Äúbackground‚Äù) to associate with each pixel.\n",
        "* In brain research, random walks and reinforced random walks are used to model cascades of neuron firing in the brain.\n",
        "Random walks have also been used to sample massive online graphs such as online social networks.\n",
        "\n",
        "* **Random Walks in Financial Time Series**\n",
        "\n",
        "  * **Is time series a random walk?**: Your time series may be a random walk. Some ways to check if your time series is a random walk are as follows:\n",
        "The time series shows a strong temporal dependence that decays linearly or in a similar pattern.\n",
        "  * The time series is non-stationary and making it stationary shows no obviously learnable structure in the data.\n",
        "The persistence model provides the best source of reliable predictions.\n",
        "  * This last point is key for time series forecasting. Baseline forecasts with the persistence model quickly flesh out whether you can do significantly better. If you can‚Äôt, you‚Äôre probably working with a random walk. Many time series are random walks, particularly those of security prices over time. The random walk hypothesis is a theory that stock market prices are a random walk and cannot be predicted.\n",
        "  * \"A random walk is one in which future steps or directions cannot be predicted on the basis of past history. When the term is applied to the stock market, it means that short-run changes in stock prices are unpredictable.\" - Page 26, A Random Walk down Wall Street: The Time-tested Strategy for Successful Investing. https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/\n",
        "\n",
        "* https://towardsdatascience.com/random-walks-with-python-8420981bc4bc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbYRNP_zXe4"
      },
      "source": [
        "###### *Wiener Process (Brownian Motion)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldqy2UtMpa8P"
      },
      "source": [
        "**Wiener Process (Brownian Motion)**\n",
        "\n",
        "* Video: Physics of Randomness: https://youtu.be/5jBVYvHeG2c\n",
        "\n",
        "* Brownian Motion: Flower particles move randomly because they are hit by particles that move. And particles move faster with more heat.\n",
        "\n",
        "* Same in astrophysics with stars under the gravitational influence of smaller stars, big stars move a bit randomly\n",
        "\n",
        "* Same with stock market where the price is influenced by many factors all the time (and hence the price moves randomly up and down)\n",
        "\n",
        "**The Wiener process is a real valued continuous-time (continuous state-space) stochastic process**\n",
        "\n",
        "* W<sub>0</sub> = 0 (P-almost certain)\n",
        "* The Wiener process has (stochastically) independent increments.\n",
        "* The increases are therefore stationary and normally distributed with the expected value zero and the variance t - s.\n",
        "* The individual paths are (P-) almost certainly continuous.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* In physics it is used to study Brownian motion, the diffusion of minute particles suspended in fluid, and other types of diffusion via the Fokker‚ÄìPlanck and Langevin equations.\n",
        "* It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman‚ÄìKac formula, a solution to the Schr√∂dinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology.\n",
        "* It is also prominent in the mathematical theory of finance, in particular the Black‚ÄìScholes option pricing model.\n",
        "\n",
        "**Properties of a Wiener Process**\n",
        "\n",
        "1. The Wiener process belongs to the family of **Markov processes** and there specifically to the class of **Levy processes**. It also fulfills the strong markov property. It is one of the best known L√©vy processes (**c√†dl√†g** stochastic processes with stationary independent increments).\n",
        "2. The Wiener Process is a **special Gaussian process** with an expected value function E(W<sub>t</sub>)  = 0 and and the covariance function Cov (W<sub>s</sub>, W<sub>t</sub>) = min (s,t)\n",
        "3. The Wiener process is a (continuous time) **martingale** (L√©vy characterisation: the Wiener process is an almost surely continuous martingale with W0 = 0 and quadratic variation [Wt, Wt] = t, which means that Wt2 ‚àí t is also a martingale).\n",
        "4. The Wiener process is a **Levy process** with steady paths and constant expectation 0.\n",
        "\n",
        "*Another characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent N(0, 1) random variables. This representation can be obtained using the Karhunen‚ÄìLo√®ve theorem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9PQvsR1bKfq"
      },
      "source": [
        "**Wiener process as a limit of random walk (& Differences between Wiener Process & Random Walk)**\n",
        "\n",
        "* https://www.quora.com/What-is-an-intuitive-explanation-of-a-Wiener-process?top_ans=3955819\n",
        "\n",
        "* A Wiener process (~ Brownian motion) is the **integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments.\n",
        "\n",
        "* Let X<sub>1</sub>, X<sub>2</sub>, X<sub>n</sub> be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1.  The central limit theorem asserts that W<sup>(n)</sup> (1) converges in distribution to a standard Gaussian random variable W(1) as n ‚Üí ‚àû.\n",
        "\n",
        "* [Donsker's theorem](https://en.m.wikipedia.org/wiki/Donsker%27s_theorem) asserts that as n ‚Üí ‚àû , W<sub>n</sub> approaches a Wiener process, which explains the ubiquity of Brownian motion. **Donsker's invariance principle** states that: As random variables taking values in the Skorokhod space D [0,1], the random function W<sup>(n)</sup> converges in distribution to a standard Brownian motion W := (W(t))<sub>t ‚àà [0,1]</sub> as n ‚Üí ‚àû.\n",
        "\n",
        "![Donsker's Invariance Principle](https://upload.wikimedia.org/wikipedia/commons/8/8c/Donskers_invariance_principle.gif)\n",
        "\n",
        "*Donsker's invariance principle for simple random walk on Z*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/wiener.jpg\" alt=\"wiener\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKFiwhd7X_5I"
      },
      "source": [
        "**Differences to other stochastic processes**\n",
        "\n",
        "**Random Walk**\n",
        "\n",
        "\n",
        "* **A Wiener process is the [scaling limit](https://en.m.wikipedia.org/wiki/Scaling_limit) of random walk in dimension 1**. The **convergence of a random walk toward the Wiener process is controlled by the central limit theorem**, and by **Donsker's theorem**. The **Green's function** of the diffusion equation that controls the Wiener process, suggests that, **after a large number of steps, the random walk converges toward a Wiener process**.\n",
        "\n",
        "* A random walk is a discrete fractal (a function with integer dimensions; 1, 2, ...), but a **Wiener process trajectory is a true fractal**, and there is a connection between the two (a Wiener process walk is a fractal of **Hausdorff dimension** 2).\n",
        "\n",
        "* Unlike the random walk, a **Wiener Process is scale invariant**. A Wiener process enjoys many symmetries random walk does not. For example, a **Wiener process walk is invariant to rotations, but the random walk is not**, since the underlying grid is not. This means that in many cases, problems on a random walk are easier to solve by translating them to a Wiener process, solving the problem there, and then translating back.\n",
        "\n",
        "**(Gaussian) White Noise**\n",
        "\n",
        "* The Wiener process is used to represent the integral (from time zero to time t) of a zero mean, unit variance, delta correlated **<u>Gaussian</u> white noise process**\n",
        "\n",
        "**Brownian Motion**\n",
        "\n",
        "* **\"Brownian motion\" is a phenomenon that can be modeled with a Wiener Process**, because a Wiener process is a stochastic process with similar behavior to Brownian motion, the physical phenomenon of a minute particle diffusing in a fluid.\n",
        "\n",
        "* The Brownian motion process (and the Poisson process in one dimension) are both examples of **Markov processes in continuous time**\n",
        "\n",
        "* It≈ç also paved the way for the Wiener process from physics to other sciences: the **stochastic differential equations** he set up made it possible to adapt the Brownian motion to more statistical problems.\n",
        "\n",
        "* The **geometric Brownian motion** derived from a stochastic differential equation solves the problem that the **Wiener process, regardless of its starting value, almost certainly reaches negative values over time, which is impossible for stocks**. Since the development of the famous **Black-Scholes model**, the geometric Brownian movement has been the standard.\n",
        "\n",
        "**Ornstein-Uhlenbeck-Process**\n",
        "\n",
        "* The problem raised by the **[non-rectifiable paths](https://en.m.wikipedia.org/wiki/Arc_length)** of the Wiener process in the modeling of Brownian paths leads to the Ornstein-Uhlenbeck process and also makes the need for a theory of stochastic integration and differentiation clear\n",
        "* here it is not the motion but the speed of the particle as one that is not rectifiable process derived from the Wiener process, from which one obtains rectifiable particle paths through integration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0K1gPOoznzE"
      },
      "source": [
        "###### *Gaussian Process*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoS5J618HxN"
      },
      "source": [
        "* A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. **every finite linear combination of them is normally distributed**.\n",
        "\n",
        "* The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
        "\n",
        "* Gaussian Processes are a class of stationary, zero-mean stochastic processes which are completely dependent on their autocovariance functions. This class of models can be used for both regression and classification tasks.\n",
        "\n",
        "* Gaussian Processes provide estimates about uncertainty, for example giving an estimate of how sure an algorithm is that an item belongs to a class or not.\n",
        "\n",
        "* In order to deal with situations which embed a certain degree of uncertainty is typically made use of probability distributions.\n",
        "\n",
        "* Gaussian processes can allow us to describe probability distributions of which we can later update the distribution using Bayes Rule once we gather new training data.\n",
        "\n",
        "* **Relation to other Stochastic Processes**\n",
        "\n",
        "  * A **Wiener process (~ Brownian motion)** is the integral of a white noise generalized Gaussian process. It is not stationary, but it has stationary increments.\n",
        "\n",
        "  * The **fractional Brownian motion** is a Gaussian process whose covariance function is a generalisation of that of the Wiener process.\n",
        "\n",
        "  * The **Ornstein‚ÄìUhlenbeck** process is a stationary Gaussian process.\n",
        "\n",
        "  * The **Brownian bridge** is (like the Ornstein‚ÄìUhlenbeck process) an example of a Gaussian process whose increments are not independent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOw3me6qbku"
      },
      "source": [
        "###### *Further Stochastic Processes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEr4Ub47xgN1"
      },
      "source": [
        "**Brownian Bridge**\n",
        "\n",
        "* A Brownian bridge is a **continuous Gaussian process** with X<sub>0</sub> = X<sub>1</sub> = 0, and with mean and covariance functions given in (c) and (d), respectively.\n",
        "\n",
        "* The **Brownian bridge is (like the Ornstein‚ÄìUhlenbeck process)** an example of a Gaussian process **whose increments are not independent**.\n",
        "\n",
        "* There are several ways of constructing a Brownian bridge from a standard Brownian motion [Source](https://www.randomservices.org/random/brown/Bridge.html)\n",
        "\n",
        "* **Applications: Path Simulation for Stock Shares**: The simple Monte Carlo method with Euler method supplemented by the Brownian bridge correction for the possibility of falling below or exceeding the barriers between discretization times.\n",
        "\n",
        "  * By merely discreetly viewing (simulating) the (log) share price, those paths can also lead to a positive final payment in which the share price between the selected times k delta t has exceeded the lower barrier or exceeded the upper barrier without this is noticed in the discretized model.\n",
        "\n",
        "  * To calculate the probability of such an unnoticed barrier violation, Brownian Bridge is used (with the help of the independence and stationarity of its growth).\n",
        "\n",
        "  * With the help of the statements about the Brown Bridge, one can formally. Specify the Monte Carlo algorithm that can be used to evaluate double barrier options without having to discretize the price path.\n",
        "\n",
        "* **Application: Bond Prices**\n",
        "\n",
        "  * Computation of bond prices in a structural default model with jumps with an unbiased Monte-Carlo simulation.\n",
        "\n",
        "  * The algorithm requires the evaluation of integrals with the density of the first-passage time of a Brownian bridge as the integrand. (Metwally and Atiya (2002) suggest an approximation of these integrals.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0CRhOchiHXb"
      },
      "source": [
        "**Ornstein-Uhlenbeck Process**\n",
        "\n",
        "> Simulating a stochastic differential equation\n",
        "\n",
        "* the Ornstein‚ÄìUhlenbeck process is a **stochastic process** with applications in financial mathematics and the physical sciences.\n",
        "\n",
        "* Its original application in physics was as a model for the **<u>velocity</u> of a massive Brownian particle under the influence of <u>friction</u>**. It is named after Leonard Ornstein and George Eugene Uhlenbeck.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is a **stationary Gauss‚ÄìMarkov process**, which means that it is a **Gaussian process, a Markov process, and is temporally homogeneous**. In fact, it is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables.\n",
        "\n",
        "* Over time, the process tends to **drift towards its mean function**: such a process is called **mean-reverting**.\n",
        "\n",
        "* The process can be considered to be a **modification of the random walk in continuous time, or Wiener process**, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the center.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can also be considered as the **continuous-time analogue of the discrete-time AR(1) process**.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can be interpreted as a **scaling limit of a discrete process**, in the same way that Brownian motion is a scaling limit of random walks.\n",
        "\n",
        "* Generalization: It is possible to extend Ornstein‚ÄìUhlenbeck processes to processes where the background driving process is a L√©vy process (instead of a simple Brownian motion).\n",
        "\n",
        "* In addition, in finance, stochastic processes are used where the volatility increases for larger values of C.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process (just like the Brownian Bridge) a is an example of a **Gaussian process whose increments are not independent**. Look for stock returns devoid of explanatory factors, and analyze the corresponding residuals as stochastic processes. (e.g. mean reverting?). Can residuals be fitted to (increments of) OU processes or other MR processes? If so, what is the typical correlation time-scale? Mean reversion days: how long does it take to converge (e.g. model distribution of days).\n",
        "\n",
        "**In Financial Mathematics**\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is one of several approaches used to model (with modifications) interest rates, currency exchange rates, and commodity prices stochastically.\n",
        "\n",
        "* The parameter Œº (mu) represents the equilibrium or mean value supported by fundamentals; œÉ (signa) the degree of volatility around it caused by shocks, and Œ∏ (theta) the rate by which these shocks dissipate and the variable reverts towards the mean.\n",
        "\n",
        "* One application of the process is a trading strategy known as pairs trade.\n",
        "\n",
        "* Stationary and mean-reverting around mean=10 (red dotted line)\n",
        "* **In financial engineering: how long does it take in average to converge? mean-reversion is an investment opportunity!**\n",
        "* Now, we are going to take a look at the time evolution of the distribution of the process. To do this, we will simulate many independent realizations of the same process in a vectorized way. We define a vector X that will contain all realizations of the process at a given time (that is, we do not keep all realizations at all times in memory). This vector will be overwritten at every time step. We will show the estimated distribution (histograms) at several points in time:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqrw3_Nqev5"
      },
      "source": [
        "**Jump diffusion**\n",
        "\n",
        "* [Jump diffusion](https://en.m.wikipedia.org/wiki/Jump_diffusion) is a stochastic process that involves jumps and diffusion.\n",
        "\n",
        "* It has important applications in magnetic reconnection, coronal mass ejections, condensed matter physics, option pricing, and pattern theory and computational vision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQP6Jd68WsL"
      },
      "source": [
        "**L√©vy Process**\n",
        "\n",
        "* A L√©vy process is a stochastic process with **[independent, stationary increments](https://de.m.wikipedia.org/wiki/Prozess_mit_unabh√§ngigen_Zuw√§chsen)** (= the course of the future of the process is independent of the past): it represents the motion of a point whose successive displacements are random and independent, and statistically identical over different time intervals of the same length.\n",
        "\n",
        "* A L√©vy process may thus be viewed as the **continuous-time analog of a random walk**.\n",
        "\n",
        "* The most well known **examples of L√©vy processes are Wiener process (~ Brownian motion), and Poisson process**. Aside from Brownian motion with drift, all other proper (that is, not deterministic) L√©vy processes have discontinuous paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAvA4JovM_-"
      },
      "source": [
        "**Bernoulli Process**\n",
        "\n",
        "* The outcomes of a Bernoulli process will follow a [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution).\n",
        "\n",
        "* https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/\n",
        "\n",
        "* A Bernoulli process is a finite or infinite sequence of binary random variables, so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1.\n",
        "\n",
        "* The component Bernoulli variables X<sub>i</sub> are [identically distributed and independent](https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). Prosaically, a Bernoulli process is a repeated coin flipping, possibly with an unfair coin (but with consistent unfairness).\n",
        "\n",
        "* Every variable X<sub>i</sub> in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided dice); this generalization is known as the **Bernoulli scheme**.\n",
        "\n",
        "* The problem of determining the process, given only a limited sample of Bernoulli trials, may be called the problem of checking whether a coin is fair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPTaHcB54U"
      },
      "source": [
        "**Poisson (Point) Process**\n",
        "\n",
        "* Poisson point process is a type of random mathematical object that consists of points randomly located on a mathematical space\n",
        "\n",
        "* Its name derives from the fact that if a collection of random points in some space forms a Poisson process, then the number of points in a region of finite size is a random variable with a Poisson distribution.\n",
        "\n",
        "* The process was discovered independently and repeatedly in several settings, including experiments on radioactive decay, telephone call arrivals and insurance mathematics. The Poisson point process is often defined on the real line, where it can be considered as a stochastic process.\n",
        "\n",
        "* On the real line, the Poisson process is a type of **continuous-time Markov process** known as a birth-death process (with just births and zero deaths) and is called a pure or simple birth process.\n",
        "\n",
        "* In this setting, it is used, for example, in queueing theory to model random events, such as the arrival of customers at a store, phone calls at an exchange or occurrence of earthquakes, distributed in time. In the plane, the point process, also known as a spatial Poisson process, can represent the locations of scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest.\n",
        "\n",
        "* In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process.\n",
        "\n",
        "* Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena where there is sufficiently strong interaction between the points. This has inspired the proposal of other point processes, some of which are constructed with the Poisson point process, that seek to capture such interaction.\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-process-everything-you-need-to-know-322aa0ab9e9a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Measure & Probability Theory*"
      ],
      "metadata": {
        "id": "ECcHaTG2nKiM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4KDiI20GSS8"
      },
      "source": [
        "###### *Measure (Ma√ü)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FMu2VsVHXMJ"
      },
      "source": [
        "* Ein Ma√ü ist in der Mathematik **eine Funktion**, die geeigneten Teilmengen einer Grundmenge Zahlen zuordnet, die als ‚ÄûMa√ü‚Äú f√ºr die Gr√∂√üe dieser Mengen interpretiert werden k√∂nnen.\n",
        "\n",
        "* Es sei $\\mathcal{A}$ eine o-Algebra √ºber einer nicht-leeren Grundmenge $\\Omega .$ Eine Funktion $\\mu: \\mathcal{A} \\rightarrow[0, \\infty]$ hei√üt Ma√ü auf $\\mathcal{A}$, wenn die beiden folgenden Bedingungen erf√ºllt sind:\n",
        "\n",
        "> $\\mu(\\emptyset)=0$\n",
        "\n",
        "> Additivit√§t: F√ºr jede Folge $\\left(A_{n}\\right)_{n \\in \\mathbb{N}}$ paarweise disjunkter Mengen aus $\\mathcal{A}$ gilt $\\mu\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right)$\n",
        "\n",
        "* Ist die o-Algebra aus dem Zusammenhang klar, so spricht man auch von einem Ma√ü auf $\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tOKtjbyH7Ai"
      },
      "source": [
        "* Eine Teilmenge von $\\Omega$, die in $\\mathcal{A}$ liegt, wird messbar genannt. F√ºr solch ein $A \\in \\mathcal{A}$ hei√üt $\\mu(A)$ das Ma√ü der Menge $A$.\n",
        "\n",
        "* Das **Tripel $(\\Omega, \\mathcal{A}, \\mu)$ wird Ma√üraum** genannt.\n",
        "\n",
        "* Das Paar $(\\Omega, \\mathcal{A})$ bestehend aus der Grundmenge und der darauf definierten $\\sigma$-Algebra **hei√üt Messraum oder auch messbarer Raum**.\n",
        "\n",
        "* **Ein Ma√ü $\\mu$ ist also eine auf einem Messraum definierte nicht-negative [$\\sigma$ -additive](https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t) [Mengenfunktion](https://de.m.wikipedia.org/wiki/Mengenfunktion) mit $\\mu(\\emptyset)=0$.**\n",
        "\n",
        "* Das Ma√ü $\\mu$ hei√üt Wahrscheinlichkeitsma√ü (oder normiertes Ma√ü), wenn zus√§tzlich $\\mu(\\Omega)=1$ gilt. Ein Ma√üraum $(\\Omega, \\mathcal{A}, \\mu)$ mit einem Wahrscheinlichkeitsma√ü $\\mu$ ist ein Wahrscheinlichkeitsraum.\n",
        "\n",
        "* Ist allgemeiner $\\mu(\\Omega)<\\infty,$ so nennt man $\\mu$ ein [endliches Ma√ü](https://de.m.wikipedia.org/wiki/Endliches_Ma√ü). Existieren abz√§hlbar viele Mengen, deren Ma√üendlich ist und deren Vereinigung ganz $\\Omega$ ergibt, dann wird $\\mu$ ein [$\\sigma$ -endliches](https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit) (oder $\\sigma$ -finites) Ma√ü genannt. $^{[4]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGw0qfYwrCJ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Messbare_Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtT12NSGYu4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbegVOVJGqzz"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1v7eQtS4gqu"
      },
      "source": [
        "**Z√§hlma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbVSW8Q6RdZ"
      },
      "source": [
        "$\\mu$ (A) :=\n",
        "* #A, wenn A endlich\n",
        "* ‚àû, sonst\n",
        "\n",
        "Rechenregeln in [0, ‚àû] :\n",
        "* x + ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* x * ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* 0 * ‚àû := 0 (in der Ma√ütheorie sinnvoll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzLXsG6C4Lx"
      },
      "source": [
        "**Dirac Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpN2hkk7KTX"
      },
      "source": [
        "p ‚àà X (Punktma√ü, weil es einem Punkt ein Ma√ü zuordnet, und sonst Null).\n",
        "\n",
        "Ist p innerhalb eine Menge A als Teilmenge von X, dann ist Diracma√ü = 1, sonst 0.\n",
        "\n",
        "Œ¥<sub>p</sub> (A) :=\n",
        "* 1, wenn p ‚àà A\n",
        "* 0, wenn sonst.\n",
        "\n",
        "(Œ¥ = delta klein)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJL1d3ifDByw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diracma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkR6CvL_C0uu"
      },
      "source": [
        "**Lebesgue Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAnnK16Dbua"
      },
      "source": [
        "* Das Lebesgue-Ma√ü ist das **Ma√ü im euklidischen Raum**, das geometrischen Objekten ihren Inhalt (L√§nge, Fl√§cheninhalt, Volumen, ‚Ä¶) zuordnet.\n",
        "\n",
        "* Das Lebesgue-Borel-Ma√ü auf der Borel-$\\sigma$-Algebra $\\mathcal{B}\\left(\\mathbb{R}^{n}\\right)$ (auch als Borel-Lebesgue-Ma√ü oder nur Bore/-Ma√ü bezeichnet) ist das eindeutige Ma√ü $\\lambda$ mit der Eigenschaft, dass es $n$ -dimensionalen\n",
        "Hyperrechtecken ihr $n$ -dimensionales Volumen zuordnet:\n",
        "\n",
        ">$\\lambda\\left(\\left[a_{1}, b_{1}\\right] \\times \\cdots \\times\\left[a_{n}, b_{n}\\right]\\right)=\\left(b_{1}-a_{1}\\right) \\cdots \\cdots\\left(b_{n}-a_{n}\\right)$\n",
        "\n",
        "* Das hei√üt, **es ist das Ma√ü, das Intervallen inre L√§nge zuordnet (im Eindimensionalen), Rechtecken ihren Fl√§cheninhalt zuordnet (im Zweidimensionalen), Quadern ihr Volumen zuordnet (im Dreidimensionalen) usw.**\n",
        "\n",
        "* Durch diese Bedingung wird der Inhalt $\\lambda(B)$ beliebiger Borel-Mengen eindeutig festgelegt. Die BorelMengen werden auch Borel-messbar oder $B$ -messbar genannt. **Das Borel-Ma√ü ist bewegungsinvariant und normiert, aber nicht vollst√§ndig.**\n",
        "\n",
        "* Das Lebesgue-Ma√ü ist das Haar-Ma√ü **auf der lokalkompakten topologischen Gruppe** $\\mathbb {R} ^{n}$ mit der Addition, die Existenz folgt daher bereits aus der Existenz des Haarma√ües.\n",
        "\n",
        "* Insbesondere ist es translationsinvariant, das bedeutet, dass sich das Ma√ü einer Menge unter Translation nicht √§ndert. Zudem ist es invariant unter Spiegelungen und Drehungen, also sogar bewegungsinvariant. Das Lebesgue-Ma√ü ist œÉ-endlich und regul√§r."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XsGc9DFRb"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymG9QHcCF1Hg"
      },
      "source": [
        "**Haarsches Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLkzlmPEF8cn"
      },
      "source": [
        "* Das Haarsche Ma√ü wurde eingef√ºhrt, um Ergebnisse der **Ma√ütheorie in der Gruppentheorie anwendbar zu machen**.\n",
        "\n",
        "* Beispiel: Das Lebesgue-Ma√ü $B$ auf $\\mathbb{R}^{n}$ und $\\mathbb{C}^{n}$ ist das Haarsche Ma√ü auf den additiven Gruppen $\\left(\\mathbb{R}^{n},+\\right)$ bzw. $\\left(\\mathbb{C}^{n},+\\right)$.\n",
        "\n",
        "* Es ist eine Verallgemeinerung des Lebesgue-Ma√ües. Das Lebesgue-Ma√ü ist ein Ma√ü auf dem euklidischen Raum, das unter Translationen invariant ist.\n",
        "\n",
        "* Der euklidische Raum ist eine lokalkompakte topologische Gruppe bez√ºglich der Addition. Das Haarsche Ma√ü ist f√ºr jede lokalkompakte (im Folgenden immer als hausdorffsch vorauszusetzende) topologische Gruppe definierbar, insbesondere also f√ºr jede Lie-Gruppe.\n",
        "\n",
        "* Lokalkompakte Gruppen mit ihren Haarschen Ma√üen werden in der [harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) untersucht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzktY0hJF4LR"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Haarsches_Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw99f_qvDl6i"
      },
      "source": [
        "**Haussdorf Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2DTGGZGDv5H"
      },
      "source": [
        "* Das Hausdorff-Ma√ü ist eine Verallgemeinerung des Lebesgue-Ma√ües auf nicht notwendig ganzzahlige Dimensionen. Mit seiner Hilfe l√§sst sich die Hausdorff-Dimension definieren, ein Dimensionsbegriff, mit dem beispielsweise fraktale Mengen untersucht werden k√∂nnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9M76qlDrVX"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Hausdorff-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V27J3X8qM9cN"
      },
      "source": [
        "**Jordan Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6tM4coFNIxa"
      },
      "source": [
        "* Mit dem Jordan-Ma√ü kann man beschr√§nkten Teilmengen des $\\mathbb {R} ^{n}$ einen Inhalt zuordnen und erh√§lt einen Integralbegriff, der dem [riemannschen Integralbegriff](https://de.m.wikipedia.org/wiki/Riemannsches_Integral) analog ist.\n",
        "\n",
        "* Das Jordan-Ma√ü ist kein Ma√ü im Sinne der Ma√ütheorie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcW8Znq_NB8I"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Jordan-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ef5-QUNE22"
      },
      "source": [
        "Siehe auch: https://de.m.wikipedia.org/wiki/Inhalt_(Ma√ütheorie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomYI_xzCurv"
      },
      "source": [
        "**Radon Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGB7cfQ9VeD"
      },
      "source": [
        "Es handelt sich um ein spezielles Ma√ü auf der Borelschen œÉ-Algebra eines Hausdorff-Raums mit bestimmten Regularit√§tseigenschaften. Der Begriff wird in der Fachliteratur jedoch nicht einheitlich verwendet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AnX69MC1wo"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Radonma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0PG2uLQIPM"
      },
      "source": [
        "**Measurable Space (Messbarer Raum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraYrSFNKL4d"
      },
      "source": [
        "Das Tupel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge (universe / Set) genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine $\\sigma$ -Algebra √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "Zusammen mit einem Ma√ü $\\mu$ wird aus einem messbaren Raum ein Ma√üraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrI52A0M-lGw"
      },
      "source": [
        "* **A measurable space or Borel space is a basic object in measure theory**. It consists of a set and a œÉ-algebra, which defines the subsets that will be measured.\n",
        "\n",
        "* Note that in contrast to a measure space, **no measure is needed for a measurable space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCr-8c-d62V"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZG-bj9KyHs"
      },
      "source": [
        "**Example**\n",
        "\n",
        "Look at the set\n",
        "\n",
        "X=\\{1,2,3\\}\n",
        "\n",
        "One possible $\\sigma$ -algebra would be\n",
        "\n",
        "$\\mathcal{A}_{1}=\\{X, \\emptyset\\}$\n",
        "\n",
        "Then $\\left(X, \\mathcal{A}_{1}\\right)$ is a measurable space.\n",
        "\n",
        "Another possible $\\sigma$ -algebra would be the power set on $X$ :\n",
        "\n",
        "$\\mathcal{A}_{2}=\\mathcal{P}(X)$\n",
        "\n",
        "With this, a second measurable space on the set $X$ is given by $\\left(X, \\mathcal{A}_{2}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kIK_3HLLTT"
      },
      "source": [
        "**Types**\n",
        "\n",
        "* If $X$ is finite or countable infinite, **the $\\sigma$ -algebra is most of the times the power set on $X$**, so $\\mathcal{A}=\\mathcal{P}(X) .$ This leads to the measurable space $(X, \\mathcal{P}(X))$\n",
        "\n",
        "* **If $X$ is a topological space, the $\\sigma$ -algebra is most commonly the Borel $\\sigma$ -algebra $\\mathcal{B}$**, so $\\mathcal{A}=\\mathcal{B}(X)$. This leads to the measurable space $(X, \\mathcal{B}(X))$ that is common for all topological spaces such as\n",
        "the real numbers $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHhdyPQ-hgE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measurable_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0juq9WKIedRw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clc0uHou3vFE"
      },
      "source": [
        "**Riemann vs Lebesgue Integral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpeX84I3z0Q"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Integralrechnung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8ajZ5T31sN"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ütheorie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3G8b13_MBD3"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Satz_von_Fubini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGhitrlOQEHM"
      },
      "source": [
        "###### *Measure Space (Ma√üraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> One important example of a measure space is a probability space."
      ],
      "metadata": {
        "id": "pPIoNoM1u-dH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsroJGMFRuY"
      },
      "source": [
        "Ein Ma√üraum ist eine spezielle mathematische Struktur, die eine essentielle Rolle in der Ma√ütheorie und dem axiomatischen Aufbau der Stochastik spielt.\n",
        "\n",
        "Das Tripel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine [$\\sigma$ -Algebra](https://de.m.wikipedia.org/wiki/Œ£-Algebra) √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "* $\\mu$ ein [Ma√ü](https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)) ist, das auf $\\mathcal{A}$ definiert ist.\n",
        "\n",
        "Alternativ kann man einen Ma√üraum auch als einen [Messraum](https://de.m.wikipedia.org/wiki/Messraum_(Mathematik)) $(\\Omega, \\mathcal{A})$ versehen mit einem Ma√ü $\\mu$ definieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqmDCJDWy4sq"
      },
      "source": [
        "A measure space is a triple $(X, \\mathcal{A}, \\mu),$ where\n",
        "\n",
        "* $X$ is a set\n",
        "\n",
        "* $\\mathcal{A}$ is a $\\sigma$ -algebra on the set $X$\n",
        "\n",
        "* $\\boldsymbol{\\mu}$ is a measure on $(X, \\mathcal{A})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34X5FmJ4d--B"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDp68lYUypzs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measure_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIqej8jGuOt"
      },
      "source": [
        "**Beispiele**\n",
        "\n",
        "* Ein einfaches Beispiel f√ºr einen Ma√üraum sind die nat√ºrlichen Zahlen als Grundmenge $\\Omega=\\mathbb{N}$, als $\\sigma$ Algebra w√§hlt man die Potenzmenge $\\mathcal{A}=\\mathcal{P}(\\mathbb{N})$ und als Ma√ü das Diracma√ü auf der $1: \\mu=\\delta_{1}$\n",
        "\n",
        "* Ein bekannter Ma√üraum ist die Grundmenge $\\mathbb{R}$, versehen mit der borelschen $\\sigma$ -Algebra $\\mathcal{B}(\\mathbb{R})$ und dem Lebesgue-Ma√ü. **Dies ist der kanonische Ma√üraum in der Integrationstheorie.**\n",
        "\n",
        "* Die in der Wahrscheinlichkeitstheorie verwendeten **[Wahrscheinlichkeitsr√§ume](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum)** $(\\Omega, \\mathcal{A}, P)$ sind allesamt Ma√ür√§ume. Sie bestehen aus der Ergebnismenge $\\Omega$, der Ereignisalgebra $\\mathcal{A}$ und dem Wahrscheinlichkeitsma√ü $P$ (= synonym 'Wahrscheinlichkeitsverteilung' oder einfach 'Verteilung'). Mit den Eigenschaften: Die drei Forderungen Normiertheit, œÉ-Additivit√§t und Werte im Intervall zwischen 0 und 1 werden auch die Kolmogorow-Axiome genannt. ([Source](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTp6RhYL8Je"
      },
      "source": [
        "**Klassen von Ma√ür√§umen**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollst√§ndiges_Ma√ü\n",
        "\n",
        "und mehr.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKgmO9kQZPo"
      },
      "source": [
        "* Verzichtet man auf Abst√§nde und Winkel, beh√§lt jedoch das Volumen geometrischer K√∂rper bei, gelangt man in das Gebiet der Ma√ütheorie. (f√ºr ein wahrscheinlichkeitsma√ü volumen = 1)\n",
        "\n",
        "* Der Ma√ütheorie gelang es, den Begriff des Volumens (oder eines anderen Ma√ües) auf eine enorm gro√üe Klasse von Mengen auszudehnen, die sogenannten messbaren Mengen. In vielen F√§llen ist es jedoch unm√∂glich, allen Mengen ein Ma√ü zuzuordnen (siehe Ma√üproblem).\n",
        "Die messbaren Mengen bilden dabei eine œÉ-Algebra. Mit Hilfen von messbaren Mengen lassen sich messbare Funktionen zwischen Messr√§umen definieren.\n",
        "\n",
        "* Um einen topologischen Raum zu einem Messraum zu machen, muss man ihn mit einer œÉ-Algebra ausstatten. Die œÉ-Algebra der Borel-Mengen ist die verbreitetste, aber nicht die einzige Wahl.\n",
        "\n",
        "* Ein Ma√üraum ist ein Messraum, der mit einem Ma√ü versehen ist. Ein euklidischer Raum mit dem [Lebesgue-Ma√ü](https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü) ist beispielsweise ein Ma√üraum. In der Integrationstheorie werden Integrierbarkeit und Integrale messbarer Funktionen auf Ma√ür√§umen definiert. Mengen vom Ma√ü null werden Nullmengen genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0omcCEyljo"
      },
      "source": [
        "* A measure space is a basic object of measure theory, a branch of mathematics that studies generalized notions of volumes.\n",
        "\n",
        "* It contains an underlying set, the subsets of this set that are feasible for measuring (the œÉ-algebra) and the method that is used for measuring (the measure).\n",
        "\n",
        "* **One important example of a measure space is a probability space**.\n",
        "\n",
        "* A measurable space consists of the first two components without a specific measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZqzchtzaSz"
      },
      "source": [
        "A **complete measure** (or, more precisely, a complete measure space) is a measure space in which every subset of every null set is measurable (having measure zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtShSxHrzeGE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Complete_measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9JLE04AcXav"
      },
      "source": [
        "**Probability Space (Wahrscheinlichkeitsraum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2axZJitcXav"
      },
      "source": [
        "Gegeben sei\n",
        "\n",
        "* eine Menge $\\Omega$, der sogenannte **Ergebnisraum**,\n",
        "\n",
        "* eine o-Algebra $\\Sigma$ auf dieser Menge, das **Ereignissystem**.\n",
        "\n",
        "Dann hei√üt eine Abbildung (=Funktion)\n",
        "\n",
        ">$P: \\Sigma \\rightarrow[0,1]$\n",
        "\n",
        "mit den Eigenschaften\n",
        "\n",
        "*  Normiertheit: Es ist $P(\\Omega)=1$\n",
        "\n",
        "* $\\sigma$-Additivit√§t: F√ºr jede abz√§hlbare Folge von paarweise disjunkten Mengen $A_{1}, A_{2}, A_{3}, \\ldots$ aus $\\Sigma$ gilt $P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right)$\n",
        "\n",
        "ein Wahrscheinlichkeitsma√ü oder eine Wahrscheinlichkeitsverteilung.\n",
        "\n",
        "Die drei Forderungen Normiertheit, $\\sigma$-Additivit√§t und Werte im Intervall zwischen O und 1 werden auch die [Kolmogorow-Axiome](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitstheorie#Axiome_von_Kolmogorow) genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qf8jvmMcXaw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3sQ4P3KcXaw"
      },
      "source": [
        "* **One important example of a measure space is a probability space.**\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum, bei dem das Ma√ü des ganzen Raums gleich 1 ist.\n",
        "\n",
        "* In der Wahrscheinlichkeitstheorie werden f√ºr die verwendeten ma√ütheoretischen Begriffe meist eigene Bezeichnungen verwendet, die der Beschreibung von Zufallsexperimenten angepasst sind: Messbare Mengen werden Ereignisse und messbare Funktionen zwischen Wahrscheinlichkeitsr√§umen werden Zufallsvariable genannt; ihre Integrale sind Erwartungswerte.\n",
        "\n",
        "* Das Produkt einer endlichen oder unendlichen Familie von Wahrscheinlichkeitsr√§umen ist wieder ein Wahrscheinlichkeitsraum. Im Gegensatz dazu ist f√ºr allgemeine Ma√ür√§ume nur das Produkt endlich vieler R√§ume definiert. Dementsprechend gibt es zahlreiche unendlichdimensionale Wahrscheinlichkeitsma√üe, beispielsweise die Normalverteilung, aber kein unendlichdimensionales Lebesgue-Ma√ü.\n",
        "Diese R√§ume sind weniger geometrisch. Insbesondere l√§sst sich die Idee der Dimension, wie sie in der einen oder anderen Form auf alle anderen R√§ume anwendbar ist, nicht auf Messr√§ume, Ma√ür√§ume und Wahrscheinlichkeitsr√§ume anwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFqotKf4cXaw"
      },
      "source": [
        "* Es handelt sich um ein mathematisches Modell zur Beschreibung von Zufallsexperimenten. Hierbei werden die verschiedenen m√∂glichen Ausg√§nge des Experiments zu einer Menge zusammengefasst. Teilmengen dieser Ergebnismenge k√∂nnen dann unter bestimmten Voraussetzungen Zahlen zwischen 0 und 1 zugeordnet werden, die als Wahrscheinlichkeiten interpretiert werden.\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum (Œ©, Œ£, P) dessen Ma√ü P ein Wahrscheinlichkeitsma√ü ist. Im Einzelnen bedeutet das:\n",
        "\n",
        "* Œ© ist eine beliebige nichtleere Menge, genannt die Ergebnismenge. Ihre Elemente hei√üen Ergebnisse.\n",
        "\n",
        "* Œ£ (Sigma) ist eine œÉ-Algebra √ºber der Grundmenge Œ© (Omega), also eine Menge bestehend aus Teilmengen von Œ©, die Œ© enth√§lt und abgeschlossen gegen√ºber der Bildung von Komplementen und abz√§hlbaren Vereinigungen ist. Die Elemente von Œ£ hei√üen Ereignisse. Die œÉ-Algebra Œ£ selbst wird auch Ereignissystem oder Ereignisalgebra genannt.\n",
        "\n",
        "* P : Œ£ ‚Äì> [0,1] ist ein Wahrscheinlichkeitsma√ü, das hei√üt eine Mengenfunktion, die den Ereignissen Zahlen zuordnet, derart dass P(‚àÖ) = 0 ist, P (A1 ‚à™ A2 ‚à™ ‚Ä¶ ) = P(A1) + P(A2) + ‚Ä¶ f√ºr paarweise disjunkte (d. h. sich gegenseitig ausschlie√üende) Ereignisse A1, A2, ‚Ä¶ gilt (3. Kolmogorow-Axiom) und P(Œ©) = 1 ist (2. Kolmogorow-Axiom).\n",
        "\n",
        "* Der Messraum (Œ©, Œ£) wird auch Ereignisraum genannt. Ein Wahrscheinlichkeitsraum ist also ein Ereignisraum, auf dem zus√§tzlich ein Wahrscheinlichkeitsma√ü gegeben ist.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4A4TeH7SewU"
      },
      "source": [
        "###### *Measurable Space (Messraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXZ_kJWGTey"
      },
      "source": [
        "* [Messraum](https://en.wikipedia.org/wiki/Measurable_space) und Ma√üraum sind spezielle œÉ-Algebren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErN__bggDh9m"
      },
      "source": [
        "###### *Field of Sets (Algebra oder Mengensystem)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIdFJLmUG_C"
      },
      "source": [
        "**Mengensystem** oder **Mengenalgebra** oder **Fields of Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3axOdZgGNHd"
      },
      "source": [
        "* **A field of sets is a pair $\\langle X, \\mathcal{F}\\rangle$ where $X$ is a set and $\\mathcal{F}$ is an algebra over $X$**\n",
        "\n",
        "* i.e., a subset of the power set of $X$, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying $X \\in \\mathcal{F}$.\n",
        "\n",
        "* In other words, $\\mathcal{F}$ forms\n",
        "a subalgebra of the power set Boolean algebra of $X$ (with the same identity element $X \\in \\mathcal{F}$ ).\n",
        "(Many authors refer to $\\mathcal{F}$ itself as a field of sets.) Elements of $X$ are called points and those of $\\mathcal{F}$\n",
        "are called complexes and are said to be the admissible sets of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct9dwFuXEuFS"
      },
      "source": [
        "Œ© (Omega) sei eine beliebige (Grund-)Menge. Ein System $\\mathcal{A}$ (oder $\\mathcal{F}$) von Teilmengen von Œ© hei√üt eine Mengenalgebra oder Algebra √ºber Œ©, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "1. $\\mathcal{A} \\neq \\emptyset$ ( $\\mathcal{A}$ ist nicht leer)\n",
        "\n",
        "2. $A, B \\in \\mathcal{A} \\Rightarrow A \\cup B \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Vereinigung)\n",
        "\n",
        "3. $A \\in \\mathcal{A} \\Rightarrow A^{\\mathrm{c}} \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Komplementbildung $\\left.A^{c}=\\Omega \\backslash A\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ksvdizjJS-"
      },
      "source": [
        "* In der Mathematik ist (Mengen-)Algebra ein Grundbegriff der Ma√ütheorie. Er beschreibt ein nicht-leeres Mengensystem, das vereinigungs- und komplementstabil ist.\n",
        "\n",
        "* A field of sets is a **pair ‚ü®X,F‚ü©** where X is a set and F is an algebra over X i.e., a subset of the power set of X, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying X ‚àà F.\n",
        "\n",
        "* In other words, F forms a subalgebra of the power set Boolean algebra of X (with the same identity element X ‚àà F). (Many authors refer to F itself as a field of sets.)\n",
        "\n",
        "* **Elements of X are called points and those of F are called complexes and are said to be the admissible sets of X.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRO-7ajId2V"
      },
      "source": [
        "* For arbitrary set $Y$, its power set (Potenzmenge) $2^{Y}$ (or, somewhat pedantically, the pair $\\left\\langle Y, 2^{Y}\\right\\rangle$ of this set and its power set) is a field of sets.\n",
        "\n",
        "* If $Y$ is finite (namely, $n$ -element), then $2^{Y}$ is finite (namely, $2^{n}$ element).\n",
        "\n",
        "* It appears that every finite field of sets (it means, $\\langle X, \\mathcal{F}\\rangle$ with $\\mathcal{F}$ finite, while $X$ may be infinite) admits a representation of the form $\\left\\langle Y, 2^{Y}\\right\\rangle$ with finite $Y ;$ it means a function $f: X \\rightarrow Y$ that establishes a one-to-one correspondence between $\\mathcal{F}$ and $2^{Y}$ via inverse image:\n",
        "$S=f^{-1}[B]=\\{x \\in X \\mid f(x) \\in B\\}$ where $S \\in \\mathcal{F}$ and $B \\in 2^{Y}$ (that is, $B \\subset Y$ ).\n",
        "\n",
        "* One notable consequence: the number of complexes, if finite, is always of the form $2^{n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkRhwrZHSGp"
      },
      "source": [
        "**Beispiele f√ºr Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFXRKkuHYNd"
      },
      "source": [
        "* F√ºr jede beliebige Menge $\\Omega$ ist $\\{\\emptyset, \\Omega\\}$ die kleinste und die Potenzmenge $\\mathcal{P}(\\Omega)$ die gr√∂√ütm√∂gliche Mengenalgebra.\n",
        "* Jede $\\sigma$ -Algebra ist eine Mengenalgebra.\n",
        "* F√ºr jede Menge $\\Omega$ ist das Mengensystem $\\mathcal{A}=\\left\\{A \\subseteq \\Omega \\mid A \\text { endlich oder } A^{c} \\text { endlich }\\right\\}$ eine Mengenalgebra. Wenn $\\Omega$ unendich ist, dann ist $\\mathcal{A}$ keine $\\sigma$ -Algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded5mK7pcYoX"
      },
      "source": [
        "**Separative and compact fields of sets: towards Stone duality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7fxFxMgcbMU"
      },
      "source": [
        "* A field of sets is called **separative (or differentiated)** if and only if for every pair of distinct points there is a complex containing one and not the other.\n",
        "\n",
        "* A field of sets is called **compact** if and only if for every proper filter over X the intersection of all the complexes contained in the filter is non-empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL_pBvidGa5"
      },
      "source": [
        "Given a field of sets $\\mathbf{X}=\\langle X, \\mathcal{F}\\rangle$ the complexes form a base for a topology. We denote by $T(\\mathbf{X})$ the corresponding topological space, $\\langle X, \\mathcal{T}\\rangle$ where $\\mathcal{T}$ is the topology formed by taking arbitrary unions of complexes. Then\n",
        "\n",
        "1. $T(\\mathbf{X})$ is always a [zero-dimensional space](https://en.m.wikipedia.org/wiki/Zero-dimensional_space)\n",
        "\n",
        "2. $T(\\mathbf{X})$ is a [Hausdorff space](https://en.m.wikipedia.org/wiki/Hausdorff_space) if and only if $\\mathbf{X}$ is separative.\n",
        "\n",
        "3. $T(\\mathbf{X})$ is a compact space with compact open sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is compact.\n",
        "\n",
        "4. $T(\\mathbf{X})$ is a Boolean space with clopen sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is both separative and compact (in which case it is described as being descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I7YepgQlABC"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Field_of_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8T8hpoiWFR"
      },
      "source": [
        "###### *œÉ-algebra (Sigma Algebra)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpgaCc5SSQx"
      },
      "source": [
        "**œÉ-algebra (Sigma Algebra)**\n",
        "\n",
        "* https://mathepedia.de/Sigma-Algebren.html\n",
        "\n",
        "* Sei $\\Omega \\neq \\emptyset$ eine Menge, $\\mathfrak{P}(\\Omega)$ die Potenzmenge und $\\mathcal{F} \\subseteq \\mathfrak{P}(\\Omega)$ ein Mengensystem (=field of sets).\n",
        "\n",
        "Definition\n",
        "\n",
        "$\\mathcal{F}$ hei√üt Algebra, wenn folgende Eigenschaften gelten:\n",
        "\n",
        "(1) $\\quad \\emptyset \\in \\mathcal{F}$\n",
        "\n",
        "(2) $\\quad A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}$\n",
        "\n",
        "(3) $\\quad A, B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}$\n",
        "\n",
        "$\\mathcal{F}$ hei√üt $\\sigma$ -Algebra, wenn die Punkte (1) und (2) gelten\n",
        "und zus√§tzlich\n",
        "\n",
        "(4) $\\quad A_{1}, A_{2}, \\ldots \\in \\mathcal{F} \\Rightarrow \\bigcup A_{k} \\in \\mathcal{F}$\n",
        "\n",
        "gilt.\n",
        "\n",
        "Algebren sind bez√ºglich der endlichen Vereinigung abgeschlossene Mengensysteme und $\\sigma$ -Algebren sind bez√ºglich der abz√§hlbaren Vereinigung abgeschlossene\n",
        "Mengensysteme. Wegen (1) und (2) gilt stets $\\Omega \\in \\mathcal{F}$.\n",
        "\n",
        "* If an algebra over a **set is closed under countable unions** (hence also under countable intersections), it is called a **sigma algebra** and **the corresponding field of sets (Mengensystem) is called a measurable space**. The complexes of a measurable space are called measurable sets.\n",
        "\n",
        "* * **A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it.** (Alternative: $\\langle$ Œ© , $\\mathcal{F}$, $\\mu$ $\\rangle$)\n",
        "\n",
        "* If $\\mu$ is in fact a probability measure we speak of a probability space and call its underlying measurable space a sample space.\n",
        "\n",
        "* The points of a sample space are called samples and represent potential outcomes while the measurable sets (complexes) are called events and represent properties of outcomes for which we wish to assign probabilities. (Many use the term sample space simply for the underlying set of a probability space, particularly in the case where every subset is an event.)\n",
        "\n",
        "* Measure spaces and probability spaces play a foundational role in measure theory and probability theory respectively.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebra_(Mengensystem)\n",
        "\n",
        "**Pain Point**\n",
        "\n",
        "* How to define a messbare Menge?\n",
        "\n",
        "* Wenn wir eine Menge aus den reellen Zahlen haben und mochten eine Teilbereich [a bis b] messen, dann brauchen wir einen allgemeinen Massbegriff unabhangig von der konkreten Menge. Hier kommt Masstheorie und Sigma-Algebra.\n",
        "\n",
        "**Definition**\n",
        "\n",
        "* A œÉ-algebra defines the **set of events that can be measured**, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".\n",
        "\n",
        "* Exkurs: Ergebnis vs Ereignis. Die m√∂glichen Ausg√§nge eines Zufallsexperimentes nennt man **Ergebnisse** (zB auf einem W√ºfel die Zahlen 1,2,3..). Wenn man alle m√∂glichen Ergebnisse eines Zufallsexperimentes in einer Menge zusammenfasst, erh√§lt man die **Ergebnismenge**. Sie wird √ºblicherweise mit dem Symbol Œ© (sprich Omega) bezeichnet. Beim W√ºrfeln ist Œ©= {1; 2; 3; 4; 5; 6} die Ergebnismenge. Jede Zusammenfassung von einem oder mehreren Ergebnissen eines Zufallsexperimentes in einer Menge nennt man **Ereignis** (zB auf einem W√ºrfel die Menge an geraden Zahlen {2,4,6} und ungeraden Zahlen {1,3,5}.\n",
        "\n",
        "* Eine Sigma-Algebra F ist ein System, um alle m√∂glichen **Ereignisse** (nicht Ergebnisse!) eines Zufallsexperiment zu beschreiben. Ereignisse sind an sich selbst Mengen, die man wie jede Menge vereinigen oder schneiden bzw. auch das Komplement bilden kann um so das Gegenereignis zu erhalten. Fasst man hier alle m√∂glichen Kombinationen an Ereignissen in einer Menge zusammen, bekommt man eine Menge, die wiederum Mengen als Elemente enth√§lt - eine Menge von Mengen sozusagen. Oft sagt man dazu auch einfach Mengensystem. Welche Eigenschaften ein Mengensystem genau haben muss, damit es eine Sigma-Algebra ist steht weiter unten.\n",
        "\n",
        "* Beispiel: Gl√ºcksrad mit blau, rot und gr√ºn. Dann haben wir folglich drei Ergebnisse, die wir auch abk√ºrzen k√∂nnen: Œ©={B,R,G}. Generell kann man sich schon merken: Œ© und ‚àÖ sind immer Elemente einer Sigma-Algebra. Daher haben wir hier 8 m√∂gliche Teilmengen von Œ©, die wir als Ereignis betrachten k√∂nnen und demnach als Menge in der Sigma-Algebra zusammenfassen (Potenzmenge von Omega): F ={‚àÖ, {B}, {R}, {G}, {B,R}, {B,G}, {R,G}, {B,R,G}} ([Source](https://www.massmatics.de/merkzettel/#!876:Ereignisraum_&_Sigma-Algebra)).\n",
        "\n",
        "* Bei diskreten Ergebnismengen kann man f√ºr die Sigma-Algebra immer die Potenzmenge P(Œ©)nehmen und hat demnach dann stets diesen **Ereignisraum: (Œ©,P(Œ©))**\n",
        "\n",
        "* Und f√ºr die reellen Zahlen gibt es die sogenannte **Borelsche Sigma-Algebra B**, die man dann auch in der Regel benutzt. Ist die Ergebnismenge Œ© eine Teilmenge der reellen Zahlen (oder ‚Ñù selbst), so nehmen wir die Borelsche-Sigma B und der Ereignisraum lautet (Œ©,B).\n",
        "\n",
        "* Wenn wir eine Sigma Algebra A gegeben haben, dann heisst jede Teilmenge in diesem Mengensystem (jedes Element aus dieser Sigma Algebra A) eine messbare Teilmenge (=die Mengen die wir messen wollen).\n",
        "\n",
        "* **<u>Die Elemente der Sigma Algebra sind die messbaren Teilmengen von unserer Grundmenge X</u>** (Und messbar ist der wesentliche Begriff). Das ist zB die Menge an vergangenen Trading-Events am Finanzmarkt bis zum Zeitpunkt t.\n",
        "\n",
        "* Sigma Algebra ist ein **Mengensystem von einer Teilmenge einer gegebenen Grundmenge** = der Raum, **den wir beschreiben wollen** (mit drei Eigenschaften). Die Menge einer Sigma-Algebra nennt man ‚Äû**messbare Teilmengen**‚Äú.\n",
        "\n",
        "**Eigenschaften**\n",
        "\n",
        "* **A $\\subseteq$ P(X) (=Potenzmenge) heisst Sigma Algebra, wenn gilt** (Die Mengen, die in dieser Sigma Algebra liegen, das sind jene, die folgende drei Eigenschaften erf√ºllen, und sind die, die wir messen wollen (=diesen Mengen wollen wir ein Mass zuordnen). Potenzmenge selbst soll eine Sigma Algebra sein. Sollten gewissen Eigenschaften der Potenzmenge fordern). **<u>A collection of subsets</u> A is called a œÉ-algebra on a set X if the following properties are met:**\n",
        "\n",
        "1. **A contains X (the set itself)**: $\\quad \\phi, X \\in A$ (Leere Menge (sollte L√§nge oder Volumen Null haben) und ganze Grundmenge selbst haben wir im Mengensystem / sollen messbar sein. Das ist was Sigma Algebra sagt). **Œ© ‚àà F (Ergebnismenge muss enthalten sein)**\n",
        "\n",
        "2. **If A contains a subset S, then A also contains the complement of S**: $A \\in A \\Rightarrow A^{c}:=X \\backslash A \\in A$ (Irgendein Element in der Algebra: dann sollte auch dessen Komplement im Mengensystem enthalten sein.) Hiermit ist auch Regel 1 eingeschlossen! Deswegen liegt auch die leere Menge (Gegenereignis von Œ©) in F.\n",
        "\n",
        "3. **Consider a countable collection of subsets. If each subset is included in A, then A must also contain their reunion.**: $A_{i} \\in A$ fur i $\\in N \\Rightarrow \\bigcup_{i=1}^{\\infty} A_{i} \\in A$ ((Letzter Punkt macht das Sigma aus): Abz√§hlbarkeit, abz√§hlbare Summe (A i‚Äòs aus unseren Mengensystem A): wir haben endlich viele bzw. abz√§hlbar viele, dann k√∂nnen wir die Vereinigung bilden / abziehbare Vereinigung. Die abz√§hlbare Vereinigung soll wieder in der Sigma Algebra liegen = Wenn wir L√§ngen haben, dann sollten wir die auch addieren k√∂nnen, auch wenn sich die Addition bis unendlich streckt! (blick auf messbarkeit))\n",
        "\n",
        "Having defined such a œÉ-algebra A, we call **the elements of œÉ-algebra A measurable sets** and the couple (X, A) a measurable space. An arbitrary set X can be a member of a multitude of œÉ-algebras. We denote the set of all œÉ-algebras that contain X with M(X). The **intersection of all those œÉ-algebras is called the œÉ-algebra generated by X**.\n",
        "\n",
        "**A œÉ-algebra (also œÉ-field) on a set X is a collection Œ£ of subsets of X that includes X itself, is closed under complement, and is closed under countable unions**. The definition implies that it also includes the empty subset and that it is closed under countable intersections. The pair (X, Œ£) is called a measurable space or Borel space. A œÉ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of finitely many subsets, which is a weaker condition.\n",
        "\n",
        "**Borel‚Äòsche Sigma-Algebra**\n",
        "\n",
        "* T ist ein topologischer Raum (oder ein metrischer Raum im engeren Sinn.) und X eine Menge darin. ‚ÄûOffene Mengen‚Äú.\n",
        "\n",
        "* Die Borel‚Äôsche Sigma Algebra auf topologischen Raum X ist jene kleinste Sigma Algebra, die von den offenen Mengen erzeugt wird.\n",
        "\n",
        "* B(X) := (T)\n",
        "\n",
        "**Measurable function**\n",
        "\n",
        "* **A set is measurable when it‚Äôs included in a œÉ-algebra.**\n",
        "\n",
        "* We can also extend the ‚Äúmeasurable‚Äù attribute to functions. Here‚Äôs how:\n",
        "\n",
        "* Let‚Äôs consider (X, A) and (Y, B) two measurable spaces. A function f from A to B is called measurable if every set from B comes from applying f to a set from A. Formally, we say that for any element S of B, the pre-image of S under the function f is in A.\n",
        "\n",
        "\n",
        "**Application**\n",
        "\n",
        "* The main use of œÉ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a œÉ-algebra.\n",
        "\n",
        "* This concept is important in mathematical analysis as the **foundation for Lebesgue integration**, and in probability theory, where it is **interpreted as the collection of events which can be assigned probabilities**.\n",
        "\n",
        "* Also, **in probability, œÉ-algebras are pivotal in the definition of conditional expectation**.\n",
        "\n",
        "* In statistics, (sub) œÉ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "1. **Minimum**: Sigma Algebra A enth√§lt leere Menge und Grundmenge selbst (kleinste Sigma Algebra die m√∂glich ist): A = {ùúô,X}\n",
        "2. **Maximum**: Sigma Algebra enth√§lt die Potenzmenge (beinhaltet alle Teilmengen von X): A = P(X)\n",
        "\n",
        "* If {A1, A2, A3, ‚Ä¶} is a countable partition of X then the **collection of all unions of sets in the partition** (including the empty set) is a œÉ-algebra.\n",
        "\n",
        "* A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the œÉ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. \"coarsest\") œÉ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZiS2oORjId"
      },
      "source": [
        "###### *Measure-theoretic probability theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-XQOB0Rk43"
      },
      "source": [
        "**Measure-theoretic probability theory**\n",
        "\n",
        "* The raison d'√™tre of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.\n",
        "\n",
        "* Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density.\n",
        "\n",
        "* The modern approach to probability theory solves these problems using measure theory to define the probability space:\n",
        "\n",
        "Given any set $\\Omega$ (also called sample space) and a $\\sigma$ -algebra $\\mathcal{F}$ on it, a measure $P$ defined on $\\mathcal{F}$ is\n",
        "called a probability measure if $P(\\Omega)=1$\n",
        "\n",
        "If $\\mathcal{F}$ is the Borel $\\sigma$ -algebra on the set of real numbers, then there is a unique probability measure on\n",
        "$\\mathcal{F}$ for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf.\n",
        "\n",
        "This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.\n",
        "\n",
        "The probability of a set $E$ in the $\\sigma$ -algebra $\\mathcal{F}$ is defined as\n",
        "\n",
        "$P(E)=\\int_{\\omega \\in E} \\mu_{F}(d \\omega)$\n",
        "\n",
        "where the integration is with respect to the measure $\\mu_{F}$ induced by $F$\n",
        "\n",
        "Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside R<sup>n</sup>, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.\n",
        "\n",
        "When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure.\n",
        "\n",
        "* Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes.\n",
        "\n",
        "* Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure.\n",
        "\n",
        "* If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Probability_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S4z_HeLRbtc"
      },
      "source": [
        "###### *Filtrations*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdKxs43TtLO"
      },
      "source": [
        "**Filtrations**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Filtrierung_(Wahrscheinlichkeitstheorie)\n",
        "\n",
        "* In martingale theory and the theory of stochastic processes, a **filtration is an increasing sequence of œÉ-algebras on a measurable space**.\n",
        "\n",
        "* That is, given a measurable space $(\\Omega, \\mathcal{F}),$ a filtration is a sequence of $\\sigma$ -algebras $\\left\\{\\mathcal{F}_{t}\\right\\}_{t \\geq 0}$ with $\\mathcal{F}_{t} \\subseteq \\mathcal{F}$ where each $t$ is a non-negative real number and\n",
        "\n",
        "> $t_{1} \\leq t_{2} \\Longrightarrow \\mathcal{F}_{t_{1}} \\subseteq \\mathcal{F}_{t_{2}}$\n",
        "\n",
        "* The exact range of the \"times\" $t$ will usually depend on context: the set of values for $t$ might be discrete or continuous, bounded or unbounded. For example,\n",
        "\n",
        "> $t \\in\\{0,1, \\ldots, N\\}, \\mathbb{N}_{0},[0, T]$ or $[0,+\\infty)$\n",
        "\n",
        "* **A œÉ-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".**\n",
        "\n",
        "* **Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information**.\n",
        "\n",
        "* A typical example is in mathematical finance, where a filtration represents the information available up to and including each time t, and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n",
        "\n",
        "A Filtration is a growing sequence of sigma algebras\n",
        "\n",
        "> $\\mathcal{F}_{1} \\subseteq \\mathcal{F}_{2} \\ldots \\subseteq \\mathcal{F}_{n}$\n",
        "\n",
        "When talking of martingales we need to talk of conditional expectations, and in particular conditional expectations w.r.t œÉ algebra's. So whenever we write\n",
        "\n",
        "> $E\\left[Y_{n} \\mid X_{1}, X_{2}, \\ldots, X_{n}\\right]$\n",
        "\n",
        "which can be written as\n",
        "\n",
        "> $E\\left[Y_{n+1} \\mid \\mathcal{F}_{n}\\right]$\n",
        "\n",
        "where Fùëõ is a sigma algebra that makes random variables\n",
        "\n",
        "> $X_{1}, \\ldots, X_{n}$\n",
        "\n",
        "measurable. Finally a flitration F1,‚Ä¶Fn is simply an increasing sequence of sigma algebras. That is **we are conditioning on growing amounts of information**.\n",
        "\n",
        "> **Der Begriff der Filtrierung ist unerl√§sslich, um, ausgehend vom Begriff des stochastischen Prozesses, wichtige Begriffe wie Martingale oder Stoppzeiten einzuf√ºhren.**\n",
        "\n",
        "* Als Menge $T$ wird wie bei stochastischen Prozessen meist $\\mathbb{R}_{+}$ oder $\\mathbb{N}_{0}$ gew√§hlt und $t \\in T$ als Zeitpunkt interpretiert.\n",
        "\n",
        "* **$\\sigma$ -Algebren modellieren verf√ºgbare Information**. Die Mengen der $\\sigma$ -Algebra $\\mathcal{F}_{t}$ geben zu jedem Zeitpunkt $t$ an, wie viele Informationen zur Zeit bekannt sind. F√ºr jedes Ereignis $A \\subseteq \\Omega$ bedeutet $A \\in \\mathcal{F}_{t}$ √ºbersetzt, dass zum Zeitpunkt $t$ die Frage $,$ ist $\\omega \\in A ?^{\\prime \\prime}$ eindeutig mit $,$ ja\" oder $,$ nein\" beantwortet werden kann.\n",
        "\n",
        "* Dass die Filtrierung stets aufsteigend geordnet ist, bedeutet demnach, **dass eine einmal erlangte Information nicht mehr verloren geht.**\n",
        "\n",
        "* Ist ein stochastischer Prozess $\\left(X_{t}\\right)_{t \\in T}$ an eine Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ adaptiert, bedeutet dies also, dass der Verlauf der Funktion $s \\mapsto X_{s}(\\omega)$ im Intervall $[0, t]$ zum Zeitpunkt $t$ (f√ºr beliebiges, aber unbekanntes $\\omega \\in \\Omega$ und in Hinsicht auf die durch Ereignisse $A \\in \\mathcal{F}_{s}, s \\in[0, t]$ formulierbaren Fragen bekannt ist.\n",
        "\n",
        "* Der Begriff wird aufgrund seiner Bedeutung in den meisten fortgeschrittenen Lehrb√ºchern √ºber stochastische Prozesse definiert. In einigen Lehrb√ºchern, zum Beispiel im Buch Probability von Albert N. Schirjajew, wird der Begriff aus didaktischen Gr√ºnden zun√§chst umfassend f√ºr Prozesse mit diskreten\n",
        "Werten in diskreter Zeit eingef√ºhrt.\n",
        "\n",
        "**Filtration in Finance**\n",
        "\n",
        "* In a multiperiod market, information about the market scenario is revealed in stages.\n",
        "\n",
        "* Some events may be completely determined by the end of the first trading period, others by the end of the second, and others not until the termination of all trading.\n",
        "\n",
        "* This suggests the following classification of events: for each t ‚â§ T ,\n",
        "\n",
        "(1) Ft = {all events determined in the first t trading periods}.\n",
        "\n",
        "* The finite sequence (Ft)0‚â§t‚â§T is a filtration of the space Œ© of market scenarios.\n",
        "\n",
        "* In general, a filtration of a set Œ© (not necessarily finite) is defined to be a collection Ft, indexed by a time parameter t (time may be either discrete or continuous), such that\n",
        "\n",
        "(a) each Ft is a œÉ‚àíalgebra of subsets (events) of Œ©; and\n",
        "\n",
        "(b) if s<t then Fs ‚äÜFt.\n",
        "\n",
        "**Beispiel**\n",
        "\n",
        "* Betrachtet man als Beispiel einen Wahrscheinlichkeitsraum $(\\mathbb{Z}, \\mathcal{P}(\\mathbb{Z}), P)$ mit abz√§hlbarer Grundmenge $\\mathbb{Z}$ die standardm√§√üig mit der Potenzmenge als $\\sigma$ -Algebra ausgestattet ist, so w√§re eine m√∂gliche Filtrierung beispielsweise\n",
        "\n",
        "> $\\mathcal{F}_{n}:=\\sigma(\\mathcal{P}(\\{-n, \\ldots, n\\}))$\n",
        "\n",
        "* Sie modelliert die Informationen, dass man bis zum n-ten Zeitschritt sich bis zu n Schritte vom Ursprung entfernt hat und w√§re beispielsweise die passende Filtrierung f√ºr einen einfachen symmetrischen Random\n",
        "Walk.\n",
        "\n",
        "**Filtration and Stochastic Processes**\n",
        "\n",
        "*  https://almostsure.wordpress.com/2009/11/08/filtrations-and-adapted-processes/\n",
        "\n",
        "* In mathematics, a filtration $\\mathcal{F}$ is an indexed family $\\left(S_{i}\\right)_{i \\in I}$ of subobjects of a given algebraic structure $S,$ with the index $i$ running over some totally ordered index set $I$, subject to the condition\n",
        "that\n",
        "\n",
        "> if $i \\leq j$ in $I,$ then $S_{i} \\subset S_{j}$\n",
        "\n",
        "* If the index i is the time parameter of some stochastic process, then the filtration can be interpreted as **representing all historical but not future information available about the stochastic process**, with the algebraic structure S<sub>i</sub> gaining in complexity with time.\n",
        "\n",
        "* Hence, a process that is adapted to a filtration F, is also called **non-anticipating**, i.e. one that cannot see into the future.\n",
        "\n",
        "* Eine Filtrierung (auch Filtration, Filterung oder Filtern) ist in der Theorie der stochastischen Prozesse eine Familie von verschachtelten œÉ-Algebren. Sie modelliert die zu verschiedenen Zeitpunkten verf√ºgbaren Informationen zum Verlauf eines Zufallsprozesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "###### *Martingale*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFV1b0SVIDD"
      },
      "source": [
        "**Martingale**\n",
        "\n",
        "* Let (Œ©,F,P) be a probability space and (Ft)0‚â§t‚â§T or (Ft)0‚â§t<‚àû a filtration by sub- œÉ‚àíalgebras of F. An adapted sequence Xt of integrable random variables is defined to be a\n",
        "\n",
        "  * martingale if E(Xt+1|Ft) = Xt ‚àÄt (=for all t).\n",
        "  * submartingale if E(Xt+1|Ft) ‚â• Xt ‚àÄt.\n",
        "  * supermartingale if E(Xt+1|Ft) ‚â§ Xt ‚àÄt.\n",
        "\n",
        "> A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it. If $\\mu$ is in fact a probability measure we speak of a probability space and call its\n",
        "underlying measurable space a sample space. The points of a sample space are called samples\n",
        "and represent potential outcomes while the measurable sets (complexes) are called events and\n",
        "represent properties of outcomes for which we wish to assign probabilities. (Many use the term\n",
        "sample space simply for the underlying set of a probability space, particularly in the case where\n",
        "every subset is an event.) Measure spaces and probability spaces play a foundational role in\n",
        "measure theory and probability theory respectively.\n",
        "\n",
        "* In probability theory, a martingale is a sequence of random variables (for example **a stochastic process**) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value.\n",
        "\n",
        "* A **martingale is characterized by the fact that it is fair on average**. Martingales arise naturally from the modeling of fair gambling.(In a fair game of chance, **the expected value of each win is zero**)\n",
        "\n",
        "* The closely related to the martingales are the super martingales, which are stochastic processes with an average loss and submartingales, which are stochastic processes with an average gain.\n",
        "\n",
        "* The property of being a (sub- / super-) martingale does not belong to stochastic processes alone, but always to a stochastic process **in combination with filtration**. Therefore, the filtration should always be specified.\n",
        "\n",
        "A **basic definition** of a discrete-time martingale is a discrete-time stochastic process (i.e., a sequence of random variables) X1, X2, X3, ... that satisfies for any time n,\n",
        "\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|X_{n}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(X_{n+1} \\mid X_{1}, \\ldots, X_{n}\\right)=X_{n}\n",
        "\\end{array}$\n",
        "\n",
        "That is, the conditional expected value of the next observation, given all the past observations, is equal to the most recent observation.\n",
        "\n",
        "A **continuous-time martingale** with respect to the stochastic process X<sub>t</sub> is a stochastic process Y<sub>t</sub> such that for all t\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|Y_{t}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(Y_{t} \\mid\\left\\{X_{\\tau}, \\tau \\leq s\\right\\}\\right)=Y_{s} \\quad \\forall s \\leq t\n",
        "\\end{array}$\n",
        "\n",
        "This expresses the property that the conditional expectation of an observation at time t, given all the observations up to time s, is equal to the observation at time s (of course, provided that s ‚â§ t). Note that the second property implies that Yn is measurable with respect to X1 ‚Ä¶ Xn.\n",
        "\n",
        "In **full generality**, a stochastic process Y : T √ó Œ© ‚Üí S taking value in a [Banach space](https://en.m.wikipedia.org/wiki/Banach_space) S is a martingale with respect to a filtration Œ£‚àó and probability measure P if\n",
        "\n",
        "**Examples of martingales**\n",
        "\n",
        "* An unbiased random walk (in any number of dimensions)\n",
        "\n",
        "* A Wiener process Wt is a martingale, and for a Wiener process the processes W<sub>t</sub><sup>2</sup> - t and the geometric Brownian movement without drift are martingales.\n",
        "\n",
        "* Stopped Brownian motion, which can be used to model the trajectory of such games\n",
        "\n",
        "* A gambler's fortune (capital) is a martingale if all the betting games which the gambler plays are fair. To be more specific: suppose Xn is a gambler's fortune after n tosses of a fair coin, where the gambler wins USD 1 if the coin comes up heads and loses USD 1 if it comes up tails. The gambler's conditional expected fortune after the next trial, given the history, is equal to their present fortune. This sequence is thus a martingale.\n",
        "\n",
        "* If { Nt : t ‚â• 0 } is a Poisson process with intensity Œª, then the compensated Poisson process { Nt ‚àí Œªt : t ‚â• 0 } is a continuous-time martingale with [right-continuous/left-limit](https://en.m.wikipedia.org/wiki/Classification_of_discontinuities) sample paths.\n",
        "\n",
        "* ([Likelihood-ratio testing](https://en.m.wikipedia.org/wiki/Likelihood-ratio_test) in statistics) A random variable X is thought to be distributed according either to probability density f or to a different probability density g. A random sample X1, ..., Xn is taken. Let Yn be the \"likelihood ratio\":\n",
        "\n",
        "> $Y_{n}=\\prod_{i=1}^{n} \\frac{g\\left(X_{i}\\right)}{f\\left(X_{i}\\right)}$\n",
        "\n",
        "If X is actually distributed according to the density f rather than according to g, then { Yn : n = 1, 2, 3, ... } is a martingale with respect to { Xn : n = 1, 2, 3, ... }.\n",
        "\n",
        "* [**Stopped Brownian Motion**](https://en.m.wikipedia.org/wiki/Stopped_process#Brownian_motion): a stopped process is a stochastic process that is forced to assume the same value after a prescribed (possibly random) time.\n",
        "\n",
        "Martingale Property vs Markov Property\n",
        "\n",
        "* In order to formally define the concept of Brownian motion and utilise it as a basis for an asset price model, it is necessary to define the Markov and Martingale properties. These provide an intuition as to how an asset price will behave over time.\n",
        "\n",
        "* The **Markov property** states that a stochastic process essentially has \"no memory\". This means that the conditional probability distribution of the future states of the process are independent of any previous state, with the exception of the current state.\n",
        "\n",
        "* The **Martingale property** states that the future expectation of a stochastic process is equal to the current value, given all known information about the prior events.\n",
        "\n",
        "* https://www.quantstart.com/articles/The-Markov-and-Martingale-Properties/\n",
        "\n",
        "**Simulate Martingale Process**: Toss a coin. toss results (1=lose 0=win). The first step is to find the edges of the losing runs, (steps + edges). You then need to take the difference of the sizes of the steps and shove those values back into the original data. When you take a cumsum of toss2 it gives you the current length of your losing streak. Your bet is then 2 ** cumsum(toss2)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Adapted (Stochastic) Process*"
      ],
      "metadata": {
        "id": "wc37qhDWywkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPqovDyR3ll"
      },
      "source": [
        "[Adapted (Stochastic) Process](https://de.m.wikipedia.org/wiki/Adaptierter_stochastischer_Prozess)\n",
        "\n",
        "* for exmaple in Finance\n",
        "\n",
        "* The share prices of assets in a multiperiod market depend on market scenarios, but evolve in such a way that their values at any time t, being observable at time t, do not depend on the unobservable post-t futures of the scenarios.\n",
        "\n",
        "* Thus, the price process St of a traded asset is **adapted to the natural filtration** (Ft)0‚â§t‚â§T defined by (1).\n",
        "\n",
        "* In general, a sequence Xt of random variables is said to be **adapted to a filtration** (Ft)0‚â§t‚â§T if, for each t, the random variable Xt is **Ft‚àímeasurable**, that is, if all events of the form {œâ : Xt(œâ) ‚àà B}, where **B is a Borel** subset of the real numbers R, are members of the œÉ‚àíalgebra Ft."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stopping Time*"
      ],
      "metadata": {
        "id": "_j3LHUI5y6Co"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWnh7gmSp_P"
      },
      "source": [
        "**Stopping Time**\n",
        "\n",
        "* [Stoppzeit (Markov Moment)](\n",
        "https://de.m.wikipedia.org/wiki/Stoppzeit): In der Stochastik bezeichnet der Begriff der Stoppzeit eine spezielle Art von Zufallsvariablen, die auf filtrierten Wahrscheinlichkeitsr√§umen definiert werden.\n",
        "\n",
        "  * Stoppzeiten sind nicht nur von Bedeutung f√ºr die Theorie der stochastischen Prozesse (beispielsweise bei der Lokalisierung von Prozessklassen oder Untersuchungen von gestoppten Prozessen), sondern auch von praktischer Relevanz, etwa f√ºr das Problem des optimalen Aus√ºbungszeitpunkts f√ºr amerikanische Optionen.\n",
        "\n",
        "  * Eine Stoppzeit kann man als die Wartezeit interpretieren, die vergeht, bis ein bestimmtes zuf√§lliges Ereignis eintritt. Wenn wie √ºblich die Filtrierung die vorhandene Information zu verschiedenen Zeitpunkten angibt, bedeutet die obige Bedingung also, dass zu jeder Zeit bekannt sein soll, ob dieses Ereignis bereits eingetreten ist oder nicht.\n",
        "\n",
        "* Optional Stopping Theorem: Das [Optional Stopping Theorem](https://de.m.wikipedia.org/wiki/Optional_Stopping_Theorem) ist ein mathematischer Satz √ºber Martingale, eine spezielle Klasse von stochastischen Prozessen, und damit der Wahrscheinlichkeitstheorie zuzuordnen.\n",
        "\n",
        "* [Optional Sampling Theorem](https://de.m.wikipedia.org/wiki/Optional_Sampling_Theorem): Eine popul√§re Version dieses Theorems besagt, dass es bei einem fairen, sich wiederholenden Spiel keine Abbruchstrategie gibt, mit der man seinen Gesamtgewinn verbessern kann.\n",
        "\n",
        "* [Starke Markoweigenschaft](https://de.m.wikipedia.org/wiki/Starke_Markoweigenschaft)\n",
        "\n",
        "* Filtrierung von Stoppzeiten:\n",
        "\n",
        "  * Eine Stoppzeit $\\tau: \\Omega \\rightarrow[0, \\infty]$ bez√ºglich einer beliebigen Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in[0, \\infty)}$ erzeugt in Analogie zur nat√ºrlichen Filtrierung eine $\\sigma$ -Algebra, die sogenannte $\\sigma$ -Algebra der $\\tau$ -Vergangenheit\n",
        "\n",
        "  > $\\mathcal{F}_{\\tau}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t}\\right\\} \\text { mit } \\mathcal{F}_{\\infty}=\\sigma\\left(\\bigcup_{t \\in[0, \\infty)} \\mathcal{F}_{t}\\right)$\n",
        "\n",
        "  * Sei nun $\\left(\\tau_{j}\\right)_{j \\in J}$ eine geordnete Familie von Stoppzeiten mit $P\\left(\\tau_{i} \\leq \\tau_{j}\\right)=1$ f√ºr alle $i, j \\in J$ mit $i \\leq j$ dann ist die Familie $\\left(\\mathcal{F}_{\\tau_{j}}\\right)_{j \\in J}$ eine Filtrierung, diese ist beim Studium von Stoppzeiten stochastischer Prozesse von Bedeutung.\n",
        "\n",
        "  * In Analogie erzeugt man die rechtsstetige Version der Filtrierung $\\left(\\mathcal{F}_{\\tau_{j}+}\\right)_{j \\in J}$ wobei:\n",
        "\n",
        "  > $\\mathcal{F}_{r+}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t+}\\right\\} \\text { und } \\mathcal{F}_{t+}=\\bigcap_{u \\in(t, \\infty)} \\mathcal{F}_{u}$\n",
        "\n",
        "  * Es gilt immer $\\mathcal{F}_{\\tau} \\subseteq \\mathcal{F}_{r+}$\n",
        "\n",
        "* [Vorhersagbarer Prozess](https://de.m.wikipedia.org/wiki/Vorhersagbarer_Prozess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *œÉ-Algebra der œÑ-Vergangenheit*"
      ],
      "metadata": {
        "id": "qCTiyeYsy0Y0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhsdp5wBSiAq"
      },
      "source": [
        "**œÉ-Algebra der œÑ-Vergangenheit**\n",
        "\n",
        "* Die [œÉ-Algebra der œÑ-Vergangenheit](https://de.m.wikipedia.org/wiki/Œ£-Algebra_der_œÑ-Vergangenheit) ist ein **Mengensystem**, sowie ein von der Stoppzeit abgeleitetes Konzept\n",
        "\n",
        "* Die œÉ-Algebra der œÑ-Vergangenheit ist eine **spezielle œÉ-Algebra**, welche √ºber die Filtrierung und die Stoppzeit definiert wird. Sie findet beispielsweise Anwendung bei der Definition der starken Markow-Eigenschaft und dem Optional Sampling Theorem.\n",
        "\n",
        "* Sie entsteht durch Kombination einer Filtrierung mit einer Stoppzeit und findet meist Anwendung bei Aussagen √ºber gestoppte Prozesse, also stochastische Prozesse, die an einem zuf√§lligen Zeitpunkt angehalten werden. Zu diesen Aussagen geh√∂ren beispielsweise das Optional Stopping Theorem, das Optional Sampling Theorem und die Definition der starken Markow-Eigenschaft.\n",
        "\n",
        "* Gegeben sei ein Wahrscheinlichkeitsraum $(\\Omega, \\mathcal{A}, P)$ sowie eine Filtrierung $\\mathbb{F}=\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ bez√ºglich der Ober- $\\sigma$ -Algebra $\\mathcal{A}$ und eine Stoppzeit $\\tau$ bez√ºglich $\\mathbb{F}$. Dann hei√üt\n",
        "\n",
        "$\\mathcal{F}_{\\tau}=\\left\\{A \\in \\mathcal{A} \\mid A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t} \\text { f√ºr alle } t \\in T\\right\\}$\n",
        "\n",
        "die $\\sigma$ -Algebra der $\\tau$ -Vergangenheit."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*(Vector) Norms for Regularization*"
      ],
      "metadata": {
        "id": "fk2nIlNKjMY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- List of 17 similarity metrics: https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681\n",
        "- Advantages and disadvantages of 9 distance metrics: https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "Mahalanobis distance is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables."
      ],
      "metadata": {
        "id": "qLE1qfquKDn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Regression*"
      ],
      "metadata": {
        "id": "F4uWOy1kiJIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms/notebook\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Residual_sum_of_squares"
      ],
      "metadata": {
        "id": "4x7F_UT4iLrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "h8VPlSUFiaHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "z7hExSBmilec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "2LHQ82bvinIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "CuQf_KldipMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "aPzEvAhTirT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "sCrxJgguiyCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "zym317e9i6qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "mTcFGaEmi8kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "fiBHTDOMi-iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "E7EvqMAjjAgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost, loss, risk or error function*"
      ],
      "metadata": {
        "id": "F1FnAXf0DeCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost, loss, risk or error function**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/cost.jpg)\n",
        "\n",
        "* The loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set.\n",
        "\n",
        "* Also: objective function, error, cost & loss function. A loss function measures the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).\n",
        "gradient of cost function tells each weight how to change to improve overall prediction\n",
        "MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.\n",
        "\n",
        "* We want to find the local minimum of the cost function\n",
        "\n",
        "*  Quadratic cost (mean squared error MSE):\n",
        "also maximum likelihood, and sum squared error.\n",
        "Most common. Used in regression.\n",
        "Mean squared error is appropriate to regression (line/curve fitting) where the goal is to minimize the mean squared error between the training set (points) and the fitted curve.\n",
        "\n",
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n",
        "\n",
        "* In most cases, our parametric model defines a distribution [‚Ä¶] and we simply use the **principle of maximum likelihood**. This means we use the cross-entropy between the training data and the model‚Äôs predictions as the cost function.\n",
        "\n",
        "* It is important, therefore, that the function faithfully represent our design goals. If we choose a poor error function and obtain unsatisfactory results, the fault is ours for badly specifying the goal of the search.\n",
        "\n",
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "* Maximum likelihood seeks to find the optimum values for the parameters by maximizing a likelihood function derived from the training data.\n",
        "\n",
        "* Given input, the model is trying to make predictions that **match the data distribution of the target variable**. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
        "\n",
        "* One way to interpret maximum likelihood estimation is to view it as **minimizing the dissimilarity** between the empirical distribution [‚Ä¶] defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. [‚Ä¶] **Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions**.\n",
        "\n",
        "* Under appropriate conditions, the maximum likelihood estimator has the **property of consistency** [‚Ä¶], meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter.\n",
        "\n",
        "* Under the framework maximum likelihood, the error between two probability distributions is measured using cross-entropy. Under maximum likelihood estimation, we would seek a set of model weights that minimize the difference between the model‚Äôs predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
        "\n",
        "When using the framework of maximum likelihood estimation, we will implement a cross-entropy loss function, which often in practice means:\n",
        "* a **cross-entropy** loss function for classification problems and\n",
        "* a **mean squared error** loss function for regression problems.\n",
        "\n",
        "* Under the framework of maximum likelihood estimation and assuming a **Gaussian distribution for the target variable**, mean squared error can be considered the cross-entropy between the distribution of the model predictions and the distribution of the target variable.\n",
        "\n",
        "* Many authors use the term ‚Äúcross-entropy‚Äù to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer.\n",
        "\n",
        "* Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
        "\n",
        "* For example, **mean squared error is the cross-entropy between the empirical distribution and a Gaussian model**\n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "* https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "* https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "* https://allenkunle.me/deriving-ml-cost-functions-part1\n",
        "\n",
        "\n",
        "* Properties of ideal Cost functions:\n",
        "  * smooth,\n",
        "  * continuous,\n",
        "  * symmetric (but i.e. Non-symmetric losses: e.g., for spam classification)\n",
        "  * differentiable\n",
        "\n",
        "**Similarity learning** is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, identity of indiscernibles, symmetry and subadditivity (or the triangle inequality). **In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.**\n",
        "\n",
        "> $\\min _{W}\\left\\{L(W):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(W ; x_{i}, y_{i}\\right)+\\lambda r(W)\\right\\}$\n",
        "\n",
        "**Similarity Learning & Distance Metric Learning**\n",
        "\n",
        "* √Ñhnlichkeitsma√üe werden f√ºr nominal oder ordinal skalierte Variablen genutzt\n",
        "\n",
        "* Distanzma√üe werden f√ºr metrisch skalierte Variablen (d. h. f√ºr Intervall- und Verh√§ltnisskala) genutzt.\n",
        "\n",
        "Complete list of [Loss / Cost Functions in TF](https://www.tensorflow.org/api_docs/python/tf/keras/losses/)"
      ],
      "metadata": {
        "id": "w3-c_-luhCKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Regression & Forecasting (mostly distance-based)*"
      ],
      "metadata": {
        "id": "DjYXE5LZhFXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions that belong to the category \"distance-based\" are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.\n",
        "\n",
        "> Great overview: http://juliaml.github.io/LossFunctions.jl/stable/losses/distance/\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/regression_loss.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm_tzCTnhHNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Linear) Least Squares**\n",
        "\n",
        "* Least Squares: Deren Parameter werden so bestimmt, dass die Summe der Abweichungsquadrate e der Beobachtungen y von den Werten der Funktion minimiert wird.\n",
        "\n",
        "* Da die Kleinste-Quadrate-Sch√§tzung die Residuenquadratsumme minimiert, ist es dasjenige Sch√§tzverfahren, welches das [Bestimmtheitsma√ü](https://de.wikipedia.org/wiki/Bestimmtheitsma√ü) maximiert.\n",
        "\n",
        "* Das Bestimmtheitsma√ü der Regression, auch empirisches Bestimmtheitsma√ü, ist eine dimensionslose Ma√üzahl die den Anteil der Variabilit√§t in den Messwerten der abh√§ngigen Variablen ausdr√ºckt, der durch das lineare Modell ‚Äûerkl√§rt‚Äú wird. Mithilfe dieser Definition k√∂nnen die Extremwerte f√ºr das Bestimmtheitsma√ü aufgezeigt werden. F√ºr das\n",
        "Bestimmtheitsma√ü gilt, dass es umso nƒÉher am Wert 1 ist, je kleiner die Residuenquadratsumme ist. Es wird maximal gleich 1 wenn $\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=0$ ist, also alle Residuen null sind. In diesem Fall ist die Anpassung an die Daten perfekt, was bedeutet, dass f√ºr jede Beobachtung $y_{i}=\\hat{y}_{i}$ ist.\n",
        "\n",
        "* [Least Squares](https://en.wikipedia.org/wiki/Least_squares) / [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) & [Linear Least Squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
      ],
      "metadata": {
        "id": "cNJH7IHehJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gauss‚ÄìMarkov theorem (BLUE)**\n",
        "\n",
        "*  states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, **if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero**.\n",
        "\n",
        "* stellt eine theoretische Rechtfertigung der Methode der kleinsten Quadrate dar\n",
        "\n",
        "* Der Satz besagt, dass in einem linearen Regressionsmodell, in dem die **St√∂rgr√∂√üen (error term) einen Erwartungswert von null und eine konstante Varianz haben sowie unkorreliert sind** (Annahmen des klassischen linearen Regressionsmodells), der Kleinste-Quadrate-Sch√§tzer ‚Äì vorausgesetzt er existiert ‚Äì ein bester linearer erwartungstreuer Sch√§tzer ist (englisch Best Linear Unbiased Estimator, kurz: BLUE).\n",
        "\n",
        "* Hierbei bedeutet der ‚Äûbeste‚Äú, dass er ‚Äì innerhalb der Klasse der linearen erwartungstreuen Sch√§tzer ‚Äì die ‚Äûkleinste‚Äú Kovarianzmatrix aufweist und somit minimalvariant ist. Die St√∂rgr√∂√üen m√ºssen nicht notwendigerweise normalverteilt sein. Sie m√ºssen im Fall der verallgemeinerten Kleinste-Quadrate-Sch√§tzung auch nicht unabh√§ngig und identisch verteilt sein.\n",
        "\n",
        "The Gauss-Markov assumptions concern the set of error random variables, $\\varepsilon_{i}:$\n",
        "\n",
        "1. They have mean zero: $\\mathrm{E}\\left[\\varepsilon_{i}\\right]=0$\n",
        "\n",
        "2. They are homoscedastic, that is all have the same finite variance: $\\operatorname{Var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}<\\infty$ for all $i$,\n",
        "3. Distinct error terms are uncorrelated: $\\operatorname{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0, \\forall i \\neq j$.\n",
        "\n",
        "A linear estimator of $\\beta_{j}$ is a linear combination $\\widehat{\\beta}_{j}=c_{1 j} y_{1}+\\cdots+c_{n j} y_{n}$\n",
        "\n",
        "* The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n",
        "\n",
        "* The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the [James‚ÄìStein estimator](https://en.wikipedia.org/wiki/James‚ÄìStein_estimator) (which also drops linearity), [ridge regression(Tikhonov_regularization)](https://en.wikipedia.org/wiki/Tikhonov_regularization), or simply any [degenerate estimator](https://en.wikipedia.org/wiki/Degenerate_distribution).\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem"
      ],
      "metadata": {
        "id": "wqLJa7izhK3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "* Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.\n",
        "\n",
        "* ‚ÄúOrdinary Least Squares‚Äù (OLS) method is used to find the best line intercept (b) and the slope (m). [in y = mx + b, m is the slope and b the intercept]\n",
        "\n",
        "\n",
        "> $m=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
        "\n",
        "> $b=\\bar{y}-m * \\bar{x}$\n",
        "\n",
        "* In other words ‚Üí with OLS Linear Regression the goal is to find the line (or hyperplane) that minimizes the vertical offsets. We define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
        "\n",
        "* OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
        "\n",
        "* The OLS method minimizes the sum of squared residuals, and leads to a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression) for the estimated value of the unknown parameter vector Œ≤.\n",
        "\n",
        "* It is important to point out though that OLS method will work for a univariate dataset (ie., single independent variables and single dependent variables). Multivariate dataset contains a single independent variables set and multiple dependent variables sets, requiring a machine learning algorithm called ‚ÄúGradient Descent‚Äù.\n",
        "\n",
        "* [Wiki](https://en.wikipedia.org/wiki/Ordinary_least_squares) & [Medium](https://medium.com/@jorgesleonel/linear-regression-307937441a8b)"
      ],
      "metadata": {
        "id": "YvnQ8gFphMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Least Squares (WLS)**\n",
        "\n",
        "* are used when heteroscedasticity is present in the error terms of the model.\n",
        "* https://en.wikipedia.org/wiki/Weighted_least_squares"
      ],
      "metadata": {
        "id": "0xWob81xhO3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized Least Squares (GLS)**\n",
        "\n",
        "* is an extension of the OLS method, that **allows efficient estimation of Œ≤ when either heteroscedasticity, or correlations, or both are present among the error terms of the model**, as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
        "\n",
        "* To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ith case is inversely proportional to var(Œµi). This special case of GLS is called \"weighted least squares\".\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Generalized_least_squares"
      ],
      "metadata": {
        "id": "L1U8vBsUhRcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE, SAE & SSE**\n",
        "\n",
        "**Sum of Errors (SE)** the difference in the predicted value and the actual value.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma(\\hat{Y}-Y)$\n",
        "\n",
        "Errors terms cancel each other out.\n",
        "\n",
        "**Sum of Absolute Errors (SAE)** takes the absolute values of the errors for all iterations.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma (|\\hat{Y}-Y|)$\n",
        "\n",
        "This loss function is not differentiable at 0.\n",
        "\n",
        "**Sum of Squared Errors (SSE)** is differentiable at all points and gives non-negative errors. But you could argue that why cannot we go for higher orders like 4th order or so. Then what if we consider to take 4th order loss function, which would look like:\n",
        "\n",
        "$\\mathbf{L}=\\left[\\Sigma(\\hat{Y}-Y)^{2}\\right]$\n",
        "\n",
        "The gradient of the loss function will vanish at minima & maxima. And the error will grow with the sample size.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/sumoferrors.png)\n",
        "\n",
        "* Minimizing Sum of Squared Errors / SSE ([wiki](https://de.m.wikipedia.org/wiki/Residuenquadratsumme) and [medium](https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511)).  We can think of the SSE loss as the (unscaled) variance of the model errors.\n",
        "* Therefore **minimizing the SEE loss is equivalent to minimizing the variance of the model residuals**. For this reason, the sum of squares loss is often referred to as the Residual Sum of Squares error (RSS) for linear models. We can think of minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.\n",
        "* Ideal when distribution of residuals in normal: the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem) states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the [best unbiased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) for the parameters .\n",
        "* Problem: Because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss\n"
      ],
      "metadata": {
        "id": "QegFiqDEhTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE (L2) & RMSE (Squared Euclidean Distance)**\n",
        "\n",
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis.\n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/3d-function-2.svg/566px-3d-function-2.svg.png)\n",
        "\n",
        "*A paraboloid, the graph of squared Euclidean distance from the origin*\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/3d-function-5.svg/566px-3d-function-5.svg.png)\n",
        "\n",
        "*A cone, the graph of Euclidean distance from the origin in the plane*"
      ],
      "metadata": {
        "id": "nPIA8NHThWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "$\\mathrm{MSE}={\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Mean Squared Error (L2 or Quadratic Loss). Error decreases as we increase our sample data as the distribution of our data becomes more and more narrower (referring to normal distribution). The more data we have, the less is the error.\n",
        "* Can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better. It is always non ‚Äì negative and values close to zero are better. The MSE is the second moment of the error (about the origin) and thus incorporates both the variance of the estimator and its bias.\n",
        "* Problem: Sensitive to outliers and the order of loss is more than that of the data. As my data is of order 1 and the loss function, MSE has an order of 2 (squared). So we cannot directly correlate data with the error.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate)\n",
        "\n",
        "**Mean Squared Logarithmic Error (MSLR)**\n",
        "\n",
        "* Mean Squared Logarithmic Error\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\n",
        "\n",
        "**RMSE** (Root-Mean-Square Error)\n",
        "\n",
        "$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Root-Mean-Square Error is the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
        "* The **RMSE is directly interpretable in terms of measurement units**, and so is a better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit. Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable\n",
        "* https://www.sciencedirect.com/science/article/pii/S096014811831231X\n",
        "* The **RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian**.\n",
        "https://www.geosci-model-dev-discuss.net/7/C473/2014/gmdd-7-C473-2014-supplement.pdf\n",
        "* When both metrics are calculated, the MAE tends to be much smaller than the RMSE because the RMSE penalizes large errors while the MAE gives the same weight to all errors.\n",
        "* They summarized that the **RMSE tends to become increasingly larger than the MAE** (but not necessarily in a monotonic fashion) as the distribution of error magnitudes becomes more variable. The RMSE tends to 1 grow larger than the MAE with n2 since its lower limit is fixed at the MAE and its upper 11 limit (n2 ¬∑ MAE) increases with n2 .\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Root-mean-square_deviation) & [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)"
      ],
      "metadata": {
        "id": "VTqsbCowhYav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE (L1) & MAPE**\n",
        "\n",
        "$\\mathrm{MAE}=\\frac{1}{n} \\sum_{j=1}^{n}\\left|y_{j}-\\hat{y}_{j}\\right|$\n",
        "\n",
        "* If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. MBE can convey useful information, but should be interpreted cautiously because positive and negative errors will cancel out.\n",
        "\n",
        "* Mean Absolute Error (L1 Loss)\n",
        "* Computes the mean of absolute difference between labels and predictions\n",
        "* measures the average magnitude of the errors in a set of predictions, without considering their direction. It‚Äôs the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "* On some regression problems, the **distribution of the target variable may be mostly Gaussian, but may have outliers**, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "* Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Extremwerte als Ausrei√üer mit geringerem Einfluss auf das Modell ansehen: MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously receive unrealistically huge negative/positive values in our training environment, but not our testing environment).\n"
      ],
      "metadata": {
        "id": "eX6fU9qohaM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE vs MSE**\n",
        "\n",
        "* One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values.\n",
        "\n",
        "* This isn‚Äôt good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate.\n",
        "\n",
        "* The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/mae_vs_mse.PNG)"
      ],
      "metadata": {
        "id": "F71Ij_KchcFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "$\\mathrm{M}=\\frac{1}{n} \\sum_{t=1}^{n}\\left|\\frac{A_{t}-F_{t}}{A_{t}}\\right|$\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is a statistical measure of **how accurate a forecast** system is.\n",
        "\n",
        "* It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values. Where At is the actual value and Ft is the forecast value.\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is the most common measure used to forecast error, and works best if there are no extremes to the data (and no zeros).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "MZzrxESQheD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
        "\n",
        "* There are 3 different definitions of sMAPE. Two of them are below:\n",
        "\n",
        "$\\operatorname{SMAPE}=\\frac{100 \\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(\\left|A_{t}\\right|+\\left|F_{t}\\right|\\right) / 2}$\n",
        "\n",
        "* Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
        "\n",
        "* At is the actual value and Ft is the forecast value\n",
        "\n",
        "* The absolute‚ÄÖdifference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n",
        "\n",
        "* Armstrong's original definition is as follows:\n",
        "\n",
        "$\\mathrm{SMAPE (old)}=\\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(A_{t}+F_{t}\\right) / 2}$\n",
        "\n",
        "* The problem is that it can be negative (if ${\\displaystyle A_{t}+F_{t}<0}$) or even undefined (if ${\\displaystyle A_{t}+F_{t}=0}$). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator.\n",
        "\n",
        "* In contrast to the mean‚ÄÖabsolute‚ÄÖpercentage‚ÄÖerror, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator)\n",
        "\n",
        "* One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the second SMAPE formula:\n",
        "\n",
        "  * Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76%\n",
        "\n",
        "  * Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) & [Wiki2](https://wiki2.org/en/Symmetric_mean_absolute_percentage_error) & [other](https://www.brightworkresearch.com/the-problem-with-using-smape-for-forecast-error-measurement/)"
      ],
      "metadata": {
        "id": "QsaM9Z0whfuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean absolute scaled error (MASE)**\n",
        "\n",
        "* mean absolute scaled error (MASE) is a measure of the accuracy of forecasts.\n",
        "\n",
        "*  It is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast. It was proposed in 2005.\n",
        "\n",
        "* The mean absolute scaled error has the following desirable propertie: [Wiki](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error)"
      ],
      "metadata": {
        "id": "6HSFPTdVhhon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss (Smooth Mean Absolute Error)**\n",
        "\n",
        "* TLDR: will better find a minimum than L1, but less exposed to outliers than L2. However one has to tune the hyperparameter delta. The larger (3+), the more it is L2, the smaller (1), the more it is L1.\n",
        "\n",
        "* The Huber loss **combines the best properties of MSE and MAE** (Mean Absolute Error). It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter.\n",
        "\n",
        "* It's **less sensitive to outliers** in data than the squared error loss. It‚Äôs **also differentiable at 0**. It‚Äôs basically absolute error, which becomes quadratic when error is small.  How small that error has to be to make it quadratic depends on a hyperparameter ùõø.\n",
        "\n",
        "* Once differentiable.\n",
        "\n",
        "$L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
        "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
        "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
        "\\end{array}\\right.$\n",
        "\n",
        "* **Huber loss approaches MSE when ùõø ~ 0 and MAE when ùõø ~ ‚àû**\n",
        "\n",
        "* The choice of delta is critical because it determines what you‚Äôre willing to consider as an outlier. Residuals larger than delta are minimized with L1 (which is less sensitive to large outliers), while residuals smaller than delta are minimized ‚Äúappropriately‚Äù with L2.\n",
        "\n",
        "* One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n",
        "Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it‚Äôs more robust to outliers than MSE. Therefore, **it combines good properties from both MSE and MAE**.\n",
        "\n",
        "* However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Huber_loss) * [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
        "\n",
        "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
        "\n",
        "* The biggest problem with using MAE to train neural networks is the constant large gradient, which may cause the minimum point to be missed when the gradient descent is about to end. For MSE, the gradient will decrease as the loss decreases, making the result more accurate.\n",
        "\n",
        "* In this case, Huber loss is very useful. It will fall near the minimum value due to the decreasing gradient. It is more robust to outliers than MSE. Therefore, Huber loss combines the advantages of MSE and MAE. However, the problem with Huber loss is that we may need to constantly adjust the hyperparameters\n",
        "\n",
        "* https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "* When you compare this statement with the benefits and disbenefits of both the MAE and the MSE, you‚Äôll gain some insights about how to adapt this delta parameter:\n",
        "\n",
        "* **If your dataset contains large outliers**, it‚Äôs likely that your model will not be able to predict them correctly at once. In fact, it might take quite some time for it to recognize these, if it can do so at all. This results in large errors between predicted values and actual targets, because they‚Äôre outliers. Since MSE squares errors, large outliers will distort your loss value significantly. If outliers are present, you likely don‚Äôt want to use MSE. Huber loss will still be useful, but you‚Äôll have to use small values for ùõø.\n",
        "\n",
        "* If it does not contain many outliers, it‚Äôs likely that it will generate quite accurate predictions from the start ‚Äì or at least, from some epochs after starting the training process. In this case, you may observe that the errors are very small overall. Then, one can argue, it may be worthwhile to let the largest small errors contribute more significantly to the error than the smaller ones. In this case, MSE is actually useful; hence, with Huber loss, you‚Äôll likely want to use quite large values for ùõø.\n",
        "\n",
        "* If you don‚Äôt know, you can always start somewhere in between ‚Äì for example, in the plot above, ùõø = 1 represented MAE quite accurately, while ùõø = 3 tends to go towards MSE already. What if you used ùõø = 1.5 instead? You may benefit from both worlds.\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/\n",
        "\n",
        "* For target = 0, the loss increases when the error increases. However, the speed with which it increases depends on this ùõø value. In fact, Grover (2019) writes about this as follows: Huber loss approaches MAE when ùõø ~ 0 and MSE when ùõø ~ ‚àû (large numbers.)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/320px-Huber_loss.svg.png)\n",
        "\n",
        "*Huber loss (green,\n",
        "Œ¥\n",
        "=\n",
        "1) and squared error loss (blue) as a function of\n",
        "y\n",
        "‚àí\n",
        "f\n",
        "(\n",
        "x\n",
        ")*\n",
        "\n",
        "![huber](https://raw.githubusercontent.com/deltorobarba/repo/master/huberloss.jpg)"
      ],
      "metadata": {
        "id": "ZBE611qIhjXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-Cosh Loss**\n",
        "\n",
        "* TLDR: Similar to MAE, will not be affected by outliers. Log-Cosh has all the points of Huber loss, and no need to set hyperparameters. Compared with Huber, Log-Cosh derivation is more complicated, requires more computation, and is not used much in deep learning.\n",
        "\n",
        "* * Log-cosh is another function used in regression tasks that‚Äôs smoother than L2 (is smoothed towards large errors (presumably caused by outliers) so that the final error score isn‚Äôt impacted thoroughly.)\n",
        "* Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. ‚ÄúLog-cosh is the logarithm of the hyperbolic cosine of the prediction error.‚Äù (Grover, 2019). Oops, that‚Äôs not intuitive but nevertheless quite important ‚Äì this is the maths behind Logcosh loss:\n",
        "\n",
        "> $\\log \\cosh (t)=\\sum_{p \\in P} \\log (\\cosh (p-t))$\n",
        "\n",
        "* Similar to Huber Loss, but twice differentiable everywhere\n",
        "* [Wiki Hyperbolic Functions](https://en.m.wikipedia.org/wiki/Hyperbolic_functions), [TF Class](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh), [Machinecurve](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "* However, Log-Cosh is second-order differentiable everywhere, which is still very useful in some machine learning models. For example, XGBoost uses Newton's method to find the best advantage. Newton's method requires solving the second derivative (Hessian). Therefore, for machine learning frameworks such as XGBoost, the second order of the loss function is differentiable. But the Log-cosh loss is not perfect, and there are still some problems. For example, if the error is large, the first step and Hessian will become fixed, which leads to the lack of split points in XGBoost.\n",
        "\n",
        "https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "![logcosh](https://raw.githubusercontent.com/deltorobarba/repo/master/logcosh.jpeg)"
      ],
      "metadata": {
        "id": "vP9h6RbchlyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile Loss (Pinball Loss)**\n",
        "\n",
        "* TLDR: for Heteroskedastizit√§t, i.e. for risk management when variance changes. Quantile Loss, you can set different quantiles to control the proportion of overestimation and underestimation in loss.\n",
        "\n",
        "* estimates conditional ‚Äúquantile‚Äù of a response variable given certain values of predictor variables\n",
        "* is an extension of MAE (**when quantile is 50th percentile, it‚Äôs MAE**)\n",
        "* Im Gegensatz zur Kleinste-Quadrate-Sch√§tzung, die den Erwartungswert der Zielgr√∂√üe sch√§tzt, ist die Quantilsregression dazu geeignet, ihre Quantile zu sch√§tzen.\n",
        "* Fitting models for many percentiles, you can estimate the entire conditional distribution. Often, the answers to important questions are found by modeling percentiles in the tails of the distribution. For that reason **quantile regression provides critical insights in financial risk management & fraud detection**.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Quantilsregression), [TF Class](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/PinballLoss) & [TF Function](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/pinball_loss)\n",
        "\n",
        "* project where predictions were subject to high uncertainty. The client required for their decision to be driven by both the predicted machine learning output and a measure of the potential prediction error. The quantile regression loss function solves this and similar problems by replacing a single value prediction by prediction intervals.\n",
        "\n",
        "* The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
        "\n",
        "* For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.\n",
        "\n",
        "* The usual regression algorithm is to fit the expected or median training data, and the quantile loss function can be used to fit different quantiles of training data by giving different quantiles.\n",
        "\n",
        "![sdd](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss.jpg)\n",
        "\n",
        "* Set different quantiles to fit different straight lines: This function is a piecewise functio., Œ≥ is the quantile coefficient. y is the true value, f(x) is the predicted value. According to the size of the predicted value and the true value, there are two cases to consider.\n",
        "\n",
        "* y> f(x) For overestimation, the predicted value is greater than the true value;\n",
        "\n",
        "* y< f(x) to underestimate, the predicted value is smaller than the real value.\n",
        "\n",
        "* Use different pass coefficients to control the weight of overestimation and underestimation in the entire loss value.\n",
        "\n",
        "* Especially when Œ≥=0.5 When the quantile loss degenerates into the mean absolute error MAE, **MAE can also be regarded as a special case of quantile loss-median loss**. The picture below is taken with different median points [0.25,0.5,0.7] Obtaining different quantile loss function curves can also be seen as MAE at 0.5.\n",
        "\n",
        "![fgfgf](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss2.jpg)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Pinball_Loss_Function.svg/320px-Pinball_Loss_Function.svg.png)\n",
        "\n",
        "*Pinball-Verlustfunktion mit\n",
        "œÑ\n",
        "=0,9. F√ºr\n",
        "Œµ\n",
        "<\n",
        "0 betr√§gt der Fehler\n",
        "‚àí\n",
        "0\n",
        ",\n",
        "1\n",
        "Œµ, f√ºr\n",
        "Œµ\n",
        "‚â•\n",
        "0 betr√§gt er\n",
        "0\n",
        ",\n",
        "9\n",
        "Œµ.*\n",
        "\n",
        "* https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/"
      ],
      "metadata": {
        "id": "geqM6xx5hnqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson Loss**\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-103abfddc312\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459\n"
      ],
      "metadata": {
        "id": "-qbix6xwhpXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "UvgTnPqOhs3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "xZOPqB8Bhuad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "6CYGi1_hhwM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "nhpMRXmPhyIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "> In machine learning, many optimization problems formulated as minimization problems wrt Kullback-Leibler divergence.\n",
        "interpreted as *information projections*, uniqueness projections proved by a generalization of the Pythagoras' theorem.\n",
        "PDF: https://Inkd.in/g9ETtTQp\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "iM9yRTVah0Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "_cMWr-Beh3wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "0kz2AVcBh5at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "R2dM3lxKh7QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "vlb98XbTh9Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "5yivnYJoh-uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Regularization*"
      ],
      "metadata": {
        "id": "G1TQJG1ufr-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*An L2-regularized version of the cost function used in SGD for NN [Source](https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db)*\n",
        "\n",
        "> $J_{\\text {regularited }}=\\underbrace{-\\frac{1}{m} \\sum_{i=1}^m\\left(y^{(i)} \\log \\left(a^{[L](i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-a^{[L](i)}\\right)\\right)}_{\\text {crossentropy cost }}+\\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum_l \\sum_l \\sum_j W_{i, j}^{[m 2}}_{\\text {L. reguatization cos }}$\n"
      ],
      "metadata": {
        "id": "OCuAdZfBf4BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "* Regularization is a technique for preventing a model from overfitting\n",
        "* Overfitting = a complicated model that gives worse predictions than a simpler model\n",
        "* Solution: e.g. preventing over-fitting by penalizing a model for having large weights (A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output)\n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called [weight regularization](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/).\n",
        "\n",
        "**Trivial Regularization Approaches**\n",
        "\n",
        "* Add more data\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data)\n",
        "* Use ensemble models\n",
        "\n",
        "**Particular Regularization Techniques**\n",
        "* Weight regularization\n",
        "* Vectornorm (L1, L2, or Elastic Net): Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.\n",
        "* Dropout\n",
        "* Jitter (add noise)\n",
        "* Batch size\n",
        "* Early stopping (this is not a formal regularization method, but can effectively limit overfitting).\n",
        "\n",
        "**Overfitting: Consider Variance-Bias-Tradeoff**: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)\n",
        "\n",
        "**Benefits of regularization from a mathematical optimization point of view**\n",
        "\n",
        "* Minimize a cost function. Neural networks are non-convex cost functions. Numerical optimization methods (gradient descent) can easily get stuck in local minima (stationary points)\n",
        "\n",
        "* Regularization can be used as a way of ‚Äöconvexifying‚Äò a non-convex cost function.\n",
        "\n",
        "* The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible.\n",
        "\n",
        "**Theoretical Foundation**\n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ],
      "metadata": {
        "id": "cjZzhH2ef5uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 (Lasso) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds ‚Äúabsolute value of magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
        "* Learn more on [Google Course](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda): Regularization for Simplicity: Lambda\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "\n",
        "$d_{1} \\equiv d_{\\mathrm{SAD}}:(x, y) \\mapsto\\|x-y\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|$\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature‚Äôs coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn‚Äôt easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**:\n",
        "  * if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero.\n",
        "  * very useful when you want to understand exactly which features are contributing to a decision.\n",
        "  * if you can ignore the ouliers in your dataset or you need them to be there.\n",
        "  * use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features¬† at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUERgs6Hf8C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 (Ridge) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds ‚Äúsquared magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it‚Äôs important how lambda is chosen.\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply ‚Äúweight decay,‚Äù with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn‚Äôt give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge.\n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ],
      "metadata": {
        "id": "sfAtZ0ekf-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Analytical Equivalence**\n",
        "\n",
        "* Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "* https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n",
        "\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed Œª2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+Œª2)."
      ],
      "metadata": {
        "id": "Dnq1CBbfgAFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "\n",
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(k√ºnstliches_neuronales_Netz)\\\n",
        "\n",
        "**Is dropout outdated?**\n",
        "\n",
        "Neural Network: ¬†Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don‚Äôt Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance.\n",
        "\n",
        "**Use Dropout along with L1/L2 Regularization?**\n",
        "\n",
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ‚Ñì 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of ‚Äúswitching off‚Äù some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network‚Äôs over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can ‚Äúwork together‚Äù or rather end up ‚Äúfighting each other‚Äù. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014)"
      ],
      "metadata": {
        "id": "dC-z_kHhgB8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jitter (Noise)**\n",
        "\n",
        "* adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise"
      ],
      "metadata": {
        "id": "DW1FugwRgDxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization & Size**\n",
        "\n",
        "* https://towardsdatascience.com/understanding-batch-normalization-for-neural-networks-1cd269786fa6\n",
        "\n",
        "* Small batches can oÔ¨Äer a regularizing eÔ¨Äect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process.\n",
        "\n",
        "* Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)\n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        "* Batch normalization is another method to regularize a (convolutional) network.\n",
        "* On top of a regularizing effect, batch normalization also gives your convolutional network a resistance to vanishing gradient during training. This can decrease training time and result in better performance.\n",
        "* Batch Normalization Combats Vanishing Gradient\n",
        "* Batch normalization replaces¬†dropout.\n",
        "* Even if you don‚Äôt need to worry about overfitting there are many benefits to implementing batch normalization. Because of this, and its regularizing effect, batch normalization has largely replaced dropout in modern convolutional architectures.\n",
        "* ‚ÄúWe presented an algorithm for constructing, training, and performing inference with batch-normalized networks. The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.‚Äù -[Ioffe and Svegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "Why use batches?\n",
        "To avoid that small datasets increase overfitting to this datasets and worsen overall accuracy. But batch size shouldnt be too big either (computation time, speed of convergence of an algorithm)\n",
        "\n",
        "* Research 1: a low batch size means a very noisy gradient (because computed on a very small subset of the dataset), and a high learning rate means noisy steps.\n",
        "https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914\n",
        "\n",
        "* Research 2: How do you choose your batch size in deep learning/SGD?¬†- An interesting concept so-called \"generalization gap\": Train longer, generalize better: closing the generalization gap in large batch training of neural networks:\n",
        "https://arxiv.org/abs/1705.08741\n",
        "\n",
        "**Covariate Shift**\n",
        "\n",
        "pending...\n",
        "\n"
      ],
      "metadata": {
        "id": "PokQre2wgFWs"
      }
    }
  ]
}