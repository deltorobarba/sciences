{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N6PdQRiaMifD",
        "DNrV4mJdako6",
        "w5ac_7baamGV",
        "YxoRP4A8aoSb"
      ],
      "authorship_tag": "ABX9TyNs4Puzsqv6ARy/AycxOLnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Models and Agents**"
      ],
      "metadata": {
        "id": "45jfy2sWQt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Agents*"
      ],
      "metadata": {
        "id": "N6PdQRiaMifD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whitepaper: https://www.kaggle.com/whitepaper-agent-companion"
      ],
      "metadata": {
        "id": "2pzWFTB2Ml8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Google*"
      ],
      "metadata": {
        "id": "DNrV4mJdako6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini**"
      ],
      "metadata": {
        "id": "8uSky9Ara2Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma**"
      ],
      "metadata": {
        "id": "MVYVfsMza309"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imagen**"
      ],
      "metadata": {
        "id": "nu_rXPIpbDXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Veo**"
      ],
      "metadata": {
        "id": "WApjF2fzbFWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lyria**"
      ],
      "metadata": {
        "id": "O2o-WVEma5H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chirp**\n",
        "\n",
        "* https://cloud.google.com/speech-to-text/v2/docs/chirp-model"
      ],
      "metadata": {
        "id": "5mOvkYSPa6v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# performing synchronous speech recognition on a local audio file using Chirp\n",
        "\n",
        "import os\n",
        "\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "\n",
        "PROJECT_ID = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
        "\n",
        "\n",
        "def transcribe_chirp(\n",
        "    audio_file: str,\n",
        ") -> cloud_speech.RecognizeResponse:\n",
        "    \"\"\"Transcribes an audio file using the Chirp model of Google Cloud Speech-to-Text API.\n",
        "    Args:\n",
        "        audio_file (str): Path to the local audio file to be transcribed.\n",
        "            Example: \"resources/audio.wav\"\n",
        "    Returns:\n",
        "        cloud_speech.RecognizeResponse: The response from the Speech-to-Text API containing\n",
        "        the transcription results.\n",
        "\n",
        "    \"\"\"\n",
        "    # Instantiates a client\n",
        "    client = SpeechClient(\n",
        "        client_options=ClientOptions(\n",
        "            api_endpoint=\"us-central1-speech.googleapis.com\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Reads a file as bytes\n",
        "    with open(audio_file, \"rb\") as f:\n",
        "        audio_content = f.read()\n",
        "\n",
        "    config = cloud_speech.RecognitionConfig(\n",
        "        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
        "        language_codes=[\"en-US\"],\n",
        "        model=\"chirp\",\n",
        "    )\n",
        "\n",
        "    request = cloud_speech.RecognizeRequest(\n",
        "        recognizer=f\"projects/{PROJECT_ID}/locations/us-central1/recognizers/_\",\n",
        "        config=config,\n",
        "        content=audio_content,\n",
        "    )\n",
        "\n",
        "    # Transcribes the audio into text\n",
        "    response = client.recognize(request=request)\n",
        "\n",
        "    for result in response.results:\n",
        "        print(f\"Transcript: {result.alternatives[0].transcript}\")\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "PFB-kotGbXmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *OSS / 3P Models*"
      ],
      "metadata": {
        "id": "w5ac_7baamGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 4: https://ai.meta.com/blog/llama-4-multimodal-intelligence/\n"
      ],
      "metadata": {
        "id": "qmP9slIWccKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anthropic: Claude Opus 3 and Sonnet 4"
      ],
      "metadata": {
        "id": "X7nBB-dwceqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen3 https://qwenlm.github.io/blog/qwen3/"
      ],
      "metadata": {
        "id": "uuMrkLheca27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSeel v3"
      ],
      "metadata": {
        "id": "BnRNn0hlckGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Embedding Models*"
      ],
      "metadata": {
        "id": "YxoRP4A8aoSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whitepaper: https://www.kaggle.com/whitepaper-embeddings-and-vector-stores\n",
        "\n",
        "https://medium.com/@aleixlopez/introduction-to-embeddings-vector-stores-c04fe3d11953"
      ],
      "metadata": {
        "id": "-_qDQz_nMMam"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge03DgPsQtMD"
      },
      "outputs": [],
      "source": []
    }
  ]
}