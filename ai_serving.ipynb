{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKKjDX81rUXSYS1044mpOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/ai_serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Serving and Inference**"
      ],
      "metadata": {
        "id": "45jfy2sWQt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *vLLM*"
      ],
      "metadata": {
        "id": "nxcLKYTn7MuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/"
      ],
      "metadata": {
        "id": "JqgmDhaf8YJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUs https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm"
      ],
      "metadata": {
        "id": "698qgAew8MTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload model to Vertex AI, configure deployment settings, and deploy it to an endpoint using vLLM\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if \"8b\" in base_model_name.lower():\n",
        "        accelerator_type = \"NVIDIA_L4\"\n",
        "        machine_type = \"g2-standard-12\"\n",
        "        accelerator_count = 1\n",
        "    elif \"70b\" in base_model_name.lower():\n",
        "        accelerator_type = \"NVIDIA_L4\"\n",
        "        machine_type = \"g2-standard-96\"\n",
        "        accelerator_count = 8\n",
        "    elif \"405b\" in base_model_name.lower():\n",
        "        accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "        machine_type = \"a3-highgpu-8g\"\n",
        "        accelerator_count = 8\n",
        "    else:\n",
        "        raise ValueError(f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\")\n",
        "\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    vllm_args = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\", f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\", f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\", \"--disable-log-stats\"\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "        \"HF_TOKEN\": HF_TOKEN\n",
        "    }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "    print(f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\")\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint"
      ],
      "metadata": {
        "id": "fIxTt6vV-L80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute deployment\n",
        "\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241001_0916_RC00\"\n",
        "\n",
        "model_name = common_util.get_job_name_with_datetime(prefix=f\"{base_model_name}-serve-vllm\")\n",
        "gpu_memory_utilization = 0.9\n",
        "max_model_len = 4096\n",
        "max_loras = 1\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=f\"{base_model_name}-serve\"),\n",
        "    model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    max_loras=max_loras,\n",
        "    enforce_eager=True,\n",
        "    enable_lora=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ],
      "metadata": {
        "id": "e-4MI-81-Q1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloud Run https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-vllm and https://codelabs.developers.google.com/codelabs/how-to-run-inference-cloud-run-gpu-vllm#0"
      ],
      "metadata": {
        "id": "Rhj-AiGX8ChH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dockerfile\n",
        "\n",
        "FROM vllm/vllm-openai:latest\n",
        "\n",
        "ENV HF_HOME=/model-cache\n",
        "RUN --mount=type=secret,id=HF_TOKEN HF_TOKEN=$(cat /run/secrets/HF_TOKEN) \\\n",
        "    huggingface-cli download google/gemma-2-2b-it\n",
        "\n",
        "ENV HF_HUB_OFFLINE=1\n",
        "\n",
        "ENTRYPOINT python3 -m vllm.entrypoints.openai.api_server \\\n",
        "    --port ${PORT:-8000} \\\n",
        "    --model ${MODEL_NAME:-google/gemma-2-2b-it} \\\n",
        "    ${MAX_MODEL_LEN:+--max-model-len \"$MAX_MODEL_LEN\"}"
      ],
      "metadata": {
        "id": "SIjmwhy295cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cloudbuild.yaml file\n",
        "\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  id: build\n",
        "  entrypoint: 'bash'\n",
        "  secretEnv: ['HF_TOKEN']\n",
        "  args:\n",
        "    - -c\n",
        "    - |\n",
        "        SECRET_TOKEN=\"$$HF_TOKEN\" docker buildx build --tag=${_IMAGE} --secret id=HF_TOKEN .\n",
        "\n",
        "availableSecrets:\n",
        "  secretManager:\n",
        "  - versionName: 'projects/${PROJECT_ID}/secrets/HF_TOKEN/versions/latest'\n",
        "    env: 'HF_TOKEN'\n",
        "\n",
        "images: [\"${_IMAGE}\"]\n",
        "\n",
        "substitutions:\n",
        "  _IMAGE: 'us-central1-docker.pkg.dev/${PROJECT_ID}/vllm-gemma-2-2b-it-repo/vllm-gemma-2-2b-it'\n",
        "\n",
        "options:\n",
        "  dynamicSubstitutions: true\n",
        "  machineType: \"E2_HIGHCPU_32\""
      ],
      "metadata": {
        "id": "tY36xbxl99pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit build\n",
        "\n",
        "gcloud builds submit --config=cloudbuild.yaml\n"
      ],
      "metadata": {
        "id": "Oi-pngqW-B1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU on GKE: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-vllm-tpu"
      ],
      "metadata": {
        "id": "veTt1rVA8kFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deployment manifest saved as vllm-llama3-70b.yaml\n",
        "\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: vllm-tpu\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: vllm-tpu\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: vllm-tpu\n",
        "      annotations:\n",
        "        gke-gcsfuse/volumes: \"true\"\n",
        "        gke-gcsfuse/cpu-limit: \"0\"\n",
        "        gke-gcsfuse/memory-limit: \"0\"\n",
        "        gke-gcsfuse/ephemeral-storage-limit: \"0\"\n",
        "    spec:\n",
        "      serviceAccountName: KSA_NAME\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-tpu-topology: 2x4\n",
        "        cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice\n",
        "      containers:\n",
        "      - name: vllm-tpu\n",
        "        image: docker.io/vllm/vllm-tpu:73aa7041bfee43581314e6f34e9a657137ecc092\n",
        "        command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n",
        "        args:\n",
        "        - --host=0.0.0.0\n",
        "        - --port=8000\n",
        "        - --tensor-parallel-size=8\n",
        "        - --max-model-len=4096\n",
        "        - --model=meta-llama/Llama-3.1-70B\n",
        "        - --download-dir=/data\n",
        "        - --max-num-batched-tokens=512\n",
        "        - --max-num-seqs=128\n",
        "        env:\n",
        "        - name: HUGGING_FACE_HUB_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        - name: VLLM_XLA_CACHE_PATH\n",
        "          value: \"/data\"\n",
        "        - name: VLLM_USE_V1\n",
        "          value: \"1\"\n",
        "        ports:\n",
        "        - containerPort: 8000\n",
        "        resources:\n",
        "          limits:\n",
        "            google.com/tpu: 8\n",
        "        readinessProbe:\n",
        "          tcpSocket:\n",
        "            port: 8000\n",
        "          initialDelaySeconds: 15\n",
        "          periodSeconds: 10\n",
        "        volumeMounts:\n",
        "        - name: gcs-fuse-csi-ephemeral\n",
        "          mountPath: /data\n",
        "        - name: dshm\n",
        "          mountPath: /dev/shm\n",
        "      volumes:\n",
        "      - name: gke-gcsfuse-cache\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      - name: gcs-fuse-csi-ephemeral\n",
        "        csi:\n",
        "          driver: gcsfuse.csi.storage.gke.io\n",
        "          volumeAttributes:\n",
        "            bucketName: GSBUCKET\n",
        "            mountOptions: \"implicit-dirs,file-cache:enable-parallel-downloads:true,file-cache:parallel-downloads-per-file:100,file-cache:max-parallel-downloads:-1,file-cache:download-chunk-size-mb:10,file-cache:max-size-mb:-1\"\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: vllm-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: vllm-tpu\n",
        "  type: LoadBalancer\n",
        "  ports:\n",
        "    - name: http\n",
        "      protocol: TCP\n",
        "      port: 8000\n",
        "      targetPort: 8000"
      ],
      "metadata": {
        "id": "NYC5uuUH9ggH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU https://cloud.google.com/tpu/docs/tutorials/LLM/vllm-inference-v6e"
      ],
      "metadata": {
        "id": "AwmnuolW8nQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the vLLM benchmarking script\n",
        "\n",
        "export MODEL=\"meta-llama/Llama-3.1-8B\"\n",
        "pip install pandas\n",
        "pip install datasets\n",
        "python benchmarks/benchmark_serving.py \\\n",
        "  --backend vllm \\\n",
        "  --model $MODEL  \\\n",
        "  --dataset-name random \\\n",
        "  --random-input-len 1820 \\\n",
        "  --random-output-len 128 \\\n",
        "  --random-prefix-len 0"
      ],
      "metadata": {
        "id": "gp3pT3dR9Otl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU https://docs.vllm.ai/en/stable/getting_started/quickstart.html"
      ],
      "metadata": {
        "id": "98wtB_Tc9Dig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *HexLLM*"
      ],
      "metadata": {
        "id": "HJTBC_0W7Noh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://cloud.google.com/blog/products/ai-machine-learning/hex-llm-on-tpus-in-vertex-ai-model-garden/?hl=en\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm"
      ],
      "metadata": {
        "id": "qQqZs_2H7VWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge03DgPsQtMD"
      },
      "outputs": [],
      "source": [
        "hexllm_args = [\n",
        "    \"--host=0.0.0.0\",\n",
        "    \"--port=7080\",\n",
        "    \"--model=meta-llama/Llama-3.1-8B\",\n",
        "    \"--data_parallel_size=1\",\n",
        "    \"--tensor_parallel_size=4\",\n",
        "    \"--num_hosts=4\",\n",
        "    \"--hbm_utilization_factor=0.9\",\n",
        "    \"--enable_prefix_cache_hbm\",      # Prefox caching\n",
        "]\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=model_name,\n",
        "    serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
        "    serving_container_command=[\"python\", \"-m\", \"hex_llm.server.api_server\"],\n",
        "    serving_container_args=hexllm_args,\n",
        "    serving_container_ports=[7080],\n",
        "    serving_container_predict_route=\"/generate\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables=env_vars,\n",
        "    serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "    serving_container_deployment_timeout=7200,\n",
        "    location=TPU_DEPLOYMENT_REGION,\n",
        ")\n",
        "\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    machine_type=machine_type,\n",
        "    tpu_topology=\"4x4\",\n",
        "    deploy_request_timeout=1800,\n",
        "    service_account=service_account,\n",
        "    min_replica_count=min_replica_count,\n",
        "    max_replica_count=max_replica_count,\n",
        ")"
      ]
    }
  ]
}