{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G76hTQm13NKf",
        "H495iuyB3a4l",
        "IbQ5pdlD1Q_a",
        "-ZXm6SgN5N9V",
        "4SaVH1fe3rjf",
        "8L-S0vdm3x1l"
      ],
      "authorship_tag": "ABX9TyM5D7eusjGe/zSybVvGR1FB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Machine Learning ü¶ãüñ≤Ô∏è**"
      ],
      "metadata": {
        "id": "9wUgGWqn1Pc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_4000.png)"
      ],
      "metadata": {
        "id": "LRrfRNva1PAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cloudskillsboost.google/paths/1283"
      ],
      "metadata": {
        "id": "NqJrkppR34Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Agents**"
      ],
      "metadata": {
        "id": "iI4a0vUV44BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Google Agent Development Kit*"
      ],
      "metadata": {
        "id": "G76hTQm13NKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://google.github.io/adk-docs/#what-is-agent-development-kit\n",
        "\n",
        "* https://github.com/sokart/adk-walkthrough\n",
        "\n",
        "* https://medium.com/@sokratis.kartakis/from-zero-to-multi-agents-a-beginners-guide-to-google-agent-development-kit-adk-b56e9b5f7861\n",
        "\n",
        "* https://www.youtube.com/watch?v=zgrOwow_uTQ"
      ],
      "metadata": {
        "id": "wXQ6yE1t3TZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Agent2Agent Protocoll*"
      ],
      "metadata": {
        "id": "H495iuyB3a4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\n",
        "\n",
        "* https://github.com/google/A2A"
      ],
      "metadata": {
        "id": "VkUZSkeu3Zyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Generative AI**"
      ],
      "metadata": {
        "id": "Ql0tM1s24-JH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Training & Tuning*"
      ],
      "metadata": {
        "id": "IbQ5pdlD1Q_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Continuous pretraining: bring your own custom LoRA (end Q2) and tokenspace\n",
        "* Full fine-tuning: In April in Model Garden\n",
        "* Parameter-efficient fine-tuning (PEFT with LoRA or qLoRA): bitsandbytes for 8bit qLoRA (k-bit quantization)\n",
        "  * Gemini: Gemini Parameter-efficient tuning (e.g LoRA), Supervised fine-tuning on Gemini 2.0 Flash, Codelab for SFT\n",
        "  * 3P: PEFT for LLama3, Video: Llama 3, PEFT for Mistral. Soon: Managed PEFT for open-weight models in Model Garden\n",
        "  * Fine-tuning LLMs with PEFT and LoRA, Code!, [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models\n",
        "* Reinforcement Fine-tuning (ReFT): article (can be used in conjunction with PEFT. For example, use LoRA as your parameter efficient fine-tuning technique, and then use a reinforcement learning reward system to guide the training of the LoRA adapters\n",
        "Instruction finetune LLMs\n",
        "* Direct Preference Optimization: directly optimizes the LLM's policy using a dataset of human preferences. https://arxiv.org/pdf/2305.18290.pdf\n",
        "* Multi-turn Conversations with Action-Based Contrastive Self-Training (ACT): https://arxiv.org/pdf/2406.00222,  email\n",
        "See: https://huggingface.co/docs/trl/en/index .  ACT consistently outperforms standard in-context learning, fine-tuning and DPO methods in three diverse conversational tasks (PACIFIC, Abg-CoQA, and AmbigSQL). https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud?e=48754805?utm_source%3Dcgc-blo"
      ],
      "metadata": {
        "id": "_PQKxxnd2Yzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Serving*"
      ],
      "metadata": {
        "id": "-ZXm6SgN5N9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Interface: FastAPI, Gradio. Special LLM: vLLM, Hex-LLM, Ollama (local). General inference: Triton, kserve, SaxML with PyTorch (multi-host TPUs)\n",
        "\tTriton + GKE Autopilot for inference https://www.youtube.com/watch?v=HT2_jdMw6u0\n",
        "* Considerations: Stream or batch (e.g. vLLM batch - inference only with large throughput single endpoint), autoscaling (e.g. Prometheus in GKE), TCO"
      ],
      "metadata": {
        "id": "imuikZ1O6X6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Serving via Vertex AI**\n",
        "  * Serving multiple LoRA adapters of Open Models on Vertex AI with Hugging Face\n",
        "  * NEW: Multi-hosting and serving LLMs faster on Vertex AI\n",
        "  * Register and Versionize Models (predictive AI): Vertex AI Model Registry and Import models to Vertex AI\n",
        "* **Serving via Model Garden**\n",
        "  * Model Garden. Deploy Models from Models Garden (code). Deploy and inference Gemma\n",
        "  * Introducing the new Vertex AI Model Garden CLI and SDK\n",
        "  * Serving with Hex-LLM on TPU via Model Garden (code example)\n",
        "  * Feature: speculative decoding (research)\n",
        "* **Serving via Cloud Run**\n",
        "  * GPU on Cloud Run: fully managed, with no extra drivers or libraries needed\n",
        "  * Open weight models: How to deploy Llama 3.2-1B-Instruct model with Google Cloud Run\n",
        "* **Serving via GKE**\n",
        "  * Serve an LLM using GPUs on GKE with vLLM  and Serve an LLM using TPUs on GKE with vLLM\n",
        "  * Recipe for GPU inference via AI Hypercompute (on GKE). Try with Llama 70B instead of 405B just change single configuration setting\n",
        "* **Serving via DataFlow ML**\n",
        "  * Run ML inference by using vLLM on GPUs | Dataflow ML\n"
      ],
      "metadata": {
        "id": "GKDnpPGw5UHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Science and Infrastructure**"
      ],
      "metadata": {
        "id": "Dd7EdPX646jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *TPU Ironwood*"
      ],
      "metadata": {
        "id": "4SaVH1fe3rjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling to 9,216 chips *per pod* for a 42.5 Exaflops, it's not just powerful ‚Äì it's in a league of its own, boasting >24x the compute of the current top supercomputer, El Capitan"
      ],
      "metadata": {
        "id": "yCUgEpf74d8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/"
      ],
      "metadata": {
        "id": "nx1nUj_r3qGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Google Science Platform*"
      ],
      "metadata": {
        "id": "8L-S0vdm3x1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.google/products/google-cloud/scientific-research-tools-ai/"
      ],
      "metadata": {
        "id": "l1xSMZu83xNN"
      }
    }
  ]
}