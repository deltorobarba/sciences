{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G76hTQm13NKf",
        "8sIpq5OYIQbi",
        "IbQ5pdlD1Q_a",
        "-ZXm6SgN5N9V",
        "Dd7EdPX646jK"
      ],
      "authorship_tag": "ABX9TyOzeJDgnWGx/oFqXyF1NI4D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Machine Learning ü¶ãüñ≤Ô∏è**"
      ],
      "metadata": {
        "id": "9wUgGWqn1Pc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_4000.png)"
      ],
      "metadata": {
        "id": "LRrfRNva1PAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook is an attempt to structure different aspects of machine learning. Additionally, it contains a collection of links for technical repo's used for customer reference.*"
      ],
      "metadata": {
        "id": "dZI62XdRxFIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-adk\n",
        "!pip install google-genai\n",
        "!pip install google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "ZKGapxnLrtVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">- - - *Google Agent Development Kit - - -*"
      ],
      "metadata": {
        "id": "G76hTQm13NKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Agent Development Kit**\n",
        "* https://google.github.io/adk-docs/#what-is-agent-development-kit\n",
        "* https://github.com/sokart/adk-walkthrough\n",
        "* https://medium.com/@sokratis.kartakis/from-zero-to-multi-agents-a-beginners-guide-to-google-agent-development-kit-adk-b56e9b5f7861\n",
        "* https://www.youtube.com/watch?v=zgrOwow_uTQ"
      ],
      "metadata": {
        "id": "wXQ6yE1t3TZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A2A (Agent2Agent Protocol)**\n",
        "* **Documentation**\n",
        "  * https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\n",
        "  * https://github.com/google/A2A\n",
        "  * https://google.github.io/A2A/#/\n",
        "* **A2A (Agent2Agent Protocol)** for agent-agent collaboration\n",
        "  * Dynamic, multimodal communication between different agents without sharing memory, resources, and tools\n",
        "  * Open standard driven by community.\n",
        "  * Samples available using Google ADK, LangGraph, Crew.AI\n",
        "* **MCP (Model Context Protocol)** for tools and resources\n",
        "  * Connect agents to tools, APIs, and resources with structured inputs/outputs.\n",
        "  * Google ADK supports MCP tools. Enabling wide range of MCP servers to be used with agents.\n",
        "\n",
        "![ggg](https://google.github.io/A2A/images/a2a_mcp_readme.png)"
      ],
      "metadata": {
        "id": "qtzwrhVFbZhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*- - - Models - - -*"
      ],
      "metadata": {
        "id": "8sIpq5OYIQbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Large Language Models**\n",
        "* **Gemini**\n",
        "  * https://cloud.google.com/vertex-ai/generative-ai/docs/live-api\n",
        "  * Model Optimizer: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/vertex-ai-model-optimizer (Don't know which Gemini model to use? ü§î Vertex AI Model Optimizer is here!Vertex AI Model Optimizer is a smart model router that helps you selecting the most appropriate Gemini model based on your cost and quality preferences for each prompt with your for specific use case.)\n",
        "  * Rate Limits: https://ai.google.dev/gemini-api/docs/rate-limits#current-rate-limits\n",
        "* **Llama**: https://ai.meta.com/blog/llama-4-multimodal-intelligence/\n",
        "\n",
        "**Small Language Models**\n",
        "* Gemma: https://blog.google/technology/developers/gemma-3/\n"
      ],
      "metadata": {
        "id": "r-2MjuYLqHgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*- - - Training & Tuning - - -*"
      ],
      "metadata": {
        "id": "IbQ5pdlD1Q_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Introduction to Tuning: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models*\n",
        "\n",
        "**Model Garden**\n",
        "* **PEFT**: https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models\n",
        "  * Example: **Mistral**: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral_peft_tuning.ipynb\n",
        "  * Example: **LLama3** https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_finetuning.ipynb\n",
        "* Managed PEFT for open-weight models: pending\n",
        "* **Full finetuning**: released\n",
        "* **DPO - Direct Preference Optimization**: directly optimizes the LLM's policy using a dataset of human preferences. https://arxiv.org/pdf/2305.18290.pdf (pending)\n",
        "* **Continuous Pretraining**: bring your own custom LoRA (end Q2) and tokenspace\n",
        "\n",
        "**Vertex AI**\n",
        "* **PEFT**:\n",
        "  * Example: **Llama2** fine-tuning with LoRA (and serving on TPUv5e) https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\n",
        "* **Supervised Finetuning**:\n",
        "  * Example: **SFT for Gemini** https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning\n",
        "  * Code example: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\n",
        "* **RLHF Tuning**: https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud?e=48754805\n",
        "* Ray on Vertex (managed): https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview\n",
        "  * Example with Gemma: https://developers.googleblog.com/en/get-started-with-gemma-on-ray-on-vertex-ai/\n",
        "  * Example: Scale with RoV https://medium.com/google-cloud/ray-on-vertex-ai-lets-get-it-started-7a9f8360ea25\n",
        "\n",
        "**Other Options**\n",
        "* Ray on GKE (with kuberay): https://cloud.google.com/kubernetes-engine/docs/add-on/ray-on-gke/concepts/overview\n",
        "* [Axolotl](https://axolotl.ai/) (finetune multiple GPU)\n",
        "* [Unsloth](https://unsloth.ai/) (finetune single GPU)\n",
        "* Dask\n",
        "* Spark\n",
        "* Infinipod\n",
        "* Horovod (distributed training): https://cloud.google.com/vertex-ai/docs/training/distributed-training\n",
        "* Cluster Director for Slurm: https://cloud.google.com/ai-hypercomputer/docs/cluster-director\n",
        "\n",
        "*Pending: * **ACT - Action-Based Contrastive Self-Training** for Multi-turn Conversations: https://arxiv.org/pdf/2406.00222, See: https://huggingface.co/docs/trl/en/index .  ACT consistently outperforms standard in-context learning, fine-tuning and DPO methods in three diverse conversational tasks (PACIFIC, Abg-CoQA, and AmbigSQL)*\n",
        "* **PEFT - Parameter-efficient fine-tuning** (with LoRA or qLoRA: https://arxiv.org/abs/2106.09685):\n",
        "  * bitsandbytes for 8bit qLoRA (k-bit quantization): https://huggingface.co/docs/bitsandbytes/index\n",
        "  * Code: https://codelabs.developers.google.com/llm-finetuning-supervised#0\n",
        "  * **Code**: [Use PEFT & bitsandbytes to finetune a LoRa checkpoint](https://colab.research.google.com/drive/14xo6sj4dARk8lXZbOifHEn1f_70qNAwy?usp=sharing#scrollTo=7650BSUPZh0Y)\n",
        "  * Video: [Fine-tuning LLMs with PEFT and LoRA](https://www.youtube.com/watch?v=Us5ZFp16PaU&t=393s)\n"
      ],
      "metadata": {
        "id": "7zYq3k7xHwx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16, #attention heads\n",
        "    lora_alpha=32, #alpha scaling\n",
        "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "nZCA41p_jB4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/vertex_0001.png)"
      ],
      "metadata": {
        "id": "PAXBE05TiGnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*- - - Serving - - -*"
      ],
      "metadata": {
        "id": "-ZXm6SgN5N9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Interface**:\n",
        "  * FastAPI and Gradio\n",
        "  * Special LLM:\n",
        "    * vLLM\n",
        "    * Hex-LLM\n",
        "    * Ollama (local)\n",
        "  * General inference: Triton, kserve, SaxML with PyTorch (multi-host TPUs). Example Triton + GKE Autopilot for inference https://www.youtube.com/watch?v=HT2_jdMw6u0\n",
        "* **Considerations**:\n",
        "  * Stream or batch (e.g. vLLM batch - inference only with large throughput single endpoint),\n",
        "  * autoscaling (e.g. Prometheus in GKE)\n",
        "  * TCO\n",
        "* **Serving via Vertex AI**\n",
        "  * Serving multiple LoRA adapters of Open Models on Vertex AI with Hugging Face\n",
        "  * NEW: Multi-hosting and serving LLMs faster on Vertex AI\n",
        "  * Register and Versionize Models (predictive AI): Vertex AI Model Registry and Import models to Vertex AI\n",
        "* **Serving via Model Garden**\n",
        "  * Model Garden. Deploy Models from Models Garden (code). Deploy and inference Gemma\n",
        "  * Introducing the new Vertex AI Model Garden CLI and SDK\n",
        "  * Serving with Hex-LLM on TPU via Model Garden (code example)\n",
        "  * Feature: speculative decoding (research)\n",
        "* **Serving via Cloud Run**\n",
        "  * GPU on Cloud Run: fully managed, with no extra drivers or libraries needed\n",
        "  * Open weight models: How to deploy Llama 3.2-1B-Instruct model with Google Cloud Run\n",
        "* **Serving via GKE**\n",
        "  * Serve an LLM using GPUs on GKE with vLLM  and Serve an LLM using TPUs on GKE with vLLM\n",
        "  * Recipe for GPU inference via AI Hypercompute (on GKE). Try with Llama 70B instead of 405B just change single configuration setting\n",
        "* **Serving via DataFlow ML**\n",
        "  * Run ML inference by using vLLM on GPUs | Dataflow ML\n"
      ],
      "metadata": {
        "id": "GKDnpPGw5UHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*- - - Observability & Evaluation - - -*"
      ],
      "metadata": {
        "id": "wFgNAsqXIBFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Monitoring</u> tells you <u>if</u> there's a problem**.\n",
        "* Monitoring involves tracking high-level metrics related to LLM performance.. Answers: \"Is system running smoothly?\" \"Are there performance bottlenecks?\" Key metrics include latency, throughput, error rates, and resource utilization. It sets up alerts for when those metrics deviate from expected values.\n",
        "* Metrics: Request Volume (incl. identify usage anomaly, sudden spikes, drops), Request Duration (network latency, response time),  Costs, Tokens Counters\n",
        "* General monitoring: https://cloud.google.com/monitoring/docs\n",
        "* Predictive AI Monitoring: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview\n",
        "* Generative AI Monitoring: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-observability"
      ],
      "metadata": {
        "id": "-Sf1wjTFIIQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://cloud.google.com/static/vertex-ai/generative-ai/docs/images/observability-dashboard.png)"
      ],
      "metadata": {
        "id": "PpYbGl3fdNZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Tracing</u> helps understand <u>why</u> the problem occurred.**   \n",
        "* Traceability provides a detailed, granular view of the execution flow within the LLM application. It helps understand how individual requests are processed and how different components interact. Answer: \"What steps did the LLM take to generate this response?\" and \"Where did an error occur in the process?\"\n",
        "* Traces: Request Metadata: Temperature, top_p, Model Name or Version, Prompt Details. Response Metadata: Tokens, Cost, Response Details."
      ],
      "metadata": {
        "id": "Iy03s6XkIK3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Evaluation</u> helps you to understand <u>how good</u> the LLM is performing**\n",
        "* Read: https://medium.com/google-cloud/llms-evaluation-on-gcp-9186fad73f22\n",
        "* Evaluation focuses on assessing the quality and accuracy of the LLM's outputs. It involves measuring metrics related to: Accuracy and relevance, Bias and fairness, Hallucinations, Safety and security. Evaluation is crucial for ensuring that the LLM is performing as expected and meeting desired standards\n",
        "* Providers: LangSmith, Traceloop, Arize, Ragas, MLflow, Alibi\n",
        "* **Overview**: https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview\n",
        "* **Judge Model**: https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluate-judge-model: After running your LLM evaluation, you can now compare the judge model's outputs against human preferences using metrics like balanced accuracy, F1 score, and a confusion matrix.\n",
        "* **AutoSxS** (pairwise model-based evaluation: https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval\n",
        "* **Computation-based evaluation**: https://cloud.google.com/vertex-ai/generative-ai/docs/models/computation-based-eval-pipeline\n"
      ],
      "metadata": {
        "id": "Duo9_CDvb_QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a pointwise metric with two criteria: Fluency and Entertaining.\n",
        "custom_text_quality = PointwiseMetric(\n",
        "    metric=\"custom_text_quality\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "            \"fluency\": (\n",
        "                \"Sentences flow smoothly and are easy to read, avoiding awkward\"\n",
        "                \" phrasing or run-on sentences. Ideas and sentences connect\"\n",
        "                \" logically, using transitions effectively where needed.\"\n",
        "            ),\n",
        "            \"entertaining\": (\n",
        "                \"Short, amusing text that incorporates emojis, exclamations and\"\n",
        "                \" questions to convey quick and spontaneous communication and\"\n",
        "                \" diversion.\"\n",
        "            ),\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"1\": \"The response performs well on both criteria.\",\n",
        "            \"0\": \"The response is somewhat aligned with both criteria\",\n",
        "            \"-1\": \"The response falls short on both criteria\",\n",
        "        },\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "pHdtyU4xdxSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*- - - Miscellaneous - - -*"
      ],
      "metadata": {
        "id": "Dd7EdPX646jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TPU Ironwood: https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/ - Scaling to 9,216 chips *per pod* for a 42.5 Exaflops, it's not just powerful ‚Äì it's in a league of its own, boasting >24x the compute of the current top supercomputer, El Capitan\n",
        "* Google Science Platform: https://blog.google/products/google-cloud/scientific-research-tools-ai/\n",
        "* https://www.thealgorithmicbridge.com/p/google-is-winning-on-every-ai-front\n",
        "* Learn: https://www.cloudskillsboost.google/paths/1283"
      ],
      "metadata": {
        "id": "nx1nUj_r3qGR"
      }
    }
  ]
}