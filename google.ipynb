{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXh1SboT7YIQuXYaLpFnT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Machine Learning ðŸ¦‹ðŸ–²ï¸**"
      ],
      "metadata": {
        "id": "9wUgGWqn1Pc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_4000.png)"
      ],
      "metadata": {
        "id": "LRrfRNva1PAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Google Agent Development Kit*"
      ],
      "metadata": {
        "id": "G76hTQm13NKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://google.github.io/adk-docs/#what-is-agent-development-kit\n",
        "\n",
        "* https://github.com/sokart/adk-walkthrough\n",
        "\n",
        "* https://medium.com/@sokratis.kartakis/from-zero-to-multi-agents-a-beginners-guide-to-google-agent-development-kit-adk-b56e9b5f7861\n",
        "\n",
        "* https://www.youtube.com/watch?v=zgrOwow_uTQ"
      ],
      "metadata": {
        "id": "wXQ6yE1t3TZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Agent2Agent Protocoll*"
      ],
      "metadata": {
        "id": "H495iuyB3a4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\n",
        "\n",
        "* https://github.com/google/A2A"
      ],
      "metadata": {
        "id": "VkUZSkeu3Zyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *TPU Ironwood*"
      ],
      "metadata": {
        "id": "4SaVH1fe3rjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling to 9,216 chips *per pod* for a 42.5 Exaflops, it's not just powerful â€“ it's in a league of its own, boasting >24x the compute of the current top supercomputer, El Capitan"
      ],
      "metadata": {
        "id": "yCUgEpf74d8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/"
      ],
      "metadata": {
        "id": "nx1nUj_r3qGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Google Science Platform*"
      ],
      "metadata": {
        "id": "8L-S0vdm3x1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.google/products/google-cloud/scientific-research-tools-ai/"
      ],
      "metadata": {
        "id": "l1xSMZu83xNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Cloudskillboost*"
      ],
      "metadata": {
        "id": "_4tCRBFp35qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cloudskillsboost.google/paths/1283"
      ],
      "metadata": {
        "id": "NqJrkppR34Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Tuning Language Models*"
      ],
      "metadata": {
        "id": "IbQ5pdlD1Q_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Continuous pretraining: bring your own custom LoRA (end Q2) and tokenspace\n",
        "* Full fine-tuning: In April in Model Garden\n",
        "* Parameter-efficient fine-tuning (PEFT with LoRA or qLoRA): bitsandbytes for 8bit qLoRA (k-bit quantization)\n",
        "  * Gemini: Gemini Parameter-efficient tuning (e.g LoRA), Supervised fine-tuning on Gemini 2.0 Flash, Codelab for SFT\n",
        "  * 3P: PEFT for LLama3, Video: Llama 3, PEFT for Mistral. Soon: Managed PEFT for open-weight models in Model Garden\n",
        "  * Fine-tuning LLMs with PEFT and LoRA, Code!, [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models\n",
        "* Reinforcement Fine-tuning (ReFT): article (can be used in conjunction with PEFT. For example, use LoRA as your parameter efficient fine-tuning technique, and then use a reinforcement learning reward system to guide the training of the LoRA adapters\n",
        "Instruction finetune LLMs\n",
        "* Direct Preference Optimization: directly optimizes the LLM's policy using a dataset of human preferences. https://arxiv.org/pdf/2305.18290.pdf\n",
        "* Multi-turn Conversations with Action-Based Contrastive Self-Training (ACT): https://arxiv.org/pdf/2406.00222,  email\n",
        "See: https://huggingface.co/docs/trl/en/index .  ACT consistently outperforms standard in-context learning, fine-tuning and DPO methods in three diverse conversational tasks (PACIFIC, Abg-CoQA, and AmbigSQL). https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud?e=48754805?utm_source%3Dcgc-blo"
      ],
      "metadata": {
        "id": "_PQKxxnd2Yzh"
      }
    }
  ]
}